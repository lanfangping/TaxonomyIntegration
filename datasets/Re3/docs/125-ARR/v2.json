{
    "nodes": [
        {
            "ix": "125-ARR_v2_0",
            "content": "DEGREE: A Data-Efficient Generation-Based Event Extraction Model",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_2",
            "content": "Event extraction requires high-quality expert human annotations, which are usually expensive. Therefore, learning a data-efficient event extraction model that can be trained with only a few labeled examples has become a crucial challenge. In this paper, we focus on low-resource end-to-end event extraction and propose DE-GREE, a data-efficient model that formulates event extraction as a conditional generation problem. Given a passage and a manually designed prompt, DEGREE learns to summarize the events mentioned in the passage into a natural sentence that follows a predefined pattern. The final event predictions are then extracted from the generated sentence with a deterministic algorithm. DEGREE has three advantages to learn well with less training data. First, our designed prompts provide semantic guidance for DEGREE to leverage label semantics and thus better capture the event arguments. Moreover, DEGREE is capable of using additional weaklysupervised information, such as the description of events encoded in the prompts. Finally, DE-GREE learns triggers and arguments jointly in an end-to-end manner, which encourages the model to better utilize the shared knowledge and dependencies among them. Our experimental results demonstrate the strong performance of DEGREE for low-resource event extraction.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "125-ARR_v2_4",
            "content": "Event extraction (EE) aims to extract events, each of which consists of a trigger and several participants (arguments) with their specific roles, from a given passage. For example, in Figure 1, a Justice:Execute event is triggered by the word \"execution\" and this event contains three argument roles, including an Agent (Indonesia) who carries out the execution, a Person who is executed (convicts), and a Place where the event occurs (not mentioned in the passage). Previous work Figure 1: Two examples of events (Justice:Execute and Justice:Appeal) extracted from the given passage. Fincke et al., 2021) usually divides EE into two subtasks: (1) event detection, which identifies event triggers and their types, and (2) event argument extraction, which extracts the arguments and their roles for given event triggers. EE has been shown to benefit a wide range of applications, e.g., building knowledge graphs , question answering (Berant et al., 2014;, and other downstream studies (Han et al., 2019a;Hogenboom et al., 2016;.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_5",
            "content": "Most prior works on EE rely on a large amount of annotated data for training (Nguyen and Grishman, 2015;Nguyen et al., 2016;Han et al., 2019b;Du and Cardie, 2020;Paolini et al., 2021). However, high-quality event annotations are expensive to obtain. For example, the ACE 2005 corpus (Doddington et al., 2004), one of the most widely used EE datasets, requires two rounds of annotations by linguistics experts. The high annotation costs make these models hard to be extended to new domains and new event types. Therefore, how to learn a data-efficient EE model trained with only a few annotated examples is a crucial challenge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_6",
            "content": "In this paper, we focus on low-resource event extraction, where only a small amount of training examples are available for training. We propose DEGREE (Data-Efficient GeneRation-Based Event Extraction), a generation-based model that takes a passage and a manually designed prompt as the input, and learns to summarize the passage into a",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_7",
            "content": "The event is related to conflict and some violent physical act.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_8",
            "content": "Similar triggers such as war, attack, terrorism.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_9",
            "content": "Event trigger is <Trigger>. \\n some attacker attacked some facility, someone, or some organization by some way in somewhere.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_10",
            "content": "Event trigger is detonated. \\n Palestinian attacked jeep and soldiers by bomb in Gaza Strip. The input of DEGREE consists of the given passage and our design prompt that contains an event type description, event keywords, and a E2E template. DEGREE is trained to generate an output to fill in the placeholders (underlined words) in the E2E template with triggers and arguments. The final event prediction is then decoded from the generated output.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_11",
            "content": "Passage",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_12",
            "content": "natural sentence following a predefined template, as illustrated in Figure 2. The event triggers and arguments can then be extracted from the generated sentence by using a deterministic algorithm. DEGREE enjoys the following advantages to learn well with less training data. First, the framework provides label semantics via the designed template in the prompts. As the example in Figure 2 shows, the word \"somewhere\" in the prompt guides the model to predict words being similar to location for the role Place. In addition, the sentence structure of the template and the word \"attacked\" depict the semantic relation between the role Attacker and the role Target. With these kinds of guidance, DEGREE can make more accurate predictions with less training examples. Second, the prompts can incorporate additional weaksupervision signal about the task, such as the description of the event and similar keywords. These resources are usually readily available. For example, in our experiments, we take the information from the annotation guideline, which is provided along with the dataset. This information facilitates DEGREE to learn under a low-resource situation. Finally, DEGREE is designed for end-to-end event extraction and can solve event detection and event argument extraction at the same time. Leveraging the shared knowledge and dependencies between the two tasks makes our model more data-efficient.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_13",
            "content": "Existing works on EE usually have only one or two of above-mentioned advantages. For exam-ple, previous classification-based models (Nguyen et al., 2016;Wang et al., 2019;Yang et al., 2019b;Wadden et al., 2019; can hardly encode label semantics and other weak supervision signals. Recently proposed generation-based models for event extraction solved the problem in a pipeline fashion; therefore, they cannot leverage shared knowledge between subtasks (Paolini et al., 2021;. Furthermore, their generated outputs are not natural sentences, which hinders the utilization of label semantics (Paolini et al., 2021;Lu et al., 2021). As a result, our model DEGREE can achieve significantly better performance than prior approaches on low-resource event extraction, as we will demonstrate in Section 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_14",
            "content": "Our contributions can be summarized as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_15",
            "content": "\u2022 We propose DEGREE, a generation-based event extraction model that learns well with less data by better incorporating label semantics and shared knowledge between subtasks (Section 2). \u2022 Experiments on ACE 2005 (Doddington et al., 2004) and ERE-EN (Song et al., 2015) demonstrate the strong performance of DEGREE in the low-resource setting (Section 3). \u2022 We present comprehensive ablation studies in both the low-resource and the high-resource setting to better understand the strengths and weaknesses of our model (Section 4).",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_16",
            "content": "Our code and models can be found at https: //github.com/PlusLabNLP/DEGREE.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_17",
            "content": "Data-Efficient Event Extraction",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "125-ARR_v2_18",
            "content": "We introduce DEGREE, a generation-based model for low-resource event extraction. Unlike previous works , which separate event extraction into two pipelined tasks (event detection and event argument extraction), DEGREE is designed for the end-to-end event extraction and predict event triggers and arguments at the same time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_19",
            "content": "The DEGREE Model",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "125-ARR_v2_20",
            "content": "We formulate event extraction as a conditional generation problem. As illustrated in Figure 2, given a passage and our designed prompt, DEGREE generates an output following a particular format. The final predictions of event triggers and argument roles can be then parsed from the generated output with a deterministic algorithm. Compared to previous classification-based models (Wang et al., 2019;Yang et al., 2019b;Wadden et al., 2019;, the generation framework provides a flexible way to include additional information and guidance. By designing appropriate prompts, we encourage DEGREE to better capture the dependencies between entities and, therefore, to reduce the number of training examples needed.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_21",
            "content": "The desired prompt not only provides information but also defines the output format. As shown in Figure 2, it contains the following components:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_22",
            "content": "\u2022 Event type definition describes the definition for the given event type. 1 For example, \"The event is related to conflict and some violent physical act.\" describes a Conflict:Attack event. \u2022 Event keywords presents some words that are semantically related to the given event type. For example, war, attack, and terrorism are three event keywords for the Conflict:Attack event. In practice, we collect three words that appear as the triggers in the example sentences from the annotation guidelines. \u2022 E2E template defines the expected output format and can be separated into two parts. The first part is called ED template, which is designed as \"Event trigger is <Trigger>\", where \"<Trigger>\" is a special token serving as a placeholder. The second part is the EAE template, which differs based on the given event type. For example, in Figure 2, the EAE template for a Conflict:Attack event is \"some attacker attacked some facility, someone, or some organization by some way in somewhere\". Each underlined string starting with \"some-\" serves as a placeholder corresponding to an argument role for a Conflict:Attack event. For instance, \"some way\" corresponds to the role Instrument and \"somewhere\" corresponds to the role Place. Notice that every event type has its own EAE template.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_23",
            "content": "We list three EAE templates in Table 1. The full list of EAE templates and the construction details can be found in Appendix A.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_24",
            "content": "Training",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "125-ARR_v2_25",
            "content": "The training objective of DEGREE is to generate an output that replaces the placeholders in E2E template with the gold labels. Take Figure 2 as an example, DEGREE is expected to replace \"<Trigger>\" with the gold trigger (detonated), replace \"some attacker\" with the gold argument for role Attacker (Palestinian), and replace \"some way\" with the gold argument for role Instrument (bomb).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_26",
            "content": "If there are multiple arguments for the same role, they are concatenated with \"and\"; if there is no predicted argument for one role, the model should keep the corresponding placeholder (i.e, \"some-\" in the E2E template). For the case that there are multiple triggers for the given event type in the input passage, DEGREE is trained to generate the output text that contains multiple E2E template such that each E2E template corresponds to one trigger and its argument roles. The hyperparameter settings are detailed in Appendix B.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_27",
            "content": "Inference",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "125-ARR_v2_28",
            "content": "We enumerate all event types and generate an output for each event type. After we obtain the generated sentences, we compare the outputs with E2E template to determine the predicted triggers and arguments in string format. Finally, we apply string matching to convert the predicted string to span offsets in the passage. If the predicted string appears in the passage multiple times, we choose all span offsets that match for trigger predictions and choose the one closest to the given trigger span for argument predictions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_29",
            "content": "Discussion",
            "ntype": "title",
            "meta": {
                "section": "2.4"
            }
        },
        {
            "ix": "125-ARR_v2_30",
            "content": "Notice that the E2E template plays an important role for DEGREE. First, it serves as the control signal and defines the expected output format. Second, it provides label semantics to help DEGREE make accurate predictions. Those placeholders (words starting with \"some-\") in the E2E template give DEGREE some hints about the entity types of arguments. For instance, when seeing \"somewhere\", DEGREE tends to generate a location rather than a person. In addition, the words other than \"some-\" describe the relationships between roles. For example, DEGREE knows the relationship between the role Attacker and the role Target (who is attacking and who is attacked) due to E2E template. This guidance helps DEGREE learn the dependencies between entities. Unlike previous generation-based approaches (Paolini et al., 2021;, we intentionally write E2E templates in natural sentences. This not only uses label semantics better but also makes the model easier to leverage the knowledge from the pre-trained decoder. In Section 4, we will provide experiments to demonstrate the advantage of using natural sentences.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_31",
            "content": "Cost of template constructing. DEGREE does require human effort to design the templates; however, writing those templates is much easier and more effortless than collecting complicated event annotations. As shown in Table 1, we keep the EAE templates as simple and short as possible. Therefore, it takes only about one minute for people who are not linguistic experts to compose a template. In fact, several prior works Du and Cardie, 2020;) also use constructed templates as weakly-supervised signals to improve models. In Section 4, we will study how different templates affect the performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_32",
            "content": "Efficiency Considerations. DEGREE requires to enumerate all event types during inference, which could cause efficiency considerations when extending to applications that contain many event types. This issue is minor for our experiments on the two datasets (ACE 2005 and ERE-EN), which are relatively small scales in terms of the number of event types. Due to the high cost of annotations, there is hardly any public datasets for end-to-end event extraction on a large scale, 2 and we cannot provide a more thorough studies when the experiments scale up. We leave the work on benchmarking and improving the efficiency of DEGREE in the scenario considering more diverse and comprehensive types of events as future work.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_33",
            "content": "DEGREE in Pipeline Framework",
            "ntype": "title",
            "meta": {
                "section": "2.5"
            }
        },
        {
            "ix": "125-ARR_v2_34",
            "content": "DEGREE is flexible and can be easily modified to DEGREE(PIPE), which first focuses event detection (ED) and then solves event argument extraction (EAE). DEGREE(PIPE) consists of two models: (1) DEGREE(ED), which aims to exact event triggers for the given event type, and (2) DE-GREE(EAE), which identifies argument roles for the given event type and the corresponding trigger. DEGREE(ED) and DEGREE(EAE) are similar to DEGREE but with different prompts and output formats. We describe the difference as follows.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_35",
            "content": "\u2022 Event type definition is the same as the ones for DEGREE. \u2022 Event keywords is the same as the one for DE-GREE. \u2022 ED template is designed as \"Event trigger is <Trigger>\", which is actually the first part of the E2E template.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_36",
            "content": "Similar to DEGREE, the objective of DEGREE(ED) is to generate an output that replaces \"<Trigger>\" in the ED template with event triggers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_37",
            "content": "\u2022 Event type definition is the same as the one for DEGREE. \u2022 Query trigger is a string that indicates the trigger word for the given event type. For example, \"The event trigger word is detonated\" points out that \"detonated\" is the given trigger. \u2022 EAE template is an event-type-specific template mentioned previously. It is actually the second part of E2E template.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_38",
            "content": "Similar to DEGREE, the goal for DEGREE(EAE) is to generate an outputs that replace the placeholders in EAE template with event arguments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_39",
            "content": "In Section 3, we will compare DEGREE with DEGREE(PIPE) to study the benefit of dealing with event extraction in an end-to-end manner under the low-resource setting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_40",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "125-ARR_v2_41",
            "content": "We conduct experiments for low-resource event extraction to study how DEGREE performs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_42",
            "content": "Experimental Settings",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "125-ARR_v2_43",
            "content": "Datasets. We consider ACE 2005 (Doddington et al., 2004) and follow the pre-processing in Wadden et al. ( 2019) and , resulting in two variants: ACE05-E and ACE05-E + . Both contain 33 event types and 22 argument roles. In addition, we consider ERE-EN (Song et al., 2015) and adopt the pre-processing in , which keeps 38 event types and 21 argument roles.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_44",
            "content": "Data split for low-resource setting. We generate different proportions (1%, 2%, 3%, 5%, 10%, 20%, 30%, and 50%) of training data to study the influence of the size of the training set and use the original development set and test set for evaluation. Appendix C lists more details about the split generation process and the data statistics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_45",
            "content": "Evaluation metrics. We consider the same criteria in prior works (Wadden et al., 2019;. ( 1) Trigger F1-score: an trigger is correctly identified (Tri-I) if its offset matches the gold one; it is correctly classified (Tri-C) if its event type also matches the gold one. (2) Argument F1-score: an argument is correctly identified (Arg-I) if its offset and event type match the gold ones; it is correctly classified (Arg-C) if its role matches as well.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_46",
            "content": "Compared baselines. We consider the following classification-based models: (1) OneIE , the current state-of-the-art (SOTA) EE model trained with designed global features. ( 2) BERT_QA (Du and Cardie, 2020), which views EE tasks as a sequence of extractive question answering problems. Since it learns a classifier to indicate the position of the predicted span, we view it as a classification model. We also consider the following generation-based models: (3) TANL (Paolini et al., 2021), which treats EE tasks as translation tasks between augmented natural languages. (4) Text2Event (Lu et al., 2021), a sequence-tostructure model that converts the input passage to a tree-like event structure. Note that the outputs of both generation-based baselines are not natural sentences. Therefore, it is more difficult for them to utilize the label semantics. All the implementation details can be found in Appendix D. It is worth noting that we train OneIE with named entity annotations, as the original papers suggest, while the other models are trained without entity annotations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_47",
            "content": "Main Results",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "125-ARR_v2_48",
            "content": "Table 2 shows the trigger classification F1-scores and the argument classification F1-scores in three data sets with different proportions of training data. The results are visualized in Figure 3. Since our task is end-to-end event extraction, the argument classification F1-score is the more important metric that we considered when comparing models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_49",
            "content": "From the figure and the table, we can observe that both DEGREE and DEGREE(PIPE) outperform all other baselines when using less than 10% of the training data. The performance gap becomes much more significant under the extremely low data situation. For example, when only 1% of the training data is available, DEGREE and DEGREE(PIPE) achieve more than 15 points of improvement in trigger classification F1 scores and more than 5 points in argument classification F1 scores. This demonstrates the effectiveness of our design. The generation-based model with carefully designed prompts is able to utilize the label semantics and the additional weakly supervised signals, thus helping learning under the low-resource regime.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_50",
            "content": "Another interesting finding is that DEGREE and DEGREE(PIPE) seem to be more beneficial for predicting arguments than for predicting triggers. For example, OneIE, the strongest baseline, requires 20% of training data to achieve competitive performance on trigger prediction to DEGREE and DEGREE(PIPE); however, it requires about 50% of training data to achieve competitive performance in predicting arguments. The reason is that the ability to capture dependencies becomes more important for argument prediction than trigger prediction since arguments are usually strongly dependent on each other compared to triggers. Therefore, the improvements of our models for argument prediction are more significant.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_51",
            "content": "Furthermore, we observe that DEGREE is slightly better than DEGREE(PIPE) under the lowresource setting. This provides empirical evidence on the benefit of jointly predicting triggers and arguments in a low-resource setting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_52",
            "content": "Finally, we perform additional experiments on few-shot and zero-shot experiments. The results can be found in Appendix E.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_53",
            "content": "High-Resource Event Extraction",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "125-ARR_v2_54",
            "content": "Although we focus on data-efficient learning for low-resource event extraction, to better understand the advantages and disadvantages of our model, we additionally study DEGREE in the high-resource setting for controlled comparisons.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_55",
            "content": "Compared baselines. In addition to the EE models mentioned above: OneIE , BERT_QA (Du and Cardie, 2020), TANL (Paolini et al., 2021) Results for event extraction. Table 3 shows the results of high-resource event extraction. In terms of trigger predictions (Tri-C), DEGREE and DE-GREE(PIPE) outperforms all the baselines except for OneIE, the current state-of-the-art model. For argument predictions (Arg-C), our models have slightly better performance than OneIE in two out of the three datasets. When enough training exam- 3 We follow the original paper and use TAPKEY as their event detection model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_56",
            "content": "Ablation Studies",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "125-ARR_v2_57",
            "content": "In this section, we present comprehensive ablation studies to justify our design. To better understand the contribution of each component in the designed prompt and their effects on the different tasks, we ablate DEGREE(EAE) and DEGREE(ED) for both low-resource and high-resource situations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_58",
            "content": "Impacts of components in prompts. Table 5 lists the performance changes when removing the components in the prompts for event detection on ACE05-E. The performance decreases whenever removing any one of event type definition, event keywords, and ED template. The results suggest that three components are all necessary. any one of event type definition, query trigger, and EAE template leads to performance drops, which validates their necessity. We observe that query trigger plays the most important role among the three and when less training data is given, the superiority of leveraging any of these weaklysupervised signal becomes more obvious.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_59",
            "content": "Effects of different template designs. To verify the importance of using natural sentences as outputs, we study three variants of EAE templates:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_60",
            "content": "\u2022 Natural sentence. Our proposed templates described in Section 2, e.g., \"somebody was born in somewhere.\", where \"somebody\" and \"somewhere\" are placeholders that can be replaced by the corresponding arguments. \u2022 Natural sentence with special tokens. It is similar to the natural sentence one except for using role-specific special tokens instead of \"some-\" words. For example, \"<Person> was born in <Place>.\" We consider this to study the label semantics of roles. \u2022 HTML-like sentence with special tokens. To study the importance of using natural sentence, we also consider HTML-like sentence, e.g., \"<Person> </Person> <Place> </Place>\".",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_61",
            "content": "The model aims to put argument predictions between the corresponding HTML tags.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_62",
            "content": "The results of all variants of EAE templates on ACE05-E are shown in Table 7. We notice that writing templates in a natural language style get better performance, especially when only a few data is available (10% of data). This shows our design's capability to leverage pre-trained knowledge in the generation process. Additionally, there are over 1 F1 score performance drops when replacing natural language placeholders with special tokens. This confirms that leveraging label semantics for different roles is beneficial. Sensitivity to template design. Finally, we study how sensitive our model is to the template. In addition to the original design of templates for event argument extraction, we compose other two sets of templates with different constructing rules (e.g., different word choices and different orders of roles). Table 8 shows the results of using different sets of templates. We observe a performance fluctuation when using different templates, which indicates that the quality of templates does affect the performance to a certain degree. Therefore, we need to be cautious when designing templates. However, even though our model could be sensitive to the template design, it still outperforms OneIE and BART-Gen, which are the best classification-based model and the best generation-based baseline, respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_63",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "125-ARR_v2_64",
            "content": "Fully supervised event extraction. Event extraction has been studied for over a decade (Ahn, 2006;Ji and Grishman, 2008) and most traditional event extraction works follow the fully supervised setting (Nguyen et al., 2016;Sha et al., 2018;Nguyen and Nguyen, 2019;Yang et al., 2019b;. Many of them use classification-based models and use pipeline-style frameworks to extract events (Nguyen et al., 2016;Yang et al., 2019b;Wadden et al., 2019). To better leverage shared knowledge in event triggers and arguments, some works propose incorporating global features to jointly decide triggers and arguments Li et al., 2013;Yang and Mitchell, 2016).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_65",
            "content": "Recently, few generation-based event extraction models have been proposed (Paolini et al., 2021;Huang et al., , 2022. TANL (Paolini et al., 2021) treats event extraction as translation tasks between augmented natural languages. Their predicted target-augmented language embed labels into the input passage via using brackets and vertical bar symbols. TempGen ) is a template-based role-filler entity extraction model, which generate outputs that fill role entities into non-natural templated sequences. The output sequence designs of TANL and Temp-Gen hinder the models from fully leveraging label semantics, unlike DEGREE that generates natural sentences. BART-Gen is also a generation-based model focusing on documentlevel event argument extraction. They solve event extraction with a pipeline, which prevents knowledge sharing across subtasks. All these fully supervised methods can achieve substantial performance with a large amount of annotated data. However, their designs are not specific for low-resource scenarios, hence, these models can not enjoy all the benefits that DEGREE obtains for low-resource event extraction at the same time, as we mentioned in Section 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_66",
            "content": "Low-resource event extraction. It has been a growing interest in event extraction in a scenario with less data. uses a machine reading comprehension formulation to conduct event extraction in a low-resource regime. Text2Event (Lu et al., 2021), a sequence-tostructure generation paradigm, first presents events in a linearized format, and then trains a generative model to generate the linearized event sequence. Text2Event's unnatural output format hinders the model from fully leveraging pre-trained knowledge. Hence, their model falls short on the cases with only extremely low data being available (as shown in Section 3).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_67",
            "content": "Another thread of works are using meta-learning to deal with the less label challenge (Deng et al., 2020;Shen et al., 2021;Cong et al., 2021). However, their methods can only be applied to event detection, which differs from our main focus on studying end-to-end event extraction.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_68",
            "content": "Conclusion & Future Work",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "125-ARR_v2_69",
            "content": "In this paper, we present DEGREE, a data-efficient generation-based event extraction model. DEGREE requires less training data because it better utilizes label semantics as well as weakly-supervised information, and captures better dependencies by jointly predicting triggers and arguments. Our experimental results and ablation studies show the superiority of DEGREE for low-resource event extraction.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_70",
            "content": "DEGREE assumes that some weakly-supervised information (the description of events, similar keywords, and human-written templates) is accessible or not expensive for the users to craft. This assumption may holds for most situations. We leave the automation of template construction for future work, which can further ease the needed efforts when deploying DEGREE in a large-scale corpus.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_71",
            "content": "DEGREE fine-tunes the pre-trained generative language model (Lewis et al., 2020). Therefore, the generated output is potentially affected by the corpus for pre-training. Although with a low possibility, it is possible for our model to accidentally generate some malicious, counterfactual, and biased sentences, which may cause ethics concerns.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_72",
            "content": "We suggest carefully examining those potential issues before deploying the model in any real-world applications.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_73",
            "content": "Our strategy to create an EAE template is first identifying all valid argument roles for the event type, 4 such as Attacker, Target, Instrument, and Place roles. Then, for each argument role, according to the semantics of the role type, we select natural and fluent words to form its placeholder (e.g., some way for Instrument). This design aims to provide a simple way to help the model learn both the roles' label semantics and the event structure. Finally, we create a natural language sentence that connects all these placeholders. Notice that we try to keep the template as simple and short as possible. Table 9 lists all designed EAE templates for ACE05-E and ACE05-E + . The EAE templates of ERE-EN can be found in Table 10.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_74",
            "content": "Given a passage, its annotated event types are consider as positive event types. During training, we additionally sample m event types that are not related to the passage as the negative examples, where m is a hyper-parameter. In our experiments, m is usually set to 13 or 15.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_75",
            "content": "For all of DEGREE, DEGREE(ED), and DE-GREE(EAE), we fine-tune the pre-trained BARTlarge (Lewis et al., 2020) with Huggingface package (Wolf et al., 2020). The number of parameters is around 406 millions. We train DEGREE with our machine that equips 128 AMD EPYC 7452 32-Core Processor, 4 NVIDIA A100 GPUs, and 792G RAM. We consider AdamW optimizer (Loshchilov and Hutter, 2019) with learning rate set to 10 \u22125 and the weight decay set to 10 \u22125 . We set the batch size to 6 for DEGREE(EAE) and 32 for DEGREE(ED) and DEGREE. The number of training epochs is 45. It takes around 2 hours, 18 hours, 22 hours to train DEGREE(EAE), DEGREE(ED), and DEGREE, respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_76",
            "content": "We do hyper-parameter search on m, the number of negative examples, from {3, 5, 7, 10, 13, 15, 18, 21}, and our preliminary trials shows that m less than 10 are usually less useful. For the learning rate and the weight decay, we tune it based on our preliminary experiment for event argument extraction from {10 \u22125 , 10 \u22124 }, while they are both fixed to 10 \u22125 for all the experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_77",
            "content": "We consider ACE 2005 5 (Doddington et al., 2004) and ERE 6 (Song et al., 2015). Both consider LDC User Agreement for Non-Members 7 as the licenses. Both datasets are created for entity, relation, and event extraction while our focus is only event extraction in this paper. In the original ACE 2005 dataset, it contains data for English, Chinese, and Arabic and we only take the English data for our experiment. In the original ERE dataset, it contains data for English, and Chinese and we only take the English data for our experiment as well.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_78",
            "content": "Because both datasets contain event like Justice:Execute and Life:Die, it is possible that some offensive words (e.g., killed) would appear in the passage. Also, some real names may appear in the passage as well (e.g., Palestinian president, Mahmoud Abbas). How to accurately identify these kinds of information is part of the goal of the task. Therefore, we do not take any changes on the datasets for protecting or anonymizing.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_79",
            "content": "We split the training data based on documents, which is a more realistic setup compared to splitting data by instance. Table 11 lists the statistics of ACE05-E, ACE05-E + , and ERE-EN. Specifically, we try to make each proportion of data contain as many event types as possible.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_80",
            "content": "This section describes the implementation details for all baselines we use. We run the experiments with three different random seeds and report the best value.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_81",
            "content": "\u2022 DyGIE++: we use their released pre-trained model 8 for evaluation. \u2022 OneIE: we use their provided code 9 to train the model with default parameters. \u2022 BERT_QA: we use their provided code 10 to train the model with default parameters. \u2022 TANL: we use their provided code 11 to train the model. We conduct the experiments with two variations: (1) using their default parameters, and (2) using their default parameters but with more training epochs. We observe that the second variant works better. As a result, we report the number obtained from the second setting. \u2022 Text2Event: we use their official code 12 to train the model with the provided parameter setting. \u2022 dbRNN: we directly report the experimental results from their paper. \u2022 Joint3EE: we directly report the experimental results from their paper. \u2022 MQAEE: we directly report the experimental results from their paper. \u2022 BART-Gen: we report the experimental results from their released appendix. 13",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_82",
            "content": "Life:Injure somebody or some organization led to some victim injured by some way in somewhere. Life:Die somebody or some organization led to some victim died by some way in somewhere.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_83",
            "content": "Movement:Transport something was sent to somewhere from some place by some vehicle. somebody or some organization was responsible for the transport. Transaction:Transfer-Ownership someone got something from some seller in somewhere. Transaction:Transfer-Money someone paid some other in somewhere. Business:Start-Org somebody or some organization launched some organzation in somewhere. Business:Merge-Org some organzation was merged. Business:Declare-Bankruptcy some organzation declared bankruptcy. Business:End-Org some organzation dissolved.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_84",
            "content": "Conflict:Attack some attacker attacked some facility, someone, or some organization by some way in somewhere. Conflict:Demonstrate some people or some organization protest at somewhere. Contact:Meet some people or some organization met at somewhere. Contact:Phone-Write some people or some organization called or texted messages at somewhere.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_85",
            "content": "Personnel:Start-Position somebody got new job and was hired by some people or some organization in somewhere. Personnel:End-Position somebody stopped working for some people or some organization at somewhere. Personnel:Nominate somebody was nominated by somebody or some organization to do a job. Life:Be-Born somebody was born in somewhere. Life:Marry somebody got married in somewhere. Life:Divorce somebody divorced in somewhere.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_86",
            "content": "Life:Injure somebody or some organization led to some victim injured by some way in somewhere. Life:Die somebody or some organization led to some victim died by some way in somewhere.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_87",
            "content": "Movement:Transport-Person somebody was moved to somewhere from some place by some way. somebody or some organization was responsible for the movement.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_88",
            "content": "Movement:Transport-Artifact something was sent to somewhere from some place. somebody or some organization was responsible for the transport. Business:Start-Org somebody or some organization launched some organzation in somewhere. Business:Merge-Org some organzation was merged. Business:Declare-Bankruptcy some organzation declared bankruptcy. Business:End-Org some organzation dissolved.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_89",
            "content": "Conflict:Attack some attacker attacked some facility, someone, or some organization by some way in somewhere. Conflict:Demonstrate some people or some organization protest at somewhere. Contact:Meet some people or some organization met at somewhere. Contact:Correspondence some people or some organization contacted each other at somewhere.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_90",
            "content": "Contact:Broadcast some people or some organization made announcement to some publicity at somewhere. Contact:Contact some people or some organization talked to each other at somewhere. Manufacture:Artifact something was built by somebody or some organization in somewhere.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_91",
            "content": "Personnel:Start-Position somebody got new job and was hired by some people or some organization in somewhere. Personnel:End-Position somebody stopped working for some people or some organization at somewhere. Personnel:Nominate somebody was nominated by somebody or some organization to do a job.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_92",
            "content": "Personnel:Elect somebody was elected a position, and the election was voted by somebody or some organization in somewhere.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_93",
            "content": "Transaction:Transfer-Ownership The ownership of something from someone was transferred to some other at somewhere.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_94",
            "content": "In order to further test our models' generaliability, we additionally conduct zero-shot and fewshot experiments on the ACE05-E dataset with DEGREE(ED) and DEGREE(EAE).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_95",
            "content": "Settings. We first select the top n common event types as \"seen\" types and use the rest as \"unseen/rare\" types, where the top common types are listed in Table 12. To simulate a zero-shot scenario, we remove all events with \"unseen/rare\" types from the training data. To simulate a few-shot scenario, we keep only k event examples for each \"unseen/rare\" type (denoted as k-shot). During the evaluation, we calculate micro F1-scores only for these \"unseen/rare\" types. Compared baselines. We consider the following baselines: (1) BERT_QA (Du and Cardie, 2020) (2) OneIE (3) Matching baseline, a proposed baseline that makes trigger predictions by performing string matching between the input passage and the event keywords. (4) Lemmatization baseline, another proposed baseline that performs string matching on lemmatized input passage and the event keywords. (Note: (3) and ( 4) are baselines only for event detection tasks.)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_96",
            "content": "Experimental results. Figure 4, Table 13, and Table 14 show the results of n = 5 and n = 10.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_97",
            "content": "From the two subfigures in the left column, we see that DEGREE(ED) achieves promising results in the zero-shot setting. In fact, it performs better than BERT_QA trained in the 10-shot setting and OneIE trained in the 5-shot setting. This demonstrates the great potential of DEGREE(ED) to discover new event types. Interestingly, we observe that our two proposed baselines perform surprisingly well, suggesting that the trigger annotations in ACE05-E are actually not diverse. Despite their impressive performance, DEGREE(ED) still outperforms the matching baseline by over 4.7% absolute trigger classification F1 in both n = 5 and n = 10 cases in zero-shot scenario. Additionally, with only one training instance for each unseen type, DEGREE(ED) can outperform both proposed baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_98",
            "content": "Next, we compare the results for the event argument extraction task. From the two middle subfigures, we observe that when given gold triggers, our model performs much better than all baselines with a large margin. Lastly, we train models for both trigger and argument extraction and report the final argument classification scores in the two right subfigures. We justify that our model has strong generalizability to unseen event types and it can outperform BERT_QA and OneIE even when they are both trained in 5-shot settings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "125-ARR_v2_99",
            "content": "David Ahn, The stages of event extraction, 2006, Proceedings of the Workshop on Annotating and Reasoning about Time and Events, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "David Ahn"
                ],
                "title": "The stages of event extraction",
                "pub_date": "2006",
                "pub_title": "Proceedings of the Workshop on Annotating and Reasoning about Time and Events",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_100",
            "content": "Jonathan Berant, Vivek Srikumar, Pei-Chun Chen, Abby Vander Linden, Brittany Harding, Brad Huang, Peter Clark, Christopher Manning, Modeling biological processes for reading comprehension, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Jonathan Berant",
                    "Vivek Srikumar",
                    "Pei-Chun Chen",
                    "Abby Vander Linden",
                    "Brittany Harding",
                    "Brad Huang",
                    "Peter Clark",
                    "Christopher Manning"
                ],
                "title": "Modeling biological processes for reading comprehension",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
                "pub": "EMNLP"
            }
        },
        {
            "ix": "125-ARR_v2_101",
            "content": "Xin Cong, Shiyao Cui, Bowen Yu, Tingwen Liu, Yubin Wang, Bin Wang, Few-shot event detection with prototypical amortized conditional random field, 2021, Findings of the Association for Computational Linguistics: ACL/IJCNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Xin Cong",
                    "Shiyao Cui",
                    "Bowen Yu",
                    "Tingwen Liu",
                    "Yubin Wang",
                    "Bin Wang"
                ],
                "title": "Few-shot event detection with prototypical amortized conditional random field",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL/IJCNLP",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_102",
            "content": "Shumin Deng, Ningyu Zhang, Jiaojian Kang, Yichi Zhang, Wei Zhang, Huajun Chen, Metalearning with dynamic-memory-based prototypical network for few-shot event detection, 2020, The Thirteenth ACM International Conference on Web Search and Data Mining (WSDM), .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Shumin Deng",
                    "Ningyu Zhang",
                    "Jiaojian Kang",
                    "Yichi Zhang",
                    "Wei Zhang",
                    "Huajun Chen"
                ],
                "title": "Metalearning with dynamic-memory-based prototypical network for few-shot event detection",
                "pub_date": "2020",
                "pub_title": "The Thirteenth ACM International Conference on Web Search and Data Mining (WSDM)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_103",
            "content": "George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, Ralph Weischedel, The automatic content extraction (ACE) program -tasks, data, and evaluation, 2004, Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC), .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "George Doddington",
                    "Alexis Mitchell",
                    "Mark Przybocki",
                    "Lance Ramshaw",
                    "Stephanie Strassel",
                    "Ralph Weischedel"
                ],
                "title": "The automatic content extraction (ACE) program -tasks, data, and evaluation",
                "pub_date": "2004",
                "pub_title": "Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_104",
            "content": "Xinya Du, Claire Cardie, Event extraction by answering (almost) natural questions, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Xinya Du",
                    "Claire Cardie"
                ],
                "title": "Event extraction by answering (almost) natural questions",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_105",
            "content": "UNKNOWN, None, 2021, Language model priming for cross-lingual event extraction, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Language model priming for cross-lingual event extraction",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_106",
            "content": "Rujun Han, I-Hung Hsu, Jiao Sun, Julia Baylon, Qiang Ning, Dan Roth, Nanyun Peng, ESTER: A machine reading comprehension dataset for reasoning about event semantic relations, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Rujun Han",
                    "I-Hung Hsu",
                    "Jiao Sun",
                    "Julia Baylon",
                    "Qiang Ning",
                    "Dan Roth",
                    "Nanyun Peng"
                ],
                "title": "ESTER: A machine reading comprehension dataset for reasoning about event semantic relations",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_107",
            "content": "Rujun Han, I-Hung Hsu, Mu Yang, Aram Galstyan, Ralph Weischedel, Nanyun Peng, Deep structured neural network for event temporal relation extraction, 2019-11-03, Proceedings of the 23rd Conference on Computational Natural Language Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Rujun Han",
                    "I-Hung Hsu",
                    "Mu Yang",
                    "Aram Galstyan",
                    "Ralph Weischedel",
                    "Nanyun Peng"
                ],
                "title": "Deep structured neural network for event temporal relation extraction",
                "pub_date": "2019-11-03",
                "pub_title": "Proceedings of the 23rd Conference on Computational Natural Language Learning",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_108",
            "content": "Rujun Han, Qiang Ning, Nanyun Peng, Joint event and temporal relation extraction with shared representations and structured prediction, 2019, 2019 Conference on Empirical Methods in Natural Language Processing, EMNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Rujun Han",
                    "Qiang Ning",
                    "Nanyun Peng"
                ],
                "title": "Joint event and temporal relation extraction with shared representations and structured prediction",
                "pub_date": "2019",
                "pub_title": "2019 Conference on Empirical Methods in Natural Language Processing",
                "pub": "EMNLP"
            }
        },
        {
            "ix": "125-ARR_v2_109",
            "content": "Frederik Hogenboom, Flavius Frasincar, Uzay Kaymak, Franciska De, Jong , Emiel Caron, A survey of event extraction methods from text for decision support systems, 2016, Decis. Support Syst, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Frederik Hogenboom",
                    "Flavius Frasincar",
                    "Uzay Kaymak",
                    "Franciska De",
                    "Jong ",
                    "Emiel Caron"
                ],
                "title": "A survey of event extraction methods from text for decision support systems",
                "pub_date": "2016",
                "pub_title": "Decis. Support Syst",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_110",
            "content": "Kuan-Hao Huang, I-Hung Hsu, Premkumar Natarajan, Kai-Wei Chang, Nanyun Peng, Multilingual generative language models for zero-shot crosslingual event argument extraction, 2022, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Kuan-Hao Huang",
                    "I-Hung Hsu",
                    "Premkumar Natarajan",
                    "Kai-Wei Chang",
                    "Nanyun Peng"
                ],
                "title": "Multilingual generative language models for zero-shot crosslingual event argument extraction",
                "pub_date": "2022",
                "pub_title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_111",
            "content": "Hsiang Kung, Nanyun Huang,  Peng, Document-level event extraction with efficient end-to-end learning of cross-event dependencies, 2021, The 3rd Workshop on Narrative Understanding (NAACL 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Hsiang Kung",
                    "Nanyun Huang",
                    " Peng"
                ],
                "title": "Document-level event extraction with efficient end-to-end learning of cross-event dependencies",
                "pub_date": "2021",
                "pub_title": "The 3rd Workshop on Narrative Understanding (NAACL 2021",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_112",
            "content": "Kung-Hsiang Huang, Sam Tang, Nanyun Peng, Document-level entity-based extraction as template generation, 2021-07-11, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Kung-Hsiang Huang",
                    "Sam Tang",
                    "Nanyun Peng"
                ],
                "title": "Document-level entity-based extraction as template generation",
                "pub_date": "2021-07-11",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_113",
            "content": "Kung-Hsiang Huang, Mu Yang, Nanyun Peng, Biomedical event extraction with hierarchical knowledge graphs, 2020, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)-Findings, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Kung-Hsiang Huang",
                    "Mu Yang",
                    "Nanyun Peng"
                ],
                "title": "Biomedical event extraction with hierarchical knowledge graphs",
                "pub_date": "2020",
                "pub_title": "the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)-Findings",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_114",
            "content": "Heng Ji, Ralph Grishman, Refining event extraction through cross-document inference, 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Heng Ji",
                    "Ralph Grishman"
                ],
                "title": "Refining event extraction through cross-document inference",
                "pub_date": "2008",
                "pub_title": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_115",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Mike Lewis",
                    "Yinhan Liu",
                    "Naman Goyal",
                    "Marjan Ghazvininejad",
                    "Abdelrahman Mohamed",
                    "Omer Levy",
                    "Veselin Stoyanov",
                    "Luke Zettlemoyer"
                ],
                "title": "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_116",
            "content": "Fayuan Li, Weihua Peng, Yuguang Chen, Quan Wang, Lu Pan, Yajuan Lyu, Yong Zhu, Event extraction as multi-turn question answering, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Fayuan Li",
                    "Weihua Peng",
                    "Yuguang Chen",
                    "Quan Wang",
                    "Lu Pan",
                    "Yajuan Lyu",
                    "Yong Zhu"
                ],
                "title": "Event extraction as multi-turn question answering",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_117",
            "content": "Qi Li, Ji Heng, Liang Huang, Joint event extraction via structured prediction with global features, 2013, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Qi Li",
                    "Ji Heng",
                    "Liang Huang"
                ],
                "title": "Joint event extraction via structured prediction with global features",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_118",
            "content": "Sha Li, Ji Heng, Jiawei Han, Document-level event argument extraction by conditional generation, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Sha Li",
                    "Ji Heng",
                    "Jiawei Han"
                ],
                "title": "Document-level event argument extraction by conditional generation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_119",
            "content": "Ying Lin, Heng Ji, Fei Huang, Lingfei Wu, A joint neural model for information extraction with global features, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Ying Lin",
                    "Heng Ji",
                    "Fei Huang",
                    "Lingfei Wu"
                ],
                "title": "A joint neural model for information extraction with global features",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_120",
            "content": "Jian Liu, Yubo Chen, Kang Liu, Wei Bi, Xiaojiang Liu, Event extraction as machine reading comprehension, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Jian Liu",
                    "Yubo Chen",
                    "Kang Liu",
                    "Wei Bi",
                    "Xiaojiang Liu"
                ],
                "title": "Event extraction as machine reading comprehension",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_121",
            "content": "Ilya Loshchilov, Frank Hutter, Decoupled weight decay regularization, 2019-05-06, 7th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Ilya Loshchilov",
                    "Frank Hutter"
                ],
                "title": "Decoupled weight decay regularization",
                "pub_date": "2019-05-06",
                "pub_title": "7th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_122",
            "content": "Yaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, Jialong Tang, Annan Li, Le Sun, Meng Liao, Shaoyi Chen, Text2event: Controllable sequence-tostructure generation for end-to-end event extraction, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL/IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Yaojie Lu",
                    "Hongyu Lin",
                    "Jin Xu",
                    "Xianpei Han",
                    "Jialong Tang",
                    "Annan Li",
                    "Le Sun",
                    "Meng Liao",
                    "Shaoyi Chen"
                ],
                "title": "Text2event: Controllable sequence-tostructure generation for end-to-end event extraction",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL/IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_123",
            "content": "Kyunghyun Thien Huu Nguyen, Ralph Cho,  Grishman, Joint event extraction via recurrent neural networks, 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Kyunghyun Thien Huu Nguyen",
                    "Ralph Cho",
                    " Grishman"
                ],
                "title": "Joint event extraction via recurrent neural networks",
                "pub_date": "2016",
                "pub_title": "The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_124",
            "content": "Huu Thien, Ralph Nguyen,  Grishman, Event detection and domain adaptation with convolutional neural networks, 2015, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Huu Thien",
                    "Ralph Nguyen",
                    " Grishman"
                ],
                "title": "Event detection and domain adaptation with convolutional neural networks",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_125",
            "content": "Minh Trung, Thien Nguyen,  Huu Nguyen, One for all: Neural joint modeling of entities and events, 2019, The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference (AAAI), .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Minh Trung",
                    "Thien Nguyen",
                    " Huu Nguyen"
                ],
                "title": "One for all: Neural joint modeling of entities and events",
                "pub_date": "2019",
                "pub_title": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference (AAAI)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_126",
            "content": "Giovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro Achille, Rishita Anubhai, C\u00edcero Nogueira, Bing Santos, Stefano Xiang,  Soatto, Structured prediction as translation between augmented natural languages, 2021, 9th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Giovanni Paolini",
                    "Ben Athiwaratkun",
                    "Jason Krone",
                    "Jie Ma",
                    "Alessandro Achille",
                    "Rishita Anubhai",
                    "C\u00edcero Nogueira",
                    "Bing Santos",
                    "Stefano Xiang",
                    " Soatto"
                ],
                "title": "Structured prediction as translation between augmented natural languages",
                "pub_date": "2021",
                "pub_title": "9th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_127",
            "content": "Lei Sha, Feng Qian, Baobao Chang, Zhifang Sui, Jointly extracting event triggers and arguments by dependency-bridge RNN and tensor-based argument interaction, 2018, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI), .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Lei Sha",
                    "Feng Qian",
                    "Baobao Chang",
                    "Zhifang Sui"
                ],
                "title": "Jointly extracting event triggers and arguments by dependency-bridge RNN and tensor-based argument interaction",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_128",
            "content": "Shirong Shen, Tongtong Wu, Guilin Qi, Yuan-Fang Li, Gholamreza Haffari, Sheng Bi, Adaptive knowledge-enhanced bayesian meta-learning for fewshot event detection, 2021, Findings of the Association for Computational Linguistics: ACL/IJCNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Shirong Shen",
                    "Tongtong Wu",
                    "Guilin Qi",
                    "Yuan-Fang Li",
                    "Gholamreza Haffari",
                    "Sheng Bi"
                ],
                "title": "Adaptive knowledge-enhanced bayesian meta-learning for fewshot event detection",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL/IJCNLP",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_129",
            "content": "Zhiyi Song, Ann Bies, Stephanie Strassel, Tom Riese, Justin Mott, Joe Ellis, Jonathan Wright, Seth Kulick, Neville Ryant, Xiaoyi Ma, From light to rich ERE: annotation of entities, relations, and events, 2015, Proceedings of the The 3rd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, (EVENTS@HLP-NAACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Zhiyi Song",
                    "Ann Bies",
                    "Stephanie Strassel",
                    "Tom Riese",
                    "Justin Mott",
                    "Joe Ellis",
                    "Jonathan Wright",
                    "Seth Kulick",
                    "Neville Ryant",
                    "Xiaoyi Ma"
                ],
                "title": "From light to rich ERE: annotation of entities, relations, and events",
                "pub_date": "2015",
                "pub_title": "Proceedings of the The 3rd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, (EVENTS@HLP-NAACL)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_130",
            "content": "Jiao Sun, Nanyun Peng, Men are elected, women are married: Events gender bias on wikipedia, 2021-08-01, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Jiao Sun",
                    "Nanyun Peng"
                ],
                "title": "Men are elected, women are married: Events gender bias on wikipedia",
                "pub_date": "2021-08-01",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "125-ARR_v2_131",
            "content": "David Wadden, Ulme Wennberg, Yi Luan, Hannaneh Hajishirzi, Entity, relation, and event extraction with contextualized span representations, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "David Wadden",
                    "Ulme Wennberg",
                    "Yi Luan",
                    "Hannaneh Hajishirzi"
                ],
                "title": "Entity, relation, and event extraction with contextualized span representations",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": "EMNLP-IJCNLP"
            }
        },
        {
            "ix": "125-ARR_v2_132",
            "content": "Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong Han, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin, Jie Zhou, MAVEN: A massive general domain event detection dataset, 2020-11-16, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Xiaozhi Wang",
                    "Ziqi Wang",
                    "Xu Han",
                    "Wangyi Jiang",
                    "Rong Han",
                    "Zhiyuan Liu",
                    "Juanzi Li",
                    "Peng Li",
                    "Yankai Lin",
                    "Jie Zhou"
                ],
                "title": "MAVEN: A massive general domain event detection dataset",
                "pub_date": "2020-11-16",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_133",
            "content": "Xiaozhi Wang, Ziqi Wang, Xu Han, Zhiyuan Liu, Juanzi Li, Peng Li, Maosong Sun, Jie Zhou, Xiang Ren, HMEAE: hierarchical modular event argument extraction, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Xiaozhi Wang",
                    "Ziqi Wang",
                    "Xu Han",
                    "Zhiyuan Liu",
                    "Juanzi Li",
                    "Peng Li",
                    "Maosong Sun",
                    "Jie Zhou",
                    "Xiang Ren"
                ],
                "title": "HMEAE: hierarchical modular event argument extraction",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": "EMNLP-IJCNLP"
            }
        },
        {
            "ix": "125-ARR_v2_134",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest,  Rush, Huggingface's transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Thomas Wolf",
                    "Lysandre Debut",
                    "Victor Sanh",
                    "Julien Chaumond",
                    "Clement Delangue",
                    "Anthony Moi",
                    "Pierric Cistac",
                    "Tim Rault",
                    "Remi Louf",
                    "Morgan Funtowicz",
                    "Joe Davison",
                    "Sam Shleifer",
                    "Clara Patrick Von Platen",
                    "Yacine Ma",
                    "Julien Jernite",
                    "Canwen Plu",
                    "Teven Xu",
                    "Sylvain Scao",
                    "Mariama Gugger",
                    "Quentin Drame",
                    "Alexander Lhoest",
                    " Rush"
                ],
                "title": "Huggingface's transformers: State-of-the-art natural language processing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_135",
            "content": "Bishan Yang, Tom Mitchell, Joint extraction of events and entities within a document context, 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Bishan Yang",
                    "Tom Mitchell"
                ],
                "title": "Joint extraction of events and entities within a document context",
                "pub_date": "2016",
                "pub_title": "The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_136",
            "content": "Sen Yang, Dawei Feng, Linbo Qiao, Zhigang Kan, Dongsheng Li, Exploring pre-trained language models for event extraction and generation, 2019-07-28, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Sen Yang",
                    "Dawei Feng",
                    "Linbo Qiao",
                    "Zhigang Kan",
                    "Dongsheng Li"
                ],
                "title": "Exploring pre-trained language models for event extraction and generation",
                "pub_date": "2019-07-28",
                "pub_title": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "125-ARR_v2_137",
            "content": "Sen Yang, Dawei Feng, Linbo Qiao, Zhigang Kan, Dongsheng Li, Exploring pre-trained language models for event extraction and generation, 2019, Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Sen Yang",
                    "Dawei Feng",
                    "Linbo Qiao",
                    "Zhigang Kan",
                    "Dongsheng Li"
                ],
                "title": "Exploring pre-trained language models for event extraction and generation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "125-ARR_v2_138",
            "content": "Hongming Zhang, Xin Liu, Haojie Pan, Yangqiu Song, Cane Wing, -Ki Leung, ASER: A largescale eventuality knowledge graph, 2020, The Web Conference, WWW.",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Hongming Zhang",
                    "Xin Liu",
                    "Haojie Pan",
                    "Yangqiu Song",
                    "Cane Wing",
                    "-Ki Leung"
                ],
                "title": "ASER: A largescale eventuality knowledge graph",
                "pub_date": "2020",
                "pub_title": "The Web Conference",
                "pub": "WWW"
            }
        },
        {
            "ix": "125-ARR_v2_139",
            "content": "UNKNOWN, None, , Table 14: Full results of zero/few-shot event argument extraction on, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Table 14: Full results of zero/few-shot event argument extraction on",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "125-ARR_v2_0@0",
            "content": "DEGREE: A Data-Efficient Generation-Based Event Extraction Model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_0",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_2@0",
            "content": "Event extraction requires high-quality expert human annotations, which are usually expensive.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_2",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_2@1",
            "content": "Therefore, learning a data-efficient event extraction model that can be trained with only a few labeled examples has become a crucial challenge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_2",
            "start": 94,
            "end": 237,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_2@2",
            "content": "In this paper, we focus on low-resource end-to-end event extraction and propose DE-GREE, a data-efficient model that formulates event extraction as a conditional generation problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_2",
            "start": 239,
            "end": 419,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_2@3",
            "content": "Given a passage and a manually designed prompt, DEGREE learns to summarize the events mentioned in the passage into a natural sentence that follows a predefined pattern.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_2",
            "start": 421,
            "end": 589,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_2@4",
            "content": "The final event predictions are then extracted from the generated sentence with a deterministic algorithm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_2",
            "start": 591,
            "end": 696,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_2@5",
            "content": "DEGREE has three advantages to learn well with less training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_2",
            "start": 698,
            "end": 763,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_2@6",
            "content": "First, our designed prompts provide semantic guidance for DEGREE to leverage label semantics and thus better capture the event arguments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_2",
            "start": 765,
            "end": 901,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_2@7",
            "content": "Moreover, DEGREE is capable of using additional weaklysupervised information, such as the description of events encoded in the prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_2",
            "start": 903,
            "end": 1037,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_2@8",
            "content": "Finally, DE-GREE learns triggers and arguments jointly in an end-to-end manner, which encourages the model to better utilize the shared knowledge and dependencies among them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_2",
            "start": 1039,
            "end": 1212,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_2@9",
            "content": "Our experimental results demonstrate the strong performance of DEGREE for low-resource event extraction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_2",
            "start": 1214,
            "end": 1317,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_4@0",
            "content": "Event extraction (EE) aims to extract events, each of which consists of a trigger and several participants (arguments) with their specific roles, from a given passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_4",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_4@1",
            "content": "For example, in Figure 1, a Justice:Execute event is triggered by the word \"execution\" and this event contains three argument roles, including an Agent (Indonesia) who carries out the execution, a Person who is executed (convicts), and a Place where the event occurs (not mentioned in the passage).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_4",
            "start": 168,
            "end": 465,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_4@2",
            "content": "Previous work Figure 1: Two examples of events (Justice:Execute and Justice:Appeal) extracted from the given passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_4",
            "start": 467,
            "end": 583,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_4@3",
            "content": "Fincke et al., 2021) usually divides EE into two subtasks: (1) event detection, which identifies event triggers and their types, and (2) event argument extraction, which extracts the arguments and their roles for given event triggers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_4",
            "start": 585,
            "end": 818,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_4@4",
            "content": "EE has been shown to benefit a wide range of applications, e.g., building knowledge graphs , question answering (Berant et al., 2014;, and other downstream studies (Han et al., 2019a;Hogenboom et al., 2016;.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_4",
            "start": 820,
            "end": 1026,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_5@0",
            "content": "Most prior works on EE rely on a large amount of annotated data for training (Nguyen and Grishman, 2015;Nguyen et al., 2016;Han et al., 2019b;Du and Cardie, 2020;Paolini et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_5",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_5@1",
            "content": "However, high-quality event annotations are expensive to obtain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_5",
            "start": 185,
            "end": 248,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_5@2",
            "content": "For example, the ACE 2005 corpus (Doddington et al., 2004), one of the most widely used EE datasets, requires two rounds of annotations by linguistics experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_5",
            "start": 250,
            "end": 408,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_5@3",
            "content": "The high annotation costs make these models hard to be extended to new domains and new event types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_5",
            "start": 410,
            "end": 508,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_5@4",
            "content": "Therefore, how to learn a data-efficient EE model trained with only a few annotated examples is a crucial challenge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_5",
            "start": 510,
            "end": 625,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_6@0",
            "content": "In this paper, we focus on low-resource event extraction, where only a small amount of training examples are available for training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_6",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_6@1",
            "content": "We propose DEGREE (Data-Efficient GeneRation-Based Event Extraction), a generation-based model that takes a passage and a manually designed prompt as the input, and learns to summarize the passage into a",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_6",
            "start": 133,
            "end": 335,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_7@0",
            "content": "The event is related to conflict and some violent physical act.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_7",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_8@0",
            "content": "Similar triggers such as war, attack, terrorism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_8",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_9@0",
            "content": "Event trigger is <Trigger>. \\n some attacker attacked some facility, someone, or some organization by some way in somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_9",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_10@0",
            "content": "Event trigger is detonated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_10",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_10@1",
            "content": "\\n Palestinian attacked jeep and soldiers by bomb in Gaza Strip.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_10",
            "start": 28,
            "end": 91,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_10@2",
            "content": "The input of DEGREE consists of the given passage and our design prompt that contains an event type description, event keywords, and a E2E template.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_10",
            "start": 93,
            "end": 240,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_10@3",
            "content": "DEGREE is trained to generate an output to fill in the placeholders (underlined words) in the E2E template with triggers and arguments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_10",
            "start": 242,
            "end": 376,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_10@4",
            "content": "The final event prediction is then decoded from the generated output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_10",
            "start": 378,
            "end": 446,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_11@0",
            "content": "Passage",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_11",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_12@0",
            "content": "natural sentence following a predefined template, as illustrated in Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_12",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_12@1",
            "content": "The event triggers and arguments can then be extracted from the generated sentence by using a deterministic algorithm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_12",
            "start": 78,
            "end": 195,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_12@2",
            "content": "DEGREE enjoys the following advantages to learn well with less training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_12",
            "start": 197,
            "end": 273,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_12@3",
            "content": "First, the framework provides label semantics via the designed template in the prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_12",
            "start": 275,
            "end": 361,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_12@4",
            "content": "As the example in Figure 2 shows, the word \"somewhere\" in the prompt guides the model to predict words being similar to location for the role Place.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_12",
            "start": 363,
            "end": 510,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_12@5",
            "content": "In addition, the sentence structure of the template and the word \"attacked\" depict the semantic relation between the role Attacker and the role Target.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_12",
            "start": 512,
            "end": 662,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_12@6",
            "content": "With these kinds of guidance, DEGREE can make more accurate predictions with less training examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_12",
            "start": 664,
            "end": 763,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_12@7",
            "content": "Second, the prompts can incorporate additional weaksupervision signal about the task, such as the description of the event and similar keywords.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_12",
            "start": 765,
            "end": 908,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_12@8",
            "content": "These resources are usually readily available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_12",
            "start": 910,
            "end": 955,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_12@9",
            "content": "For example, in our experiments, we take the information from the annotation guideline, which is provided along with the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_12",
            "start": 957,
            "end": 1085,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_12@10",
            "content": "This information facilitates DEGREE to learn under a low-resource situation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_12",
            "start": 1087,
            "end": 1162,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_12@11",
            "content": "Finally, DEGREE is designed for end-to-end event extraction and can solve event detection and event argument extraction at the same time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_12",
            "start": 1164,
            "end": 1300,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_12@12",
            "content": "Leveraging the shared knowledge and dependencies between the two tasks makes our model more data-efficient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_12",
            "start": 1302,
            "end": 1408,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_13@0",
            "content": "Existing works on EE usually have only one or two of above-mentioned advantages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_13",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_13@1",
            "content": "For exam-ple, previous classification-based models (Nguyen et al., 2016;Wang et al., 2019;Yang et al., 2019b;Wadden et al., 2019; can hardly encode label semantics and other weak supervision signals.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_13",
            "start": 81,
            "end": 279,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_13@2",
            "content": "Recently proposed generation-based models for event extraction solved the problem in a pipeline fashion; therefore, they cannot leverage shared knowledge between subtasks (Paolini et al., 2021;. Furthermore, their generated outputs are not natural sentences, which hinders the utilization of label semantics (Paolini et al., 2021;Lu et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_13",
            "start": 281,
            "end": 627,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_13@3",
            "content": "As a result, our model DEGREE can achieve significantly better performance than prior approaches on low-resource event extraction, as we will demonstrate in Section 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_13",
            "start": 629,
            "end": 795,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_14@0",
            "content": "Our contributions can be summarized as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_14",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_15@0",
            "content": "\u2022 We propose DEGREE, a generation-based event extraction model that learns well with less data by better incorporating label semantics and shared knowledge between subtasks (Section 2). \u2022 Experiments on ACE 2005 (Doddington et al., 2004) and ERE-EN (Song et al., 2015) demonstrate the strong performance of DEGREE in the low-resource setting (Section 3). \u2022 We present comprehensive ablation studies in both the low-resource and the high-resource setting to better understand the strengths and weaknesses of our model (Section 4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_15",
            "start": 0,
            "end": 528,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_16@0",
            "content": "Our code and models can be found at https: //github.com/PlusLabNLP/DEGREE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_16",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_17@0",
            "content": "Data-Efficient Event Extraction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_17",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_18@0",
            "content": "We introduce DEGREE, a generation-based model for low-resource event extraction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_18",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_18@1",
            "content": "Unlike previous works , which separate event extraction into two pipelined tasks (event detection and event argument extraction), DEGREE is designed for the end-to-end event extraction and predict event triggers and arguments at the same time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_18",
            "start": 81,
            "end": 323,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_19@0",
            "content": "The DEGREE Model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_19",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_20@0",
            "content": "We formulate event extraction as a conditional generation problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_20",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_20@1",
            "content": "As illustrated in Figure 2, given a passage and our designed prompt, DEGREE generates an output following a particular format.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_20",
            "start": 67,
            "end": 192,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_20@2",
            "content": "The final predictions of event triggers and argument roles can be then parsed from the generated output with a deterministic algorithm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_20",
            "start": 194,
            "end": 328,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_20@3",
            "content": "Compared to previous classification-based models (Wang et al., 2019;Yang et al., 2019b;Wadden et al., 2019;, the generation framework provides a flexible way to include additional information and guidance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_20",
            "start": 330,
            "end": 534,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_20@4",
            "content": "By designing appropriate prompts, we encourage DEGREE to better capture the dependencies between entities and, therefore, to reduce the number of training examples needed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_20",
            "start": 536,
            "end": 706,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_21@0",
            "content": "The desired prompt not only provides information but also defines the output format.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_21",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_21@1",
            "content": "As shown in Figure 2, it contains the following components:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_21",
            "start": 85,
            "end": 143,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_22@0",
            "content": "\u2022 Event type definition describes the definition for the given event type. 1 For example, \"The event is related to conflict and some violent physical act.\" describes a Conflict:Attack event. \u2022 Event keywords presents some words that are semantically related to the given event type. For example, war, attack, and terrorism are three event keywords for the Conflict:Attack event. In practice, we collect three words that appear as the triggers in the example sentences from the annotation guidelines. \u2022 E2E template defines the expected output format and can be separated into two parts. The first part is called ED template, which is designed as \"Event trigger is <Trigger>\", where \"<Trigger>\" is a special token serving as a placeholder. The second part is the EAE template, which differs based on the given event type. For example, in Figure 2, the EAE template for a Conflict:Attack event is \"some attacker attacked some facility, someone, or some organization by some way in somewhere\". Each underlined string starting with \"some-\" serves as a placeholder corresponding to an argument role for a Conflict:Attack event. For instance, \"some way\" corresponds to the role Instrument and \"somewhere\" corresponds to the role Place. Notice that every event type has its own EAE template.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_22",
            "start": 0,
            "end": 1283,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_23@0",
            "content": "We list three EAE templates in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_23",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_23@1",
            "content": "The full list of EAE templates and the construction details can be found in Appendix A.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_23",
            "start": 40,
            "end": 126,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_24@0",
            "content": "Training",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_24",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_25@0",
            "content": "The training objective of DEGREE is to generate an output that replaces the placeholders in E2E template with the gold labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_25",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_25@1",
            "content": "Take Figure 2 as an example, DEGREE is expected to replace \"<Trigger>\" with the gold trigger (detonated), replace \"some attacker\" with the gold argument for role Attacker (Palestinian), and replace \"some way\" with the gold argument for role Instrument (bomb).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_25",
            "start": 127,
            "end": 385,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_26@0",
            "content": "If there are multiple arguments for the same role, they are concatenated with \"and\"; if there is no predicted argument for one role, the model should keep the corresponding placeholder (i.e, \"some-\" in the E2E template).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_26",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_26@1",
            "content": "For the case that there are multiple triggers for the given event type in the input passage, DEGREE is trained to generate the output text that contains multiple E2E template such that each E2E template corresponds to one trigger and its argument roles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_26",
            "start": 221,
            "end": 473,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_26@2",
            "content": "The hyperparameter settings are detailed in Appendix B.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_26",
            "start": 475,
            "end": 529,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_27@0",
            "content": "Inference",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_27",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_28@0",
            "content": "We enumerate all event types and generate an output for each event type.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_28",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_28@1",
            "content": "After we obtain the generated sentences, we compare the outputs with E2E template to determine the predicted triggers and arguments in string format.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_28",
            "start": 73,
            "end": 221,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_28@2",
            "content": "Finally, we apply string matching to convert the predicted string to span offsets in the passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_28",
            "start": 223,
            "end": 319,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_28@3",
            "content": "If the predicted string appears in the passage multiple times, we choose all span offsets that match for trigger predictions and choose the one closest to the given trigger span for argument predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_28",
            "start": 321,
            "end": 523,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_29@0",
            "content": "Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_29",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_30@0",
            "content": "Notice that the E2E template plays an important role for DEGREE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_30",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_30@1",
            "content": "First, it serves as the control signal and defines the expected output format.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_30",
            "start": 65,
            "end": 142,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_30@2",
            "content": "Second, it provides label semantics to help DEGREE make accurate predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_30",
            "start": 144,
            "end": 220,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_30@3",
            "content": "Those placeholders (words starting with \"some-\") in the E2E template give DEGREE some hints about the entity types of arguments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_30",
            "start": 222,
            "end": 349,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_30@4",
            "content": "For instance, when seeing \"somewhere\", DEGREE tends to generate a location rather than a person.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_30",
            "start": 351,
            "end": 446,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_30@5",
            "content": "In addition, the words other than \"some-\" describe the relationships between roles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_30",
            "start": 448,
            "end": 530,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_30@6",
            "content": "For example, DEGREE knows the relationship between the role Attacker and the role Target (who is attacking and who is attacked) due to E2E template.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_30",
            "start": 532,
            "end": 679,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_30@7",
            "content": "This guidance helps DEGREE learn the dependencies between entities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_30",
            "start": 681,
            "end": 747,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_30@8",
            "content": "Unlike previous generation-based approaches (Paolini et al., 2021;, we intentionally write E2E templates in natural sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_30",
            "start": 749,
            "end": 874,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_30@9",
            "content": "This not only uses label semantics better but also makes the model easier to leverage the knowledge from the pre-trained decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_30",
            "start": 876,
            "end": 1004,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_30@10",
            "content": "In Section 4, we will provide experiments to demonstrate the advantage of using natural sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_30",
            "start": 1006,
            "end": 1103,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_31@0",
            "content": "Cost of template constructing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_31",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_31@1",
            "content": "DEGREE does require human effort to design the templates; however, writing those templates is much easier and more effortless than collecting complicated event annotations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_31",
            "start": 31,
            "end": 202,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_31@2",
            "content": "As shown in Table 1, we keep the EAE templates as simple and short as possible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_31",
            "start": 204,
            "end": 282,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_31@3",
            "content": "Therefore, it takes only about one minute for people who are not linguistic experts to compose a template.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_31",
            "start": 284,
            "end": 389,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_31@4",
            "content": "In fact, several prior works Du and Cardie, 2020;) also use constructed templates as weakly-supervised signals to improve models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_31",
            "start": 391,
            "end": 519,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_31@5",
            "content": "In Section 4, we will study how different templates affect the performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_31",
            "start": 521,
            "end": 595,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_32@0",
            "content": "Efficiency Considerations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_32",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_32@1",
            "content": "DEGREE requires to enumerate all event types during inference, which could cause efficiency considerations when extending to applications that contain many event types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_32",
            "start": 27,
            "end": 194,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_32@2",
            "content": "This issue is minor for our experiments on the two datasets (ACE 2005 and ERE-EN), which are relatively small scales in terms of the number of event types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_32",
            "start": 196,
            "end": 350,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_32@3",
            "content": "Due to the high cost of annotations, there is hardly any public datasets for end-to-end event extraction on a large scale, 2 and we cannot provide a more thorough studies when the experiments scale up.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_32",
            "start": 352,
            "end": 552,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_32@4",
            "content": "We leave the work on benchmarking and improving the efficiency of DEGREE in the scenario considering more diverse and comprehensive types of events as future work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_32",
            "start": 554,
            "end": 716,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_33@0",
            "content": "DEGREE in Pipeline Framework",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_33",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_34@0",
            "content": "DEGREE is flexible and can be easily modified to DEGREE(PIPE), which first focuses event detection (ED) and then solves event argument extraction (EAE).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_34",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_34@1",
            "content": "DEGREE(PIPE) consists of two models: (1) DEGREE(ED), which aims to exact event triggers for the given event type, and (2) DE-GREE(EAE), which identifies argument roles for the given event type and the corresponding trigger.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_34",
            "start": 153,
            "end": 375,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_34@2",
            "content": "DEGREE(ED) and DEGREE(EAE) are similar to DEGREE but with different prompts and output formats.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_34",
            "start": 377,
            "end": 471,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_34@3",
            "content": "We describe the difference as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_34",
            "start": 473,
            "end": 510,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_35@0",
            "content": "\u2022 Event type definition is the same as the ones for DEGREE. \u2022 Event keywords is the same as the one for DE-GREE. \u2022 ED template is designed as \"Event trigger is <Trigger>\", which is actually the first part of the E2E template.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_35",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_36@0",
            "content": "Similar to DEGREE, the objective of DEGREE(ED) is to generate an output that replaces \"<Trigger>\" in the ED template with event triggers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_36",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_37@0",
            "content": "\u2022 Event type definition is the same as the one for DEGREE. \u2022 Query trigger is a string that indicates the trigger word for the given event type. For example, \"The event trigger word is detonated\" points out that \"detonated\" is the given trigger. \u2022 EAE template is an event-type-specific template mentioned previously. It is actually the second part of E2E template.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_37",
            "start": 0,
            "end": 364,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_38@0",
            "content": "Similar to DEGREE, the goal for DEGREE(EAE) is to generate an outputs that replace the placeholders in EAE template with event arguments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_38",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_39@0",
            "content": "In Section 3, we will compare DEGREE with DEGREE(PIPE) to study the benefit of dealing with event extraction in an end-to-end manner under the low-resource setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_39",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_40@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_40",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_41@0",
            "content": "We conduct experiments for low-resource event extraction to study how DEGREE performs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_41",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_42@0",
            "content": "Experimental Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_42",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_43@0",
            "content": "Datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_43",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_43@1",
            "content": "We consider ACE 2005 (Doddington et al., 2004) and follow the pre-processing in Wadden et al. ( 2019) and , resulting in two variants: ACE05-E and ACE05-E + .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_43",
            "start": 10,
            "end": 167,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_43@2",
            "content": "Both contain 33 event types and 22 argument roles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_43",
            "start": 169,
            "end": 218,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_43@3",
            "content": "In addition, we consider ERE-EN (Song et al., 2015) and adopt the pre-processing in , which keeps 38 event types and 21 argument roles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_43",
            "start": 220,
            "end": 354,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_44@0",
            "content": "Data split for low-resource setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_44",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_44@1",
            "content": "We generate different proportions (1%, 2%, 3%, 5%, 10%, 20%, 30%, and 50%) of training data to study the influence of the size of the training set and use the original development set and test set for evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_44",
            "start": 37,
            "end": 248,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_44@2",
            "content": "Appendix C lists more details about the split generation process and the data statistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_44",
            "start": 250,
            "end": 338,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_45@0",
            "content": "Evaluation metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_45",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_45@1",
            "content": "We consider the same criteria in prior works (Wadden et al., 2019;. ( 1) Trigger F1-score: an trigger is correctly identified (Tri-I) if its offset matches the gold one; it is correctly classified (Tri-C) if its event type also matches the gold one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_45",
            "start": 20,
            "end": 268,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_45@2",
            "content": "(2) Argument F1-score: an argument is correctly identified (Arg-I) if its offset and event type match the gold ones; it is correctly classified (Arg-C) if its role matches as well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_45",
            "start": 270,
            "end": 449,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_46@0",
            "content": "Compared baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_46",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_46@1",
            "content": "We consider the following classification-based models: (1) OneIE , the current state-of-the-art (SOTA) EE model trained with designed global features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_46",
            "start": 20,
            "end": 169,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_46@2",
            "content": "( 2) BERT_QA (Du and Cardie, 2020), which views EE tasks as a sequence of extractive question answering problems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_46",
            "start": 171,
            "end": 283,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_46@3",
            "content": "Since it learns a classifier to indicate the position of the predicted span, we view it as a classification model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_46",
            "start": 285,
            "end": 398,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_46@4",
            "content": "We also consider the following generation-based models: (3) TANL (Paolini et al., 2021), which treats EE tasks as translation tasks between augmented natural languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_46",
            "start": 400,
            "end": 567,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_46@5",
            "content": "(4) Text2Event (Lu et al., 2021), a sequence-tostructure model that converts the input passage to a tree-like event structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_46",
            "start": 569,
            "end": 694,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_46@6",
            "content": "Note that the outputs of both generation-based baselines are not natural sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_46",
            "start": 696,
            "end": 778,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_46@7",
            "content": "Therefore, it is more difficult for them to utilize the label semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_46",
            "start": 780,
            "end": 851,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_46@8",
            "content": "All the implementation details can be found in Appendix D. It is worth noting that we train OneIE with named entity annotations, as the original papers suggest, while the other models are trained without entity annotations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_46",
            "start": 853,
            "end": 1075,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_47@0",
            "content": "Main Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_47",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_48@0",
            "content": "Table 2 shows the trigger classification F1-scores and the argument classification F1-scores in three data sets with different proportions of training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_48",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_48@1",
            "content": "The results are visualized in Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_48",
            "start": 157,
            "end": 195,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_48@2",
            "content": "Since our task is end-to-end event extraction, the argument classification F1-score is the more important metric that we considered when comparing models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_48",
            "start": 197,
            "end": 350,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_49@0",
            "content": "From the figure and the table, we can observe that both DEGREE and DEGREE(PIPE) outperform all other baselines when using less than 10% of the training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_49",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_49@1",
            "content": "The performance gap becomes much more significant under the extremely low data situation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_49",
            "start": 158,
            "end": 246,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_49@2",
            "content": "For example, when only 1% of the training data is available, DEGREE and DEGREE(PIPE) achieve more than 15 points of improvement in trigger classification F1 scores and more than 5 points in argument classification F1 scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_49",
            "start": 248,
            "end": 471,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_49@3",
            "content": "This demonstrates the effectiveness of our design.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_49",
            "start": 473,
            "end": 522,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_49@4",
            "content": "The generation-based model with carefully designed prompts is able to utilize the label semantics and the additional weakly supervised signals, thus helping learning under the low-resource regime.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_49",
            "start": 524,
            "end": 719,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_50@0",
            "content": "Another interesting finding is that DEGREE and DEGREE(PIPE) seem to be more beneficial for predicting arguments than for predicting triggers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_50",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_50@1",
            "content": "For example, OneIE, the strongest baseline, requires 20% of training data to achieve competitive performance on trigger prediction to DEGREE and DEGREE(PIPE); however, it requires about 50% of training data to achieve competitive performance in predicting arguments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_50",
            "start": 142,
            "end": 407,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_50@2",
            "content": "The reason is that the ability to capture dependencies becomes more important for argument prediction than trigger prediction since arguments are usually strongly dependent on each other compared to triggers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_50",
            "start": 409,
            "end": 616,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_50@3",
            "content": "Therefore, the improvements of our models for argument prediction are more significant.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_50",
            "start": 618,
            "end": 704,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_51@0",
            "content": "Furthermore, we observe that DEGREE is slightly better than DEGREE(PIPE) under the lowresource setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_51",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_51@1",
            "content": "This provides empirical evidence on the benefit of jointly predicting triggers and arguments in a low-resource setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_51",
            "start": 104,
            "end": 222,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_52@0",
            "content": "Finally, we perform additional experiments on few-shot and zero-shot experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_52",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_52@1",
            "content": "The results can be found in Appendix E.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_52",
            "start": 82,
            "end": 120,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_53@0",
            "content": "High-Resource Event Extraction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_53",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_54@0",
            "content": "Although we focus on data-efficient learning for low-resource event extraction, to better understand the advantages and disadvantages of our model, we additionally study DEGREE in the high-resource setting for controlled comparisons.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_54",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_55@0",
            "content": "Compared baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_55",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_55@1",
            "content": "In addition to the EE models mentioned above: OneIE , BERT_QA (Du and Cardie, 2020), TANL (Paolini et al., 2021) Results for event extraction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_55",
            "start": 20,
            "end": 161,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_55@2",
            "content": "Table 3 shows the results of high-resource event extraction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_55",
            "start": 163,
            "end": 222,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_55@3",
            "content": "In terms of trigger predictions (Tri-C), DEGREE and DE-GREE(PIPE) outperforms all the baselines except for OneIE, the current state-of-the-art model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_55",
            "start": 224,
            "end": 372,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_55@4",
            "content": "For argument predictions (Arg-C), our models have slightly better performance than OneIE in two out of the three datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_55",
            "start": 374,
            "end": 495,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_55@5",
            "content": "When enough training exam- 3 We follow the original paper and use TAPKEY as their event detection model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_55",
            "start": 497,
            "end": 600,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_56@0",
            "content": "Ablation Studies",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_56",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_57@0",
            "content": "In this section, we present comprehensive ablation studies to justify our design.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_57",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_57@1",
            "content": "To better understand the contribution of each component in the designed prompt and their effects on the different tasks, we ablate DEGREE(EAE) and DEGREE(ED) for both low-resource and high-resource situations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_57",
            "start": 82,
            "end": 290,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_58@0",
            "content": "Impacts of components in prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_58",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_58@1",
            "content": "Table 5 lists the performance changes when removing the components in the prompts for event detection on ACE05-E. The performance decreases whenever removing any one of event type definition, event keywords, and ED template.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_58",
            "start": 34,
            "end": 257,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_58@2",
            "content": "The results suggest that three components are all necessary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_58",
            "start": 259,
            "end": 318,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_58@3",
            "content": "any one of event type definition, query trigger, and EAE template leads to performance drops, which validates their necessity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_58",
            "start": 320,
            "end": 445,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_58@4",
            "content": "We observe that query trigger plays the most important role among the three and when less training data is given, the superiority of leveraging any of these weaklysupervised signal becomes more obvious.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_58",
            "start": 447,
            "end": 648,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_59@0",
            "content": "Effects of different template designs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_59",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_59@1",
            "content": "To verify the importance of using natural sentences as outputs, we study three variants of EAE templates:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_59",
            "start": 39,
            "end": 143,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_60@0",
            "content": "\u2022 Natural sentence. Our proposed templates described in Section 2, e.g., \"somebody was born in somewhere.\", where \"somebody\" and \"somewhere\" are placeholders that can be replaced by the corresponding arguments. \u2022 Natural sentence with special tokens. It is similar to the natural sentence one except for using role-specific special tokens instead of \"some-\" words. For example, \"<Person> was born in <Place>.\" We consider this to study the label semantics of roles. \u2022 HTML-like sentence with special tokens. To study the importance of using natural sentence, we also consider HTML-like sentence, e.g., \"<Person> </Person> <Place> </Place>\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_60",
            "start": 0,
            "end": 639,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_61@0",
            "content": "The model aims to put argument predictions between the corresponding HTML tags.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_61",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_62@0",
            "content": "The results of all variants of EAE templates on ACE05-E are shown in Table 7.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_62",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_62@1",
            "content": "We notice that writing templates in a natural language style get better performance, especially when only a few data is available (10% of data).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_62",
            "start": 78,
            "end": 221,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_62@2",
            "content": "This shows our design's capability to leverage pre-trained knowledge in the generation process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_62",
            "start": 223,
            "end": 317,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_62@3",
            "content": "Additionally, there are over 1 F1 score performance drops when replacing natural language placeholders with special tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_62",
            "start": 319,
            "end": 441,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_62@4",
            "content": "This confirms that leveraging label semantics for different roles is beneficial.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_62",
            "start": 443,
            "end": 522,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_62@5",
            "content": "Sensitivity to template design.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_62",
            "start": 524,
            "end": 554,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_62@6",
            "content": "Finally, we study how sensitive our model is to the template.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_62",
            "start": 556,
            "end": 616,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_62@7",
            "content": "In addition to the original design of templates for event argument extraction, we compose other two sets of templates with different constructing rules (e.g., different word choices and different orders of roles).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_62",
            "start": 618,
            "end": 830,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_62@8",
            "content": "Table 8 shows the results of using different sets of templates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_62",
            "start": 832,
            "end": 894,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_62@9",
            "content": "We observe a performance fluctuation when using different templates, which indicates that the quality of templates does affect the performance to a certain degree.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_62",
            "start": 896,
            "end": 1058,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_62@10",
            "content": "Therefore, we need to be cautious when designing templates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_62",
            "start": 1060,
            "end": 1118,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_62@11",
            "content": "However, even though our model could be sensitive to the template design, it still outperforms OneIE and BART-Gen, which are the best classification-based model and the best generation-based baseline, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_62",
            "start": 1120,
            "end": 1333,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_63@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_63",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_64@0",
            "content": "Fully supervised event extraction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_64",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_64@1",
            "content": "Event extraction has been studied for over a decade (Ahn, 2006;Ji and Grishman, 2008) and most traditional event extraction works follow the fully supervised setting (Nguyen et al., 2016;Sha et al., 2018;Nguyen and Nguyen, 2019;Yang et al., 2019b;. Many of them use classification-based models and use pipeline-style frameworks to extract events (Nguyen et al., 2016;Yang et al., 2019b;Wadden et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_64",
            "start": 35,
            "end": 441,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_64@2",
            "content": "To better leverage shared knowledge in event triggers and arguments, some works propose incorporating global features to jointly decide triggers and arguments Li et al., 2013;Yang and Mitchell, 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_64",
            "start": 443,
            "end": 642,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_65@0",
            "content": "Recently, few generation-based event extraction models have been proposed (Paolini et al., 2021;Huang et al., , 2022.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_65",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_65@1",
            "content": "TANL (Paolini et al., 2021) treats event extraction as translation tasks between augmented natural languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_65",
            "start": 118,
            "end": 226,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_65@2",
            "content": "Their predicted target-augmented language embed labels into the input passage via using brackets and vertical bar symbols.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_65",
            "start": 228,
            "end": 349,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_65@3",
            "content": "TempGen ) is a template-based role-filler entity extraction model, which generate outputs that fill role entities into non-natural templated sequences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_65",
            "start": 351,
            "end": 501,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_65@4",
            "content": "The output sequence designs of TANL and Temp-Gen hinder the models from fully leveraging label semantics, unlike DEGREE that generates natural sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_65",
            "start": 503,
            "end": 655,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_65@5",
            "content": "BART-Gen is also a generation-based model focusing on documentlevel event argument extraction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_65",
            "start": 657,
            "end": 750,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_65@6",
            "content": "They solve event extraction with a pipeline, which prevents knowledge sharing across subtasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_65",
            "start": 752,
            "end": 845,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_65@7",
            "content": "All these fully supervised methods can achieve substantial performance with a large amount of annotated data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_65",
            "start": 847,
            "end": 955,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_65@8",
            "content": "However, their designs are not specific for low-resource scenarios, hence, these models can not enjoy all the benefits that DEGREE obtains for low-resource event extraction at the same time, as we mentioned in Section 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_65",
            "start": 957,
            "end": 1176,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_66@0",
            "content": "Low-resource event extraction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_66",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_66@1",
            "content": "It has been a growing interest in event extraction in a scenario with less data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_66",
            "start": 31,
            "end": 110,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_66@2",
            "content": "uses a machine reading comprehension formulation to conduct event extraction in a low-resource regime.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_66",
            "start": 112,
            "end": 213,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_66@3",
            "content": "Text2Event (Lu et al., 2021), a sequence-tostructure generation paradigm, first presents events in a linearized format, and then trains a generative model to generate the linearized event sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_66",
            "start": 215,
            "end": 411,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_66@4",
            "content": "Text2Event's unnatural output format hinders the model from fully leveraging pre-trained knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_66",
            "start": 413,
            "end": 511,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_66@5",
            "content": "Hence, their model falls short on the cases with only extremely low data being available (as shown in Section 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_66",
            "start": 513,
            "end": 625,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_67@0",
            "content": "Another thread of works are using meta-learning to deal with the less label challenge (Deng et al., 2020;Shen et al., 2021;Cong et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_67",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_67@1",
            "content": "However, their methods can only be applied to event detection, which differs from our main focus on studying end-to-end event extraction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_67",
            "start": 143,
            "end": 279,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_68@0",
            "content": "Conclusion & Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_68",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_69@0",
            "content": "In this paper, we present DEGREE, a data-efficient generation-based event extraction model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_69",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_69@1",
            "content": "DEGREE requires less training data because it better utilizes label semantics as well as weakly-supervised information, and captures better dependencies by jointly predicting triggers and arguments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_69",
            "start": 92,
            "end": 289,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_69@2",
            "content": "Our experimental results and ablation studies show the superiority of DEGREE for low-resource event extraction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_69",
            "start": 291,
            "end": 401,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_70@0",
            "content": "DEGREE assumes that some weakly-supervised information (the description of events, similar keywords, and human-written templates) is accessible or not expensive for the users to craft.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_70",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_70@1",
            "content": "This assumption may holds for most situations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_70",
            "start": 185,
            "end": 230,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_70@2",
            "content": "We leave the automation of template construction for future work, which can further ease the needed efforts when deploying DEGREE in a large-scale corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_70",
            "start": 232,
            "end": 385,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_71@0",
            "content": "DEGREE fine-tunes the pre-trained generative language model (Lewis et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_71",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_71@1",
            "content": "Therefore, the generated output is potentially affected by the corpus for pre-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_71",
            "start": 82,
            "end": 168,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_71@2",
            "content": "Although with a low possibility, it is possible for our model to accidentally generate some malicious, counterfactual, and biased sentences, which may cause ethics concerns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_71",
            "start": 170,
            "end": 342,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_72@0",
            "content": "We suggest carefully examining those potential issues before deploying the model in any real-world applications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_72",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_73@0",
            "content": "Our strategy to create an EAE template is first identifying all valid argument roles for the event type, 4 such as Attacker, Target, Instrument, and Place roles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_73",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_73@1",
            "content": "Then, for each argument role, according to the semantics of the role type, we select natural and fluent words to form its placeholder (e.g., some way for Instrument).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_73",
            "start": 162,
            "end": 327,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_73@2",
            "content": "This design aims to provide a simple way to help the model learn both the roles' label semantics and the event structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_73",
            "start": 329,
            "end": 449,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_73@3",
            "content": "Finally, we create a natural language sentence that connects all these placeholders.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_73",
            "start": 451,
            "end": 534,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_73@4",
            "content": "Notice that we try to keep the template as simple and short as possible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_73",
            "start": 536,
            "end": 607,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_73@5",
            "content": "Table 9 lists all designed EAE templates for ACE05-E and ACE05-E + .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_73",
            "start": 609,
            "end": 676,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_73@6",
            "content": "The EAE templates of ERE-EN can be found in Table 10.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_73",
            "start": 678,
            "end": 730,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_74@0",
            "content": "Given a passage, its annotated event types are consider as positive event types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_74",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_74@1",
            "content": "During training, we additionally sample m event types that are not related to the passage as the negative examples, where m is a hyper-parameter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_74",
            "start": 81,
            "end": 225,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_74@2",
            "content": "In our experiments, m is usually set to 13 or 15.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_74",
            "start": 227,
            "end": 275,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_75@0",
            "content": "For all of DEGREE, DEGREE(ED), and DE-GREE(EAE), we fine-tune the pre-trained BARTlarge (Lewis et al., 2020) with Huggingface package (Wolf et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_75",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_75@1",
            "content": "The number of parameters is around 406 millions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_75",
            "start": 155,
            "end": 202,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_75@2",
            "content": "We train DEGREE with our machine that equips 128 AMD EPYC 7452 32-Core Processor, 4 NVIDIA A100 GPUs, and 792G RAM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_75",
            "start": 204,
            "end": 318,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_75@3",
            "content": "We consider AdamW optimizer (Loshchilov and Hutter, 2019) with learning rate set to 10 \u22125 and the weight decay set to 10 \u22125 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_75",
            "start": 320,
            "end": 444,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_75@4",
            "content": "We set the batch size to 6 for DEGREE(EAE) and 32 for DEGREE(ED) and DEGREE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_75",
            "start": 446,
            "end": 521,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_75@5",
            "content": "The number of training epochs is 45.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_75",
            "start": 523,
            "end": 558,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_75@6",
            "content": "It takes around 2 hours, 18 hours, 22 hours to train DEGREE(EAE), DEGREE(ED), and DEGREE, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_75",
            "start": 560,
            "end": 662,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_76@0",
            "content": "We do hyper-parameter search on m, the number of negative examples, from {3, 5, 7, 10, 13, 15, 18, 21}, and our preliminary trials shows that m less than 10 are usually less useful.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_76",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_76@1",
            "content": "For the learning rate and the weight decay, we tune it based on our preliminary experiment for event argument extraction from {10 \u22125 , 10 \u22124 }, while they are both fixed to 10 \u22125 for all the experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_76",
            "start": 182,
            "end": 384,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_77@0",
            "content": "We consider ACE 2005 5 (Doddington et al., 2004) and ERE 6 (Song et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_77",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_77@1",
            "content": "Both consider LDC User Agreement for Non-Members 7 as the licenses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_77",
            "start": 80,
            "end": 146,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_77@2",
            "content": "Both datasets are created for entity, relation, and event extraction while our focus is only event extraction in this paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_77",
            "start": 148,
            "end": 271,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_77@3",
            "content": "In the original ACE 2005 dataset, it contains data for English, Chinese, and Arabic and we only take the English data for our experiment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_77",
            "start": 273,
            "end": 409,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_77@4",
            "content": "In the original ERE dataset, it contains data for English, and Chinese and we only take the English data for our experiment as well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_77",
            "start": 411,
            "end": 542,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_78@0",
            "content": "Because both datasets contain event like Justice:Execute and Life:Die, it is possible that some offensive words (e.g., killed) would appear in the passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_78",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_78@1",
            "content": "Also, some real names may appear in the passage as well (e.g., Palestinian president, Mahmoud Abbas).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_78",
            "start": 156,
            "end": 256,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_78@2",
            "content": "How to accurately identify these kinds of information is part of the goal of the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_78",
            "start": 258,
            "end": 343,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_78@3",
            "content": "Therefore, we do not take any changes on the datasets for protecting or anonymizing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_78",
            "start": 345,
            "end": 428,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_79@0",
            "content": "We split the training data based on documents, which is a more realistic setup compared to splitting data by instance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_79",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_79@1",
            "content": "Table 11 lists the statistics of ACE05-E, ACE05-E + , and ERE-EN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_79",
            "start": 119,
            "end": 183,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_79@2",
            "content": "Specifically, we try to make each proportion of data contain as many event types as possible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_79",
            "start": 185,
            "end": 277,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_80@0",
            "content": "This section describes the implementation details for all baselines we use.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_80",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_80@1",
            "content": "We run the experiments with three different random seeds and report the best value.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_80",
            "start": 76,
            "end": 158,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_81@0",
            "content": "\u2022 DyGIE++: we use their released pre-trained model 8 for evaluation. \u2022 OneIE: we use their provided code 9 to train the model with default parameters. \u2022 BERT_QA: we use their provided code 10 to train the model with default parameters. \u2022 TANL: we use their provided code 11 to train the model. We conduct the experiments with two variations: (1) using their default parameters, and (2) using their default parameters but with more training epochs. We observe that the second variant works better. As a result, we report the number obtained from the second setting. \u2022 Text2Event: we use their official code 12 to train the model with the provided parameter setting. \u2022 dbRNN: we directly report the experimental results from their paper. \u2022 Joint3EE: we directly report the experimental results from their paper. \u2022 MQAEE: we directly report the experimental results from their paper. \u2022 BART-Gen: we report the experimental results from their released appendix. 13",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_81",
            "start": 0,
            "end": 959,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_82@0",
            "content": "Life:Injure somebody or some organization led to some victim injured by some way in somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_82",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_82@1",
            "content": "Life:Die somebody or some organization led to some victim died by some way in somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_82",
            "start": 95,
            "end": 182,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_83@0",
            "content": "Movement:Transport something was sent to somewhere from some place by some vehicle.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_83",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_83@1",
            "content": "somebody or some organization was responsible for the transport.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_83",
            "start": 84,
            "end": 147,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_83@2",
            "content": "Transaction:Transfer-Ownership someone got something from some seller in somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_83",
            "start": 149,
            "end": 231,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_83@3",
            "content": "Transaction:Transfer-Money someone paid some other in somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_83",
            "start": 233,
            "end": 296,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_83@4",
            "content": "Business:Start-Org somebody or some organization launched some organzation in somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_83",
            "start": 298,
            "end": 385,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_83@5",
            "content": "Business:Merge-Org some organzation was merged.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_83",
            "start": 387,
            "end": 433,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_83@6",
            "content": "Business:Declare-Bankruptcy some organzation declared bankruptcy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_83",
            "start": 435,
            "end": 499,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_83@7",
            "content": "Business:End-Org some organzation dissolved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_83",
            "start": 501,
            "end": 544,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_84@0",
            "content": "Conflict:Attack some attacker attacked some facility, someone, or some organization by some way in somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_84",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_84@1",
            "content": "Conflict:Demonstrate some people or some organization protest at somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_84",
            "start": 110,
            "end": 184,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_84@2",
            "content": "Contact:Meet some people or some organization met at somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_84",
            "start": 186,
            "end": 248,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_84@3",
            "content": "Contact:Phone-Write some people or some organization called or texted messages at somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_84",
            "start": 250,
            "end": 341,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_85@0",
            "content": "Personnel:Start-Position somebody got new job and was hired by some people or some organization in somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_85",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_85@1",
            "content": "Personnel:End-Position somebody stopped working for some people or some organization at somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_85",
            "start": 110,
            "end": 207,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_85@2",
            "content": "Personnel:Nominate somebody was nominated by somebody or some organization to do a job.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_85",
            "start": 209,
            "end": 295,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_85@3",
            "content": "Life:Be-Born somebody was born in somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_85",
            "start": 297,
            "end": 340,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_85@4",
            "content": "Life:Marry somebody got married in somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_85",
            "start": 342,
            "end": 386,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_85@5",
            "content": "Life:Divorce somebody divorced in somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_85",
            "start": 388,
            "end": 431,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_86@0",
            "content": "Life:Injure somebody or some organization led to some victim injured by some way in somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_86",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_86@1",
            "content": "Life:Die somebody or some organization led to some victim died by some way in somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_86",
            "start": 95,
            "end": 182,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_87@0",
            "content": "Movement:Transport-Person somebody was moved to somewhere from some place by some way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_87",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_87@1",
            "content": "somebody or some organization was responsible for the movement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_87",
            "start": 87,
            "end": 149,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_88@0",
            "content": "Movement:Transport-Artifact something was sent to somewhere from some place.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_88",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_88@1",
            "content": "somebody or some organization was responsible for the transport.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_88",
            "start": 77,
            "end": 140,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_88@2",
            "content": "Business:Start-Org somebody or some organization launched some organzation in somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_88",
            "start": 142,
            "end": 229,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_88@3",
            "content": "Business:Merge-Org some organzation was merged.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_88",
            "start": 231,
            "end": 277,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_88@4",
            "content": "Business:Declare-Bankruptcy some organzation declared bankruptcy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_88",
            "start": 279,
            "end": 343,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_88@5",
            "content": "Business:End-Org some organzation dissolved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_88",
            "start": 345,
            "end": 388,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_89@0",
            "content": "Conflict:Attack some attacker attacked some facility, someone, or some organization by some way in somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_89",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_89@1",
            "content": "Conflict:Demonstrate some people or some organization protest at somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_89",
            "start": 110,
            "end": 184,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_89@2",
            "content": "Contact:Meet some people or some organization met at somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_89",
            "start": 186,
            "end": 248,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_89@3",
            "content": "Contact:Correspondence some people or some organization contacted each other at somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_89",
            "start": 250,
            "end": 339,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_90@0",
            "content": "Contact:Broadcast some people or some organization made announcement to some publicity at somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_90",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_90@1",
            "content": "Contact:Contact some people or some organization talked to each other at somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_90",
            "start": 101,
            "end": 183,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_90@2",
            "content": "Manufacture:Artifact something was built by somebody or some organization in somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_90",
            "start": 185,
            "end": 271,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_91@0",
            "content": "Personnel:Start-Position somebody got new job and was hired by some people or some organization in somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_91",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_91@1",
            "content": "Personnel:End-Position somebody stopped working for some people or some organization at somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_91",
            "start": 110,
            "end": 207,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_91@2",
            "content": "Personnel:Nominate somebody was nominated by somebody or some organization to do a job.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_91",
            "start": 209,
            "end": 295,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_92@0",
            "content": "Personnel:Elect somebody was elected a position, and the election was voted by somebody or some organization in somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_92",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_93@0",
            "content": "Transaction:Transfer-Ownership The ownership of something from someone was transferred to some other at somewhere.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_93",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_94@0",
            "content": "In order to further test our models' generaliability, we additionally conduct zero-shot and fewshot experiments on the ACE05-E dataset with DEGREE(ED) and DEGREE(EAE).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_94",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_95@0",
            "content": "Settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_95",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_95@1",
            "content": "We first select the top n common event types as \"seen\" types and use the rest as \"unseen/rare\" types, where the top common types are listed in Table 12.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_95",
            "start": 10,
            "end": 161,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_95@2",
            "content": "To simulate a zero-shot scenario, we remove all events with \"unseen/rare\" types from the training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_95",
            "start": 163,
            "end": 265,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_95@3",
            "content": "To simulate a few-shot scenario, we keep only k event examples for each \"unseen/rare\" type (denoted as k-shot).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_95",
            "start": 267,
            "end": 377,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_95@4",
            "content": "During the evaluation, we calculate micro F1-scores only for these \"unseen/rare\" types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_95",
            "start": 379,
            "end": 465,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_95@5",
            "content": "Compared baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_95",
            "start": 467,
            "end": 485,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_95@6",
            "content": "We consider the following baselines: (1) BERT_QA (Du and Cardie, 2020) (2) OneIE (3) Matching baseline, a proposed baseline that makes trigger predictions by performing string matching between the input passage and the event keywords.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_95",
            "start": 487,
            "end": 720,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_95@7",
            "content": "(4) Lemmatization baseline, another proposed baseline that performs string matching on lemmatized input passage and the event keywords. (Note: (3) and ( 4) are baselines only for event detection tasks.)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_95",
            "start": 722,
            "end": 923,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_96@0",
            "content": "Experimental results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_96",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_96@1",
            "content": "Figure 4, Table 13, and Table 14 show the results of n = 5 and n = 10.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_96",
            "start": 22,
            "end": 91,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_97@0",
            "content": "From the two subfigures in the left column, we see that DEGREE(ED) achieves promising results in the zero-shot setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_97",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_97@1",
            "content": "In fact, it performs better than BERT_QA trained in the 10-shot setting and OneIE trained in the 5-shot setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_97",
            "start": 120,
            "end": 231,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_97@2",
            "content": "This demonstrates the great potential of DEGREE(ED) to discover new event types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_97",
            "start": 233,
            "end": 312,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_97@3",
            "content": "Interestingly, we observe that our two proposed baselines perform surprisingly well, suggesting that the trigger annotations in ACE05-E are actually not diverse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_97",
            "start": 314,
            "end": 474,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_97@4",
            "content": "Despite their impressive performance, DEGREE(ED) still outperforms the matching baseline by over 4.7% absolute trigger classification F1 in both n = 5 and n = 10 cases in zero-shot scenario.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_97",
            "start": 476,
            "end": 665,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_97@5",
            "content": "Additionally, with only one training instance for each unseen type, DEGREE(ED) can outperform both proposed baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_97",
            "start": 667,
            "end": 784,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_98@0",
            "content": "Next, we compare the results for the event argument extraction task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_98",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_98@1",
            "content": "From the two middle subfigures, we observe that when given gold triggers, our model performs much better than all baselines with a large margin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_98",
            "start": 69,
            "end": 212,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_98@2",
            "content": "Lastly, we train models for both trigger and argument extraction and report the final argument classification scores in the two right subfigures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_98",
            "start": 214,
            "end": 358,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_98@3",
            "content": "We justify that our model has strong generalizability to unseen event types and it can outperform BERT_QA and OneIE even when they are both trained in 5-shot settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_98",
            "start": 360,
            "end": 526,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_99@0",
            "content": "David Ahn, The stages of event extraction, 2006, Proceedings of the Workshop on Annotating and Reasoning about Time and Events, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_99",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_100@0",
            "content": "Jonathan Berant, Vivek Srikumar, Pei-Chun Chen, Abby Vander Linden, Brittany Harding, Brad Huang, Peter Clark, Christopher Manning, Modeling biological processes for reading comprehension, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_100",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_101@0",
            "content": "Xin Cong, Shiyao Cui, Bowen Yu, Tingwen Liu, Yubin Wang, Bin Wang, Few-shot event detection with prototypical amortized conditional random field, 2021, Findings of the Association for Computational Linguistics: ACL/IJCNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_101",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_102@0",
            "content": "Shumin Deng, Ningyu Zhang, Jiaojian Kang, Yichi Zhang, Wei Zhang, Huajun Chen, Metalearning with dynamic-memory-based prototypical network for few-shot event detection, 2020, The Thirteenth ACM International Conference on Web Search and Data Mining (WSDM), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_102",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_103@0",
            "content": "George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, Ralph Weischedel, The automatic content extraction (ACE) program -tasks, data, and evaluation, 2004, Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_103",
            "start": 0,
            "end": 284,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_104@0",
            "content": "Xinya Du, Claire Cardie, Event extraction by answering (almost) natural questions, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_104",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_105@0",
            "content": "UNKNOWN, None, 2021, Language model priming for cross-lingual event extraction, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_105",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_106@0",
            "content": "Rujun Han, I-Hung Hsu, Jiao Sun, Julia Baylon, Qiang Ning, Dan Roth, Nanyun Peng, ESTER: A machine reading comprehension dataset for reasoning about event semantic relations, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_106",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_107@0",
            "content": "Rujun Han, I-Hung Hsu, Mu Yang, Aram Galstyan, Ralph Weischedel, Nanyun Peng, Deep structured neural network for event temporal relation extraction, 2019-11-03, Proceedings of the 23rd Conference on Computational Natural Language Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_107",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_108@0",
            "content": "Rujun Han, Qiang Ning, Nanyun Peng, Joint event and temporal relation extraction with shared representations and structured prediction, 2019, 2019 Conference on Empirical Methods in Natural Language Processing, EMNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_108",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_109@0",
            "content": "Frederik Hogenboom, Flavius Frasincar, Uzay Kaymak, Franciska De, Jong , Emiel Caron, A survey of event extraction methods from text for decision support systems, 2016, Decis. Support Syst, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_109",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_110@0",
            "content": "Kuan-Hao Huang, I-Hung Hsu, Premkumar Natarajan, Kai-Wei Chang, Nanyun Peng, Multilingual generative language models for zero-shot crosslingual event argument extraction, 2022, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_110",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_111@0",
            "content": "Hsiang Kung, Nanyun Huang,  Peng, Document-level event extraction with efficient end-to-end learning of cross-event dependencies, 2021, The 3rd Workshop on Narrative Understanding (NAACL 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_111",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_112@0",
            "content": "Kung-Hsiang Huang, Sam Tang, Nanyun Peng, Document-level entity-based extraction as template generation, 2021-07-11, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_112",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_113@0",
            "content": "Kung-Hsiang Huang, Mu Yang, Nanyun Peng, Biomedical event extraction with hierarchical knowledge graphs, 2020, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)-Findings, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_113",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_114@0",
            "content": "Heng Ji, Ralph Grishman, Refining event extraction through cross-document inference, 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_114",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_115@0",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_115",
            "start": 0,
            "end": 343,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_116@0",
            "content": "Fayuan Li, Weihua Peng, Yuguang Chen, Quan Wang, Lu Pan, Yajuan Lyu, Yong Zhu, Event extraction as multi-turn question answering, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_116",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_117@0",
            "content": "Qi Li, Ji Heng, Liang Huang, Joint event extraction via structured prediction with global features, 2013, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_117",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_118@0",
            "content": "Sha Li, Ji Heng, Jiawei Han, Document-level event argument extraction by conditional generation, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_118",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_119@0",
            "content": "Ying Lin, Heng Ji, Fei Huang, Lingfei Wu, A joint neural model for information extraction with global features, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_119",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_120@0",
            "content": "Jian Liu, Yubo Chen, Kang Liu, Wei Bi, Xiaojiang Liu, Event extraction as machine reading comprehension, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_120",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_121@0",
            "content": "Ilya Loshchilov, Frank Hutter, Decoupled weight decay regularization, 2019-05-06, 7th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_121",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_122@0",
            "content": "Yaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, Jialong Tang, Annan Li, Le Sun, Meng Liao, Shaoyi Chen, Text2event: Controllable sequence-tostructure generation for end-to-end event extraction, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL/IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_122",
            "start": 0,
            "end": 373,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_123@0",
            "content": "Kyunghyun Thien Huu Nguyen, Ralph Cho,  Grishman, Joint event extraction via recurrent neural networks, 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_123",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_124@0",
            "content": "Huu Thien, Ralph Nguyen,  Grishman, Event detection and domain adaptation with convolutional neural networks, 2015, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_124",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_125@0",
            "content": "Minh Trung, Thien Nguyen,  Huu Nguyen, One for all: Neural joint modeling of entities and events, 2019, The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference (AAAI), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_125",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_126@0",
            "content": "Giovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro Achille, Rishita Anubhai, C\u00edcero Nogueira, Bing Santos, Stefano Xiang,  Soatto, Structured prediction as translation between augmented natural languages, 2021, 9th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_126",
            "start": 0,
            "end": 286,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_127@0",
            "content": "Lei Sha, Feng Qian, Baobao Chang, Zhifang Sui, Jointly extracting event triggers and arguments by dependency-bridge RNN and tensor-based argument interaction, 2018, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_127",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_128@0",
            "content": "Shirong Shen, Tongtong Wu, Guilin Qi, Yuan-Fang Li, Gholamreza Haffari, Sheng Bi, Adaptive knowledge-enhanced bayesian meta-learning for fewshot event detection, 2021, Findings of the Association for Computational Linguistics: ACL/IJCNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_128",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_129@0",
            "content": "Zhiyi Song, Ann Bies, Stephanie Strassel, Tom Riese, Justin Mott, Joe Ellis, Jonathan Wright, Seth Kulick, Neville Ryant, Xiaoyi Ma, From light to rich ERE: annotation of entities, relations, and events, 2015, Proceedings of the The 3rd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, (EVENTS@HLP-NAACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_129",
            "start": 0,
            "end": 333,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_130@0",
            "content": "Jiao Sun, Nanyun Peng, Men are elected, women are married: Events gender bias on wikipedia, 2021-08-01, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_130",
            "start": 0,
            "end": 297,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_131@0",
            "content": "David Wadden, Ulme Wennberg, Yi Luan, Hannaneh Hajishirzi, Entity, relation, and event extraction with contextualized span representations, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_131",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_132@0",
            "content": "Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong Han, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin, Jie Zhou, MAVEN: A massive general domain event detection dataset, 2020-11-16, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_132",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_133@0",
            "content": "Xiaozhi Wang, Ziqi Wang, Xu Han, Zhiyuan Liu, Juanzi Li, Peng Li, Maosong Sun, Jie Zhou, Xiang Ren, HMEAE: hierarchical modular event argument extraction, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_133",
            "start": 0,
            "end": 335,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_134@0",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest,  Rush, Huggingface's transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_134",
            "start": 0,
            "end": 509,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_135@0",
            "content": "Bishan Yang, Tom Mitchell, Joint extraction of events and entities within a document context, 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_135",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_136@0",
            "content": "Sen Yang, Dawei Feng, Linbo Qiao, Zhigang Kan, Dongsheng Li, Exploring pre-trained language models for event extraction and generation, 2019-07-28, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_136",
            "start": 0,
            "end": 254,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_137@0",
            "content": "Sen Yang, Dawei Feng, Linbo Qiao, Zhigang Kan, Dongsheng Li, Exploring pre-trained language models for event extraction and generation, 2019, Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_137",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_138@0",
            "content": "Hongming Zhang, Xin Liu, Haojie Pan, Yangqiu Song, Cane Wing, -Ki Leung, ASER: A largescale eventuality knowledge graph, 2020, The Web Conference, WWW.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_138",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "125-ARR_v2_139@0",
            "content": "UNKNOWN, None, , Table 14: Full results of zero/few-shot event argument extraction on, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "125-ARR_v2_139",
            "start": 0,
            "end": 87,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "125-ARR_v2_0",
            "tgt_ix": "125-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_0",
            "tgt_ix": "125-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_1",
            "tgt_ix": "125-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_1",
            "tgt_ix": "125-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_0",
            "tgt_ix": "125-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_2",
            "tgt_ix": "125-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_4",
            "tgt_ix": "125-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_5",
            "tgt_ix": "125-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_3",
            "tgt_ix": "125-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_3",
            "tgt_ix": "125-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_3",
            "tgt_ix": "125-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_3",
            "tgt_ix": "125-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_3",
            "tgt_ix": "125-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_6",
            "tgt_ix": "125-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_3",
            "tgt_ix": "125-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_7",
            "tgt_ix": "125-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_3",
            "tgt_ix": "125-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_8",
            "tgt_ix": "125-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_3",
            "tgt_ix": "125-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_9",
            "tgt_ix": "125-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_11",
            "tgt_ix": "125-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_12",
            "tgt_ix": "125-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_13",
            "tgt_ix": "125-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_14",
            "tgt_ix": "125-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_3",
            "tgt_ix": "125-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_3",
            "tgt_ix": "125-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_3",
            "tgt_ix": "125-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_3",
            "tgt_ix": "125-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_3",
            "tgt_ix": "125-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_3",
            "tgt_ix": "125-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_10",
            "tgt_ix": "125-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_0",
            "tgt_ix": "125-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_16",
            "tgt_ix": "125-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_17",
            "tgt_ix": "125-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_17",
            "tgt_ix": "125-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_17",
            "tgt_ix": "125-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_18",
            "tgt_ix": "125-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_20",
            "tgt_ix": "125-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_21",
            "tgt_ix": "125-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_19",
            "tgt_ix": "125-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_19",
            "tgt_ix": "125-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_19",
            "tgt_ix": "125-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_19",
            "tgt_ix": "125-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_19",
            "tgt_ix": "125-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_17",
            "tgt_ix": "125-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_23",
            "tgt_ix": "125-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_25",
            "tgt_ix": "125-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_24",
            "tgt_ix": "125-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_24",
            "tgt_ix": "125-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_24",
            "tgt_ix": "125-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_17",
            "tgt_ix": "125-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_26",
            "tgt_ix": "125-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_27",
            "tgt_ix": "125-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_27",
            "tgt_ix": "125-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_17",
            "tgt_ix": "125-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_28",
            "tgt_ix": "125-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_30",
            "tgt_ix": "125-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_31",
            "tgt_ix": "125-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_29",
            "tgt_ix": "125-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_29",
            "tgt_ix": "125-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_29",
            "tgt_ix": "125-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_29",
            "tgt_ix": "125-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_17",
            "tgt_ix": "125-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_32",
            "tgt_ix": "125-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_33",
            "tgt_ix": "125-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_33",
            "tgt_ix": "125-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_33",
            "tgt_ix": "125-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_33",
            "tgt_ix": "125-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_34",
            "tgt_ix": "125-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_38",
            "tgt_ix": "125-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_33",
            "tgt_ix": "125-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_33",
            "tgt_ix": "125-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_33",
            "tgt_ix": "125-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_36",
            "tgt_ix": "125-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_0",
            "tgt_ix": "125-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_39",
            "tgt_ix": "125-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_40",
            "tgt_ix": "125-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_40",
            "tgt_ix": "125-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_40",
            "tgt_ix": "125-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_41",
            "tgt_ix": "125-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_43",
            "tgt_ix": "125-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_44",
            "tgt_ix": "125-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_45",
            "tgt_ix": "125-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_42",
            "tgt_ix": "125-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_42",
            "tgt_ix": "125-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_42",
            "tgt_ix": "125-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_42",
            "tgt_ix": "125-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_42",
            "tgt_ix": "125-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_40",
            "tgt_ix": "125-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_46",
            "tgt_ix": "125-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_48",
            "tgt_ix": "125-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_49",
            "tgt_ix": "125-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_50",
            "tgt_ix": "125-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_51",
            "tgt_ix": "125-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_47",
            "tgt_ix": "125-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_47",
            "tgt_ix": "125-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_47",
            "tgt_ix": "125-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_47",
            "tgt_ix": "125-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_47",
            "tgt_ix": "125-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_47",
            "tgt_ix": "125-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_40",
            "tgt_ix": "125-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_52",
            "tgt_ix": "125-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_54",
            "tgt_ix": "125-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_53",
            "tgt_ix": "125-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_53",
            "tgt_ix": "125-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_53",
            "tgt_ix": "125-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_0",
            "tgt_ix": "125-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_55",
            "tgt_ix": "125-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_57",
            "tgt_ix": "125-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_58",
            "tgt_ix": "125-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_59",
            "tgt_ix": "125-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_61",
            "tgt_ix": "125-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_56",
            "tgt_ix": "125-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_56",
            "tgt_ix": "125-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_56",
            "tgt_ix": "125-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_56",
            "tgt_ix": "125-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_56",
            "tgt_ix": "125-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_56",
            "tgt_ix": "125-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_56",
            "tgt_ix": "125-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_0",
            "tgt_ix": "125-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_62",
            "tgt_ix": "125-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_64",
            "tgt_ix": "125-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_65",
            "tgt_ix": "125-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_66",
            "tgt_ix": "125-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_63",
            "tgt_ix": "125-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_63",
            "tgt_ix": "125-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_63",
            "tgt_ix": "125-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_63",
            "tgt_ix": "125-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_63",
            "tgt_ix": "125-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_0",
            "tgt_ix": "125-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_67",
            "tgt_ix": "125-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_69",
            "tgt_ix": "125-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_71",
            "tgt_ix": "125-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_70",
            "tgt_ix": "125-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_72",
            "tgt_ix": "125-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_74",
            "tgt_ix": "125-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_75",
            "tgt_ix": "125-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_73",
            "tgt_ix": "125-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_77",
            "tgt_ix": "125-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_78",
            "tgt_ix": "125-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_76",
            "tgt_ix": "125-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_80",
            "tgt_ix": "125-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_82",
            "tgt_ix": "125-ARR_v2_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_83",
            "tgt_ix": "125-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_84",
            "tgt_ix": "125-ARR_v2_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_85",
            "tgt_ix": "125-ARR_v2_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_86",
            "tgt_ix": "125-ARR_v2_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_87",
            "tgt_ix": "125-ARR_v2_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_88",
            "tgt_ix": "125-ARR_v2_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_89",
            "tgt_ix": "125-ARR_v2_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_90",
            "tgt_ix": "125-ARR_v2_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_91",
            "tgt_ix": "125-ARR_v2_92",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_92",
            "tgt_ix": "125-ARR_v2_93",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_92",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_93",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_79",
            "tgt_ix": "125-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_94",
            "tgt_ix": "125-ARR_v2_95",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_95",
            "tgt_ix": "125-ARR_v2_96",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_96",
            "tgt_ix": "125-ARR_v2_97",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_97",
            "tgt_ix": "125-ARR_v2_98",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_94",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_95",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_96",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_97",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_98",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_93",
            "tgt_ix": "125-ARR_v2_94",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "125-ARR_v2_0",
            "tgt_ix": "125-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_1",
            "tgt_ix": "125-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_2",
            "tgt_ix": "125-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_2",
            "tgt_ix": "125-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_2",
            "tgt_ix": "125-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_2",
            "tgt_ix": "125-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_2",
            "tgt_ix": "125-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_2",
            "tgt_ix": "125-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_2",
            "tgt_ix": "125-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_2",
            "tgt_ix": "125-ARR_v2_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_2",
            "tgt_ix": "125-ARR_v2_2@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_2",
            "tgt_ix": "125-ARR_v2_2@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_3",
            "tgt_ix": "125-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_4",
            "tgt_ix": "125-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_4",
            "tgt_ix": "125-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_4",
            "tgt_ix": "125-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_4",
            "tgt_ix": "125-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_4",
            "tgt_ix": "125-ARR_v2_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_5",
            "tgt_ix": "125-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_5",
            "tgt_ix": "125-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_5",
            "tgt_ix": "125-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_5",
            "tgt_ix": "125-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_5",
            "tgt_ix": "125-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_6",
            "tgt_ix": "125-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_6",
            "tgt_ix": "125-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_7",
            "tgt_ix": "125-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_8",
            "tgt_ix": "125-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_9",
            "tgt_ix": "125-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_10",
            "tgt_ix": "125-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_10",
            "tgt_ix": "125-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_10",
            "tgt_ix": "125-ARR_v2_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_10",
            "tgt_ix": "125-ARR_v2_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_10",
            "tgt_ix": "125-ARR_v2_10@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_11",
            "tgt_ix": "125-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_12",
            "tgt_ix": "125-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_12",
            "tgt_ix": "125-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_12",
            "tgt_ix": "125-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_12",
            "tgt_ix": "125-ARR_v2_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_12",
            "tgt_ix": "125-ARR_v2_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_12",
            "tgt_ix": "125-ARR_v2_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_12",
            "tgt_ix": "125-ARR_v2_12@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_12",
            "tgt_ix": "125-ARR_v2_12@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_12",
            "tgt_ix": "125-ARR_v2_12@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_12",
            "tgt_ix": "125-ARR_v2_12@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_12",
            "tgt_ix": "125-ARR_v2_12@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_12",
            "tgt_ix": "125-ARR_v2_12@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_12",
            "tgt_ix": "125-ARR_v2_12@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_13",
            "tgt_ix": "125-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_13",
            "tgt_ix": "125-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_13",
            "tgt_ix": "125-ARR_v2_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_13",
            "tgt_ix": "125-ARR_v2_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_14",
            "tgt_ix": "125-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_15",
            "tgt_ix": "125-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_16",
            "tgt_ix": "125-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_17",
            "tgt_ix": "125-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_18",
            "tgt_ix": "125-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_18",
            "tgt_ix": "125-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_19",
            "tgt_ix": "125-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_20",
            "tgt_ix": "125-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_20",
            "tgt_ix": "125-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_20",
            "tgt_ix": "125-ARR_v2_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_20",
            "tgt_ix": "125-ARR_v2_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_20",
            "tgt_ix": "125-ARR_v2_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_21",
            "tgt_ix": "125-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_21",
            "tgt_ix": "125-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_22",
            "tgt_ix": "125-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_23",
            "tgt_ix": "125-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_23",
            "tgt_ix": "125-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_24",
            "tgt_ix": "125-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_25",
            "tgt_ix": "125-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_25",
            "tgt_ix": "125-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_26",
            "tgt_ix": "125-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_26",
            "tgt_ix": "125-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_26",
            "tgt_ix": "125-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_27",
            "tgt_ix": "125-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_28",
            "tgt_ix": "125-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_28",
            "tgt_ix": "125-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_28",
            "tgt_ix": "125-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_28",
            "tgt_ix": "125-ARR_v2_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_29",
            "tgt_ix": "125-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_30",
            "tgt_ix": "125-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_30",
            "tgt_ix": "125-ARR_v2_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_30",
            "tgt_ix": "125-ARR_v2_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_30",
            "tgt_ix": "125-ARR_v2_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_30",
            "tgt_ix": "125-ARR_v2_30@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_30",
            "tgt_ix": "125-ARR_v2_30@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_30",
            "tgt_ix": "125-ARR_v2_30@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_30",
            "tgt_ix": "125-ARR_v2_30@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_30",
            "tgt_ix": "125-ARR_v2_30@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_30",
            "tgt_ix": "125-ARR_v2_30@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_30",
            "tgt_ix": "125-ARR_v2_30@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_31",
            "tgt_ix": "125-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_31",
            "tgt_ix": "125-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_31",
            "tgt_ix": "125-ARR_v2_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_31",
            "tgt_ix": "125-ARR_v2_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_31",
            "tgt_ix": "125-ARR_v2_31@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_31",
            "tgt_ix": "125-ARR_v2_31@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_32",
            "tgt_ix": "125-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_32",
            "tgt_ix": "125-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_32",
            "tgt_ix": "125-ARR_v2_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_32",
            "tgt_ix": "125-ARR_v2_32@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_32",
            "tgt_ix": "125-ARR_v2_32@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_33",
            "tgt_ix": "125-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_34",
            "tgt_ix": "125-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_34",
            "tgt_ix": "125-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_34",
            "tgt_ix": "125-ARR_v2_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_34",
            "tgt_ix": "125-ARR_v2_34@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_35",
            "tgt_ix": "125-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_36",
            "tgt_ix": "125-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_37",
            "tgt_ix": "125-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_38",
            "tgt_ix": "125-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_39",
            "tgt_ix": "125-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_40",
            "tgt_ix": "125-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_41",
            "tgt_ix": "125-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_42",
            "tgt_ix": "125-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_43",
            "tgt_ix": "125-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_43",
            "tgt_ix": "125-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_43",
            "tgt_ix": "125-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_43",
            "tgt_ix": "125-ARR_v2_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_44",
            "tgt_ix": "125-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_44",
            "tgt_ix": "125-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_44",
            "tgt_ix": "125-ARR_v2_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_45",
            "tgt_ix": "125-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_45",
            "tgt_ix": "125-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_45",
            "tgt_ix": "125-ARR_v2_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_46",
            "tgt_ix": "125-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_46",
            "tgt_ix": "125-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_46",
            "tgt_ix": "125-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_46",
            "tgt_ix": "125-ARR_v2_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_46",
            "tgt_ix": "125-ARR_v2_46@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_46",
            "tgt_ix": "125-ARR_v2_46@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_46",
            "tgt_ix": "125-ARR_v2_46@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_46",
            "tgt_ix": "125-ARR_v2_46@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_46",
            "tgt_ix": "125-ARR_v2_46@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_47",
            "tgt_ix": "125-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_48",
            "tgt_ix": "125-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_48",
            "tgt_ix": "125-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_48",
            "tgt_ix": "125-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_49",
            "tgt_ix": "125-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_49",
            "tgt_ix": "125-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_49",
            "tgt_ix": "125-ARR_v2_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_49",
            "tgt_ix": "125-ARR_v2_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_49",
            "tgt_ix": "125-ARR_v2_49@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_50",
            "tgt_ix": "125-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_50",
            "tgt_ix": "125-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_50",
            "tgt_ix": "125-ARR_v2_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_50",
            "tgt_ix": "125-ARR_v2_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_51",
            "tgt_ix": "125-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_51",
            "tgt_ix": "125-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_52",
            "tgt_ix": "125-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_52",
            "tgt_ix": "125-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_53",
            "tgt_ix": "125-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_54",
            "tgt_ix": "125-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_55",
            "tgt_ix": "125-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_55",
            "tgt_ix": "125-ARR_v2_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_55",
            "tgt_ix": "125-ARR_v2_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_55",
            "tgt_ix": "125-ARR_v2_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_55",
            "tgt_ix": "125-ARR_v2_55@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_55",
            "tgt_ix": "125-ARR_v2_55@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_56",
            "tgt_ix": "125-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_57",
            "tgt_ix": "125-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_57",
            "tgt_ix": "125-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_58",
            "tgt_ix": "125-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_58",
            "tgt_ix": "125-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_58",
            "tgt_ix": "125-ARR_v2_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_58",
            "tgt_ix": "125-ARR_v2_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_58",
            "tgt_ix": "125-ARR_v2_58@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_59",
            "tgt_ix": "125-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_59",
            "tgt_ix": "125-ARR_v2_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_60",
            "tgt_ix": "125-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_61",
            "tgt_ix": "125-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_62",
            "tgt_ix": "125-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_62",
            "tgt_ix": "125-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_62",
            "tgt_ix": "125-ARR_v2_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_62",
            "tgt_ix": "125-ARR_v2_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_62",
            "tgt_ix": "125-ARR_v2_62@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_62",
            "tgt_ix": "125-ARR_v2_62@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_62",
            "tgt_ix": "125-ARR_v2_62@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_62",
            "tgt_ix": "125-ARR_v2_62@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_62",
            "tgt_ix": "125-ARR_v2_62@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_62",
            "tgt_ix": "125-ARR_v2_62@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_62",
            "tgt_ix": "125-ARR_v2_62@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_62",
            "tgt_ix": "125-ARR_v2_62@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_63",
            "tgt_ix": "125-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_64",
            "tgt_ix": "125-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_64",
            "tgt_ix": "125-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_64",
            "tgt_ix": "125-ARR_v2_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_65",
            "tgt_ix": "125-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_65",
            "tgt_ix": "125-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_65",
            "tgt_ix": "125-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_65",
            "tgt_ix": "125-ARR_v2_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_65",
            "tgt_ix": "125-ARR_v2_65@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_65",
            "tgt_ix": "125-ARR_v2_65@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_65",
            "tgt_ix": "125-ARR_v2_65@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_65",
            "tgt_ix": "125-ARR_v2_65@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_65",
            "tgt_ix": "125-ARR_v2_65@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_66",
            "tgt_ix": "125-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_66",
            "tgt_ix": "125-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_66",
            "tgt_ix": "125-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_66",
            "tgt_ix": "125-ARR_v2_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_66",
            "tgt_ix": "125-ARR_v2_66@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_66",
            "tgt_ix": "125-ARR_v2_66@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_67",
            "tgt_ix": "125-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_67",
            "tgt_ix": "125-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_68",
            "tgt_ix": "125-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_69",
            "tgt_ix": "125-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_69",
            "tgt_ix": "125-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_69",
            "tgt_ix": "125-ARR_v2_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_70",
            "tgt_ix": "125-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_70",
            "tgt_ix": "125-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_70",
            "tgt_ix": "125-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_71",
            "tgt_ix": "125-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_71",
            "tgt_ix": "125-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_71",
            "tgt_ix": "125-ARR_v2_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_72",
            "tgt_ix": "125-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_73",
            "tgt_ix": "125-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_73",
            "tgt_ix": "125-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_73",
            "tgt_ix": "125-ARR_v2_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_73",
            "tgt_ix": "125-ARR_v2_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_73",
            "tgt_ix": "125-ARR_v2_73@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_73",
            "tgt_ix": "125-ARR_v2_73@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_73",
            "tgt_ix": "125-ARR_v2_73@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_74",
            "tgt_ix": "125-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_74",
            "tgt_ix": "125-ARR_v2_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_74",
            "tgt_ix": "125-ARR_v2_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_75",
            "tgt_ix": "125-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_75",
            "tgt_ix": "125-ARR_v2_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_75",
            "tgt_ix": "125-ARR_v2_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_75",
            "tgt_ix": "125-ARR_v2_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_75",
            "tgt_ix": "125-ARR_v2_75@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_75",
            "tgt_ix": "125-ARR_v2_75@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_75",
            "tgt_ix": "125-ARR_v2_75@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_76",
            "tgt_ix": "125-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_76",
            "tgt_ix": "125-ARR_v2_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_77",
            "tgt_ix": "125-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_77",
            "tgt_ix": "125-ARR_v2_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_77",
            "tgt_ix": "125-ARR_v2_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_77",
            "tgt_ix": "125-ARR_v2_77@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_77",
            "tgt_ix": "125-ARR_v2_77@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_78",
            "tgt_ix": "125-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_78",
            "tgt_ix": "125-ARR_v2_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_78",
            "tgt_ix": "125-ARR_v2_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_78",
            "tgt_ix": "125-ARR_v2_78@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_79",
            "tgt_ix": "125-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_79",
            "tgt_ix": "125-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_79",
            "tgt_ix": "125-ARR_v2_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_80",
            "tgt_ix": "125-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_80",
            "tgt_ix": "125-ARR_v2_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_81",
            "tgt_ix": "125-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_82",
            "tgt_ix": "125-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_82",
            "tgt_ix": "125-ARR_v2_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_83",
            "tgt_ix": "125-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_83",
            "tgt_ix": "125-ARR_v2_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_83",
            "tgt_ix": "125-ARR_v2_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_83",
            "tgt_ix": "125-ARR_v2_83@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_83",
            "tgt_ix": "125-ARR_v2_83@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_83",
            "tgt_ix": "125-ARR_v2_83@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_83",
            "tgt_ix": "125-ARR_v2_83@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_83",
            "tgt_ix": "125-ARR_v2_83@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_84",
            "tgt_ix": "125-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_84",
            "tgt_ix": "125-ARR_v2_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_84",
            "tgt_ix": "125-ARR_v2_84@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_84",
            "tgt_ix": "125-ARR_v2_84@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_85",
            "tgt_ix": "125-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_85",
            "tgt_ix": "125-ARR_v2_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_85",
            "tgt_ix": "125-ARR_v2_85@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_85",
            "tgt_ix": "125-ARR_v2_85@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_85",
            "tgt_ix": "125-ARR_v2_85@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_85",
            "tgt_ix": "125-ARR_v2_85@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_86",
            "tgt_ix": "125-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_86",
            "tgt_ix": "125-ARR_v2_86@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_87",
            "tgt_ix": "125-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_87",
            "tgt_ix": "125-ARR_v2_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_88",
            "tgt_ix": "125-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_88",
            "tgt_ix": "125-ARR_v2_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_88",
            "tgt_ix": "125-ARR_v2_88@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_88",
            "tgt_ix": "125-ARR_v2_88@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_88",
            "tgt_ix": "125-ARR_v2_88@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_88",
            "tgt_ix": "125-ARR_v2_88@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_89",
            "tgt_ix": "125-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_89",
            "tgt_ix": "125-ARR_v2_89@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_89",
            "tgt_ix": "125-ARR_v2_89@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_89",
            "tgt_ix": "125-ARR_v2_89@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_90",
            "tgt_ix": "125-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_90",
            "tgt_ix": "125-ARR_v2_90@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_90",
            "tgt_ix": "125-ARR_v2_90@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_91",
            "tgt_ix": "125-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_91",
            "tgt_ix": "125-ARR_v2_91@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_91",
            "tgt_ix": "125-ARR_v2_91@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_92",
            "tgt_ix": "125-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_93",
            "tgt_ix": "125-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_94",
            "tgt_ix": "125-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_95",
            "tgt_ix": "125-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_95",
            "tgt_ix": "125-ARR_v2_95@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_95",
            "tgt_ix": "125-ARR_v2_95@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_95",
            "tgt_ix": "125-ARR_v2_95@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_95",
            "tgt_ix": "125-ARR_v2_95@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_95",
            "tgt_ix": "125-ARR_v2_95@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_95",
            "tgt_ix": "125-ARR_v2_95@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_95",
            "tgt_ix": "125-ARR_v2_95@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_96",
            "tgt_ix": "125-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_96",
            "tgt_ix": "125-ARR_v2_96@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_97",
            "tgt_ix": "125-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_97",
            "tgt_ix": "125-ARR_v2_97@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_97",
            "tgt_ix": "125-ARR_v2_97@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_97",
            "tgt_ix": "125-ARR_v2_97@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_97",
            "tgt_ix": "125-ARR_v2_97@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_97",
            "tgt_ix": "125-ARR_v2_97@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_98",
            "tgt_ix": "125-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_98",
            "tgt_ix": "125-ARR_v2_98@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_98",
            "tgt_ix": "125-ARR_v2_98@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_98",
            "tgt_ix": "125-ARR_v2_98@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_99",
            "tgt_ix": "125-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_100",
            "tgt_ix": "125-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_101",
            "tgt_ix": "125-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_102",
            "tgt_ix": "125-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_103",
            "tgt_ix": "125-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_104",
            "tgt_ix": "125-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_105",
            "tgt_ix": "125-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_106",
            "tgt_ix": "125-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_107",
            "tgt_ix": "125-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_108",
            "tgt_ix": "125-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_109",
            "tgt_ix": "125-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_110",
            "tgt_ix": "125-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_111",
            "tgt_ix": "125-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_112",
            "tgt_ix": "125-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_113",
            "tgt_ix": "125-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_114",
            "tgt_ix": "125-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_115",
            "tgt_ix": "125-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_116",
            "tgt_ix": "125-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_117",
            "tgt_ix": "125-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_118",
            "tgt_ix": "125-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_119",
            "tgt_ix": "125-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_120",
            "tgt_ix": "125-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_121",
            "tgt_ix": "125-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_122",
            "tgt_ix": "125-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_123",
            "tgt_ix": "125-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_124",
            "tgt_ix": "125-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_125",
            "tgt_ix": "125-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_126",
            "tgt_ix": "125-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_127",
            "tgt_ix": "125-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_128",
            "tgt_ix": "125-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_129",
            "tgt_ix": "125-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_130",
            "tgt_ix": "125-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_131",
            "tgt_ix": "125-ARR_v2_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_132",
            "tgt_ix": "125-ARR_v2_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_133",
            "tgt_ix": "125-ARR_v2_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_134",
            "tgt_ix": "125-ARR_v2_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_135",
            "tgt_ix": "125-ARR_v2_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_136",
            "tgt_ix": "125-ARR_v2_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_137",
            "tgt_ix": "125-ARR_v2_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_138",
            "tgt_ix": "125-ARR_v2_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "125-ARR_v2_139",
            "tgt_ix": "125-ARR_v2_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1144,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "125-ARR",
        "version": 2
    }
}