{
    "nodes": [
        {
            "ix": "91-ARR_v2_0",
            "content": "A Novel Perspective to Look At Attention: Bi-level Attention-based Explainable Topic Modeling for News Classification",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_2",
            "content": "Many recent deep learning-based solutions have adopted the attention mechanism in various tasks in the field of NLP. However, the inherent characteristics of deep learning models and the flexibility of the attention mechanism increase the models' complexity, thus leading to challenges in model explainability. To address this challenge, we propose a novel practical framework by utilizing a two-tier attention architecture to decouple the complexity of explanation and the decision-making process. We apply it in the context of a news article classification task. The experiments on two largescaled news corpora demonstrate that the proposed model can achieve competitive performance with many state-of-the-art alternatives and illustrate its appropriateness from an explainability perspective. We release the source code here 1 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "91-ARR_v2_4",
            "content": "The attention mechanism is one of the most important components in recent deep learning-based architectures in natural language processing (NLP). In the early stages of its development, the encoderdecoder models (Bahdanau et al., 2015;Xu et al., 2015) often adopted an attention mechanism to improve the performance achieved by capturing different areas of the input sequence when generating an output in the decoding process to solve issues arising in encoding long-form inputs. Subsequently, researchers have applied the attention mechanism to large-scale corpora and developed a range of pre-trained language models (Kalyan et al., 2021), such as BERT (Devlin et al., 2019) and GPT-1 (Radford et al., 2018). This has yielded great progress across a range of NLP tasks, including sentiment analysis (Zhao et al., 2021) and news classification (Wu et al., 2021). However, the inherent characteristics of deep learning models and the flexibility of the attention mechanism increase these models' complexity, thus leading to challenges in model explainability.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_5",
            "content": "Today, there is still no consensus among researchers regarding whether attention-based models are explainable in theory. Some researchers believe that attention weights may reflect the importance of features during the decision-making process and thus can provide an explanation of their operation if we visualize features according to their weight distribution (Luong et al., 2015;Lu et al., 2018). However, other researchers have disagreed with this hypothesis. For example, Jain and Wallance's study demonstrated that learned attention weights are often uncorrelated with feature importance (Jain and Wallace, 2019). Some researchers have supported this viewpoint (Serrano and Smith, 2019), but treated with skepticism by others (Wiegreffe and Pinter, 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_6",
            "content": "In this paper, rather than validating the attention explainability theoretically, we propose a novel, practical explainable attention-based solution. Inspired by the idea of topic models (Blei et al., 2003), our proposed solution decouples the complexity of explanation and the decision-making process by adopting two attention layers to capture topicword distribution and document-topic distribution, respectively. Specifically, the first layer contains multiple attentions, and each attention is expected to focus on specific words from a topic. The attention in the second layer is then used to judge the importance of topics from the perspective of the target document. In order to further improve the model's explainability, we add an entropy constraint for each attention in the first layer. To prove the effectiveness of our proposed solution, we apply it in the context of a news article classification task and conduct experiments on two largescaled news article datasets. The results presented later in Section 4 show that our model can achieve competitive performance with many state-of-the-art transformer-based models and pre-trained language models, while also demonstrating its appropriateness from an explainability perspective.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_7",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "91-ARR_v2_8",
            "content": "Attention Mechanism",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "91-ARR_v2_9",
            "content": "The attention mechanism was first applied on machine translation tasks (Bahdanau et al., 2015) with the Seq2Seq model using RNN. To solve the dilemma in compressing long sequences by using an RNN-encoder, Bahdanau et al. (2015) introduced an attention mechanism by allowing RNNdecoder to assign attention weights to words in the input sequence. This strategy helps the decoder to effectively capture the relevant information between the hidden states of the encoder and the corresponding decoder's hidden state, which avoids information loss and makes the decoder focus on the relevant position of the input sequence. This attention mechanism is named additive attention or Tanh attention because it uses the Tanh activation function. In our work, we propose to use additive attention to discover the underlying mixture of topics within a document.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_10",
            "content": "Furthermore, Vaswani et al. (2017) proposed a transformer architecture to replace RNNs entirely with multi-head self-attention. This approach makes it possible to compute hidden representation for all input and output positions in parallel. The advantage of parallelized training has led to the emergence of many large pre-trained language models, such as BERT (Devlin et al., 2019). The improvement of using the transformer-based language model for generating representations is significant compared with popular word embedding methods such as GloVe (Pennington et al., 2014). However, along with the considerable enhancement in performance, it makes the attention-based language models difficult to interpret. One potential solution is to use attention weights to provide insights into the model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_11",
            "content": "Attention as an Explanation",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "91-ARR_v2_12",
            "content": "The visualization of attention weight alignment in (Luong et al., 2015;Vaswani et al., 2017) provides an intuitive explanation of the operation of additive attention and multi-head self-attention in machine translation tasks. But the faithfulness (i.e. accurately revealing the proper reasoning of the model) and plausibility (i.e. providing a convincing interpretation for humans) of using attention as an explanation for some tasks are still in debate, and the questioning is mainly on faithfulness (Jacovi and Goldberg, 2020). This discussion is primarily focused on a simple model for specific tasks, such as text classification, using RNN models connecting an attention layer which is typically MLP-based (Bahdanau et al., 2015). A number of researchers have challenged the usefulness of attention as an explanation (Jain and Wallace, 2019;Serrano and Smith, 2019;Bastings and Filippova, 2020), concluding that saliency methods, such as gradientbased techniques, perform much better than using attention weights as interpretations in finding the most significant features of the input sequence that yield the predicted outcome. However, Wiegreffe and Pinter (2019) claimed that, despite the fact that explanations provided by attention mechanisms are not always faithful, in practice, this does not invalidate the plausibility of using attention as an explanation. We believe that the attention mechanism can provide a plausible explanation when applied correctly for an appropriate task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_13",
            "content": "Role of Attention Mechanism",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "91-ARR_v2_14",
            "content": "Compared to simple additive attention, the Multi-Head Attention (MHA) mechanism, the core component of the big Transformer-based language model, is more complicated when attempting to interpret model behavior with complex weights distribution. Therefore, considerable work has attempted to understand the role played by the different attention heads (Rogers et al., 2020). For example, Voita et al. (2019) analyzed the patterns of attention heads by checking the survival of pruning, finding that the syntactic and positional heads are the final ones to be removed. Kovaleva et al. (2019) identified five attention patterns of MHA, while Pande et al. (2021) proposed a standardized approach for analyzing patterns of different attention heads in the context of the BERT model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_15",
            "content": "Instead of employing a complex transformer-like architecture with many MHA layers, we propose to start with a single MHA layer individually. Inspired by previous work, we focus on analyzing the role of attention heads in our architecture. We adopt a similar approach to (Lu et al., 2018) by modeling attention using topics. However, unlike the topic attention model (TAN), which uses a bag-of-words (BOW) model based on variational inference to align the topic space and word space with extracting meaningful topics (Panwar et al., 2021), we assume that these multiple attention heads represent multiple topics in terms of their semantics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_16",
            "content": "Methodology",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "91-ARR_v2_17",
            "content": "This section describes our proposed architecture Bi-level Attention-based Topical Model (BATM) as illustrated in Figure 1. It uses two attention layers to uncover a latent representation of the data and then makes use of attention weights as a form of topic distribution. We describe this architecture from the perspective of a news classification task. Our architecture consists of three components: an embedding layer, two attention layers, and a classification layer. After generating embedding vectors of words for the given news articles, we pass them to two attention layers to obtain the weight distribution of different words in each head (i.e. topic) and the weight distribution of different heads in the input articles. Then we generate the document representation vector based on these weights and finally classify the articles into different categories using a single linear layer. By analyzing the weight distribution of the attention layer on the entire news corpus, we find that some heads focus on the words related to the specific topics. These concentrated words help us understand the behavior of the attention mechanism.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_18",
            "content": "Embedding Layer",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "91-ARR_v2_19",
            "content": "There are two popular embedding methods: wordlevel embedding and contextual embedding, in general. Word-level embedding methods, such as GloVe, project different words into a word vector space and acquire a fixed-length word vector through a pre-trained embedding matrix. Contextual embedding models, such as BERT, generate different word vectors based on each word's context, so that the same word in different contexts can produce very different word vectors. For a given document x, suppose we have N tokens in total, we use an appropriate tokenizer to partition it into tokens t 1 , t 2 , . . . , t N according to the embedding method. Then we can represent the document using its embedding vectors e 1 , e 2 , . . . , e N as an input to the attention layer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_20",
            "content": "Multi-Head Attention Layer",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "91-ARR_v2_21",
            "content": "We use a multi-head attention mechanism to allow the model to focus on different positions in the document from different representation subspaces through multiple attention heads. We compute the weight distribution g k of the head vector h k through a single-layer feed-forward network first:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_22",
            "content": "g k i = v k tanh (W k e i + b k ) (1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_23",
            "content": "We then use the softmax function to get the normalized weights distribution \u03b1 k among the document:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_24",
            "content": "\u03b1 k i = e g k i N j e g k j (2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_25",
            "content": "Finally, the head vector h k is the weighted sum of word embedding vectors using the weights \u03b1 k , given by",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_26",
            "content": "h k = N i \u03b1 k i e i(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_27",
            "content": "where trained parameters are",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_28",
            "content": "v k \u2208 R D k , W k \u2208 R E\u00d7D k , and b k \u2208 R D k . D k",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_29",
            "content": "is the projected dimension of each head in the middle, and E is the embedding dimension, while the dimension of head vector h k is E which is the same as embedding vector e i from Eqn. 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_30",
            "content": "Additive Attention Layer",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "91-ARR_v2_31",
            "content": "For a given number of attention heads K, we have a group of head vectors H = {h 1 , h 2 , . . . , h K }, which are fed into an additive attention network to generate the document-topic distribution.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_32",
            "content": "\u00b5 k = c tanh (W H h k + b H ) \u03b2 k = e \u00b5 k K i e \u00b5 i (4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_33",
            "content": "Finally, the document representation d is the weighted sum of head vectors along with the weights distribution \u03b2 :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_34",
            "content": "d = K i \u03b2 k h k (5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_35",
            "content": "where trained parameters are",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_36",
            "content": "c \u2208 R D h , W H \u2208 R E\u00d7D h , b H \u2208 R D h",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_37",
            "content": ", and the dimension of d is also E which is the same as h k .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_38",
            "content": "Classification Layer",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "91-ARR_v2_39",
            "content": "Since the representation of each document d will be a dense vector containing a mixture of information about the document's content, we can use it as the feature vector for the final news classification task:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_40",
            "content": "y = sof tmax (W C d + b C ) (6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_41",
            "content": "Entropy Constraint",
            "ntype": "title",
            "meta": {
                "section": "3.5"
            }
        },
        {
            "ix": "91-ARR_v2_42",
            "content": "In order to further improve the explainability of our base model as described above, we now adjust the model so that each head only focuses on a specific set of words -i.e. we enforce topic-word weights distribution \u03b1 k not to spread over the document widely. We do this by computing the entropy of \u03b1 k as a part of the loss function. The entropy constraint penalizes the model when \u03b1 k has high entropy. Thus, the final loss with entropy constraint for the news classification task is:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_43",
            "content": "L = L CE (y, \u0177) + \u03bb K k E doc \u03b1 k K (7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_44",
            "content": "where L CE (y, \u0177) is the Cross-Entropy Loss between ground-truth class and predicted class, and \u03bb is a hyper-parameter to scale the magnitude of average entropy calculated by \u03b1 k . The calculation for corresponding entropy L entropy is by:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_45",
            "content": "E doc \u03b1 k = \u2212 N i \u03b1 k i log \u03b1 k i (8)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_46",
            "content": "The entropy constraint applied on document-level in Eqn. 8 changes the distribution of topic-word weights \u03b1 k . However, our goal is to find more diverse topics, which means different topics should focus on different words. Therefore, it is necessary to know how entropy decreases at the token level (i.e. across the vocabulary as shown in Figure 2), which is defined by: To distinguish between the two variants of our model, we name the basic model as BATM-Base and use BATM-EC refer to the model with entropy constraints. From Eqn. 7, it is evident that if we set \u03bb as 0, BATM-EC will be equivalent to the basic model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_47",
            "content": "E token (M i ) = \u2212 K k M k i log M k i (9)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_48",
            "content": "Generating the Topic Distribution",
            "ntype": "title",
            "meta": {
                "section": "3.6"
            }
        },
        {
            "ix": "91-ARR_v2_49",
            "content": "After training our proposed BATM model, we analyze the attention weights generated from the first attention layer (MHA) over the corpus vocabulary to generate a global topic distribution. Let us assume that there are V words in the corpus and we have K heads corresponding to K topics. The resulting topic distribution takes the form of a V \u00d7 K weight matrix, calculated from a trained MHA layer using embedded word vectors as inputs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_50",
            "content": "Moreover, to identify the most important words for each topic (which we can view as being the topic's descriptor), we extract the top-T words from the topic distribution, which can help us understand the heads and interpret them as topics. We examine the interpretations of these topic descriptors and display some examples in Section 4.5.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_51",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "91-ARR_v2_52",
            "content": "We now evaluate the BATM model on two largescale real-world datasets, and compare its performance with a number of state-of-the-art methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_53",
            "content": "Datasets",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "91-ARR_v2_54",
            "content": "We evaluate our proposed model on a news classification task and conduct extensive experiments on two public corpora. MIND (Wu et al., 2020) is a large-scale English dataset for news recommendation and categorization tasks. It contains information such as story title, abstract, and news category, but the public version does not include full article body content. We collected news articles from the Microsoft news website 2 to supplement it. There are 18 categories in the original MINDlarge dataset, but three of them only have a small number of articles (< 10). Therefore, we exclude these categories from our experiment. The second one is the News Category Dataset 3 , which contains approximately 200k news articles (each of them include a headline and a short news description) from 2012 to 2018 obtained from HuffPost. The original dataset has 41 categories, but some of these are duplicates. After merging the duplicated categories, there are 26 categories remain, which is denoted as News-26. We randomly split these two datasets into training/validation/test sets with a 80/10/10 split. Table 1 summarizes the divisions and the key statistics of the datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_55",
            "content": "Baseline Models",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "91-ARR_v2_56",
            "content": "For the purpose of assessing classification performance, we first compare the effectiveness of our BATM base model relative to a number of attentionbased and pre-trained language models:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_57",
            "content": "\u2022 BERT (Devlin et al., 2019) composes of a bidirectional encoder of transformer and is pre-trained by using a combination of masked language modeling objective and next sentence prediction on a large corpus; \u2022 DistilBERT (Sanh et al., 2019) is a small, fast, cheap, and light transformer model trained by distilling BERT base; \u2022 XLNet is an extension of the Transformer-XL model, which utilizes an autoregressive method to learn bidirectional contexts by maximizing the expected likelihood over all permutations of input sequence factorization order; \u2022 Roberta (Liu et al., 2019) is a robustly optimized BERT that modifies key hyperparameters, removing the next-sentence pre-training objective and training with much larger minibatches and learning rates; \u2022 Longformer (Beltagy et al., 2020) is based on RoBERTa (Liu et al., 2019) and uses sliding window attention and global attention to model local and global contexts; \u2022 Fastformer (Wu et al., 2021) uses additive attention to perform multi-head attention, which is more efficient than a standard transformer.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_58",
            "content": "The initial weights of these pre-trained language models (BERT, DistilBERT, XLNet, Roberta, and Longformer) are provided by Hugging Face Transformer (Wolf et al., 2020) library 4 . We use a linear classifier to receive the pooled output from previous transformer layers and then fine-tune these models to adapt them to the classification task. For the attention-based model, Fastformer, we initialize its embedding matrix using GloVe embedding and follow the hyper-parameter settings in (Wu et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_59",
            "content": "Experimental Settings",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "91-ARR_v2_60",
            "content": "In our experiments, we consider two ways to initialize our embedding matrix: GloVe embedding (Pennington et al., 2014) and context embeddings from a pre-trained language model DistilBERT (Sanh et al., 2019), where embedding weights are not fixed during the training procedure. We examine how different number of heads would influence the performance of our proposed model on the validation set, the details is shown in Figure 3. Unsurprisingly, on the MIND data set, the model needs to set a relatively larger number of topics, because the average length of news articles in the MIND dataset and its vocabulary size are much larger than the News-26 dataset, as indicated in Table 1. We identify the number of topics for MIND-15 and News-26 as 180 and 30 for the rest of experiments, respectively. We use Adam (Kingma and Ba, 2015) for model optimization, and each epoch decays the learning rate by half.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_61",
            "content": "Performance Comparison",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "91-ARR_v2_62",
            "content": "The large pre-trained transformer variants perform better than the model with GloVe embedding, both for MIND-15 and News-26. Compared to Fastformer-GloVe, our BATM-Base-GloVe model achieves a similar result (variance in 0.3% of accuracy and 0.4% of Macro-F) for MIND-15 and a better result (variance in almost 0.4% of accuracy and 0.6% of Macro-F) for News-26. The differing results in MIND-15 and News-26 are due to the length of articles. As an efficient Fastformer can take a much longer sequence as input, it is advantageous to deal with long sequences which are unavailable in a short-length news dataset such as News-26. Using the pre-trained transformer-based embedding greatly improves the performance of our proposed BATM-Base model compared to the GloVe embedding, although it adds to the difficulty of interpretation. The performance difference of the other pre-trained language models with the BATM-Base-DB model is less than 1% accuracy and approximately 2% Macro-F, both for MIND-15 and News-26. These experiments demonstrate the effectiveness of our proposed model in constructing document representations. Thus, the analysis of BATM's behavior using the topic-word distribution and document-topic distribution is essential to understanding the role of Bi-level attention layers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_63",
            "content": "Evaluation of Global Topic Representation",
            "ntype": "title",
            "meta": {
                "section": "4.5"
            }
        },
        {
            "ix": "91-ARR_v2_64",
            "content": "Besides the classification performance, we are also interested in whether each extracted topic descriptor as described in 3.6 has an intuitive meaning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_65",
            "content": "We take the top-25 highest scoring terms from each topic and calculate topic coherence scores C v (R\u00f6der et al., 2015). The average coherence scores of all topics of the BATM-Base-GloVe model are 0.58 and 0.56 on the MIND dataset and the news category datasets, respectively. Moreover, to more intuitively understand the meaning of topics mined by our model, we list a few topic examples whose coherence scores range from 0.3 to 0.8 along with a manually-assigned label in Table 3. The topics with coherence scores between 0.55 and 0.8 usually have precise meanings, such as the topic labeled as \"Partisan\" score of 0.76, where the vast majority of words are related to political activities and elections. However, some topics with a score in the range of 0.55 \u223c 0.8 are still tough to surmise the focus, as the unknown topic (labeled as \"Unknown\" with C v value is 0.61) suggest, where the correlation of topic descriptors is non-intuitive. In contrast, some low-coherence topics may contain highly relevant words as well. For example, the topic \"Schedule\" a with a score of 0.38 (under 0.55) mainly includes words related to time and arrangement, which we can comprehend the central point of these words, but the automated metric unfairly evaluates it. Therefore, with the auxiliary of topic coherence measurement and manual verification, we are firmly convinced that topic descriptors extracted by the BATM-Base-GloVe model indeed have specific meanings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_66",
            "content": "Effect of Entropy Constraints",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "91-ARR_v2_67",
            "content": "In the previous sections, the proposed BATM-Base-GloVe model demonstrates its competitive classification performance and excellent explainability.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_68",
            "content": "We now study the effect of adding an entropy constraint, as discussed in Section 3.5. In the extended model, referred to as BATM-EC, \u03bb determines the degree of constraint that is imposed, so the BATM-Base-GloVe model is a special case when \u03bb is zero. This study assumes that a good topic (a first-level of attention) should only focus on specific words related to that topic. Its weight distribution on a news article should not be flat for the whole document, while its global weight distribution should also not be widely spread out across the entire vocabulary (i.e., it should have a relatively lower entropy ). Therefore, we observe the dynamic of two entropy metrics E doc and E token (see calculation in Eqn. 8 and Eqn. 9) by setting different values of \u03bb. We present the performance and entropy changes along with the values of \u03bb in Table 4 The results meet our expectations. When \u03bb reaches le-4, both entropy indicators decrease significantly with an acceptable trade-off in classification performance. When continually increasing the impact of entropy constraints, both entropy indicators and classification performance decrease dramatically. This is reasonable, as this experiment is conducted with a fixed number of heads. When attention focuses on a minimal number of topics, and the number of topics does not increase accordingly, information within article texts is likely to be lost, affecting the classification performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_69",
            "content": "Discussion and Future Work",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "91-ARR_v2_70",
            "content": "While the variant of our proposed model, BATMbase-DB, which is initialized by the contextual embeddings, can outperform all alternatives, the meaning of its topics is much worse than BATM-Base-GloVe. Each contextual embedding learned by pre-trained language models will merge the information from its surrounding words, which increases the difficulties of the proposed attention layer to capture the topics it focuses on, thus leading to more noise in their representations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_71",
            "content": "Another challenge we will address in the future is how to balance the computation cost, topic granularity, and classification performances. As discussed in the previous sections, it will affect the model's classification performance if we only introduce entropy constraints without incrementing the number of attention heads. However, increasing the number of attention heads will lead to the proportional increment of parameters, increasing the complexity of the model and resulting in a high computation cost. We will consider increasing the number of heads and the extending entropy constraint further, to improve classification performance while maintaining strong explainability.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_72",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "91-ARR_v2_73",
            "content": "In this paper, we presented a novel approach that harnesses a bi-level attention framework to decouple the text classification process as topic capturing, topic importance recognition and decision-making process to benefit explainability. We conducted the experiments on two large-scale text corpora. Compared with a number of state-of-the-art alternatives on a text classification task, our model can not only achieve a competitive performance, but also demonstrates a strong ability to capture intuitive meanings in the form of topical features, thus improving its explainability and transparency. In addition, by initializing it with contextual embeddings, our model outperforms all the baseline models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "91-ARR_v2_74",
            "content": "UNKNOWN, None, 2015, Neural machine translation by jointly learning to align and translate, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Neural machine translation by jointly learning to align and translate",
                "pub": "CoRR"
            }
        },
        {
            "ix": "91-ARR_v2_75",
            "content": "Jasmijn Bastings, Katja Filippova, The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?, 2020, Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Jasmijn Bastings",
                    "Katja Filippova"
                ],
                "title": "The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_76",
            "content": "UNKNOWN, None, 2004, Longformer: The long-document transformer. ArXiv, abs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2004",
                "pub_title": "Longformer: The long-document transformer. ArXiv, abs",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_77",
            "content": "M David,  Blei, Y Andrew, Michael I Jordan Ng, None, 2003, Latent dirichlet allocation. the Journal of machine Learning research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "M David",
                    " Blei",
                    "Y Andrew",
                    "Michael I Jordan Ng"
                ],
                "title": null,
                "pub_date": "2003",
                "pub_title": "Latent dirichlet allocation. the Journal of machine Learning research",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_78",
            "content": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, Ruslan Salakhutdinov, Transformer-XL: Attentive language models beyond a fixed-length context, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Zihang Dai",
                    "Zhilin Yang",
                    "Yiming Yang",
                    "Jaime Carbonell",
                    "Quoc Le",
                    "Ruslan Salakhutdinov"
                ],
                "title": "Transformer-XL: Attentive language models beyond a fixed-length context",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "91-ARR_v2_79",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "91-ARR_v2_80",
            "content": "Alon Jacovi, Yoav Goldberg, Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness?, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Alon Jacovi",
                    "Yoav Goldberg"
                ],
                "title": "Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness?",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_81",
            "content": "Sarthak Jain, Byron Wallace, Attention is not Explanation, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Sarthak Jain",
                    "Byron Wallace"
                ],
                "title": "Attention is not Explanation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "91-ARR_v2_82",
            "content": "UNKNOWN, None, 2021, Ammus: A survey of transformer-based pretrained models in natural language processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Ammus: A survey of transformer-based pretrained models in natural language processing",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_83",
            "content": "UNKNOWN, None, 2015, Adam: A method for stochastic optimization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Adam: A method for stochastic optimization",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_84",
            "content": "Olga Kovaleva, Alexey Romanov, Anna Rogers, Anna Rumshisky, Revealing the dark secrets of BERT, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Olga Kovaleva",
                    "Alexey Romanov",
                    "Anna Rogers",
                    "Anna Rumshisky"
                ],
                "title": "Revealing the dark secrets of BERT",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "91-ARR_v2_85",
            "content": "UNKNOWN, None, 1907, Roberta: A robustly optimized bert pretraining approach. ArXiv, abs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": "1907",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach. ArXiv, abs",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_86",
            "content": "Yichao Lu, Ruihai Dong, Barry Smyth, Coevolutionary recommendation model: Mutual learning between ratings and reviews, 2018, Proceedings of the 2018 World Wide Web Conference, WWW '18, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Yichao Lu",
                    "Ruihai Dong",
                    "Barry Smyth"
                ],
                "title": "Coevolutionary recommendation model: Mutual learning between ratings and reviews",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 World Wide Web Conference, WWW '18",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_87",
            "content": "Thang Luong, Hieu Pham, Christopher Manning, Effective approaches to attention-based neural machine translation, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Thang Luong",
                    "Hieu Pham",
                    "Christopher Manning"
                ],
                "title": "Effective approaches to attention-based neural machine translation",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_88",
            "content": "Madhura Pande, Aakriti Budhraja, Preksha Nema, Pratyush Kumar, Mitesh Khapra, The heads hypothesis: A unifying statistical approach towards understanding multi-headed attention in bert, 2021, AAAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Madhura Pande",
                    "Aakriti Budhraja",
                    "Preksha Nema",
                    "Pratyush Kumar",
                    "Mitesh Khapra"
                ],
                "title": "The heads hypothesis: A unifying statistical approach towards understanding multi-headed attention in bert",
                "pub_date": "2021",
                "pub_title": "AAAI",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_89",
            "content": "Madhur Panwar, Shashank Shailabh, Milan Aggarwal, Balaji Krishnamurthy, TAN-NTM: Topic attention networks for neural topic modeling, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Madhur Panwar",
                    "Shashank Shailabh",
                    "Milan Aggarwal",
                    "Balaji Krishnamurthy"
                ],
                "title": "TAN-NTM: Topic attention networks for neural topic modeling",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "91-ARR_v2_90",
            "content": "Jeffrey Pennington, Richard Socher, Christopher Manning, GloVe: Global vectors for word representation, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Jeffrey Pennington",
                    "Richard Socher",
                    "Christopher Manning"
                ],
                "title": "GloVe: Global vectors for word representation",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "91-ARR_v2_91",
            "content": "UNKNOWN, None, 2018, Improving language understanding by generative pre-training, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Improving language understanding by generative pre-training",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_92",
            "content": "Michael R\u00f6der, Andreas Both, Alexander Hinneburg, Exploring the space of topic coherence measures, 2015, Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Michael R\u00f6der",
                    "Andreas Both",
                    "Alexander Hinneburg"
                ],
                "title": "Exploring the space of topic coherence measures",
                "pub_date": "2015",
                "pub_title": "Proceedings of the Eighth ACM International Conference on Web Search and Data Mining",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_93",
            "content": "Anna Rogers, Olga Kovaleva, Anna Rumshisky, A primer in bertology: What we know about how bert works, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Anna Rogers",
                    "Olga Kovaleva",
                    "Anna Rumshisky"
                ],
                "title": "A primer in bertology: What we know about how bert works",
                "pub_date": "2020",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_94",
            "content": "Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, 2019, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Victor Sanh",
                    "Lysandre Debut",
                    "Julien Chaumond",
                    "Thomas Wolf"
                ],
                "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
                "pub_date": "2019",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_95",
            "content": "Sofia Serrano, Noah Smith, Is attention interpretable?, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Sofia Serrano",
                    "Noah Smith"
                ],
                "title": "Is attention interpretable?",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_96",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_97",
            "content": "Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov, Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Elena Voita",
                    "David Talbot",
                    "Fedor Moiseev",
                    "Rico Sennrich",
                    "Ivan Titov"
                ],
                "title": "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "91-ARR_v2_98",
            "content": "Sarah Wiegreffe, Yuval Pinter, Attention is not not explanation, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Sarah Wiegreffe",
                    "Yuval Pinter"
                ],
                "title": "Attention is not not explanation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "91-ARR_v2_99",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger,  Drame, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Thomas Wolf",
                    "Lysandre Debut",
                    "Victor Sanh",
                    "Julien Chaumond",
                    "Clement Delangue",
                    "Anthony Moi",
                    "Pierric Cistac",
                    "Tim Rault",
                    "Remi Louf",
                    "Morgan Funtowicz",
                    "Joe Davison",
                    "Sam Shleifer",
                    "Clara Patrick Von Platen",
                    "Yacine Ma",
                    "Julien Jernite",
                    "Canwen Plu",
                    "Teven Xu",
                    "Sylvain Scao",
                    "Mariama Gugger",
                    " Drame"
                ],
                "title": "Transformers: State-of-the-art natural language processing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_100",
            "content": "Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, Fastformer: Additive attention can be all you need, 2021, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Chuhan Wu",
                    "Fangzhao Wu",
                    "Tao Qi",
                    "Yongfeng Huang"
                ],
                "title": "Fastformer: Additive attention can be all you need",
                "pub_date": "2021",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_101",
            "content": "Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, Ming Zhou, MIND: A large-scale dataset for news recommendation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Fangzhao Wu",
                    "Ying Qiao",
                    "Jiun-Hung Chen",
                    "Chuhan Wu",
                    "Tao Qi",
                    "Jianxun Lian",
                    "Danyang Liu",
                    "Xing Xie",
                    "Jianfeng Gao",
                    "Winnie Wu",
                    "Ming Zhou"
                ],
                "title": "MIND: A large-scale dataset for news recommendation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_102",
            "content": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, Yoshua Bengio, Show, attend and tell: Neural image caption generation with visual attention, 2015, International conference on machine learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Kelvin Xu",
                    "Jimmy Ba",
                    "Ryan Kiros",
                    "Kyunghyun Cho",
                    "Aaron Courville",
                    "Ruslan Salakhudinov",
                    "Rich Zemel",
                    "Yoshua Bengio"
                ],
                "title": "Show, attend and tell: Neural image caption generation with visual attention",
                "pub_date": "2015",
                "pub_title": "International conference on machine learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "91-ARR_v2_103",
            "content": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, V Quoc,  Le, Xlnet: Generalized autoregressive pretraining for language understanding, 2019, NeurIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Zhilin Yang",
                    "Zihang Dai",
                    "Yiming Yang",
                    "Jaime Carbonell",
                    "Ruslan Salakhutdinov",
                    "V Quoc",
                    " Le"
                ],
                "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
                "pub_date": "2019",
                "pub_title": "NeurIPS",
                "pub": null
            }
        },
        {
            "ix": "91-ARR_v2_104",
            "content": "Lingyun Zhao, Lin Li, Xinhao Zheng, Jianwei Zhang, A BERT based sentiment analysis and key entity detection approach for online financial texts, 2021, 2021 IEEE 24th International Conference on Computer Supported Cooperative Work in Design (CSCWD), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Lingyun Zhao",
                    "Lin Li",
                    "Xinhao Zheng",
                    "Jianwei Zhang"
                ],
                "title": "A BERT based sentiment analysis and key entity detection approach for online financial texts",
                "pub_date": "2021",
                "pub_title": "2021 IEEE 24th International Conference on Computer Supported Cooperative Work in Design (CSCWD)",
                "pub": "IEEE"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "91-ARR_v2_0@0",
            "content": "A Novel Perspective to Look At Attention: Bi-level Attention-based Explainable Topic Modeling for News Classification",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_0",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_2@0",
            "content": "Many recent deep learning-based solutions have adopted the attention mechanism in various tasks in the field of NLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_2",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_2@1",
            "content": "However, the inherent characteristics of deep learning models and the flexibility of the attention mechanism increase the models' complexity, thus leading to challenges in model explainability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_2",
            "start": 117,
            "end": 309,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_2@2",
            "content": "To address this challenge, we propose a novel practical framework by utilizing a two-tier attention architecture to decouple the complexity of explanation and the decision-making process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_2",
            "start": 311,
            "end": 497,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_2@3",
            "content": "We apply it in the context of a news article classification task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_2",
            "start": 499,
            "end": 563,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_2@4",
            "content": "The experiments on two largescaled news corpora demonstrate that the proposed model can achieve competitive performance with many state-of-the-art alternatives and illustrate its appropriateness from an explainability perspective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_2",
            "start": 565,
            "end": 794,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_2@5",
            "content": "We release the source code here 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_2",
            "start": 796,
            "end": 830,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_4@0",
            "content": "The attention mechanism is one of the most important components in recent deep learning-based architectures in natural language processing (NLP).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_4",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_4@1",
            "content": "In the early stages of its development, the encoderdecoder models (Bahdanau et al., 2015;Xu et al., 2015) often adopted an attention mechanism to improve the performance achieved by capturing different areas of the input sequence when generating an output in the decoding process to solve issues arising in encoding long-form inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_4",
            "start": 146,
            "end": 478,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_4@2",
            "content": "Subsequently, researchers have applied the attention mechanism to large-scale corpora and developed a range of pre-trained language models (Kalyan et al., 2021), such as BERT (Devlin et al., 2019) and GPT-1 (Radford et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_4",
            "start": 480,
            "end": 709,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_4@3",
            "content": "This has yielded great progress across a range of NLP tasks, including sentiment analysis (Zhao et al., 2021) and news classification (Wu et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_4",
            "start": 711,
            "end": 862,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_4@4",
            "content": "However, the inherent characteristics of deep learning models and the flexibility of the attention mechanism increase these models' complexity, thus leading to challenges in model explainability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_4",
            "start": 864,
            "end": 1058,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_5@0",
            "content": "Today, there is still no consensus among researchers regarding whether attention-based models are explainable in theory.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_5",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_5@1",
            "content": "Some researchers believe that attention weights may reflect the importance of features during the decision-making process and thus can provide an explanation of their operation if we visualize features according to their weight distribution (Luong et al., 2015;Lu et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_5",
            "start": 121,
            "end": 398,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_5@2",
            "content": "However, other researchers have disagreed with this hypothesis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_5",
            "start": 400,
            "end": 462,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_5@3",
            "content": "For example, Jain and Wallance's study demonstrated that learned attention weights are often uncorrelated with feature importance (Jain and Wallace, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_5",
            "start": 464,
            "end": 618,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_5@4",
            "content": "Some researchers have supported this viewpoint (Serrano and Smith, 2019), but treated with skepticism by others (Wiegreffe and Pinter, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_5",
            "start": 620,
            "end": 760,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_6@0",
            "content": "In this paper, rather than validating the attention explainability theoretically, we propose a novel, practical explainable attention-based solution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_6",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_6@1",
            "content": "Inspired by the idea of topic models (Blei et al., 2003), our proposed solution decouples the complexity of explanation and the decision-making process by adopting two attention layers to capture topicword distribution and document-topic distribution, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_6",
            "start": 150,
            "end": 414,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_6@2",
            "content": "Specifically, the first layer contains multiple attentions, and each attention is expected to focus on specific words from a topic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_6",
            "start": 416,
            "end": 546,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_6@3",
            "content": "The attention in the second layer is then used to judge the importance of topics from the perspective of the target document.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_6",
            "start": 548,
            "end": 672,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_6@4",
            "content": "In order to further improve the model's explainability, we add an entropy constraint for each attention in the first layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_6",
            "start": 674,
            "end": 796,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_6@5",
            "content": "To prove the effectiveness of our proposed solution, we apply it in the context of a news article classification task and conduct experiments on two largescaled news article datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_6",
            "start": 798,
            "end": 980,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_6@6",
            "content": "The results presented later in Section 4 show that our model can achieve competitive performance with many state-of-the-art transformer-based models and pre-trained language models, while also demonstrating its appropriateness from an explainability perspective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_6",
            "start": 982,
            "end": 1243,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_7@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_7",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_8@0",
            "content": "Attention Mechanism",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_8",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_9@0",
            "content": "The attention mechanism was first applied on machine translation tasks (Bahdanau et al., 2015) with the Seq2Seq model using RNN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_9",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_9@1",
            "content": "To solve the dilemma in compressing long sequences by using an RNN-encoder, Bahdanau et al. (2015) introduced an attention mechanism by allowing RNNdecoder to assign attention weights to words in the input sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_9",
            "start": 129,
            "end": 343,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_9@2",
            "content": "This strategy helps the decoder to effectively capture the relevant information between the hidden states of the encoder and the corresponding decoder's hidden state, which avoids information loss and makes the decoder focus on the relevant position of the input sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_9",
            "start": 345,
            "end": 616,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_9@3",
            "content": "This attention mechanism is named additive attention or Tanh attention because it uses the Tanh activation function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_9",
            "start": 618,
            "end": 733,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_9@4",
            "content": "In our work, we propose to use additive attention to discover the underlying mixture of topics within a document.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_9",
            "start": 735,
            "end": 847,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_10@0",
            "content": "Furthermore, Vaswani et al. (2017) proposed a transformer architecture to replace RNNs entirely with multi-head self-attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_10",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_10@1",
            "content": "This approach makes it possible to compute hidden representation for all input and output positions in parallel.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_10",
            "start": 128,
            "end": 239,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_10@2",
            "content": "The advantage of parallelized training has led to the emergence of many large pre-trained language models, such as BERT (Devlin et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_10",
            "start": 241,
            "end": 382,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_10@3",
            "content": "The improvement of using the transformer-based language model for generating representations is significant compared with popular word embedding methods such as GloVe (Pennington et al., 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_10",
            "start": 384,
            "end": 576,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_10@4",
            "content": "However, along with the considerable enhancement in performance, it makes the attention-based language models difficult to interpret.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_10",
            "start": 578,
            "end": 710,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_10@5",
            "content": "One potential solution is to use attention weights to provide insights into the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_10",
            "start": 712,
            "end": 797,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_11@0",
            "content": "Attention as an Explanation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_11",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_12@0",
            "content": "The visualization of attention weight alignment in (Luong et al., 2015;Vaswani et al., 2017) provides an intuitive explanation of the operation of additive attention and multi-head self-attention in machine translation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_12",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_12@1",
            "content": "But the faithfulness (i.e. accurately revealing the proper reasoning of the model) and plausibility (i.e. providing a convincing interpretation for humans) of using attention as an explanation for some tasks are still in debate, and the questioning is mainly on faithfulness (Jacovi and Goldberg, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_12",
            "start": 226,
            "end": 528,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_12@2",
            "content": "This discussion is primarily focused on a simple model for specific tasks, such as text classification, using RNN models connecting an attention layer which is typically MLP-based (Bahdanau et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_12",
            "start": 530,
            "end": 733,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_12@3",
            "content": "A number of researchers have challenged the usefulness of attention as an explanation (Jain and Wallace, 2019;Serrano and Smith, 2019;Bastings and Filippova, 2020), concluding that saliency methods, such as gradientbased techniques, perform much better than using attention weights as interpretations in finding the most significant features of the input sequence that yield the predicted outcome.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_12",
            "start": 735,
            "end": 1131,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_12@4",
            "content": "However, Wiegreffe and Pinter (2019) claimed that, despite the fact that explanations provided by attention mechanisms are not always faithful, in practice, this does not invalidate the plausibility of using attention as an explanation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_12",
            "start": 1133,
            "end": 1368,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_12@5",
            "content": "We believe that the attention mechanism can provide a plausible explanation when applied correctly for an appropriate task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_12",
            "start": 1370,
            "end": 1492,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_13@0",
            "content": "Role of Attention Mechanism",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_13",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_14@0",
            "content": "Compared to simple additive attention, the Multi-Head Attention (MHA) mechanism, the core component of the big Transformer-based language model, is more complicated when attempting to interpret model behavior with complex weights distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_14",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_14@1",
            "content": "Therefore, considerable work has attempted to understand the role played by the different attention heads (Rogers et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_14",
            "start": 244,
            "end": 371,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_14@2",
            "content": "For example, Voita et al. (2019) analyzed the patterns of attention heads by checking the survival of pruning, finding that the syntactic and positional heads are the final ones to be removed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_14",
            "start": 373,
            "end": 564,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_14@3",
            "content": "Kovaleva et al. (2019) identified five attention patterns of MHA, while Pande et al. (2021) proposed a standardized approach for analyzing patterns of different attention heads in the context of the BERT model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_14",
            "start": 566,
            "end": 775,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_15@0",
            "content": "Instead of employing a complex transformer-like architecture with many MHA layers, we propose to start with a single MHA layer individually.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_15",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_15@1",
            "content": "Inspired by previous work, we focus on analyzing the role of attention heads in our architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_15",
            "start": 141,
            "end": 237,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_15@2",
            "content": "We adopt a similar approach to (Lu et al., 2018) by modeling attention using topics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_15",
            "start": 239,
            "end": 322,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_15@3",
            "content": "However, unlike the topic attention model (TAN), which uses a bag-of-words (BOW) model based on variational inference to align the topic space and word space with extracting meaningful topics (Panwar et al., 2021), we assume that these multiple attention heads represent multiple topics in terms of their semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_15",
            "start": 324,
            "end": 638,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_16@0",
            "content": "Methodology",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_16",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_17@0",
            "content": "This section describes our proposed architecture Bi-level Attention-based Topical Model (BATM) as illustrated in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_17",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_17@1",
            "content": "It uses two attention layers to uncover a latent representation of the data and then makes use of attention weights as a form of topic distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_17",
            "start": 123,
            "end": 270,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_17@2",
            "content": "We describe this architecture from the perspective of a news classification task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_17",
            "start": 272,
            "end": 352,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_17@3",
            "content": "Our architecture consists of three components: an embedding layer, two attention layers, and a classification layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_17",
            "start": 354,
            "end": 469,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_17@4",
            "content": "After generating embedding vectors of words for the given news articles, we pass them to two attention layers to obtain the weight distribution of different words in each head (i.e. topic) and the weight distribution of different heads in the input articles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_17",
            "start": 471,
            "end": 728,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_17@5",
            "content": "Then we generate the document representation vector based on these weights and finally classify the articles into different categories using a single linear layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_17",
            "start": 730,
            "end": 892,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_17@6",
            "content": "By analyzing the weight distribution of the attention layer on the entire news corpus, we find that some heads focus on the words related to the specific topics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_17",
            "start": 894,
            "end": 1054,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_17@7",
            "content": "These concentrated words help us understand the behavior of the attention mechanism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_17",
            "start": 1056,
            "end": 1139,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_18@0",
            "content": "Embedding Layer",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_18",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_19@0",
            "content": "There are two popular embedding methods: wordlevel embedding and contextual embedding, in general.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_19",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_19@1",
            "content": "Word-level embedding methods, such as GloVe, project different words into a word vector space and acquire a fixed-length word vector through a pre-trained embedding matrix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_19",
            "start": 99,
            "end": 270,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_19@2",
            "content": "Contextual embedding models, such as BERT, generate different word vectors based on each word's context, so that the same word in different contexts can produce very different word vectors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_19",
            "start": 272,
            "end": 460,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_19@3",
            "content": "For a given document x, suppose we have N tokens in total, we use an appropriate tokenizer to partition it into tokens t 1 , t 2 , . . . , t N according to the embedding method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_19",
            "start": 462,
            "end": 638,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_19@4",
            "content": "Then we can represent the document using its embedding vectors e 1 , e 2 , . . . , e N as an input to the attention layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_19",
            "start": 640,
            "end": 761,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_20@0",
            "content": "Multi-Head Attention Layer",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_20",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_21@0",
            "content": "We use a multi-head attention mechanism to allow the model to focus on different positions in the document from different representation subspaces through multiple attention heads.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_21",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_21@1",
            "content": "We compute the weight distribution g k of the head vector h k through a single-layer feed-forward network first:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_21",
            "start": 181,
            "end": 292,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_22@0",
            "content": "g k i = v k tanh (W k e i + b k ) (1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_22",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_23@0",
            "content": "We then use the softmax function to get the normalized weights distribution \u03b1 k among the document:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_23",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_24@0",
            "content": "\u03b1 k i = e g k i N j e g k j (2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_24",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_25@0",
            "content": "Finally, the head vector h k is the weighted sum of word embedding vectors using the weights \u03b1 k , given by",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_25",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_26@0",
            "content": "h k = N i \u03b1 k i e i(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_26",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_27@0",
            "content": "where trained parameters are",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_27",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_28@0",
            "content": "v k \u2208 R D k , W k \u2208 R E\u00d7D k , and b k \u2208 R D k . D k",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_28",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_29@0",
            "content": "is the projected dimension of each head in the middle, and E is the embedding dimension, while the dimension of head vector h k is E which is the same as embedding vector e i from Eqn.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_29",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_29@1",
            "content": "3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_29",
            "start": 185,
            "end": 186,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_30@0",
            "content": "Additive Attention Layer",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_30",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_31@0",
            "content": "For a given number of attention heads K, we have a group of head vectors H = {h 1 , h 2 , . . . , h K }, which are fed into an additive attention network to generate the document-topic distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_31",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_32@0",
            "content": "\u00b5 k = c tanh (W H h k + b H ) \u03b2 k = e \u00b5 k K i e \u00b5 i (4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_32",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_33@0",
            "content": "Finally, the document representation d is the weighted sum of head vectors along with the weights distribution \u03b2 :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_33",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_34@0",
            "content": "d = K i \u03b2 k h k (5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_34",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_35@0",
            "content": "where trained parameters are",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_35",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_36@0",
            "content": "c \u2208 R D h , W H \u2208 R E\u00d7D h , b H \u2208 R D h",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_36",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_37@0",
            "content": ", and the dimension of d is also E which is the same as h k .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_37",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_38@0",
            "content": "Classification Layer",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_38",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_39@0",
            "content": "Since the representation of each document d will be a dense vector containing a mixture of information about the document's content, we can use it as the feature vector for the final news classification task:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_39",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_40@0",
            "content": "y = sof tmax (W C d + b C ) (6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_40",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_41@0",
            "content": "Entropy Constraint",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_41",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_42@0",
            "content": "In order to further improve the explainability of our base model as described above, we now adjust the model so that each head only focuses on a specific set of words -i.e. we enforce topic-word weights distribution \u03b1 k not to spread over the document widely.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_42",
            "start": 0,
            "end": 258,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_42@1",
            "content": "We do this by computing the entropy of \u03b1 k as a part of the loss function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_42",
            "start": 260,
            "end": 333,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_42@2",
            "content": "The entropy constraint penalizes the model when \u03b1 k has high entropy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_42",
            "start": 335,
            "end": 403,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_42@3",
            "content": "Thus, the final loss with entropy constraint for the news classification task is:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_42",
            "start": 405,
            "end": 485,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_43@0",
            "content": "L = L CE (y, \u0177) + \u03bb K k E doc \u03b1 k K (7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_43",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_44@0",
            "content": "where L CE (y, \u0177) is the Cross-Entropy Loss between ground-truth class and predicted class, and \u03bb is a hyper-parameter to scale the magnitude of average entropy calculated by \u03b1 k .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_44",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_44@1",
            "content": "The calculation for corresponding entropy L entropy is by:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_44",
            "start": 181,
            "end": 238,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_45@0",
            "content": "E doc \u03b1 k = \u2212 N i \u03b1 k i log \u03b1 k i (8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_45",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_46@0",
            "content": "The entropy constraint applied on document-level in Eqn. 8 changes the distribution of topic-word weights \u03b1 k .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_46",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_46@1",
            "content": "However, our goal is to find more diverse topics, which means different topics should focus on different words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_46",
            "start": 112,
            "end": 222,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_46@2",
            "content": "Therefore, it is necessary to know how entropy decreases at the token level (i.e. across the vocabulary as shown in Figure 2), which is defined by: To distinguish between the two variants of our model, we name the basic model as BATM-Base and use BATM-EC refer to the model with entropy constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_46",
            "start": 224,
            "end": 522,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_46@3",
            "content": "From Eqn. 7, it is evident that if we set \u03bb as 0, BATM-EC will be equivalent to the basic model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_46",
            "start": 524,
            "end": 619,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_47@0",
            "content": "E token (M i ) = \u2212 K k M k i log M k i (9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_47",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_48@0",
            "content": "Generating the Topic Distribution",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_48",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_49@0",
            "content": "After training our proposed BATM model, we analyze the attention weights generated from the first attention layer (MHA) over the corpus vocabulary to generate a global topic distribution. Let us assume that there are V words in the corpus and we have K heads corresponding to K topics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_49",
            "start": 0,
            "end": 284,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_49@1",
            "content": "The resulting topic distribution takes the form of a V \u00d7 K weight matrix, calculated from a trained MHA layer using embedded word vectors as inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_49",
            "start": 286,
            "end": 433,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_50@0",
            "content": "Moreover, to identify the most important words for each topic (which we can view as being the topic's descriptor), we extract the top-T words from the topic distribution, which can help us understand the heads and interpret them as topics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_50",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_50@1",
            "content": "We examine the interpretations of these topic descriptors and display some examples in Section 4.5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_50",
            "start": 240,
            "end": 338,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_51@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_51",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_52@0",
            "content": "We now evaluate the BATM model on two largescale real-world datasets, and compare its performance with a number of state-of-the-art methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_52",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_53@0",
            "content": "Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_53",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_54@0",
            "content": "We evaluate our proposed model on a news classification task and conduct extensive experiments on two public corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_54",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_54@1",
            "content": "MIND (Wu et al., 2020) is a large-scale English dataset for news recommendation and categorization tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_54",
            "start": 118,
            "end": 222,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_54@2",
            "content": "It contains information such as story title, abstract, and news category, but the public version does not include full article body content.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_54",
            "start": 224,
            "end": 363,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_54@3",
            "content": "We collected news articles from the Microsoft news website 2 to supplement it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_54",
            "start": 365,
            "end": 442,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_54@4",
            "content": "There are 18 categories in the original MINDlarge dataset, but three of them only have a small number of articles (< 10).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_54",
            "start": 444,
            "end": 564,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_54@5",
            "content": "Therefore, we exclude these categories from our experiment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_54",
            "start": 566,
            "end": 624,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_54@6",
            "content": "The second one is the News Category Dataset 3 , which contains approximately 200k news articles (each of them include a headline and a short news description) from 2012 to 2018 obtained from HuffPost.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_54",
            "start": 626,
            "end": 825,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_54@7",
            "content": "The original dataset has 41 categories, but some of these are duplicates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_54",
            "start": 827,
            "end": 899,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_54@8",
            "content": "After merging the duplicated categories, there are 26 categories remain, which is denoted as News-26.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_54",
            "start": 901,
            "end": 1001,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_54@9",
            "content": "We randomly split these two datasets into training/validation/test sets with a 80/10/10 split.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_54",
            "start": 1003,
            "end": 1096,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_54@10",
            "content": "Table 1 summarizes the divisions and the key statistics of the datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_54",
            "start": 1098,
            "end": 1169,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_55@0",
            "content": "Baseline Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_55",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_56@0",
            "content": "For the purpose of assessing classification performance, we first compare the effectiveness of our BATM base model relative to a number of attentionbased and pre-trained language models:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_56",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_57@0",
            "content": "\u2022 BERT (Devlin et al., 2019) composes of a bidirectional encoder of transformer and is pre-trained by using a combination of masked language modeling objective and next sentence prediction on a large corpus; \u2022 DistilBERT (Sanh et al., 2019) is a small, fast, cheap, and light transformer model trained by distilling BERT base; \u2022 XLNet is an extension of the Transformer-XL model, which utilizes an autoregressive method to learn bidirectional contexts by maximizing the expected likelihood over all permutations of input sequence factorization order; \u2022 Roberta (Liu et al., 2019) is a robustly optimized BERT that modifies key hyperparameters, removing the next-sentence pre-training objective and training with much larger minibatches and learning rates; \u2022 Longformer (Beltagy et al., 2020) is based on RoBERTa (Liu et al., 2019) and uses sliding window attention and global attention to model local and global contexts; \u2022 Fastformer (Wu et al., 2021) uses additive attention to perform multi-head attention, which is more efficient than a standard transformer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_57",
            "start": 0,
            "end": 1061,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_58@0",
            "content": "The initial weights of these pre-trained language models (BERT, DistilBERT, XLNet, Roberta, and Longformer) are provided by Hugging Face Transformer (Wolf et al., 2020) library 4 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_58",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_58@1",
            "content": "We use a linear classifier to receive the pooled output from previous transformer layers and then fine-tune these models to adapt them to the classification task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_58",
            "start": 181,
            "end": 342,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_58@2",
            "content": "For the attention-based model, Fastformer, we initialize its embedding matrix using GloVe embedding and follow the hyper-parameter settings in (Wu et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_58",
            "start": 344,
            "end": 504,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_59@0",
            "content": "Experimental Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_59",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_60@0",
            "content": "In our experiments, we consider two ways to initialize our embedding matrix: GloVe embedding (Pennington et al., 2014) and context embeddings from a pre-trained language model DistilBERT (Sanh et al., 2019), where embedding weights are not fixed during the training procedure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_60",
            "start": 0,
            "end": 275,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_60@1",
            "content": "We examine how different number of heads would influence the performance of our proposed model on the validation set, the details is shown in Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_60",
            "start": 277,
            "end": 427,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_60@2",
            "content": "Unsurprisingly, on the MIND data set, the model needs to set a relatively larger number of topics, because the average length of news articles in the MIND dataset and its vocabulary size are much larger than the News-26 dataset, as indicated in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_60",
            "start": 429,
            "end": 681,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_60@3",
            "content": "We identify the number of topics for MIND-15 and News-26 as 180 and 30 for the rest of experiments, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_60",
            "start": 683,
            "end": 795,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_60@4",
            "content": "We use Adam (Kingma and Ba, 2015) for model optimization, and each epoch decays the learning rate by half.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_60",
            "start": 797,
            "end": 902,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_61@0",
            "content": "Performance Comparison",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_61",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_62@0",
            "content": "The large pre-trained transformer variants perform better than the model with GloVe embedding, both for MIND-15 and News-26.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_62",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_62@1",
            "content": "Compared to Fastformer-GloVe, our BATM-Base-GloVe model achieves a similar result (variance in 0.3% of accuracy and 0.4% of Macro-F) for MIND-15 and a better result (variance in almost 0.4% of accuracy and 0.6% of Macro-F) for News-26.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_62",
            "start": 125,
            "end": 359,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_62@2",
            "content": "The differing results in MIND-15 and News-26 are due to the length of articles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_62",
            "start": 361,
            "end": 439,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_62@3",
            "content": "As an efficient Fastformer can take a much longer sequence as input, it is advantageous to deal with long sequences which are unavailable in a short-length news dataset such as News-26.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_62",
            "start": 441,
            "end": 625,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_62@4",
            "content": "Using the pre-trained transformer-based embedding greatly improves the performance of our proposed BATM-Base model compared to the GloVe embedding, although it adds to the difficulty of interpretation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_62",
            "start": 627,
            "end": 827,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_62@5",
            "content": "The performance difference of the other pre-trained language models with the BATM-Base-DB model is less than 1% accuracy and approximately 2% Macro-F, both for MIND-15 and News-26.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_62",
            "start": 829,
            "end": 1008,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_62@6",
            "content": "These experiments demonstrate the effectiveness of our proposed model in constructing document representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_62",
            "start": 1010,
            "end": 1120,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_62@7",
            "content": "Thus, the analysis of BATM's behavior using the topic-word distribution and document-topic distribution is essential to understanding the role of Bi-level attention layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_62",
            "start": 1122,
            "end": 1293,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_63@0",
            "content": "Evaluation of Global Topic Representation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_63",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_64@0",
            "content": "Besides the classification performance, we are also interested in whether each extracted topic descriptor as described in 3.6 has an intuitive meaning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_64",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_65@0",
            "content": "We take the top-25 highest scoring terms from each topic and calculate topic coherence scores C v (R\u00f6der et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_65",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_65@1",
            "content": "The average coherence scores of all topics of the BATM-Base-GloVe model are 0.58 and 0.56 on the MIND dataset and the news category datasets, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_65",
            "start": 120,
            "end": 274,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_65@2",
            "content": "Moreover, to more intuitively understand the meaning of topics mined by our model, we list a few topic examples whose coherence scores range from 0.3 to 0.8 along with a manually-assigned label in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_65",
            "start": 276,
            "end": 480,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_65@3",
            "content": "The topics with coherence scores between 0.55 and 0.8 usually have precise meanings, such as the topic labeled as \"Partisan\" score of 0.76, where the vast majority of words are related to political activities and elections.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_65",
            "start": 482,
            "end": 704,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_65@4",
            "content": "However, some topics with a score in the range of 0.55 \u223c 0.8 are still tough to surmise the focus, as the unknown topic (labeled as \"Unknown\" with C v value is 0.61) suggest, where the correlation of topic descriptors is non-intuitive.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_65",
            "start": 706,
            "end": 940,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_65@5",
            "content": "In contrast, some low-coherence topics may contain highly relevant words as well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_65",
            "start": 942,
            "end": 1022,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_65@6",
            "content": "For example, the topic \"Schedule\" a with a score of 0.38 (under 0.55) mainly includes words related to time and arrangement, which we can comprehend the central point of these words, but the automated metric unfairly evaluates it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_65",
            "start": 1024,
            "end": 1253,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_65@7",
            "content": "Therefore, with the auxiliary of topic coherence measurement and manual verification, we are firmly convinced that topic descriptors extracted by the BATM-Base-GloVe model indeed have specific meanings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_65",
            "start": 1255,
            "end": 1456,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_66@0",
            "content": "Effect of Entropy Constraints",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_66",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_67@0",
            "content": "In the previous sections, the proposed BATM-Base-GloVe model demonstrates its competitive classification performance and excellent explainability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_67",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_68@0",
            "content": "We now study the effect of adding an entropy constraint, as discussed in Section 3.5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_68",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_68@1",
            "content": "In the extended model, referred to as BATM-EC, \u03bb determines the degree of constraint that is imposed, so the BATM-Base-GloVe model is a special case when \u03bb is zero.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_68",
            "start": 86,
            "end": 249,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_68@2",
            "content": "This study assumes that a good topic (a first-level of attention) should only focus on specific words related to that topic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_68",
            "start": 251,
            "end": 374,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_68@3",
            "content": "Its weight distribution on a news article should not be flat for the whole document, while its global weight distribution should also not be widely spread out across the entire vocabulary (i.e., it should have a relatively lower entropy ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_68",
            "start": 376,
            "end": 614,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_68@4",
            "content": "Therefore, we observe the dynamic of two entropy metrics E doc and E token (see calculation in Eqn. 8 and Eqn. 9) by setting different values of \u03bb.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_68",
            "start": 616,
            "end": 762,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_68@5",
            "content": "We present the performance and entropy changes along with the values of \u03bb in Table 4 The results meet our expectations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_68",
            "start": 764,
            "end": 882,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_68@6",
            "content": "When \u03bb reaches le-4, both entropy indicators decrease significantly with an acceptable trade-off in classification performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_68",
            "start": 884,
            "end": 1010,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_68@7",
            "content": "When continually increasing the impact of entropy constraints, both entropy indicators and classification performance decrease dramatically.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_68",
            "start": 1012,
            "end": 1151,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_68@8",
            "content": "This is reasonable, as this experiment is conducted with a fixed number of heads.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_68",
            "start": 1153,
            "end": 1233,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_68@9",
            "content": "When attention focuses on a minimal number of topics, and the number of topics does not increase accordingly, information within article texts is likely to be lost, affecting the classification performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_68",
            "start": 1235,
            "end": 1440,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_69@0",
            "content": "Discussion and Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_69",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_70@0",
            "content": "While the variant of our proposed model, BATMbase-DB, which is initialized by the contextual embeddings, can outperform all alternatives, the meaning of its topics is much worse than BATM-Base-GloVe.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_70",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_70@1",
            "content": "Each contextual embedding learned by pre-trained language models will merge the information from its surrounding words, which increases the difficulties of the proposed attention layer to capture the topics it focuses on, thus leading to more noise in their representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_70",
            "start": 200,
            "end": 473,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_71@0",
            "content": "Another challenge we will address in the future is how to balance the computation cost, topic granularity, and classification performances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_71",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_71@1",
            "content": "As discussed in the previous sections, it will affect the model's classification performance if we only introduce entropy constraints without incrementing the number of attention heads.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_71",
            "start": 140,
            "end": 324,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_71@2",
            "content": "However, increasing the number of attention heads will lead to the proportional increment of parameters, increasing the complexity of the model and resulting in a high computation cost.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_71",
            "start": 326,
            "end": 510,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_71@3",
            "content": "We will consider increasing the number of heads and the extending entropy constraint further, to improve classification performance while maintaining strong explainability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_71",
            "start": 512,
            "end": 683,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_72@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_72",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_73@0",
            "content": "In this paper, we presented a novel approach that harnesses a bi-level attention framework to decouple the text classification process as topic capturing, topic importance recognition and decision-making process to benefit explainability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_73",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_73@1",
            "content": "We conducted the experiments on two large-scale text corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_73",
            "start": 239,
            "end": 299,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_73@2",
            "content": "Compared with a number of state-of-the-art alternatives on a text classification task, our model can not only achieve a competitive performance, but also demonstrates a strong ability to capture intuitive meanings in the form of topical features, thus improving its explainability and transparency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_73",
            "start": 301,
            "end": 598,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_73@3",
            "content": "In addition, by initializing it with contextual embeddings, our model outperforms all the baseline models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_73",
            "start": 600,
            "end": 705,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_74@0",
            "content": "UNKNOWN, None, 2015, Neural machine translation by jointly learning to align and translate, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_74",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_75@0",
            "content": "Jasmijn Bastings, Katja Filippova, The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?, 2020, Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_75",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_76@0",
            "content": "UNKNOWN, None, 2004, Longformer: The long-document transformer. ArXiv, abs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_76",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_77@0",
            "content": "M David,  Blei, Y Andrew, Michael I Jordan Ng, None, 2003, Latent dirichlet allocation. the Journal of machine Learning research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_77",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_78@0",
            "content": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, Ruslan Salakhutdinov, Transformer-XL: Attentive language models beyond a fixed-length context, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_78",
            "start": 0,
            "end": 295,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_79@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_79",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_80@0",
            "content": "Alon Jacovi, Yoav Goldberg, Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness?, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_80",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_81@0",
            "content": "Sarthak Jain, Byron Wallace, Attention is not Explanation, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_81",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_82@0",
            "content": "UNKNOWN, None, 2021, Ammus: A survey of transformer-based pretrained models in natural language processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_82",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_83@0",
            "content": "UNKNOWN, None, 2015, Adam: A method for stochastic optimization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_83",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_84@0",
            "content": "Olga Kovaleva, Alexey Romanov, Anna Rogers, Anna Rumshisky, Revealing the dark secrets of BERT, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_84",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_85@0",
            "content": "UNKNOWN, None, 1907, Roberta: A robustly optimized bert pretraining approach. ArXiv, abs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_85",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_86@0",
            "content": "Yichao Lu, Ruihai Dong, Barry Smyth, Coevolutionary recommendation model: Mutual learning between ratings and reviews, 2018, Proceedings of the 2018 World Wide Web Conference, WWW '18, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_86",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_87@0",
            "content": "Thang Luong, Hieu Pham, Christopher Manning, Effective approaches to attention-based neural machine translation, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_87",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_88@0",
            "content": "Madhura Pande, Aakriti Budhraja, Preksha Nema, Pratyush Kumar, Mitesh Khapra, The heads hypothesis: A unifying statistical approach towards understanding multi-headed attention in bert, 2021, AAAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_88",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_89@0",
            "content": "Madhur Panwar, Shashank Shailabh, Milan Aggarwal, Balaji Krishnamurthy, TAN-NTM: Topic attention networks for neural topic modeling, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_89",
            "start": 0,
            "end": 314,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_90@0",
            "content": "Jeffrey Pennington, Richard Socher, Christopher Manning, GloVe: Global vectors for word representation, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_90",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_91@0",
            "content": "UNKNOWN, None, 2018, Improving language understanding by generative pre-training, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_91",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_92@0",
            "content": "Michael R\u00f6der, Andreas Both, Alexander Hinneburg, Exploring the space of topic coherence measures, 2015, Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_92",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_93@0",
            "content": "Anna Rogers, Olga Kovaleva, Anna Rumshisky, A primer in bertology: What we know about how bert works, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_93",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_94@0",
            "content": "Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, 2019, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_94",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_95@0",
            "content": "Sofia Serrano, Noah Smith, Is attention interpretable?, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_95",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_96@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_96",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_97@0",
            "content": "Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov, Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_97",
            "start": 0,
            "end": 306,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_98@0",
            "content": "Sarah Wiegreffe, Yuval Pinter, Attention is not not explanation, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_98",
            "start": 0,
            "end": 289,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_99@0",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger,  Drame, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_99",
            "start": 0,
            "end": 463,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_100@0",
            "content": "Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, Fastformer: Additive attention can be all you need, 2021, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_100",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_101@0",
            "content": "Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, Ming Zhou, MIND: A large-scale dataset for news recommendation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_101",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_102@0",
            "content": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, Yoshua Bengio, Show, attend and tell: Neural image caption generation with visual attention, 2015, International conference on machine learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_102",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_103@0",
            "content": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, V Quoc,  Le, Xlnet: Generalized autoregressive pretraining for language understanding, 2019, NeurIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_103",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "91-ARR_v2_104@0",
            "content": "Lingyun Zhao, Lin Li, Xinhao Zheng, Jianwei Zhang, A BERT based sentiment analysis and key entity detection approach for online financial texts, 2021, 2021 IEEE 24th International Conference on Computer Supported Cooperative Work in Design (CSCWD), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "91-ARR_v2_104",
            "start": 0,
            "end": 253,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "91-ARR_v2_0",
            "tgt_ix": "91-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_0",
            "tgt_ix": "91-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_1",
            "tgt_ix": "91-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_1",
            "tgt_ix": "91-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_0",
            "tgt_ix": "91-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_2",
            "tgt_ix": "91-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_4",
            "tgt_ix": "91-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_5",
            "tgt_ix": "91-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_3",
            "tgt_ix": "91-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_3",
            "tgt_ix": "91-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_3",
            "tgt_ix": "91-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_3",
            "tgt_ix": "91-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_0",
            "tgt_ix": "91-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_6",
            "tgt_ix": "91-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_7",
            "tgt_ix": "91-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_7",
            "tgt_ix": "91-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_9",
            "tgt_ix": "91-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_8",
            "tgt_ix": "91-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_8",
            "tgt_ix": "91-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_8",
            "tgt_ix": "91-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_7",
            "tgt_ix": "91-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_10",
            "tgt_ix": "91-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_11",
            "tgt_ix": "91-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_11",
            "tgt_ix": "91-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_7",
            "tgt_ix": "91-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_12",
            "tgt_ix": "91-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_14",
            "tgt_ix": "91-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_13",
            "tgt_ix": "91-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_13",
            "tgt_ix": "91-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_13",
            "tgt_ix": "91-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_0",
            "tgt_ix": "91-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_15",
            "tgt_ix": "91-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_16",
            "tgt_ix": "91-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_16",
            "tgt_ix": "91-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_16",
            "tgt_ix": "91-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_17",
            "tgt_ix": "91-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_18",
            "tgt_ix": "91-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_18",
            "tgt_ix": "91-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_16",
            "tgt_ix": "91-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_19",
            "tgt_ix": "91-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_21",
            "tgt_ix": "91-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_22",
            "tgt_ix": "91-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_23",
            "tgt_ix": "91-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_24",
            "tgt_ix": "91-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_25",
            "tgt_ix": "91-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_26",
            "tgt_ix": "91-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_27",
            "tgt_ix": "91-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_28",
            "tgt_ix": "91-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_20",
            "tgt_ix": "91-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_20",
            "tgt_ix": "91-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_20",
            "tgt_ix": "91-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_20",
            "tgt_ix": "91-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_20",
            "tgt_ix": "91-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_20",
            "tgt_ix": "91-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_20",
            "tgt_ix": "91-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_20",
            "tgt_ix": "91-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_20",
            "tgt_ix": "91-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_20",
            "tgt_ix": "91-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_16",
            "tgt_ix": "91-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_29",
            "tgt_ix": "91-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_31",
            "tgt_ix": "91-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_32",
            "tgt_ix": "91-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_33",
            "tgt_ix": "91-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_34",
            "tgt_ix": "91-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_35",
            "tgt_ix": "91-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_36",
            "tgt_ix": "91-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_30",
            "tgt_ix": "91-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_30",
            "tgt_ix": "91-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_30",
            "tgt_ix": "91-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_30",
            "tgt_ix": "91-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_30",
            "tgt_ix": "91-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_30",
            "tgt_ix": "91-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_30",
            "tgt_ix": "91-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_30",
            "tgt_ix": "91-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_16",
            "tgt_ix": "91-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_37",
            "tgt_ix": "91-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_39",
            "tgt_ix": "91-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_38",
            "tgt_ix": "91-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_38",
            "tgt_ix": "91-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_38",
            "tgt_ix": "91-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_16",
            "tgt_ix": "91-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_40",
            "tgt_ix": "91-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_42",
            "tgt_ix": "91-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_43",
            "tgt_ix": "91-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_44",
            "tgt_ix": "91-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_45",
            "tgt_ix": "91-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_46",
            "tgt_ix": "91-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_41",
            "tgt_ix": "91-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_41",
            "tgt_ix": "91-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_41",
            "tgt_ix": "91-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_41",
            "tgt_ix": "91-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_41",
            "tgt_ix": "91-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_41",
            "tgt_ix": "91-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_41",
            "tgt_ix": "91-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_16",
            "tgt_ix": "91-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_47",
            "tgt_ix": "91-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_49",
            "tgt_ix": "91-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_48",
            "tgt_ix": "91-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_48",
            "tgt_ix": "91-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_48",
            "tgt_ix": "91-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_0",
            "tgt_ix": "91-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_50",
            "tgt_ix": "91-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_51",
            "tgt_ix": "91-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_51",
            "tgt_ix": "91-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_51",
            "tgt_ix": "91-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_52",
            "tgt_ix": "91-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_53",
            "tgt_ix": "91-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_53",
            "tgt_ix": "91-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_51",
            "tgt_ix": "91-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_54",
            "tgt_ix": "91-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_56",
            "tgt_ix": "91-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_55",
            "tgt_ix": "91-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_55",
            "tgt_ix": "91-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_55",
            "tgt_ix": "91-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_55",
            "tgt_ix": "91-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_51",
            "tgt_ix": "91-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_58",
            "tgt_ix": "91-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_59",
            "tgt_ix": "91-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_59",
            "tgt_ix": "91-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_51",
            "tgt_ix": "91-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_60",
            "tgt_ix": "91-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_61",
            "tgt_ix": "91-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_61",
            "tgt_ix": "91-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_51",
            "tgt_ix": "91-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_62",
            "tgt_ix": "91-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_64",
            "tgt_ix": "91-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_63",
            "tgt_ix": "91-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_63",
            "tgt_ix": "91-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_63",
            "tgt_ix": "91-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_0",
            "tgt_ix": "91-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_65",
            "tgt_ix": "91-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_67",
            "tgt_ix": "91-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_66",
            "tgt_ix": "91-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_66",
            "tgt_ix": "91-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_66",
            "tgt_ix": "91-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_0",
            "tgt_ix": "91-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_68",
            "tgt_ix": "91-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_70",
            "tgt_ix": "91-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_69",
            "tgt_ix": "91-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_69",
            "tgt_ix": "91-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_69",
            "tgt_ix": "91-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_0",
            "tgt_ix": "91-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_71",
            "tgt_ix": "91-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_72",
            "tgt_ix": "91-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_72",
            "tgt_ix": "91-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "91-ARR_v2_0",
            "tgt_ix": "91-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_1",
            "tgt_ix": "91-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_2",
            "tgt_ix": "91-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_2",
            "tgt_ix": "91-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_2",
            "tgt_ix": "91-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_2",
            "tgt_ix": "91-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_2",
            "tgt_ix": "91-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_2",
            "tgt_ix": "91-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_3",
            "tgt_ix": "91-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_4",
            "tgt_ix": "91-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_4",
            "tgt_ix": "91-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_4",
            "tgt_ix": "91-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_4",
            "tgt_ix": "91-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_4",
            "tgt_ix": "91-ARR_v2_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_5",
            "tgt_ix": "91-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_5",
            "tgt_ix": "91-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_5",
            "tgt_ix": "91-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_5",
            "tgt_ix": "91-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_5",
            "tgt_ix": "91-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_6",
            "tgt_ix": "91-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_6",
            "tgt_ix": "91-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_6",
            "tgt_ix": "91-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_6",
            "tgt_ix": "91-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_6",
            "tgt_ix": "91-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_6",
            "tgt_ix": "91-ARR_v2_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_6",
            "tgt_ix": "91-ARR_v2_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_7",
            "tgt_ix": "91-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_8",
            "tgt_ix": "91-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_9",
            "tgt_ix": "91-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_9",
            "tgt_ix": "91-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_9",
            "tgt_ix": "91-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_9",
            "tgt_ix": "91-ARR_v2_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_9",
            "tgt_ix": "91-ARR_v2_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_10",
            "tgt_ix": "91-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_10",
            "tgt_ix": "91-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_10",
            "tgt_ix": "91-ARR_v2_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_10",
            "tgt_ix": "91-ARR_v2_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_10",
            "tgt_ix": "91-ARR_v2_10@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_10",
            "tgt_ix": "91-ARR_v2_10@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_11",
            "tgt_ix": "91-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_12",
            "tgt_ix": "91-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_12",
            "tgt_ix": "91-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_12",
            "tgt_ix": "91-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_12",
            "tgt_ix": "91-ARR_v2_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_12",
            "tgt_ix": "91-ARR_v2_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_12",
            "tgt_ix": "91-ARR_v2_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_13",
            "tgt_ix": "91-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_14",
            "tgt_ix": "91-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_14",
            "tgt_ix": "91-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_14",
            "tgt_ix": "91-ARR_v2_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_14",
            "tgt_ix": "91-ARR_v2_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_15",
            "tgt_ix": "91-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_15",
            "tgt_ix": "91-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_15",
            "tgt_ix": "91-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_15",
            "tgt_ix": "91-ARR_v2_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_16",
            "tgt_ix": "91-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_17",
            "tgt_ix": "91-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_17",
            "tgt_ix": "91-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_17",
            "tgt_ix": "91-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_17",
            "tgt_ix": "91-ARR_v2_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_17",
            "tgt_ix": "91-ARR_v2_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_17",
            "tgt_ix": "91-ARR_v2_17@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_17",
            "tgt_ix": "91-ARR_v2_17@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_17",
            "tgt_ix": "91-ARR_v2_17@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_18",
            "tgt_ix": "91-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_19",
            "tgt_ix": "91-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_19",
            "tgt_ix": "91-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_19",
            "tgt_ix": "91-ARR_v2_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_19",
            "tgt_ix": "91-ARR_v2_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_19",
            "tgt_ix": "91-ARR_v2_19@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_20",
            "tgt_ix": "91-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_21",
            "tgt_ix": "91-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_21",
            "tgt_ix": "91-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_22",
            "tgt_ix": "91-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_23",
            "tgt_ix": "91-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_24",
            "tgt_ix": "91-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_25",
            "tgt_ix": "91-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_26",
            "tgt_ix": "91-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_27",
            "tgt_ix": "91-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_28",
            "tgt_ix": "91-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_29",
            "tgt_ix": "91-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_29",
            "tgt_ix": "91-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_30",
            "tgt_ix": "91-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_31",
            "tgt_ix": "91-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_32",
            "tgt_ix": "91-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_33",
            "tgt_ix": "91-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_34",
            "tgt_ix": "91-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_35",
            "tgt_ix": "91-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_36",
            "tgt_ix": "91-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_37",
            "tgt_ix": "91-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_38",
            "tgt_ix": "91-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_39",
            "tgt_ix": "91-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_40",
            "tgt_ix": "91-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_41",
            "tgt_ix": "91-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_42",
            "tgt_ix": "91-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_42",
            "tgt_ix": "91-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_42",
            "tgt_ix": "91-ARR_v2_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_42",
            "tgt_ix": "91-ARR_v2_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_43",
            "tgt_ix": "91-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_44",
            "tgt_ix": "91-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_44",
            "tgt_ix": "91-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_45",
            "tgt_ix": "91-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_46",
            "tgt_ix": "91-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_46",
            "tgt_ix": "91-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_46",
            "tgt_ix": "91-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_46",
            "tgt_ix": "91-ARR_v2_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_47",
            "tgt_ix": "91-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_48",
            "tgt_ix": "91-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_49",
            "tgt_ix": "91-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_49",
            "tgt_ix": "91-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_50",
            "tgt_ix": "91-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_50",
            "tgt_ix": "91-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_51",
            "tgt_ix": "91-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_52",
            "tgt_ix": "91-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_53",
            "tgt_ix": "91-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_54",
            "tgt_ix": "91-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_54",
            "tgt_ix": "91-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_54",
            "tgt_ix": "91-ARR_v2_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_54",
            "tgt_ix": "91-ARR_v2_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_54",
            "tgt_ix": "91-ARR_v2_54@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_54",
            "tgt_ix": "91-ARR_v2_54@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_54",
            "tgt_ix": "91-ARR_v2_54@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_54",
            "tgt_ix": "91-ARR_v2_54@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_54",
            "tgt_ix": "91-ARR_v2_54@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_54",
            "tgt_ix": "91-ARR_v2_54@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_54",
            "tgt_ix": "91-ARR_v2_54@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_55",
            "tgt_ix": "91-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_56",
            "tgt_ix": "91-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_57",
            "tgt_ix": "91-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_58",
            "tgt_ix": "91-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_58",
            "tgt_ix": "91-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_58",
            "tgt_ix": "91-ARR_v2_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_59",
            "tgt_ix": "91-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_60",
            "tgt_ix": "91-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_60",
            "tgt_ix": "91-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_60",
            "tgt_ix": "91-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_60",
            "tgt_ix": "91-ARR_v2_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_60",
            "tgt_ix": "91-ARR_v2_60@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_61",
            "tgt_ix": "91-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_62",
            "tgt_ix": "91-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_62",
            "tgt_ix": "91-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_62",
            "tgt_ix": "91-ARR_v2_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_62",
            "tgt_ix": "91-ARR_v2_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_62",
            "tgt_ix": "91-ARR_v2_62@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_62",
            "tgt_ix": "91-ARR_v2_62@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_62",
            "tgt_ix": "91-ARR_v2_62@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_62",
            "tgt_ix": "91-ARR_v2_62@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_63",
            "tgt_ix": "91-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_64",
            "tgt_ix": "91-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_65",
            "tgt_ix": "91-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_65",
            "tgt_ix": "91-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_65",
            "tgt_ix": "91-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_65",
            "tgt_ix": "91-ARR_v2_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_65",
            "tgt_ix": "91-ARR_v2_65@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_65",
            "tgt_ix": "91-ARR_v2_65@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_65",
            "tgt_ix": "91-ARR_v2_65@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_65",
            "tgt_ix": "91-ARR_v2_65@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_66",
            "tgt_ix": "91-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_67",
            "tgt_ix": "91-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_68",
            "tgt_ix": "91-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_68",
            "tgt_ix": "91-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_68",
            "tgt_ix": "91-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_68",
            "tgt_ix": "91-ARR_v2_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_68",
            "tgt_ix": "91-ARR_v2_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_68",
            "tgt_ix": "91-ARR_v2_68@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_68",
            "tgt_ix": "91-ARR_v2_68@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_68",
            "tgt_ix": "91-ARR_v2_68@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_68",
            "tgt_ix": "91-ARR_v2_68@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_68",
            "tgt_ix": "91-ARR_v2_68@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_69",
            "tgt_ix": "91-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_70",
            "tgt_ix": "91-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_70",
            "tgt_ix": "91-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_71",
            "tgt_ix": "91-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_71",
            "tgt_ix": "91-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_71",
            "tgt_ix": "91-ARR_v2_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_71",
            "tgt_ix": "91-ARR_v2_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_72",
            "tgt_ix": "91-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_73",
            "tgt_ix": "91-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_73",
            "tgt_ix": "91-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_73",
            "tgt_ix": "91-ARR_v2_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_73",
            "tgt_ix": "91-ARR_v2_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_74",
            "tgt_ix": "91-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_75",
            "tgt_ix": "91-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_76",
            "tgt_ix": "91-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_77",
            "tgt_ix": "91-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_78",
            "tgt_ix": "91-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_79",
            "tgt_ix": "91-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_80",
            "tgt_ix": "91-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_81",
            "tgt_ix": "91-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_82",
            "tgt_ix": "91-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_83",
            "tgt_ix": "91-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_84",
            "tgt_ix": "91-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_85",
            "tgt_ix": "91-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_86",
            "tgt_ix": "91-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_87",
            "tgt_ix": "91-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_88",
            "tgt_ix": "91-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_89",
            "tgt_ix": "91-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_90",
            "tgt_ix": "91-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_91",
            "tgt_ix": "91-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_92",
            "tgt_ix": "91-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_93",
            "tgt_ix": "91-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_94",
            "tgt_ix": "91-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_95",
            "tgt_ix": "91-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_96",
            "tgt_ix": "91-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_97",
            "tgt_ix": "91-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_98",
            "tgt_ix": "91-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_99",
            "tgt_ix": "91-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_100",
            "tgt_ix": "91-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_101",
            "tgt_ix": "91-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_102",
            "tgt_ix": "91-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_103",
            "tgt_ix": "91-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "91-ARR_v2_104",
            "tgt_ix": "91-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 700,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "91-ARR",
        "version": 2
    }
}