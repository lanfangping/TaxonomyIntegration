{
    "nodes": [
        {
            "ix": "41-ARR_v2_0",
            "content": "Contrastive Learning for Prompt-Based Few-Shot Language Learners",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_2",
            "content": "The impressive performance of GPT-3 using natural language prompts and in-context learning has inspired work on better fine-tuning of moderately-sized models under this paradigm. Following this line of work, we present a contrastive learning framework that clusters inputs from the same class for better generality of models trained with only limited examples. Specifically, we propose a supervised contrastive framework that clusters inputs from the same class under different augmented \"views\" and repel the ones from different classes. We create different \"views\" of an example by appending it with different language prompts and contextual demonstrations. Combining a contrastive loss with the standard masked language modeling (MLM) loss in prompt-based few-shot learners, the experimental results show that our method can improve over the state-of-the-art methods in a diverse set of 15 language tasks. Our framework makes minimal assumptions on the task or the base model, and can be applied to many recent methods with little modification. The code will be made available at: https://github.com/yiren-jian/LM-SupCon.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "41-ARR_v2_4",
            "content": "The prompt-based fine-tuning method reduces the gap between pre-training and fine-tuning by forming the fine-tuning task into a masking language problem. A language prompt is a piece of text appended to the query input enabling the model to come up with better predictions (Schick and Sch\u00fctze, 2021;Tam et al., 2021). For instance, by feeding a language model with \"The story is not worth reading, a truly one.\", the model assigns a higher probability for the blank to be filled with \"terrible\" than \"great\". Here, \"a truly one.\" is called the template of the prompt and \"terrible\" or \"great\" is the label word. Recently, LM-BFF shows that appending demonstrations (e.g.\"This is an amazing movie, a truly great one\") to inputs can help the model to better understand the label word, leading to further improved results.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_5",
            "content": "In this work, we show that Supervised Contrastive Learning (SupCon) (Khosla et al., 2020) at the feature space can be beneficial during the finetuning of prompt-based few-shot language learners, with proper data augmentation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_6",
            "content": "Data augmentation is the key component of SupCon. While there exists many augmentation techniques like Cutmix (Yun et al., 2019), Mixup (Zhang et al., 2018) in computer vision and EDA (Wei and Zou, 2019), AEDA (Karimi et al., 2021) for text, data augmentation remains challenging.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_7",
            "content": "However, prompt-based few-shot learners with demonstrations actually provide us with a natural way to create multiple \"views\" (augmentations) of a single example, i.e., for a fixed set of label words, we can sample different templates and different demonstrations to append to the input text (shown in Figure 1). This allows us to construct diverse input texts that are consistent and complete. By applying SupCon to cluster the above two example inputs with very different contents but the same label, our method is able to obtain an additional supervision at the feature space which is crucial if we are only given a few labeled examples.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_8",
            "content": "The main contributions of our paper are:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_9",
            "content": "\u2022 A Supervised Contrastive Learning framework for prompt-based few-shot learners. \u2022 An effective data augmentation method using prompts for contrastive learning with promptbased learners.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_10",
            "content": "Related Work & Background",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "41-ARR_v2_11",
            "content": "Few-shot Learning is often tackled by meta learning (Li and Zhang, 2021;Bansal et al., 2020;Sharaf et al., 2020;Jian et al., 2020;Jian and Gao, 2021), data augmentation Jian and Torresani, 2022;Arthaud et al., 2021;Wei et al., 2021; Encoder MLM head SupCon(+) SupCon(-)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_12",
            "content": "The story is not worth reading, a truly [MASK] one. This is an amazing movie, a t r u l y g r e a t o n e . T h e c l a s s h a s n o attendance, a truly terrible one.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_13",
            "content": "Prof. Figure 1: Overview of our proposed method. Besides the standard prompt-base MLM loss on label words \"great\" and \"terrible\", we introduce a SupCon loss on multi-views of input text. The positive pair is sentences (with sampled templates and/or demonstrations) in the same class, e.g. sent 1 and sent 3 , or itself with a different template and demonstrations, e.g. sent 1 and sent 2 . The negative sentence pair is input sentences (with sampled templates and/or demonstrations) in different classes, e.g. sent 1 and sent 0 . Kumar et al., 2019). Inspired by the in-context learning of GPT-3, prompt-based fine-tuning Tam et al., 2021;Schick and Sch\u00fctze, 2021) recently becomes dominant in NLP. Basu et al. (2021) applies contrastive learning in their few-shot semi-supervised intent classification, by using EDA (Wei and Zou, 2019) as augmentation method. Different from Basu et al. (2021), our method applies to prompt-based fine-tuning, and we show in experiments that our proposed augmentation outperforms EDA. Supervised Contrastive Loss. SupCon is a special form of contrastive learning (Chen et al., 2020a,b;Tian et al., 2020a,b,c;Liu et al., 2021;Xiong et al., 2020) that clusters two augmented batches at the class level in the feature space. Let x2k\u22121 , x2k be two augmented views of an input batch x k ; and z 2k , z 2k\u22121 to be the features of x2k\u22121 , x2k . Then SupCon loss can be computed as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_14",
            "content": "L SupCon = SupCon(z 2k\u22121 , z 2k , y k ) (1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_15",
            "content": "where y k is the label for batch x k . The details of SupCon can be found in Khosla et al. (2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_16",
            "content": "3 Method Fine-tuning with prompts and demonstrations. Prompt-based methods treat a classification problem as a masked language modeling (MLM) problem. They take as input a sentence (sent) and a masked template (temp) (i.e., x prompt = sent, temp([mask])), and find the best token to fill in the [mask]. This leads to a MLM loss L MLM = MLM(x prompt , y), where y is the label word corresponding to x prompt . LM-BFF further appends demonstrations of label words to improve the results: x prompt+demo = sent 0 , temp 0 ([mask]), sent i , temp 0 (word i ) , where word i is the label word for sent i , and sent i is sampled from the training set. Then the classification loss becomes:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_17",
            "content": "L MLM = MLM(x prompt+demo , y)(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_18",
            "content": "More mathematical formulation can be found in LM-BFF or our Appendix B.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_19",
            "content": "Language-based Supervised Contrastive Loss. For applying SupCon on multi-views of an input text, we need to first obtain two views of a text:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_20",
            "content": "x 1 =sent 0 , temp 0 ([mask]), sent i , temp 0 (word i ) x 2 =sent 0 , temp j ([mask]), sent k , temp j (word k )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_21",
            "content": "where x 1 is identical to x prompt+demo in LM-BFF. We sample a new template (temp j ), demonstration (sent k ) and the corresponding label word (word k ) to replace those in x 1 , to create a second view of input x 2 . With x 1 and x 2 , we can compute SupCon loss by Equation 1. The total loss is then",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_22",
            "content": "L total = L MLM + L SupCon (3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_23",
            "content": "See our Appendix C for more mathematical details.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_24",
            "content": "Computational overhead. We show the algorithm of our method in Algorithm 1. In general, our method learns from L total = L MLM + L SupCon , whereas baseline LM-BFF learns with L MLM only.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_25",
            "content": "Learning from L SupCon requires one additional forward and backward pass (highlighted in blue in Algorithm 1), leading to an increase of computational cost by \u00d71.5. L SupCon = SupCon(output 1 , output 2 ) 20:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_26",
            "content": "L SupCon .backward()",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_27",
            "content": "optimizer.step() 22: end for 4 Experiments Evaluation datasets and protocol. We evaluate our method on 15 classification tasks studied in LM-BFF and follow the same setup as them to allow fair comparisons (see Appendix A for more training details). Contrastive learning algorithms benefit from large batch training. Thus, we report baselines with the same large batch size as ours.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_28",
            "content": "Our method uses a single prompt/template (primary prompt) for the prediction of each task, and a set of prompts (auxiliary prompts) for generating multi-views of inputs for contrastive learning. The primary prompts we used are shown in Appendix D. The auxiliary prompts can be either manually designed or generated by a searching algorithm. In this work, we use the top-20 generated prompts from LM-BFF's project page and we randomly sample templates in these 20 prompts to produce second views of our inputs. Unless otherwise noted, we apply both random templates and random demonstrations to create second views of inputs for the contrastive learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_29",
            "content": "Main results on 15 tasks",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "41-ARR_v2_30",
            "content": "We use RoBERTa-base (see Appendix E for RoBERTa-large). We compare ours with LM-BFF (a method w/ demonstrations) and PET (Schick and Sch\u00fctze, 2021) (a method w/o demonstration).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_31",
            "content": "Table 1 shows that our SupCon loss can consistently boost the performance of baseline promptbased fine-tuning method LM-BFF. The introduction of SupCon loss has a maximum improvement of 6.3% in QQP and an average improvement of 2.5% across 15 tasks, likely due to the more generalized representations learned by SupCon. On average, the greater improvements by our model can be seen on the more difficult tasks (see Appendix 5 for more detail).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_32",
            "content": "We want to emphasize that the input for baseline LM-BFF already appends different randomly sampled demonstrations at each tuning iteration. Thus, the improvement of our method can not be attributed to the diversity of inputs when learning from L MLM of Equation 3, but to the L SupCon .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_33",
            "content": "Table 1 also shows that our method works well even for prompt-based methods without demonstrations. PET, which is a method without demonstrations, works consistently worse than LM-BFF. However, with the additional SupCon loss, the fewshot performances of PET can be increased by an average of 2.3%. And the gap between having and not having demonstrations can be largely closed (see LM-BFF vs. PET+ours in Table 1). In some tasks, e.g., SST-2, SST-5, QNLI, QQP, RTE MRPC, MR, and CR, the contribution of our SupCon loss can be even larger than the sole use of the demonstrations for label words. We further show that our method outperforms two latest methods that are designed to improve prompt-based language models. In ADAPET (Tam et al., 2021), the authors replace the traditional CrossEntropy loss with Decoupling Label Loss and Label Condition Loss in the prompt-based finetuning method PET, without demonstrations. Contextual Calibration (Zhao et al., 2021) calibrates the output probabilities by considering context-free inputs, i.e., \" \" or \"N/A\". (Further see Appendix I)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_34",
            "content": "SupCon vs. other losses",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "41-ARR_v2_35",
            "content": "From Table 2 we observe that on 12 tasks our L SupCon outperforms the other losses, while performs on-par in other tasks. Contextual Calibration does not achieve good results overall. We speculate two reasons for this. First, Contextual Calibration is designed for large models without fine-tuning like GPT (zero-shot setting). Second, the form of in-context learning in Contextual Calibration is different from the demonstrations we study here.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_36",
            "content": "Ensemble vs. our single model",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "41-ARR_v2_37",
            "content": "Our method uses 20 generated templates (auxiliary prompts) to construct multi-views of input sentences. But only a single prompt (primary prompt) and one set of label words are used for main predictions. Thus, there is only a single model from our method. Here, we compare our model to an ensemble comprised of 20 models trained separately with the 20 prompts. From Table 3, we find that our method even outperforms the ensemble with 20\u00d7 more number of parameters, showing that it is a more efficient way to make use of the generated prompts. We speculate that because of the over-fitting nature of few-shot learners, members in the ensemble fail to produce substantial diverse prediction distributions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_38",
            "content": "Figure 2: The average improvements achieved by our method on the top K hardest tasks, where K goes from 1 to 15.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_39",
            "content": "Improvements vs. Task Difficulty",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "41-ARR_v2_40",
            "content": "Here, we show that the improvements achieved by our method are greater for tasks with higher difficulty. To show this, we first sort the 15 tasks by base (LM-BFF) performance and use this ranking as a proxy for the difficulty of the task. Next, we report the average improvements achieved by our method on the top K hardest tasks, where K goes from 1 to 15. Figure 2 shows these results. The first bar corresponds to the improvement achieved by our method on the hardest task, the second bar corresponds to the average improvement achieved by our method on the hardest and second-hardest tasks, and so on. The last bar corresponds to the average improvement on all 15 tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_41",
            "content": "6 Comparative Experiments",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_42",
            "content": "Input augmentation",
            "ntype": "title",
            "meta": {
                "section": "6.1"
            }
        },
        {
            "ix": "41-ARR_v2_43",
            "content": "The success of contrastive learning heavily relies on the data augmentation. Our method takes advantage of prompt-based language learners and naturally creates multi-views of a single input by appending it with different templates and/or demonstrations. Compared to EDA which includes synonym replacement (SR), random insertion (RI), random swap (RS) and random deletion (RD), our strategy for augmentation does not lead to incomplete and inconsistent sentences, while introducing adequate variations for effective learning. The results in Table 4 are obtained by applying SR, RI, RS, RD, EDA for 10% of input tokens (Results for 20% are in Appendix F). In contrast to ours, EDA, etc., for SupCon lead to worse performances than the baseline method in many tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_44",
            "content": "Variable templates, demonstrations",
            "ntype": "title",
            "meta": {
                "section": "6.2"
            }
        },
        {
            "ix": "41-ARR_v2_45",
            "content": "So far, we have shown the results by our method generating multi-views of inputs by appending both random templates and demonstrations. However, we find that in some tasks fixed templates with random demonstrations or random templates with fixed demonstration lead to even stronger performances (see Table 5). For example, sampling demonstrations with fixed templates for MRPC achieves a very strong result (80.0), outperforming all other methods in Table 4",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_46",
            "content": "Limitations",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "41-ARR_v2_47",
            "content": "Since SupCon clusters examples on class level, our framework applies only to classification tasks. Also, our framework requires large GPU memory, as SupCon is an in-batch contrastive loss that needs a large batch size.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_48",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "8"
            }
        },
        {
            "ix": "41-ARR_v2_49",
            "content": "We proposed a novel supervised contrastive learning framework and an effective augmentation method using prompts that can boost the performance of prompt-based language learners and outperform recent work on 15 few-shot tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_50",
            "content": "Ethical Considerations",
            "ntype": "title",
            "meta": {
                "section": "9"
            }
        },
        {
            "ix": "41-ARR_v2_51",
            "content": "As far as we are aware, our proposed work does not have any ethical considerations. However, our work relies on pre-trained language models, which have been shown to be biased in prior work (Liang et al., 2021). As such, users of such models should be aware of and if possible address such issues. The data and the code for this work will be made available to aid reproducibility.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_52",
            "content": "We use the same learning rate of 1e \u22125 for MLM loss as LM-BFF. To take full advantage of SupCon, we apply large batch sizes (16,32,40). We show the batch size and learning rate for SupCon in Table A.1. Note that for results of LM-BFF shown in the main paper, we use the same large batch size of our method to allow for fair comparisons.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_53",
            "content": "We set the batch size to be dividable by the total number of examples in the task and small enough to fit into the GPU memory. The experiments with RoBERTa-base are carried out on one NVIDIA RTX-A6000 with 48 GB of memory. Experiments with RoBERTa-large require 4x NVIDIA RTX-8000 (or RTX-A6000) with 192 (4x 48) GB of momery.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_54",
            "content": "Following LM-BFF, our fine-tuning runs a maximum of 1000 steps.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_55",
            "content": "We also consider LM-BFF as our baseline method due to its state-of-the-art performance in a wide range of few-shot tasks. The given masked language model M first encodes the input sentence x in into a sequence of tokens xin and maps xin to a sequence of hidden states {h 1 , h 2 , ...h L }, where L is the length of the sequence and h \u2208 R d , where d is the dimension of the hidden states. For example, in prompt-base fine-tuning, for single sentence text x in e.g., \"The story is not worth reading.\"), the input with the prompt (e.g., \"a truly [MASK] one .\") takes the form of",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_56",
            "content": "x prompt = [CLS]x in , a truly[MASK]one.[SEP] \u2261 T (x in )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_57",
            "content": "Then, the model decides whether it is more likely to put the label word \"great\" or \"terrible\" at the [MASK] position. Fine-tuning with this fill-in-theblank framework has been shown to be superior to standard fine-tuning (Schick and Sch\u00fctze, 2021). By mapping the label space Y to the label words where V(y) denotes the label word for class y, the prediction of the model M for class y \u2208 Y can be written as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_58",
            "content": "p(y|x in ) = p([MASK] = V(y)|x prompt ) (4) = exp(w V(y) \u2022 h [MASK] ) y \u2208Y exp(w V(y ) \u2022 h [MASK] )(5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_59",
            "content": "where w is the weight vector of MLM head.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_60",
            "content": "In LM-BFF, the authors further append demonstrations to the input x prompt to help the model better understand what is \"great\" and \"terrible\". Formally, x prompt \u2261 T (x in ) and T (x c in , y c ) denote T (x c in ) with [MASK] replaced by the label word V(y c ). Then, the input to LM-BFF takes the form of",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_61",
            "content": "T (x in ) \u2295 T (x 1 in , y 1 ) \u2295 ... \u2295 T (x |Y| in , y |Y| ) (6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_62",
            "content": "In this paper, we use random sampling for the demonstrations, i.e., x c in is randomly chosen from the training set. The masked language modeling loss is then",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_63",
            "content": "L MLM = (x in ,y)\u2208D train \u2212 log p(y|x in )(7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_64",
            "content": "C Language-based Supervised Contrastive Loss",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_65",
            "content": "Our method extends the loss L MLM with an additional Supervised Contrastive Loss (SupCon). For applying SupCon on multi-views of an input text, we need to first obtain a second view of a text:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_66",
            "content": "x2k = Aug(x 2k\u22121 )(8)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_67",
            "content": "As we show in ablations, traditional data augmentation for text does not work well in the contrastive framework. Thus, we propose obtaining a second view by randomly changing the templates and/or demonstrations:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_68",
            "content": "x2k\u22121 = T t 0 (x in ) \u2295 ... \u2295 Tt 0 (x c in , y c ) \u2295 ... (9) x2k = T t j (x in ) \u2295 ... \u2295 Tt j (x c in , y c ) \u2295 ... (10",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_69",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_70",
            "content": "where T denotes a set of pre-defined templates, t j \u2208 T and t j = t 0 . xc in is another randomly sampled example as the demonstration text and xc in = x in . This strategy serves as a perfect form of augmentation for our purpose as it does not generate incomplete or inconsistent sentences, and since we do not edit the main input, the label for that input stays the same. Furthermore, x2k has a substantial variation from x2k\u22121 , which allows for effective contrastive learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_71",
            "content": "Table D.1 shows the primary prompts we used for each task. Those prompts are manually chosen by LM-BFF .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_72",
            "content": "While we use RoBERTa-base to conduct extensive experiments in our main study and ablations, here we compare our framework to LM-BFF using RoBERTa-large. We also include results directly reported from LM-BFF (henceforth referred to as LM-BFF \u2020) , though the comparison between them could be unfair since the results reported in the original LM-BFF paper are:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_73",
            "content": "\u2022 obtained with an additional sampling strategy to select similar demonstrations (section 6.2 of their paper), which put our results at a disadvantage. \u2022 obtained from a set of batch sizes (2,4,8) and learning rates (1e \u22125 , 2e \u22125 , 5e \u22125 ) and the best models are selected from the validation set.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_74",
            "content": "Whereas we show experimental results with models trained with a fixed batch size and a learning rate of 1e \u22125 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_75",
            "content": "Nevertheless, we show the state-of-the-art results we achieved in Table E.1. We only marginally under-perform LM-BFF \u2020 in 2 tasks, possibly due to the reasons listed above. 87.8 (0.9) 88.1 (0.3) 87.5 (1.0) 88.5 (1.0) 87.9 (0.5) 89.4 (1.0)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_76",
            "content": "In the main paper, we compare our augmentation strategy (random templates and random demonstrations) to standard augmentation techniques on 10% of input tokens for creating multi-view of inputs to apply the SupCon loss. Here, we show additional experimental results with synonym replacement (SR), random insertion (RI), random swapping (RS), random deletion (RD) and EDA (Wei and Zou, 2019) (with SR, RI, RS and RD all together) at 20% of input tokens. Same as before, the model under-performs when using standard augmentations. Results are shown in Table F",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_77",
            "content": "For results in Table 2, we adapt the open-source code from ADAPET to our codebase. In original ADAPET, there are multiple label words corresponding to a label class (e.g positive class: \"great\", \"good\", \"nice\"). To make a fair comparison to LM-BFF and ours, we only apply one label word corresponding to a label class (e.g positive class: \"great\"). The original Label Condition Loss implemented in ADAPET has a hyper-parameter \u03b1 to control the percentage of input tokens used for masked language modeling (see ADAPET (Tam et al., 2021) for more details). To match the training objective in LM-BFF with only one [MASK] token, we also set the Label Condition Loss to apply to one random token of inputs, i.e., \u03b1 = 1 len(input) .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "41-ARR_v2_78",
            "content": "Farid Arthaud, Rachel Bawden, Alexandra Birch, Few-shot learning through contextual data augmentation, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Farid Arthaud",
                    "Rachel Bawden",
                    "Alexandra Birch"
                ],
                "title": "Few-shot learning through contextual data augmentation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "41-ARR_v2_79",
            "content": "Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai, Andrew Mccallum, Self-supervised meta-learning for few-shot natural language classification tasks, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Trapit Bansal",
                    "Rishikesh Jha",
                    "Tsendsuren Munkhdalai",
                    "Andrew Mccallum"
                ],
                "title": "Self-supervised meta-learning for few-shot natural language classification tasks",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "41-ARR_v2_80",
            "content": "UNKNOWN, None, 2021, Semi-supervised few-shot intent classification and slot filling, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Semi-supervised few-shot intent classification and slot filling",
                "pub": "CoRR"
            }
        },
        {
            "ix": "41-ARR_v2_81",
            "content": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, A simple framework for contrastive learning of visual representations, 2020, Proceedings of the 37th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Ting Chen",
                    "Simon Kornblith",
                    "Mohammad Norouzi",
                    "Geoffrey Hinton"
                ],
                "title": "A simple framework for contrastive learning of visual representations",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 37th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "41-ARR_v2_82",
            "content": "Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey Hinton, Big self-supervised models are strong semi-supervised learners, 2020, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Ting Chen",
                    "Simon Kornblith",
                    "Kevin Swersky",
                    "Mohammad Norouzi",
                    "Geoffrey Hinton"
                ],
                "title": "Big self-supervised models are strong semi-supervised learners",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "41-ARR_v2_83",
            "content": "Tianyu Gao, Adam Fisch, Danqi Chen, Making pre-trained language models better few-shot learners, 2021, Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Tianyu Gao",
                    "Adam Fisch",
                    "Danqi Chen"
                ],
                "title": "Making pre-trained language models better few-shot learners",
                "pub_date": "2021",
                "pub_title": "Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "41-ARR_v2_84",
            "content": "Yiren Jian, Karim Ahmed, Lorenzo Torresani, Task meta-transfer from limited parallel labels, 2020, Meta-Learning workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Yiren Jian",
                    "Karim Ahmed",
                    "Lorenzo Torresani"
                ],
                "title": "Task meta-transfer from limited parallel labels",
                "pub_date": "2020",
                "pub_title": "Meta-Learning workshop",
                "pub": null
            }
        },
        {
            "ix": "41-ARR_v2_85",
            "content": "UNKNOWN, None, 2021, Metapix: Domain transfer for semantic segmentation by meta pixel weighting. Image and Vision Computing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Metapix: Domain transfer for semantic segmentation by meta pixel weighting. Image and Vision Computing",
                "pub": null
            }
        },
        {
            "ix": "41-ARR_v2_86",
            "content": "Yiren Jian, Chongyang Gao, Soroush Vosoughi, Embedding hallucination for few-shot language learning, 2022, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Yiren Jian",
                    "Chongyang Gao",
                    "Soroush Vosoughi"
                ],
                "title": "Embedding hallucination for few-shot language learning",
                "pub_date": "2022",
                "pub_title": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "41-ARR_v2_87",
            "content": "Yiren Jian, Lorenzo Torresani, Label hallucination for few-shot classification, 2022, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Yiren Jian",
                    "Lorenzo Torresani"
                ],
                "title": "Label hallucination for few-shot classification",
                "pub_date": "2022",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "41-ARR_v2_88",
            "content": "Akbar Karimi, Leonardo Rossi, Andrea Prati, AEDA: an easier data augmentation technique for text classification, 2021-11-20, Findings of the Association for Computational Linguistics: EMNLP 2021, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Akbar Karimi",
                    "Leonardo Rossi",
                    "Andrea Prati"
                ],
                "title": "AEDA: an easier data augmentation technique for text classification",
                "pub_date": "2021-11-20",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2021",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "41-ARR_v2_89",
            "content": "Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan, Supervised contrastive learning, 2020, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Prannay Khosla",
                    "Piotr Teterwak",
                    "Chen Wang",
                    "Aaron Sarna",
                    "Yonglong Tian",
                    "Phillip Isola",
                    "Aaron Maschinot",
                    "Ce Liu",
                    "Dilip Krishnan"
                ],
                "title": "Supervised contrastive learning",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "41-ARR_v2_90",
            "content": "Varun Kumar, Hadrien Glaude, Cyprien De Lichy, Wlliam Campbell, A closer look at feature space data augmentation for few-shot intent classification, 2019, Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Varun Kumar",
                    "Hadrien Glaude",
                    "Cyprien De Lichy",
                    "Wlliam Campbell"
                ],
                "title": "A closer look at feature space data augmentation for few-shot intent classification",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP",
                "pub": null
            }
        },
        {
            "ix": "41-ARR_v2_91",
            "content": "Judith , Li , Jiong Zhang, Semi-supervised meta-learning for cross-domain few-shot intent classification, 2021, MetaNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Judith ",
                    "Li ",
                    "Jiong Zhang"
                ],
                "title": "Semi-supervised meta-learning for cross-domain few-shot intent classification",
                "pub_date": "2021",
                "pub_title": "MetaNLP",
                "pub": null
            }
        },
        {
            "ix": "41-ARR_v2_92",
            "content": "Chiyu Paul Pu Liang, Louis-Philippe Wu, Ruslan Morency,  Salakhutdinov, Towards understanding and mitigating social biases in language models, 2021, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Chiyu Paul Pu Liang",
                    "Louis-Philippe Wu",
                    "Ruslan Morency",
                    " Salakhutdinov"
                ],
                "title": "Towards understanding and mitigating social biases in language models",
                "pub_date": "2021",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "41-ARR_v2_93",
            "content": "UNKNOWN, None, 2021, Bootstrapping semantic segmentation with regional contrast, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Bootstrapping semantic segmentation with regional contrast",
                "pub": null
            }
        },
        {
            "ix": "41-ARR_v2_94",
            "content": "Timo Schick, Hinrich Sch\u00fctze, Exploiting cloze-questions for few-shot text classification and natural language inference, 2021, EACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Timo Schick",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "Exploiting cloze-questions for few-shot text classification and natural language inference",
                "pub_date": "2021",
                "pub_title": "EACL",
                "pub": null
            }
        },
        {
            "ix": "41-ARR_v2_95",
            "content": "Amr Sharaf, Hany Hassan, Hal Daum\u00e9, Iii , Meta-learning for few-shot nmt adaptation, 2020, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Amr Sharaf",
                    "Hany Hassan",
                    "Hal Daum\u00e9",
                    "Iii "
                ],
                "title": "Meta-learning for few-shot nmt adaptation",
                "pub_date": "2020",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "41-ARR_v2_96",
            "content": "Derek Tam, Mohit Rakesh R Menon,  Bansal, Shashank Srivastava, and Colin Raffel. 2021. Improving and simplifying pattern exploiting training, , Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Derek Tam",
                    "Mohit Rakesh R Menon",
                    " Bansal"
                ],
                "title": "Shashank Srivastava, and Colin Raffel. 2021. Improving and simplifying pattern exploiting training",
                "pub_date": null,
                "pub_title": "Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "41-ARR_v2_97",
            "content": "Yonglong Tian, Dilip Krishnan, Phillip Isola, Contrastive multiview coding, 2020, Computer Vision -ECCV 2020, Springer International Publishing.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Yonglong Tian",
                    "Dilip Krishnan",
                    "Phillip Isola"
                ],
                "title": "Contrastive multiview coding",
                "pub_date": "2020",
                "pub_title": "Computer Vision -ECCV 2020",
                "pub": "Springer International Publishing"
            }
        },
        {
            "ix": "41-ARR_v2_98",
            "content": "Yonglong Tian, Dilip Krishnan, Phillip Isola, Contrastive representation distillation, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Yonglong Tian",
                    "Dilip Krishnan",
                    "Phillip Isola"
                ],
                "title": "Contrastive representation distillation",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "41-ARR_v2_99",
            "content": "UNKNOWN, None, 2020, What makes for good views for contrastive learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "What makes for good views for contrastive learning",
                "pub": null
            }
        },
        {
            "ix": "41-ARR_v2_100",
            "content": "Jason Wei, Chengyu Huang, Soroush Vosoughi, Yu Cheng, Shiqi Xu, Few-shot text classification with triplet networks, data augmentation, and curriculum learning, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Jason Wei",
                    "Chengyu Huang",
                    "Soroush Vosoughi",
                    "Yu Cheng",
                    "Shiqi Xu"
                ],
                "title": "Few-shot text classification with triplet networks, data augmentation, and curriculum learning",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "41-ARR_v2_101",
            "content": "Jason Wei, Kai Zou, EDA: Easy data augmentation techniques for boosting performance on text classification tasks, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Jason Wei",
                    "Kai Zou"
                ],
                "title": "EDA: Easy data augmentation techniques for boosting performance on text classification tasks",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "41-ARR_v2_102",
            "content": "Yuwen Xiong, Mengye Ren, Raquel Urtasun, Loco: Local contrastive representation learning, 2020, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Yuwen Xiong",
                    "Mengye Ren",
                    "Raquel Urtasun"
                ],
                "title": "Loco: Local contrastive representation learning",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "41-ARR_v2_103",
            "content": "Sangdoo Yun, Dongyoon Han, Sanghyuk Seong Joon Oh, Junsuk Chun, Youngjoon Choe,  Yoo, Cutmix: Regularization strategy to train strong classifiers with localizable features, 2019, Proceedings of the IEEE/CVF International Conference on Computer Vision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Sangdoo Yun",
                    "Dongyoon Han",
                    "Sanghyuk Seong Joon Oh",
                    "Junsuk Chun",
                    "Youngjoon Choe",
                    " Yoo"
                ],
                "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
                "pub_date": "2019",
                "pub_title": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
                "pub": null
            }
        },
        {
            "ix": "41-ARR_v2_104",
            "content": "Hongyi Zhang, Moustapha Cisse, David Yann N Dauphin,  Lopez-Paz, mixup: Beyond empirical risk minimization, 2018, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Hongyi Zhang",
                    "Moustapha Cisse",
                    "David Yann N Dauphin",
                    " Lopez-Paz"
                ],
                "title": "mixup: Beyond empirical risk minimization",
                "pub_date": "2018",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "41-ARR_v2_105",
            "content": "Tony Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, Calibrate Before Use: Improving Few-shot Performance of Language Models, 2021, International Conference on Machine Learning (ICML), .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Tony Zhao",
                    "Eric Wallace",
                    "Shi Feng",
                    "Dan Klein",
                    "Sameer Singh"
                ],
                "title": "Calibrate Before Use: Improving Few-shot Performance of Language Models",
                "pub_date": "2021",
                "pub_title": "International Conference on Machine Learning (ICML)",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "41-ARR_v2_0@0",
            "content": "Contrastive Learning for Prompt-Based Few-Shot Language Learners",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_0",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_2@0",
            "content": "The impressive performance of GPT-3 using natural language prompts and in-context learning has inspired work on better fine-tuning of moderately-sized models under this paradigm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_2",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_2@1",
            "content": "Following this line of work, we present a contrastive learning framework that clusters inputs from the same class for better generality of models trained with only limited examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_2",
            "start": 179,
            "end": 359,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_2@2",
            "content": "Specifically, we propose a supervised contrastive framework that clusters inputs from the same class under different augmented \"views\" and repel the ones from different classes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_2",
            "start": 361,
            "end": 537,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_2@3",
            "content": "We create different \"views\" of an example by appending it with different language prompts and contextual demonstrations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_2",
            "start": 539,
            "end": 658,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_2@4",
            "content": "Combining a contrastive loss with the standard masked language modeling (MLM) loss in prompt-based few-shot learners, the experimental results show that our method can improve over the state-of-the-art methods in a diverse set of 15 language tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_2",
            "start": 660,
            "end": 907,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_2@5",
            "content": "Our framework makes minimal assumptions on the task or the base model, and can be applied to many recent methods with little modification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_2",
            "start": 909,
            "end": 1046,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_2@6",
            "content": "The code will be made available at: https://github.com/yiren-jian/LM-SupCon.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_2",
            "start": 1048,
            "end": 1123,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_4@0",
            "content": "The prompt-based fine-tuning method reduces the gap between pre-training and fine-tuning by forming the fine-tuning task into a masking language problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_4",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_4@1",
            "content": "A language prompt is a piece of text appended to the query input enabling the model to come up with better predictions (Schick and Sch\u00fctze, 2021;Tam et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_4",
            "start": 154,
            "end": 316,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_4@2",
            "content": "For instance, by feeding a language model with \"The story is not worth reading, a truly one.\", the model assigns a higher probability for the blank to be filled with \"terrible\" than \"great\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_4",
            "start": 318,
            "end": 507,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_4@3",
            "content": "Here, \"a truly one.\" is called the template of the prompt and \"terrible\" or \"great\" is the label word.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_4",
            "start": 509,
            "end": 610,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_4@4",
            "content": "Recently, LM-BFF shows that appending demonstrations (e.g.\"This is an amazing movie, a truly great one\") to inputs can help the model to better understand the label word, leading to further improved results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_4",
            "start": 612,
            "end": 818,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_5@0",
            "content": "In this work, we show that Supervised Contrastive Learning (SupCon) (Khosla et al., 2020) at the feature space can be beneficial during the finetuning of prompt-based few-shot language learners, with proper data augmentation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_5",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_6@0",
            "content": "Data augmentation is the key component of SupCon.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_6",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_6@1",
            "content": "While there exists many augmentation techniques like Cutmix (Yun et al., 2019), Mixup (Zhang et al., 2018) in computer vision and EDA (Wei and Zou, 2019), AEDA (Karimi et al., 2021) for text, data augmentation remains challenging.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_6",
            "start": 50,
            "end": 279,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_7@0",
            "content": "However, prompt-based few-shot learners with demonstrations actually provide us with a natural way to create multiple \"views\" (augmentations) of a single example, i.e., for a fixed set of label words, we can sample different templates and different demonstrations to append to the input text (shown in Figure 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_7",
            "start": 0,
            "end": 311,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_7@1",
            "content": "This allows us to construct diverse input texts that are consistent and complete.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_7",
            "start": 313,
            "end": 393,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_7@2",
            "content": "By applying SupCon to cluster the above two example inputs with very different contents but the same label, our method is able to obtain an additional supervision at the feature space which is crucial if we are only given a few labeled examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_7",
            "start": 395,
            "end": 639,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_8@0",
            "content": "The main contributions of our paper are:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_8",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_9@0",
            "content": "\u2022 A Supervised Contrastive Learning framework for prompt-based few-shot learners. \u2022 An effective data augmentation method using prompts for contrastive learning with promptbased learners.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_9",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_10@0",
            "content": "Related Work & Background",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_10",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_11@0",
            "content": "Few-shot Learning is often tackled by meta learning (Li and Zhang, 2021;Bansal et al., 2020;Sharaf et al., 2020;Jian et al., 2020;Jian and Gao, 2021), data augmentation Jian and Torresani, 2022;Arthaud et al., 2021;Wei et al., 2021; Encoder MLM head SupCon(+) SupCon(-)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_11",
            "start": 0,
            "end": 268,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_12@0",
            "content": "The story is not worth reading, a truly [MASK] one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_12",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_12@1",
            "content": "This is an amazing movie, a t r u l y g r e a t o n e .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_12",
            "start": 52,
            "end": 106,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_12@2",
            "content": "T h e c l a s s h a s n o attendance, a truly terrible one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_12",
            "start": 108,
            "end": 166,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_13@0",
            "content": "Prof. Figure 1: Overview of our proposed method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_13",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_13@1",
            "content": "Besides the standard prompt-base MLM loss on label words \"great\" and \"terrible\", we introduce a SupCon loss on multi-views of input text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_13",
            "start": 49,
            "end": 185,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_13@2",
            "content": "The positive pair is sentences (with sampled templates and/or demonstrations) in the same class, e.g. sent 1 and sent 3 , or itself with a different template and demonstrations, e.g. sent 1 and sent 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_13",
            "start": 187,
            "end": 388,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_13@3",
            "content": "The negative sentence pair is input sentences (with sampled templates and/or demonstrations) in different classes, e.g. sent 1 and sent 0 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_13",
            "start": 390,
            "end": 528,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_13@4",
            "content": "Kumar et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_13",
            "start": 530,
            "end": 549,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_13@5",
            "content": "Inspired by the in-context learning of GPT-3, prompt-based fine-tuning Tam et al., 2021;Schick and Sch\u00fctze, 2021) recently becomes dominant in NLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_13",
            "start": 551,
            "end": 697,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_13@6",
            "content": "Basu et al. (2021) applies contrastive learning in their few-shot semi-supervised intent classification, by using EDA (Wei and Zou, 2019) as augmentation method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_13",
            "start": 699,
            "end": 859,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_13@7",
            "content": "Different from Basu et al. (2021), our method applies to prompt-based fine-tuning, and we show in experiments that our proposed augmentation outperforms EDA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_13",
            "start": 861,
            "end": 1017,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_13@8",
            "content": "Supervised Contrastive Loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_13",
            "start": 1019,
            "end": 1046,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_13@9",
            "content": "SupCon is a special form of contrastive learning (Chen et al., 2020a,b;Tian et al., 2020a,b,c;Liu et al., 2021;Xiong et al., 2020) that clusters two augmented batches at the class level in the feature space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_13",
            "start": 1048,
            "end": 1254,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_13@10",
            "content": "Let x2k\u22121 , x2k be two augmented views of an input batch x k ; and z 2k , z 2k\u22121 to be the features of x2k\u22121 , x2k .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_13",
            "start": 1256,
            "end": 1371,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_13@11",
            "content": "Then SupCon loss can be computed as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_13",
            "start": 1373,
            "end": 1407,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_14@0",
            "content": "L SupCon = SupCon(z 2k\u22121 , z 2k , y k ) (1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_14",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_15@0",
            "content": "where y k is the label for batch x k .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_15",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_15@1",
            "content": "The details of SupCon can be found in Khosla et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_15",
            "start": 39,
            "end": 97,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_16@0",
            "content": "3 Method Fine-tuning with prompts and demonstrations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_16",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_16@1",
            "content": "Prompt-based methods treat a classification problem as a masked language modeling (MLM) problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_16",
            "start": 54,
            "end": 149,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_16@2",
            "content": "They take as input a sentence (sent) and a masked template (temp) (i.e., x prompt = sent, temp([mask])), and find the best token to fill in the [mask].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_16",
            "start": 151,
            "end": 301,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_16@3",
            "content": "This leads to a MLM loss L MLM = MLM(x prompt , y), where y is the label word corresponding to x prompt .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_16",
            "start": 303,
            "end": 407,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_16@4",
            "content": "LM-BFF further appends demonstrations of label words to improve the results: x prompt+demo = sent 0 , temp 0 ([mask]), sent i , temp 0 (word i ) , where word i is the label word for sent i , and sent i is sampled from the training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_16",
            "start": 409,
            "end": 643,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_16@5",
            "content": "Then the classification loss becomes:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_16",
            "start": 645,
            "end": 681,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_17@0",
            "content": "L MLM = MLM(x prompt+demo , y)(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_17",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_18@0",
            "content": "More mathematical formulation can be found in LM-BFF or our Appendix B.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_18",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_19@0",
            "content": "Language-based Supervised Contrastive Loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_19",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_19@1",
            "content": "For applying SupCon on multi-views of an input text, we need to first obtain two views of a text:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_19",
            "start": 44,
            "end": 140,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_20@0",
            "content": "x 1 =sent 0 , temp 0 ([mask]), sent i , temp 0 (word i ) x 2 =sent 0 , temp j ([mask]), sent k , temp j (word k )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_20",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_21@0",
            "content": "where x 1 is identical to x prompt+demo in LM-BFF.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_21",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_21@1",
            "content": "We sample a new template (temp j ), demonstration (sent k ) and the corresponding label word (word k ) to replace those in x 1 , to create a second view of input x 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_21",
            "start": 51,
            "end": 217,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_21@2",
            "content": "With x 1 and x 2 , we can compute SupCon loss by Equation 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_21",
            "start": 219,
            "end": 278,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_21@3",
            "content": "The total loss is then",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_21",
            "start": 280,
            "end": 301,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_22@0",
            "content": "L total = L MLM + L SupCon (3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_22",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_23@0",
            "content": "See our Appendix C for more mathematical details.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_23",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_24@0",
            "content": "Computational overhead.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_24",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_24@1",
            "content": "We show the algorithm of our method in Algorithm 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_24",
            "start": 24,
            "end": 74,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_24@2",
            "content": "In general, our method learns from L total = L MLM + L SupCon , whereas baseline LM-BFF learns with L MLM only.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_24",
            "start": 76,
            "end": 186,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_25@0",
            "content": "Learning from L SupCon requires one additional forward and backward pass (highlighted in blue in Algorithm 1), leading to an increase of computational cost by \u00d71.5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_25",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_25@1",
            "content": "L SupCon = SupCon(output 1 , output 2 ) 20:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_25",
            "start": 165,
            "end": 207,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_26@0",
            "content": "L SupCon .backward()",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_26",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_27@0",
            "content": "optimizer.step() 22: end for 4 Experiments Evaluation datasets and protocol.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_27",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_27@1",
            "content": "We evaluate our method on 15 classification tasks studied in LM-BFF and follow the same setup as them to allow fair comparisons (see Appendix A for more training details).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_27",
            "start": 77,
            "end": 247,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_27@2",
            "content": "Contrastive learning algorithms benefit from large batch training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_27",
            "start": 249,
            "end": 314,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_27@3",
            "content": "Thus, we report baselines with the same large batch size as ours.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_27",
            "start": 316,
            "end": 380,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_28@0",
            "content": "Our method uses a single prompt/template (primary prompt) for the prediction of each task, and a set of prompts (auxiliary prompts) for generating multi-views of inputs for contrastive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_28",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_28@1",
            "content": "The primary prompts we used are shown in Appendix D. The auxiliary prompts can be either manually designed or generated by a searching algorithm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_28",
            "start": 195,
            "end": 339,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_28@2",
            "content": "In this work, we use the top-20 generated prompts from LM-BFF's project page and we randomly sample templates in these 20 prompts to produce second views of our inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_28",
            "start": 341,
            "end": 508,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_28@3",
            "content": "Unless otherwise noted, we apply both random templates and random demonstrations to create second views of inputs for the contrastive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_28",
            "start": 510,
            "end": 652,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_29@0",
            "content": "Main results on 15 tasks",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_29",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_30@0",
            "content": "We use RoBERTa-base (see Appendix E for RoBERTa-large).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_30",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_30@1",
            "content": "We compare ours with LM-BFF (a method w/ demonstrations) and PET (Schick and Sch\u00fctze, 2021) (a method w/o demonstration).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_30",
            "start": 56,
            "end": 176,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_31@0",
            "content": "Table 1 shows that our SupCon loss can consistently boost the performance of baseline promptbased fine-tuning method LM-BFF.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_31",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_31@1",
            "content": "The introduction of SupCon loss has a maximum improvement of 6.3% in QQP and an average improvement of 2.5% across 15 tasks, likely due to the more generalized representations learned by SupCon.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_31",
            "start": 125,
            "end": 318,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_31@2",
            "content": "On average, the greater improvements by our model can be seen on the more difficult tasks (see Appendix 5 for more detail).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_31",
            "start": 320,
            "end": 442,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_32@0",
            "content": "We want to emphasize that the input for baseline LM-BFF already appends different randomly sampled demonstrations at each tuning iteration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_32",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_32@1",
            "content": "Thus, the improvement of our method can not be attributed to the diversity of inputs when learning from L MLM of Equation 3, but to the L SupCon .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_32",
            "start": 140,
            "end": 285,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_33@0",
            "content": "Table 1 also shows that our method works well even for prompt-based methods without demonstrations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_33",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_33@1",
            "content": "PET, which is a method without demonstrations, works consistently worse than LM-BFF.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_33",
            "start": 100,
            "end": 183,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_33@2",
            "content": "However, with the additional SupCon loss, the fewshot performances of PET can be increased by an average of 2.3%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_33",
            "start": 185,
            "end": 297,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_33@3",
            "content": "And the gap between having and not having demonstrations can be largely closed (see LM-BFF vs. PET+ours in Table 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_33",
            "start": 299,
            "end": 414,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_33@4",
            "content": "In some tasks, e.g., SST-2, SST-5, QNLI, QQP, RTE MRPC, MR, and CR, the contribution of our SupCon loss can be even larger than the sole use of the demonstrations for label words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_33",
            "start": 416,
            "end": 594,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_33@5",
            "content": "We further show that our method outperforms two latest methods that are designed to improve prompt-based language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_33",
            "start": 596,
            "end": 716,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_33@6",
            "content": "In ADAPET (Tam et al., 2021), the authors replace the traditional CrossEntropy loss with Decoupling Label Loss and Label Condition Loss in the prompt-based finetuning method PET, without demonstrations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_33",
            "start": 718,
            "end": 919,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_33@7",
            "content": "Contextual Calibration (Zhao et al., 2021) calibrates the output probabilities by considering context-free inputs, i.e., \" \" or \"N/A\". (Further see Appendix I)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_33",
            "start": 921,
            "end": 1079,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_34@0",
            "content": "SupCon vs. other losses",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_34",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_35@0",
            "content": "From Table 2 we observe that on 12 tasks our L SupCon outperforms the other losses, while performs on-par in other tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_35",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_35@1",
            "content": "Contextual Calibration does not achieve good results overall.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_35",
            "start": 122,
            "end": 182,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_35@2",
            "content": "We speculate two reasons for this.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_35",
            "start": 184,
            "end": 217,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_35@3",
            "content": "First, Contextual Calibration is designed for large models without fine-tuning like GPT (zero-shot setting).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_35",
            "start": 219,
            "end": 326,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_35@4",
            "content": "Second, the form of in-context learning in Contextual Calibration is different from the demonstrations we study here.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_35",
            "start": 328,
            "end": 444,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_36@0",
            "content": "Ensemble vs. our single model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_36",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_37@0",
            "content": "Our method uses 20 generated templates (auxiliary prompts) to construct multi-views of input sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_37",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_37@1",
            "content": "But only a single prompt (primary prompt) and one set of label words are used for main predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_37",
            "start": 104,
            "end": 202,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_37@2",
            "content": "Thus, there is only a single model from our method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_37",
            "start": 204,
            "end": 254,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_37@3",
            "content": "Here, we compare our model to an ensemble comprised of 20 models trained separately with the 20 prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_37",
            "start": 256,
            "end": 359,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_37@4",
            "content": "From Table 3, we find that our method even outperforms the ensemble with 20\u00d7 more number of parameters, showing that it is a more efficient way to make use of the generated prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_37",
            "start": 361,
            "end": 541,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_37@5",
            "content": "We speculate that because of the over-fitting nature of few-shot learners, members in the ensemble fail to produce substantial diverse prediction distributions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_37",
            "start": 543,
            "end": 702,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_38@0",
            "content": "Figure 2: The average improvements achieved by our method on the top K hardest tasks, where K goes from 1 to 15.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_38",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_39@0",
            "content": "Improvements vs. Task Difficulty",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_39",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_40@0",
            "content": "Here, we show that the improvements achieved by our method are greater for tasks with higher difficulty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_40",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_40@1",
            "content": "To show this, we first sort the 15 tasks by base (LM-BFF) performance and use this ranking as a proxy for the difficulty of the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_40",
            "start": 105,
            "end": 237,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_40@2",
            "content": "Next, we report the average improvements achieved by our method on the top K hardest tasks, where K goes from 1 to 15.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_40",
            "start": 239,
            "end": 356,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_40@3",
            "content": "Figure 2 shows these results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_40",
            "start": 358,
            "end": 386,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_40@4",
            "content": "The first bar corresponds to the improvement achieved by our method on the hardest task, the second bar corresponds to the average improvement achieved by our method on the hardest and second-hardest tasks, and so on.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_40",
            "start": 388,
            "end": 604,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_40@5",
            "content": "The last bar corresponds to the average improvement on all 15 tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_40",
            "start": 606,
            "end": 673,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_41@0",
            "content": "6 Comparative Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_41",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_42@0",
            "content": "Input augmentation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_42",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_43@0",
            "content": "The success of contrastive learning heavily relies on the data augmentation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_43",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_43@1",
            "content": "Our method takes advantage of prompt-based language learners and naturally creates multi-views of a single input by appending it with different templates and/or demonstrations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_43",
            "start": 77,
            "end": 252,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_43@2",
            "content": "Compared to EDA which includes synonym replacement (SR), random insertion (RI), random swap (RS) and random deletion (RD), our strategy for augmentation does not lead to incomplete and inconsistent sentences, while introducing adequate variations for effective learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_43",
            "start": 254,
            "end": 523,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_43@3",
            "content": "The results in Table 4 are obtained by applying SR, RI, RS, RD, EDA for 10% of input tokens (Results for 20% are in Appendix F).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_43",
            "start": 525,
            "end": 652,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_43@4",
            "content": "In contrast to ours, EDA, etc., for SupCon lead to worse performances than the baseline method in many tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_43",
            "start": 654,
            "end": 762,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_44@0",
            "content": "Variable templates, demonstrations",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_44",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_45@0",
            "content": "So far, we have shown the results by our method generating multi-views of inputs by appending both random templates and demonstrations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_45",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_45@1",
            "content": "However, we find that in some tasks fixed templates with random demonstrations or random templates with fixed demonstration lead to even stronger performances (see Table 5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_45",
            "start": 136,
            "end": 308,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_45@2",
            "content": "For example, sampling demonstrations with fixed templates for MRPC achieves a very strong result (80.0), outperforming all other methods in Table 4",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_45",
            "start": 310,
            "end": 456,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_46@0",
            "content": "Limitations",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_46",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_47@0",
            "content": "Since SupCon clusters examples on class level, our framework applies only to classification tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_47",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_47@1",
            "content": "Also, our framework requires large GPU memory, as SupCon is an in-batch contrastive loss that needs a large batch size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_47",
            "start": 99,
            "end": 217,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_48@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_48",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_49@0",
            "content": "We proposed a novel supervised contrastive learning framework and an effective augmentation method using prompts that can boost the performance of prompt-based language learners and outperform recent work on 15 few-shot tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_49",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_50@0",
            "content": "Ethical Considerations",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_50",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_51@0",
            "content": "As far as we are aware, our proposed work does not have any ethical considerations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_51",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_51@1",
            "content": "However, our work relies on pre-trained language models, which have been shown to be biased in prior work (Liang et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_51",
            "start": 84,
            "end": 210,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_51@2",
            "content": "As such, users of such models should be aware of and if possible address such issues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_51",
            "start": 212,
            "end": 296,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_51@3",
            "content": "The data and the code for this work will be made available to aid reproducibility.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_51",
            "start": 298,
            "end": 379,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_52@0",
            "content": "We use the same learning rate of 1e \u22125 for MLM loss as LM-BFF.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_52",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_52@1",
            "content": "To take full advantage of SupCon, we apply large batch sizes (16,32,40).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_52",
            "start": 63,
            "end": 134,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_52@2",
            "content": "We show the batch size and learning rate for SupCon in Table A.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_52",
            "start": 136,
            "end": 200,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_52@3",
            "content": "Note that for results of LM-BFF shown in the main paper, we use the same large batch size of our method to allow for fair comparisons.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_52",
            "start": 202,
            "end": 335,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_53@0",
            "content": "We set the batch size to be dividable by the total number of examples in the task and small enough to fit into the GPU memory.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_53",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_53@1",
            "content": "The experiments with RoBERTa-base are carried out on one NVIDIA RTX-A6000 with 48 GB of memory.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_53",
            "start": 127,
            "end": 221,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_53@2",
            "content": "Experiments with RoBERTa-large require 4x NVIDIA RTX-8000 (or RTX-A6000) with 192 (4x 48) GB of momery.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_53",
            "start": 223,
            "end": 325,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_54@0",
            "content": "Following LM-BFF, our fine-tuning runs a maximum of 1000 steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_54",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_55@0",
            "content": "We also consider LM-BFF as our baseline method due to its state-of-the-art performance in a wide range of few-shot tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_55",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_55@1",
            "content": "The given masked language model M first encodes the input sentence x in into a sequence of tokens xin and maps xin to a sequence of hidden states {h 1 , h 2 , ...h L }, where L is the length of the sequence and h \u2208 R d , where d is the dimension of the hidden states.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_55",
            "start": 122,
            "end": 388,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_55@2",
            "content": "For example, in prompt-base fine-tuning, for single sentence text x in e.g., \"The story is not worth reading.\"), the input with the prompt (e.g., \"a truly [MASK] one .\") takes the form of",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_55",
            "start": 390,
            "end": 576,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_56@0",
            "content": "x prompt = [CLS]x in , a truly[MASK]one.[SEP] \u2261 T (x in )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_56",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_57@0",
            "content": "Then, the model decides whether it is more likely to put the label word \"great\" or \"terrible\" at the [MASK] position.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_57",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_57@1",
            "content": "Fine-tuning with this fill-in-theblank framework has been shown to be superior to standard fine-tuning (Schick and Sch\u00fctze, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_57",
            "start": 118,
            "end": 247,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_57@2",
            "content": "By mapping the label space Y to the label words where V(y) denotes the label word for class y, the prediction of the model M for class y \u2208 Y can be written as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_57",
            "start": 249,
            "end": 406,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_58@0",
            "content": "p(y|x in ) = p([MASK] = V(y)|x prompt ) (4) = exp(w V(y) \u2022 h [MASK] ) y \u2208Y exp(w V(y ) \u2022 h [MASK] )(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_58",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_59@0",
            "content": "where w is the weight vector of MLM head.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_59",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_60@0",
            "content": "In LM-BFF, the authors further append demonstrations to the input x prompt to help the model better understand what is \"great\" and \"terrible\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_60",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_60@1",
            "content": "Formally, x prompt \u2261 T (x in ) and T (x c in , y c ) denote T (x c in ) with [MASK] replaced by the label word V(y c ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_60",
            "start": 143,
            "end": 261,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_60@2",
            "content": "Then, the input to LM-BFF takes the form of",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_60",
            "start": 263,
            "end": 305,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_61@0",
            "content": "T (x in ) \u2295 T (x 1 in , y 1 ) \u2295 ... \u2295 T (x |Y| in , y |Y| ) (6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_61",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_62@0",
            "content": "In this paper, we use random sampling for the demonstrations, i.e., x c in is randomly chosen from the training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_62",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_62@1",
            "content": "The masked language modeling loss is then",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_62",
            "start": 117,
            "end": 157,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_63@0",
            "content": "L MLM = (x in ,y)\u2208D train \u2212 log p(y|x in )(7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_63",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_64@0",
            "content": "C Language-based Supervised Contrastive Loss",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_64",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_65@0",
            "content": "Our method extends the loss L MLM with an additional Supervised Contrastive Loss (SupCon).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_65",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_65@1",
            "content": "For applying SupCon on multi-views of an input text, we need to first obtain a second view of a text:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_65",
            "start": 91,
            "end": 191,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_66@0",
            "content": "x2k = Aug(x 2k\u22121 )(8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_66",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_67@0",
            "content": "As we show in ablations, traditional data augmentation for text does not work well in the contrastive framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_67",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_67@1",
            "content": "Thus, we propose obtaining a second view by randomly changing the templates and/or demonstrations:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_67",
            "start": 113,
            "end": 210,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_68@0",
            "content": "x2k\u22121 = T t 0 (x in ) \u2295 ... \u2295 Tt 0 (x c in , y c ) \u2295 ... (9) x2k = T t j (x in ) \u2295 ... \u2295 Tt j (x c in , y c ) \u2295 ... (10",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_68",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_69@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_69",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_70@0",
            "content": "where T denotes a set of pre-defined templates, t j \u2208 T and t j = t 0 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_70",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_70@1",
            "content": "xc in is another randomly sampled example as the demonstration text and xc in = x in .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_70",
            "start": 72,
            "end": 157,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_70@2",
            "content": "This strategy serves as a perfect form of augmentation for our purpose as it does not generate incomplete or inconsistent sentences, and since we do not edit the main input, the label for that input stays the same.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_70",
            "start": 159,
            "end": 372,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_70@3",
            "content": "Furthermore, x2k has a substantial variation from x2k\u22121 , which allows for effective contrastive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_70",
            "start": 374,
            "end": 479,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_71@0",
            "content": "Table D.1 shows the primary prompts we used for each task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_71",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_71@1",
            "content": "Those prompts are manually chosen by LM-BFF .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_71",
            "start": 59,
            "end": 103,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_72@0",
            "content": "While we use RoBERTa-base to conduct extensive experiments in our main study and ablations, here we compare our framework to LM-BFF using RoBERTa-large.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_72",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_72@1",
            "content": "We also include results directly reported from LM-BFF (henceforth referred to as LM-BFF \u2020) , though the comparison between them could be unfair since the results reported in the original LM-BFF paper are:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_72",
            "start": 153,
            "end": 356,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_73@0",
            "content": "\u2022 obtained with an additional sampling strategy to select similar demonstrations (section 6.2 of their paper), which put our results at a disadvantage. \u2022 obtained from a set of batch sizes (2,4,8) and learning rates (1e \u22125 , 2e \u22125 , 5e \u22125 ) and the best models are selected from the validation set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_73",
            "start": 0,
            "end": 297,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_74@0",
            "content": "Whereas we show experimental results with models trained with a fixed batch size and a learning rate of 1e \u22125 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_74",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_75@0",
            "content": "Nevertheless, we show the state-of-the-art results we achieved in Table E.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_75",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_75@1",
            "content": "We only marginally under-perform LM-BFF \u2020 in 2 tasks, possibly due to the reasons listed above.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_75",
            "start": 77,
            "end": 171,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_75@2",
            "content": "87.8 (0.9) 88.1 (0.3) 87.5 (1.0) 88.5 (1.0) 87.9 (0.5) 89.4 (1.0)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_75",
            "start": 173,
            "end": 237,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_76@0",
            "content": "In the main paper, we compare our augmentation strategy (random templates and random demonstrations) to standard augmentation techniques on 10% of input tokens for creating multi-view of inputs to apply the SupCon loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_76",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_76@1",
            "content": "Here, we show additional experimental results with synonym replacement (SR), random insertion (RI), random swapping (RS), random deletion (RD) and EDA (Wei and Zou, 2019) (with SR, RI, RS and RD all together) at 20% of input tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_76",
            "start": 220,
            "end": 451,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_76@2",
            "content": "Same as before, the model under-performs when using standard augmentations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_76",
            "start": 453,
            "end": 527,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_76@3",
            "content": "Results are shown in Table F",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_76",
            "start": 529,
            "end": 556,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_77@0",
            "content": "For results in Table 2, we adapt the open-source code from ADAPET to our codebase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_77",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_77@1",
            "content": "In original ADAPET, there are multiple label words corresponding to a label class (e.g positive class: \"great\", \"good\", \"nice\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_77",
            "start": 83,
            "end": 210,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_77@2",
            "content": "To make a fair comparison to LM-BFF and ours, we only apply one label word corresponding to a label class (e.g positive class: \"great\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_77",
            "start": 212,
            "end": 347,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_77@3",
            "content": "The original Label Condition Loss implemented in ADAPET has a hyper-parameter \u03b1 to control the percentage of input tokens used for masked language modeling (see ADAPET (Tam et al., 2021) for more details).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_77",
            "start": 349,
            "end": 553,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_77@4",
            "content": "To match the training objective in LM-BFF with only one [MASK] token, we also set the Label Condition Loss to apply to one random token of inputs, i.e., \u03b1 = 1 len(input) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_77",
            "start": 555,
            "end": 725,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_78@0",
            "content": "Farid Arthaud, Rachel Bawden, Alexandra Birch, Few-shot learning through contextual data augmentation, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_78",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_79@0",
            "content": "Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai, Andrew Mccallum, Self-supervised meta-learning for few-shot natural language classification tasks, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_79",
            "start": 0,
            "end": 254,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_80@0",
            "content": "UNKNOWN, None, 2021, Semi-supervised few-shot intent classification and slot filling, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_80",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_81@0",
            "content": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, A simple framework for contrastive learning of visual representations, 2020, Proceedings of the 37th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_81",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_82@0",
            "content": "Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey Hinton, Big self-supervised models are strong semi-supervised learners, 2020, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_82",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_83@0",
            "content": "Tianyu Gao, Adam Fisch, Danqi Chen, Making pre-trained language models better few-shot learners, 2021, Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_83",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_84@0",
            "content": "Yiren Jian, Karim Ahmed, Lorenzo Torresani, Task meta-transfer from limited parallel labels, 2020, Meta-Learning workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_84",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_85@0",
            "content": "UNKNOWN, None, 2021, Metapix: Domain transfer for semantic segmentation by meta pixel weighting. Image and Vision Computing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_85",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_86@0",
            "content": "Yiren Jian, Chongyang Gao, Soroush Vosoughi, Embedding hallucination for few-shot language learning, 2022, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_86",
            "start": 0,
            "end": 292,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_87@0",
            "content": "Yiren Jian, Lorenzo Torresani, Label hallucination for few-shot classification, 2022, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_87",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_88@0",
            "content": "Akbar Karimi, Leonardo Rossi, Andrea Prati, AEDA: an easier data augmentation technique for text classification, 2021-11-20, Findings of the Association for Computational Linguistics: EMNLP 2021, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_88",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_89@0",
            "content": "Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan, Supervised contrastive learning, 2020, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_89",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_90@0",
            "content": "Varun Kumar, Hadrien Glaude, Cyprien De Lichy, Wlliam Campbell, A closer look at feature space data augmentation for few-shot intent classification, 2019, Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_90",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_91@0",
            "content": "Judith , Li , Jiong Zhang, Semi-supervised meta-learning for cross-domain few-shot intent classification, 2021, MetaNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_91",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_92@0",
            "content": "Chiyu Paul Pu Liang, Louis-Philippe Wu, Ruslan Morency,  Salakhutdinov, Towards understanding and mitigating social biases in language models, 2021, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_92",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_93@0",
            "content": "UNKNOWN, None, 2021, Bootstrapping semantic segmentation with regional contrast, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_93",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_94@0",
            "content": "Timo Schick, Hinrich Sch\u00fctze, Exploiting cloze-questions for few-shot text classification and natural language inference, 2021, EACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_94",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_95@0",
            "content": "Amr Sharaf, Hany Hassan, Hal Daum\u00e9, Iii , Meta-learning for few-shot nmt adaptation, 2020, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_95",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_96@0",
            "content": "Derek Tam, Mohit Rakesh R Menon,  Bansal, Shashank Srivastava, and Colin Raffel. 2021. Improving and simplifying pattern exploiting training, , Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_96",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_97@0",
            "content": "Yonglong Tian, Dilip Krishnan, Phillip Isola, Contrastive multiview coding, 2020, Computer Vision -ECCV 2020, Springer International Publishing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_97",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_98@0",
            "content": "Yonglong Tian, Dilip Krishnan, Phillip Isola, Contrastive representation distillation, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_98",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_99@0",
            "content": "UNKNOWN, None, 2020, What makes for good views for contrastive learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_99",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_100@0",
            "content": "Jason Wei, Chengyu Huang, Soroush Vosoughi, Yu Cheng, Shiqi Xu, Few-shot text classification with triplet networks, data augmentation, and curriculum learning, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_100",
            "start": 0,
            "end": 359,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_101@0",
            "content": "Jason Wei, Kai Zou, EDA: Easy data augmentation techniques for boosting performance on text classification tasks, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_101",
            "start": 0,
            "end": 338,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_102@0",
            "content": "Yuwen Xiong, Mengye Ren, Raquel Urtasun, Loco: Local contrastive representation learning, 2020, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_102",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_103@0",
            "content": "Sangdoo Yun, Dongyoon Han, Sanghyuk Seong Joon Oh, Junsuk Chun, Youngjoon Choe,  Yoo, Cutmix: Regularization strategy to train strong classifiers with localizable features, 2019, Proceedings of the IEEE/CVF International Conference on Computer Vision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_103",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_104@0",
            "content": "Hongyi Zhang, Moustapha Cisse, David Yann N Dauphin,  Lopez-Paz, mixup: Beyond empirical risk minimization, 2018, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_104",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "41-ARR_v2_105@0",
            "content": "Tony Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, Calibrate Before Use: Improving Few-shot Performance of Language Models, 2021, International Conference on Machine Learning (ICML), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "41-ARR_v2_105",
            "start": 0,
            "end": 192,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "41-ARR_v2_0",
            "tgt_ix": "41-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_0",
            "tgt_ix": "41-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_1",
            "tgt_ix": "41-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_1",
            "tgt_ix": "41-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_0",
            "tgt_ix": "41-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_2",
            "tgt_ix": "41-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_4",
            "tgt_ix": "41-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_5",
            "tgt_ix": "41-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_6",
            "tgt_ix": "41-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_7",
            "tgt_ix": "41-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_8",
            "tgt_ix": "41-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_3",
            "tgt_ix": "41-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_3",
            "tgt_ix": "41-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_3",
            "tgt_ix": "41-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_3",
            "tgt_ix": "41-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_3",
            "tgt_ix": "41-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_3",
            "tgt_ix": "41-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_3",
            "tgt_ix": "41-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_0",
            "tgt_ix": "41-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_12",
            "tgt_ix": "41-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_13",
            "tgt_ix": "41-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_14",
            "tgt_ix": "41-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_15",
            "tgt_ix": "41-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_16",
            "tgt_ix": "41-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_17",
            "tgt_ix": "41-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_18",
            "tgt_ix": "41-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_19",
            "tgt_ix": "41-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_20",
            "tgt_ix": "41-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_21",
            "tgt_ix": "41-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_22",
            "tgt_ix": "41-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_23",
            "tgt_ix": "41-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_24",
            "tgt_ix": "41-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_25",
            "tgt_ix": "41-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_11",
            "tgt_ix": "41-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_27",
            "tgt_ix": "41-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_26",
            "tgt_ix": "41-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_0",
            "tgt_ix": "41-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_28",
            "tgt_ix": "41-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_30",
            "tgt_ix": "41-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_31",
            "tgt_ix": "41-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_32",
            "tgt_ix": "41-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_29",
            "tgt_ix": "41-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_29",
            "tgt_ix": "41-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_29",
            "tgt_ix": "41-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_29",
            "tgt_ix": "41-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_29",
            "tgt_ix": "41-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_0",
            "tgt_ix": "41-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_33",
            "tgt_ix": "41-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_34",
            "tgt_ix": "41-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_34",
            "tgt_ix": "41-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_0",
            "tgt_ix": "41-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_35",
            "tgt_ix": "41-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_37",
            "tgt_ix": "41-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_36",
            "tgt_ix": "41-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_36",
            "tgt_ix": "41-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_36",
            "tgt_ix": "41-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_0",
            "tgt_ix": "41-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_38",
            "tgt_ix": "41-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_40",
            "tgt_ix": "41-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_39",
            "tgt_ix": "41-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_39",
            "tgt_ix": "41-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_39",
            "tgt_ix": "41-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_0",
            "tgt_ix": "41-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_41",
            "tgt_ix": "41-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_42",
            "tgt_ix": "41-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_42",
            "tgt_ix": "41-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_0",
            "tgt_ix": "41-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_43",
            "tgt_ix": "41-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_44",
            "tgt_ix": "41-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_44",
            "tgt_ix": "41-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_0",
            "tgt_ix": "41-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_45",
            "tgt_ix": "41-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_46",
            "tgt_ix": "41-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_46",
            "tgt_ix": "41-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_0",
            "tgt_ix": "41-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_47",
            "tgt_ix": "41-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_48",
            "tgt_ix": "41-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_48",
            "tgt_ix": "41-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_0",
            "tgt_ix": "41-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_49",
            "tgt_ix": "41-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_52",
            "tgt_ix": "41-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_53",
            "tgt_ix": "41-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_51",
            "tgt_ix": "41-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_55",
            "tgt_ix": "41-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_56",
            "tgt_ix": "41-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_57",
            "tgt_ix": "41-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_58",
            "tgt_ix": "41-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_59",
            "tgt_ix": "41-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_60",
            "tgt_ix": "41-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_61",
            "tgt_ix": "41-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_62",
            "tgt_ix": "41-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_63",
            "tgt_ix": "41-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_64",
            "tgt_ix": "41-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_65",
            "tgt_ix": "41-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_66",
            "tgt_ix": "41-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_67",
            "tgt_ix": "41-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_68",
            "tgt_ix": "41-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_69",
            "tgt_ix": "41-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_70",
            "tgt_ix": "41-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_72",
            "tgt_ix": "41-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_74",
            "tgt_ix": "41-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_71",
            "tgt_ix": "41-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_75",
            "tgt_ix": "41-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_76",
            "tgt_ix": "41-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "41-ARR_v2_0",
            "tgt_ix": "41-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_1",
            "tgt_ix": "41-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_2",
            "tgt_ix": "41-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_2",
            "tgt_ix": "41-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_2",
            "tgt_ix": "41-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_2",
            "tgt_ix": "41-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_2",
            "tgt_ix": "41-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_2",
            "tgt_ix": "41-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_2",
            "tgt_ix": "41-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_3",
            "tgt_ix": "41-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_4",
            "tgt_ix": "41-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_4",
            "tgt_ix": "41-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_4",
            "tgt_ix": "41-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_4",
            "tgt_ix": "41-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_4",
            "tgt_ix": "41-ARR_v2_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_5",
            "tgt_ix": "41-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_6",
            "tgt_ix": "41-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_6",
            "tgt_ix": "41-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_7",
            "tgt_ix": "41-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_7",
            "tgt_ix": "41-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_7",
            "tgt_ix": "41-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_8",
            "tgt_ix": "41-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_9",
            "tgt_ix": "41-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_10",
            "tgt_ix": "41-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_11",
            "tgt_ix": "41-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_12",
            "tgt_ix": "41-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_12",
            "tgt_ix": "41-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_12",
            "tgt_ix": "41-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_13",
            "tgt_ix": "41-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_13",
            "tgt_ix": "41-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_13",
            "tgt_ix": "41-ARR_v2_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_13",
            "tgt_ix": "41-ARR_v2_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_13",
            "tgt_ix": "41-ARR_v2_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_13",
            "tgt_ix": "41-ARR_v2_13@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_13",
            "tgt_ix": "41-ARR_v2_13@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_13",
            "tgt_ix": "41-ARR_v2_13@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_13",
            "tgt_ix": "41-ARR_v2_13@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_13",
            "tgt_ix": "41-ARR_v2_13@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_13",
            "tgt_ix": "41-ARR_v2_13@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_13",
            "tgt_ix": "41-ARR_v2_13@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_14",
            "tgt_ix": "41-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_15",
            "tgt_ix": "41-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_15",
            "tgt_ix": "41-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_16",
            "tgt_ix": "41-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_16",
            "tgt_ix": "41-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_16",
            "tgt_ix": "41-ARR_v2_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_16",
            "tgt_ix": "41-ARR_v2_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_16",
            "tgt_ix": "41-ARR_v2_16@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_16",
            "tgt_ix": "41-ARR_v2_16@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_17",
            "tgt_ix": "41-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_18",
            "tgt_ix": "41-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_19",
            "tgt_ix": "41-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_19",
            "tgt_ix": "41-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_20",
            "tgt_ix": "41-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_21",
            "tgt_ix": "41-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_21",
            "tgt_ix": "41-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_21",
            "tgt_ix": "41-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_21",
            "tgt_ix": "41-ARR_v2_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_22",
            "tgt_ix": "41-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_23",
            "tgt_ix": "41-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_24",
            "tgt_ix": "41-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_24",
            "tgt_ix": "41-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_24",
            "tgt_ix": "41-ARR_v2_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_25",
            "tgt_ix": "41-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_25",
            "tgt_ix": "41-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_26",
            "tgt_ix": "41-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_27",
            "tgt_ix": "41-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_27",
            "tgt_ix": "41-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_27",
            "tgt_ix": "41-ARR_v2_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_27",
            "tgt_ix": "41-ARR_v2_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_28",
            "tgt_ix": "41-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_28",
            "tgt_ix": "41-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_28",
            "tgt_ix": "41-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_28",
            "tgt_ix": "41-ARR_v2_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_29",
            "tgt_ix": "41-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_30",
            "tgt_ix": "41-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_30",
            "tgt_ix": "41-ARR_v2_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_31",
            "tgt_ix": "41-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_31",
            "tgt_ix": "41-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_31",
            "tgt_ix": "41-ARR_v2_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_32",
            "tgt_ix": "41-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_32",
            "tgt_ix": "41-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_33",
            "tgt_ix": "41-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_33",
            "tgt_ix": "41-ARR_v2_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_33",
            "tgt_ix": "41-ARR_v2_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_33",
            "tgt_ix": "41-ARR_v2_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_33",
            "tgt_ix": "41-ARR_v2_33@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_33",
            "tgt_ix": "41-ARR_v2_33@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_33",
            "tgt_ix": "41-ARR_v2_33@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_33",
            "tgt_ix": "41-ARR_v2_33@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_34",
            "tgt_ix": "41-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_35",
            "tgt_ix": "41-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_35",
            "tgt_ix": "41-ARR_v2_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_35",
            "tgt_ix": "41-ARR_v2_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_35",
            "tgt_ix": "41-ARR_v2_35@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_35",
            "tgt_ix": "41-ARR_v2_35@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_36",
            "tgt_ix": "41-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_37",
            "tgt_ix": "41-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_37",
            "tgt_ix": "41-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_37",
            "tgt_ix": "41-ARR_v2_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_37",
            "tgt_ix": "41-ARR_v2_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_37",
            "tgt_ix": "41-ARR_v2_37@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_37",
            "tgt_ix": "41-ARR_v2_37@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_38",
            "tgt_ix": "41-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_39",
            "tgt_ix": "41-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_40",
            "tgt_ix": "41-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_40",
            "tgt_ix": "41-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_40",
            "tgt_ix": "41-ARR_v2_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_40",
            "tgt_ix": "41-ARR_v2_40@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_40",
            "tgt_ix": "41-ARR_v2_40@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_40",
            "tgt_ix": "41-ARR_v2_40@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_41",
            "tgt_ix": "41-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_42",
            "tgt_ix": "41-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_43",
            "tgt_ix": "41-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_43",
            "tgt_ix": "41-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_43",
            "tgt_ix": "41-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_43",
            "tgt_ix": "41-ARR_v2_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_43",
            "tgt_ix": "41-ARR_v2_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_44",
            "tgt_ix": "41-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_45",
            "tgt_ix": "41-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_45",
            "tgt_ix": "41-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_45",
            "tgt_ix": "41-ARR_v2_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_46",
            "tgt_ix": "41-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_47",
            "tgt_ix": "41-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_47",
            "tgt_ix": "41-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_48",
            "tgt_ix": "41-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_49",
            "tgt_ix": "41-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_50",
            "tgt_ix": "41-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_51",
            "tgt_ix": "41-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_51",
            "tgt_ix": "41-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_51",
            "tgt_ix": "41-ARR_v2_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_51",
            "tgt_ix": "41-ARR_v2_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_52",
            "tgt_ix": "41-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_52",
            "tgt_ix": "41-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_52",
            "tgt_ix": "41-ARR_v2_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_52",
            "tgt_ix": "41-ARR_v2_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_53",
            "tgt_ix": "41-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_53",
            "tgt_ix": "41-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_53",
            "tgt_ix": "41-ARR_v2_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_54",
            "tgt_ix": "41-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_55",
            "tgt_ix": "41-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_55",
            "tgt_ix": "41-ARR_v2_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_55",
            "tgt_ix": "41-ARR_v2_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_56",
            "tgt_ix": "41-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_57",
            "tgt_ix": "41-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_57",
            "tgt_ix": "41-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_57",
            "tgt_ix": "41-ARR_v2_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_58",
            "tgt_ix": "41-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_59",
            "tgt_ix": "41-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_60",
            "tgt_ix": "41-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_60",
            "tgt_ix": "41-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_60",
            "tgt_ix": "41-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_61",
            "tgt_ix": "41-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_62",
            "tgt_ix": "41-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_62",
            "tgt_ix": "41-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_63",
            "tgt_ix": "41-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_64",
            "tgt_ix": "41-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_65",
            "tgt_ix": "41-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_65",
            "tgt_ix": "41-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_66",
            "tgt_ix": "41-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_67",
            "tgt_ix": "41-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_67",
            "tgt_ix": "41-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_68",
            "tgt_ix": "41-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_69",
            "tgt_ix": "41-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_70",
            "tgt_ix": "41-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_70",
            "tgt_ix": "41-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_70",
            "tgt_ix": "41-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_70",
            "tgt_ix": "41-ARR_v2_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_71",
            "tgt_ix": "41-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_71",
            "tgt_ix": "41-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_72",
            "tgt_ix": "41-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_72",
            "tgt_ix": "41-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_73",
            "tgt_ix": "41-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_74",
            "tgt_ix": "41-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_75",
            "tgt_ix": "41-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_75",
            "tgt_ix": "41-ARR_v2_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_75",
            "tgt_ix": "41-ARR_v2_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_76",
            "tgt_ix": "41-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_76",
            "tgt_ix": "41-ARR_v2_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_76",
            "tgt_ix": "41-ARR_v2_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_76",
            "tgt_ix": "41-ARR_v2_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_77",
            "tgt_ix": "41-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_77",
            "tgt_ix": "41-ARR_v2_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_77",
            "tgt_ix": "41-ARR_v2_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_77",
            "tgt_ix": "41-ARR_v2_77@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_77",
            "tgt_ix": "41-ARR_v2_77@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_78",
            "tgt_ix": "41-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_79",
            "tgt_ix": "41-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_80",
            "tgt_ix": "41-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_81",
            "tgt_ix": "41-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_82",
            "tgt_ix": "41-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_83",
            "tgt_ix": "41-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_84",
            "tgt_ix": "41-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_85",
            "tgt_ix": "41-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_86",
            "tgt_ix": "41-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_87",
            "tgt_ix": "41-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_88",
            "tgt_ix": "41-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_89",
            "tgt_ix": "41-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_90",
            "tgt_ix": "41-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_91",
            "tgt_ix": "41-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_92",
            "tgt_ix": "41-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_93",
            "tgt_ix": "41-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_94",
            "tgt_ix": "41-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_95",
            "tgt_ix": "41-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_96",
            "tgt_ix": "41-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_97",
            "tgt_ix": "41-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_98",
            "tgt_ix": "41-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_99",
            "tgt_ix": "41-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_100",
            "tgt_ix": "41-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_101",
            "tgt_ix": "41-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_102",
            "tgt_ix": "41-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_103",
            "tgt_ix": "41-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_104",
            "tgt_ix": "41-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "41-ARR_v2_105",
            "tgt_ix": "41-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 689,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "41-ARR",
        "version": 2
    }
}