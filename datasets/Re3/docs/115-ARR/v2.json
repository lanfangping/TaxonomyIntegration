{
    "nodes": [
        {
            "ix": "115-ARR_v2_0",
            "content": "Multi-Modal Sarcasm Detection via Cross-Modal Graph Convolutional Network",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_2",
            "content": "With the increasing popularity of posting multimodal messages online, many recent studies have been carried out utilizing both textual and visual information for multi-modal sarcasm detection. In this paper, we investigate multimodal sarcasm detection from a novel perspective by constructing a cross-modal graph for each instance to explicitly draw the ironic relations between textual and visual modalities. Specifically, we first detect the objects paired with descriptions of the image modality, enabling the learning of important visual information. Then, the descriptions of the objects are served as a bridge to determine the importance of the association between the objects of image modality and the contextual words of text modality, so as to build a cross-modal graph for each multi-modal instance. Furthermore, we devise a cross-modal graph convolutional network to make sense of the incongruity relations between modalities for multi-modal sarcasm detection. Extensive experimental results and in-depth analysis show that our model achieves state-of-the-art performance in multi-modal sarcasm detection 1 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "115-ARR_v2_4",
            "content": "Sarcasm is a peculiar form of sentiment expressions, allowing individuals to express contempt sentiment or intention that is converse to the authentic/apparent sentiment information (Gibbs, 1986;Dews and Winner, 1995;Gibbs, 2007). As such, accurately detecting satirical/ironic expression could potentially improve the performance of sentiment analysis and opinion mining (Pang and Lee, 2008;Kumar Jena et al., 2020;Pan et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_5",
            "content": "In today's fast growing social media platforms, it is common to post multi-modal messages. Therefore, in addition to developing sarcasm detection models for textual data (Riloff et al., 2013;Joshi et al., 2015), it is increasingly popular to explore sarcasm detection in multi-modal data such as text and images (Schifanella et al., 2016;Cai et al., 2019). Dealing with multimodal data requires an understanding of the information presented in different modalities. As the sarcastic example shown in Figure 1 (a), text-only approaches may erroneously identify it as a positive sentiment expression due to the phrase \"wonderful weather\". This post however contains a sarcastic expression with negative sentiment, because it is accompanied by an image with \"thunderstorm clouds\". The key of effective multi-modal sarcasm detection is to accurately extract the incongruent sentiment cues from different modalities, allowing the detection of the true sentiment conveyed in the message.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_6",
            "content": "To perform multi-modal sarcasm detection on data composed of text and image, several related research efforts attempt to concatenate the textual and visual features to fuse sarcastic information (Schifanella et al., 2016), employ attention mechanism to implicitly fuse the features of different modalities based on external knowledge (Cai et al., 2019;Xu et al., 2020;Pan et al., 2020), or build interactive graphs to model the relations of different modalities (Liang et al., 2021a). Despite promising progress made by existing models, they still suffer from the following limitations: 1) Simply considering the whole image does not produce good results, mostly due to the intricate visual information presented in an image; not to mention that only particular visual patches are related to the text. As in the examples shown in Figure 1, the correct results can be easily obtained by only tracking the visual information in the bounding boxes. Therefore, discriminating key visual objects from the irrelevant ones could lead to improved learning of visual information. 2) Crucial visual information that relates to the sarcastic cues of text modality may be scattered in an image (Figure 1 (b)). As such, it is essential to focus on drawing the intricate sentiment connections between text and image modalities, allowing a good exploitation of the contradictory sentiment information between modalities for learning sarcastic clues.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_7",
            "content": "To this end, we propose a novel cross-modal graph convolutional networks (CMGCN) by constructing a cross-modal graph for each instance, where the important visual information and the related textual tokens are explicitly linked. This allows for the extraction of incongruous implications between two modalities in sarcasm detection. Concretely, instead of trying to produce a caption of the whole image, we first detect the objects of the image to capture the important visual regions and the corresponding attribute-object pairs via the approach proposed by Anderson et al. (2018). Then, we explore a novel solution to assign weights to the edges of the cross-modal graph by means of computing the word similarities between the object descriptors of the attribute-object pairs and textual words based on the WordNet (Miller, 1992). Further, to introduce the multi-modal sentiment relations into the cross-modal graphs, inspired by (Lou et al., 2021), we devise a modulating factor of sentiment relation for each edge by retrieving the affective weights of attribute descriptors (usually adjectives with affective information) and textual words from external affective knowledge (SenticNet (Cambria et al., 2020)). As such, the modulating factors can be adopted to refine the edge weights of word similarities, allowing the capture of sentiment incongruities of the cross-modal nodes in the graph. Further, in the light of crossmodal graphs, we deploy a GCN architecture to make sense of the incongruous relations across the modalities for multi-modal sarcasm detection.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_8",
            "content": "The main contributions of our work are summarized as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_9",
            "content": "\u2022 To the best of our knowledge, we are the first to explore the use of the graph model based on auxiliary object detection for modeling the contradictory sentiments between key textual and visual information in multi-modal sarcasm detection. \u2022 Using the attribute-object pairs of the image objects as the bridge, a novel approach of constructing cross-modal graphs is developed to explicitly link the two modalities by edges with the varying degree of importance. \u2022 A series of experiments on a publicly available multi-modal sarcasm detection benchmark dataset show that our proposed method achieves the state-of-the-art performance.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_10",
            "content": "2 Related Work",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_11",
            "content": "Multi-modal Sarcasm Detection",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "115-ARR_v2_12",
            "content": "Previous work of sarcasm detection has been applied to textual utterances information Tay et al., 2018;Babanejad et al., 2020 the graph-based methods, Liang et al. (2021a) deployed a heterogeneous graph structure to learn the sarcastic features from both intra-and intermodality perspectives. However, this method tried to grasp the visual information of the whole image, and meanwhile ignore the sentiment expression between different modalities. Therefore, different from (Liang et al., 2021a), we explore a novel cross-modal GCN model based on the important visual information and sentiment cues to leverage the inconsistent implications between different modalities and thus improve the performance of multimodal sarcasm detection.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_13",
            "content": "Graph Neural Networks",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "115-ARR_v2_14",
            "content": "Models based on graph neural networks (GNN), including graph convolutional network (GCN) (Kipf and Welling, 2017) and graph attention network (GAT) (Velickovic et al., 2018), have achieved promising performance in many recent research studies, such as visual representation learning (Wu et al., 2019;Xie et al., 2021), text representation learning (Yao et al., 2019;Lou et al., 2021;Liang et al., 2021bLiang et al., , 2022, and recommendation systems (Ying et al., 2018;Tan et al., 2020). Further, there are also some research studies explored graph models to deal with the multi-modal tasks, such as multi-modal sentiment detection (Yang et al., 2021), multi-modal named entity recognition , cross-modal video moment retrieval (Zeng et al., 2021), multi-modal neural machine translation (Yin et al., 2020), and multimodal sarcasm detection (Liang et al., 2021a).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_15",
            "content": "Methodology",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "115-ARR_v2_16",
            "content": "In this section, we describe our proposed Cross-Modal Graph Convolutional Networks (CMGCN) model for multi-modal sarcasm detection in details.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_17",
            "content": "As demonstrated in Figure 2, the architecture of the proposed CMGCN contains four main components: 1) Text-modality representation, which employs the pre-trained uncased BERT-base model (Devlin et al., 2019) as the text encoder to capture the hidden representation of the text-modality; 2) Image-modality representation, which deploys the pre-trained Vision Transformer (ViT) (Dosovitskiy et al., 2021) as the image encoder to capture the hidden representation of the image-modality with respect to each bounding box (visual region); 3) Cross-modal graph, which constructs a crossmodal graph for each multi-modal example based on the external affective knowledge source and the hidden representations of text and image modalities; 4) Multi-modal fusion, which fuses the representations from image and text modalities to capture the sarcastic features by means of a GCN structure and an attention mechanism.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_18",
            "content": "Text-modality Representation",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "115-ARR_v2_19",
            "content": "For text processing, given a sequence of words s = {w i } n i=1 , n is the length of the text s. We first adopt the pre-trained uncased BERT-base model (Devlin et al., 2019) to map each word w i into a d T -dimensional embedding:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_20",
            "content": "X T = [x 1 , x 2 , \u2022 \u2022 \u2022 , x n ] = BERT([CLS]s[SEP])",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_21",
            "content": "(1) Where X T is the embedding matrix of the input text. Here, the representations of tokens [CLS] and [SEP] are not utilized in constructing the cross-modal graph. Subsequently, to unify the dimensions of representations between different modalities and capture the sequential relations of the context, we utilize a bidirectional LSTM (Bi-LSTM) to learn the text-modality representation of the input text:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_22",
            "content": "T = {t 1 , t 2 , \u2022 \u2022 \u2022 , t n } = Bi-LSTM(X T ) (2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_23",
            "content": "Where t j \u2208 R 2d h denotes the hidden state vector at time step j from the bidirectional LSTM, d h denotes the dimensionality of the text-modality hidden state representation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_24",
            "content": "Image-modality Representation",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "115-ARR_v2_25",
            "content": "For image processing, given an image I, we first adopt a trained toolkit proposed by Anderson et al. (2018) to derive a series of bounding boxes (objects) paired with their attribute-object pairs. For each visual region of the bounding box I i \u2208 R L h \u00d7Lw , following (Xu et al., 2020), we first resize it to 224 \u00d7 224, i.e. L = L h = L w = 224. Subsequently, following (Dosovitskiy et al., 2021), we reshape the region",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_26",
            "content": "I i \u2208 R L\u00d7L into a sequence I i = {p j \u2208 R L/p\u00d7L/p } r j=1",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_27",
            "content": ", where r = p \u00d7 p is the number of patches. Then, we flatten and map each patch to a d I -dimensional vector with a trainable linear projection:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_28",
            "content": "z j = p j E.",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_29",
            "content": "For each sequence of image patches, a [class] token embedding z [class] \u2208 R d I is prepended for the sequence of embedded patches, and position embeddings are added to the patch embeddings to retain positional information. The input of each visual region I i is represented as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_30",
            "content": "Z i = [z [class] ; z 1 ; z 2 ; \u2022 \u2022 \u2022 ; z r ] + E pos (3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_31",
            "content": "Where Z i \u2208 R (r+1)\u00d7d I is the input matrix of the image patches, and E pos \u2208 R (r+1)\u00d7d I is the position embedding matrix. Then, we feed the input matrix Z i into the ViT encoder to acquire the representation h i of visual region I i :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_32",
            "content": "H i = ViT(Z i ), h i = H i,[class]",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_33",
            "content": "(4)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_34",
            "content": "We use the representation of the [class] token embedding to represent the visual region. Finally, the representation of the image I is defined as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_35",
            "content": "X I = {h 1 , h 2 , \u2022 \u2022 \u2022 , h m } (5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_36",
            "content": "Where m is the number of visual regions. Subsequently, we employ a trainable Linear Projection to map each v i to a 2d h -dimensional vector:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_37",
            "content": "V = {v 1 , v 2 , \u2022 \u2022 \u2022 , v m } = X I W V (6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_38",
            "content": "Where W V \u2208 R d I \u00d72d h is a trainable parameter.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_39",
            "content": "Cross-modal Graph",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "115-ARR_v2_40",
            "content": "In this section, we describe how to construct a cross-modal graph. To leverage the relations between multi-modal features, we employ a graph structure to link the textual words with the associated image objects. Here, the nodes of the crossmodal graph are the representations of text and image modalities. Many GCN-based approaches have demonstrated that the weights of the edges are crucial in graph information aggregation (Liang et al., 2021b;Yang et al., 2021;Lou et al., 2021). As such, constructing a cross-modal graph boils down to the setting of the edge weights in the graph.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_41",
            "content": "To this end, we explore a novel approach of setting the weights based on both word similarities and affective clues between textual words and the attribute-object pairs of the image regions, and the dependency tree of the text-modality. The adjacency matrix A \u2208 R (n+m)\u00d7(n+m) of the crossmodal graph is defined as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_42",
            "content": "A i,j = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 if D i,j and i < n, j < n \u03ba i,j if i < n, j \u2265 n 0 otherwise (7) \u03ba i,j = Sim(w i , o j ) \u00d7 \u03be i,j + 1 (8) \u03be i,j = \u03b3 \u2212\u03c9(w i )\u03c9(a j ) \u00d7 |\u03c9(w i ) \u2212 \u03c9(a j )| (9)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_43",
            "content": "Where D i,j indicates that there is a relation between w i and w j in the dependency tree of the sentence. Sim(\u2022) represents the computation of word similarity 2 . We set Sim(\u2022) = 0 if the return value is N one. \u03be i,j is a modulating factor refers to the sentiment relation (sentiment incongruity) between an image region and a text token. \u03c9(w i ) \u2208 [\u22121, 1] represents the affective weight of word w i retrieved from SenticNet (Cambria et al., 2020). We set \u03c9(w i ) = 0 if w i cannot be found in SenticNet. |\u2022| represents absolute value calculation. a j and o j respectively denote the attribute and the object of the bounding box j. Inspired by Kipf and Welling (2017), we construct the cross-modal graph as an undirected graph, A i,j = A j,i , and set a self-loop for each node, A i,i = 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_44",
            "content": "The intention of the cross-modal graph construction (Equations 7 and 9) is that: 1) As in the examples shown in Figure 1, the sarcastic information of text-modality may be expressed by multiple words, such as \"wonderful weather\". Therefore, we incorporate the syntax-aware relations over the dependency tree of the sentence into the cross-modal graph to advance the learning of the contextual dependencies 3 . 2) We devise a coefficient \u03ba i,j , which is associated with the affective weights, to modulate the influence of contrary sentiment relations. Here, \u03b3 > 1 is a tuned hyper-parameter to regulate the bias of inconsistent sentiment relations. That is, if the polarities of \u03c9(w i ) and \u03c9(a j ) are opposite, the value of \u03b3 is boosted, otherwise the value is shrunk. Especially, the greater the affective weights, the higher the confidence that the value of \u03b3 is boosted or shrunk. 3) We add 1 to the cross-modal edges to pay more attention to the cross-modal nodes aggregation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_45",
            "content": "Multi-modal Fusion",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "115-ARR_v2_46",
            "content": "For each instance, we explore a graph architecture to extract the crucial sarcastic clues by aggregating the correlation of nodes in the cross-modal graph. Concretely, we feed the adjacency matrix of the cross-modal graph A and the corresponding nodes' representations R of each multi-modal example into a multi-layers GCNs architecture to derive the graph representation. For each graph convolutional operation, each node in the l-th GCN layer is updated according to the hidden representations of its neighborhoods according to the adjacency matrices of the cross-modal graph, which is defined as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_47",
            "content": "G l = ReLU( \u00c3G l\u22121 W l + b l )(10)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_48",
            "content": "Where \u00c3 = D \u2212 1 2 AD \u2212 1 2 is the normalized symmetric adjacency matrix. D is the degree matrix of A, where D ii = j A i,j . G l\u22121 is the hidden graph representation evolved from the preceding GCN layer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_49",
            "content": "W l \u2208 R 2d h \u00d72d h , b l \u2208 R 2d h",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_50",
            "content": "are the trainable parameters of the l-th GCN layer. The nodes input of the first GCN layer are the concatenation of text-modality and imagemodality representations:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_51",
            "content": "G 0 = R. Here, R = {r 1 , r 2 , \u2022 \u2022 \u2022 , r n+m } = {t 1 , \u2022 \u2022 \u2022 , t n , v 1 , \u2022 \u2022 \u2022 , v m }.",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_52",
            "content": "Subsequently, inspired by , we employ a retrieval-based attention mechanism to capture the graph-oriented attention information from the concatenation of text and image representations R = {r 1 , r 2 , \u2022 \u2022 \u2022 , r n+m } by means of the graph representation g derived from the final GCN layer. The intention is to retrieve crucially associated cross-modal features that are explicitly connected in the cross-modal graph. The attention weights are computed as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_53",
            "content": "\u03b1 t = exp(\u03b2 t ) n+m i=1 exp(\u03b2 i ) , \u03b2 t = i\u2208C r \u22a4 t g i (11)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_54",
            "content": "Where C denotes a set of indices in which nodes contain cross-modal edges in the graph. \u22a4 represents the matrix transposition. The final sarcastic representation is defined as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_55",
            "content": "f = n+m t=1 \u03b1 t r t(12)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_56",
            "content": "Then, the final sarcastic representation is fed into a fully-connected layer with a softmax function to capture a probability distribution \u0177 \u2208 R dp in the sarcasm decision space:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_57",
            "content": "\u0177 = softmax(W o f + b o )(13)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_58",
            "content": "Where d p is the dimensionality of sarcasm labels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_59",
            "content": "W o \u2208 R dp\u00d72d h and b o \u2208 R dp are trainable parameters.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_60",
            "content": "Learning Objective",
            "ntype": "title",
            "meta": {
                "section": "3.5"
            }
        },
        {
            "ix": "115-ARR_v2_61",
            "content": "We minimize the cross-entropy loss via the standard gradient descent algorithm to train the model:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_62",
            "content": "min \u0398 L = \u2212 N i=1 dp j=1 y j i log\u0177 j i + \u03bb||\u0398|| 2 (14)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_63",
            "content": "where N is the training data size. y i and \u0177i respectively represent the ground-truth and estimated label distribution of instance i. \u0398 denotes all trainable parameters of the model, \u03bb represents the coefficient of L 2 -regularization.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_64",
            "content": "Experimental Settings",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "115-ARR_v2_65",
            "content": "For a fair comparison, the data preprocessing follows (Cai et al., 2019). We set the maximum number of visual regions as 10 for object detection results. That is, we select the top 10 bounding boxes with highest scores if the objects are greater than 10. We utilize the pre-trained uncased BERT-base (Devlin et al., 2019) module to embed each word of text-modality as a 768-dimensional embedding and employ the pre-trained ViT 4 (Dosovitskiy et al., 2021) to embed each visual region patch as a 768dimensional embedding, i.e. d T = d I = 768. The resolution of visual region patch is set to L p = 32, correspondingly, p = 7, r = 49. 5 The number of GCN layers is set to 2, which is the optimal depth in the pilot experiments. The dimensionality of hidden representations is set to d h = 512. The coefficient \u03bb is set to 0.00001. Adam is utilized as the optimizer with a learning rate of 0.00002, and the mini-batch size is 32. The dropout rate with 0.1 is utilized to avoid overfitting. We use early-stopping with patience of 5. We set \u03b3 = 3 to compute the modulating factor of incongruous multi-modal sentiment relations, which is the optimal hyper-parameter in the pilot experiments. Following (Cai et al., 2019), we use Accuracy, Precision, Recall, and F1-score to measure the model performance. Since the label distribution of the dataset is imbalanced, following (Pan et al., 2020), we also report Macro-average results. The experimental results of our models are averaged over 10 runs with different random seeds to ensure the final reported results are statistically stable.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_66",
            "content": "Comparison Models",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "115-ARR_v2_67",
            "content": "We compare our proposed CMGCN model with a series of strong baselines, summarized as follow: 1) Image-modality methods: These models use only visual information for sarcasm detection, including Image (Cai et al., 2019), which employs ResNet (He et al., 2016) to train a sarcasm classifier; and ViT (Dosovitskiy et al., 2021), which utilizes the '[class]' token representation of the pre-trained ViT to detect the sarcasm.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_68",
            "content": "2) Text-modality methods: These models use only textual information, including TextCNN (Kim, 2014), a deep learning model based on CNN for text classification; Bi-LSTM, a bidirectional LSTM network for text classification; SIARN (Tay et al., 2018), adopting inner-attention for textual sarcasm detection; SMSD (Xiong et al., 2019), exploring a self-matching network to capture textual incongruity information; and BERT (Devlin et al., 2019), the vanilla pre-trained uncased BERT-base taking '[CLS] text [SEP]' as input.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_69",
            "content": "3) Multi-modal methods: These models take both text-and image-modality information. Including HFM (Cai et al., 2019), a hierarchical multimodal features fusion model for multi-modal sarcasm detection; D&R Net (Xu et al., 2020), a Decomposition and Relation Network modeling both crossmodality contrast and semantic association; Res-BERT (Pan et al., 2020), concatenating image features and BERT-based text features for sarcasm prediction; Att- BERT (Pan et al., 2020), exploring an inter-modality attention and a co-attention to model the incongruity of multi-modal sarcasm detection; and InCrossMGs (Liang et al., 2021a), a graph-based model to leverage the sarcastic relations from both intra-and inter-modal perspectives.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_70",
            "content": "We also explore several variants of CMGCN to analyze the impact of different components in the ablation study: 1) w/o G denotes without crossmodal graph, which only concatenates the representations of '[class]' and '[CLS]' tokens from ViT and BERT for sarcasm detection; 2) w/o O denotes without object detection. The whole image is input into the image encoder, and the edge weights are set to 1 in the cross-modal graphs; 3) w/o S denotes without using external knowledge. All weights of edges are set to 1 in the cross-modal graph. Further, 4) w/o S w represents without using affective knowledge; 5) w/o D denotes without using syntax-aware information of text-modality in graph construction. Further, to investigate the effectiveness of our CMGCN when used with different pre-trained models, we also set the following variants: 1) -GloVe+ResNet: We replace BERT with GloVe (Pennington et al., 2014) to initialize each word into a 300-dimensional embedding and ViT with ResNet-152 (He et al., 2016) to embed each image patch as a 2048-dimensional vector.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_71",
            "content": "2) -GloVe+ViT: We use GloVe as text encoder and use ViT as image encoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_72",
            "content": "3) -BERT+ResNet: We use BERT as text encoder and use ResNet-152 as image encoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_73",
            "content": "Experimental Results",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "115-ARR_v2_74",
            "content": "Main Results",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "115-ARR_v2_75",
            "content": "We report the comparison results regarding Textmodality, Image-modality, and Text+Image modalities in Table 2. From the results, we can draw the following conclusions. 1) Our proposed CMGCN outperforms existing baselines across all metrics. This verifies the effectiveness of our proposed model in multi-modal sarcasm detection. 2) We conduct significance tests of our CMGCN over the baseline models, the results show that our CMGCN significantly outperforms the baseline models in terms of most of the evaluation metrics (with p\u2212value < 0.05). 3) Our CMGCN model performs consistently better than the previous graph-based method (InCrossMGs), which demonstrates that recognizing significant visual regions and modeling sentiment relations can lead to improved performance. 4) The methods based on text modality achieve consistently better performance than the methods based on image modality, which shows that the expression of sarcastic/nonsarcastic information primarily resides in the text modality. 5) Methods based on both image and text modalities perform better than the unimodal baselines overall. This implies that leveraging the information of both image and text modalities is more effective for multi-modal sarcasm detection.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_76",
            "content": "6) The results of macro metrics are better than other commonly used metrics overall, which indicates that models perform better in the \"negative\" class due to the imbalanced class distribution.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_77",
            "content": "Ablation Study",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "115-ARR_v2_78",
            "content": "To analyze the impact of different components of our proposed CMGCN, we conduct an ablation study and report the results in Table 3. Note that removal of cross-modal graph (w/o G) sharply degrades the performance, which verifies the significance of cross-modal in multi-modal features fusion for learning sarcastic expressions in multimodal sarcasm detection. Removal of object detection (w/o O) leads to considerable performance degradation, which demonstrates that adopting object detection to track important visual information is effective for constructing crucial relations between visual and textual information in the crossmodal graphs. From the results of w/o S and S w , we conclude that exploiting the attribute-object pair as a bridge to set edge weights based on word similarity is effective when constructing cross-modal graphs. Further, leveraging affective clues to capture multi-modal sentiment incongruity between text-and image-modality is effective in sarcasm detection, and thus leads to improved performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_79",
            "content": "In addition, removal of syntax-aware information of text-modality leads to slight performance degradation, which indicates that incorporating syntactic information in the graph makes better learning of dependency relations of textual words and thus improves the performance of sarcasm detection.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_80",
            "content": "Generalizability of Cross-modal Graph",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "115-ARR_v2_81",
            "content": "To investigate the generalizability and effectiveness of our proposed cross-modal graph when used with different pre-trained methods, we conduct experiments with five variants of our proposed CMGCN by using different text and image encoders. The experimental results are shown in Figure 3 (a). Note that the proposed cross-modal graph can directly work with various pre-trained models and performs consistently better than that without cross-modal graph (w/o G). This demonstrates the generalizability and effectiveness of our proposed cross-modal graph in multi-modal sarcasm detection. Further, from the results, we can also conclude that superior performance is obtained when using more powerful pre-trained methods, such as ViT and BERT.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_82",
            "content": "Impact of GCN Layers",
            "ntype": "title",
            "meta": {
                "section": "5.4"
            }
        },
        {
            "ix": "115-ARR_v2_83",
            "content": "In this section, we analyze the impact of the number of GCN layers on the performance of our proposed CMGCN. We vary the layer number from 1 to 6 and report the results in Figure 3 (b). Note that the 2-layer GCN architecture performs better than others overall, and thus the number of GCN layers is set to 2 in our model. Model with one layer performs worse, which indicates that a shallow graph network structure is not able to learn sarcastic features well. When the number of layers is greater than 2, the performance tends to decline. This shows that further increasing the number of layers beyond 2 degrades the model performance possibly due to the sharp increase of parameters.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_84",
            "content": "Visualization",
            "ntype": "title",
            "meta": {
                "section": "5.5"
            }
        },
        {
            "ix": "115-ARR_v2_85",
            "content": "To",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_86",
            "content": "Conclusion and Future Work",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "115-ARR_v2_87",
            "content": "This paper has proposed a novel cross-modal graph architecture for multi-modal sarcasm detection, in which the crucial visual regions can be explicitly connected to the highly correlated textual tokens for learning the incongruity sentiment of sarcastic expression. Specifically, unlike previous research efforts that simply consider the visual information of the whole image, we attempt to recognize the important visual regions via object detection results, and further devise a novel cross-modal graph to explicitly establish the connections of scattered visual regions and the associated textual tokens. More concretely, owing to the object detection results, the attribute-object pair descriptors of the objects are served as a bridge to track the highly related sarcastic cues between image and text modalities and their connection weights, and then deriving the cross-modal graphs based on external knowledge bases. Afterwards, a GCNs architecture based on a retrieval-based attention mechanism is employed to capture the key incongruity sentiment expressions across different modalities for multi-modal sarcasm detection. To the best of our knowledge, it is the first study of utilizing a cross-modal graph to extract intricate multi-modal sarcastic relations via object detection and sentiment cues from external knowledge bases. Extensive experiments on a public benchmark dataset show that our proposed approach significantly outperforms state-of-the-art baseline methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_88",
            "content": "As described in Section 3.3, the weights of edges in the cross-modal graph are computed based on both word similarities and affective clues between textual words and the attribute-object pairs of the image regions, and the dependency tree of the textmodality. The approach can be easily generalized to other sentiment-related multi-modal learning scenarios. Nevertheless, the cross-graph solution might not be generalized well to other multi-modal tasks or data genres, if there is a lack of affective knowledge or a difficulty in deriving dependency trees in low-resource settings. Therefore, future research can consider exploiting alternatively approaches to automatically learn the weights of edges in the cross-modal graph without relying on external knowledge sources.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "115-ARR_v2_89",
            "content": "Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang, Bottom-up and top-down attention for image captioning and visual question answering, 2018-06-18, 2018 IEEE Conference on Computer Vision and Pattern Recognition, IEEE Computer Society.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Peter Anderson",
                    "Xiaodong He",
                    "Chris Buehler",
                    "Damien Teney",
                    "Mark Johnson",
                    "Stephen Gould",
                    "Lei Zhang"
                ],
                "title": "Bottom-up and top-down attention for image captioning and visual question answering",
                "pub_date": "2018-06-18",
                "pub_title": "2018 IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": "IEEE Computer Society"
            }
        },
        {
            "ix": "115-ARR_v2_90",
            "content": "Nastaran Babanejad, Heidar Davoudi, Affective and contextual embedding for sarcasm detection, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Nastaran Babanejad",
                    "Heidar Davoudi"
                ],
                "title": "Affective and contextual embedding for sarcasm detection",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_91",
            "content": "Yitao Cai, Huiyu Cai, Xiaojun Wan, Multimodal sarcasm detection in Twitter with hierarchical fusion model, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Yitao Cai",
                    "Huiyu Cai",
                    "Xiaojun Wan"
                ],
                "title": "Multimodal sarcasm detection in Twitter with hierarchical fusion model",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_92",
            "content": "Erik Cambria, Yang Li, Frank Xing, Soujanya Poria, Kenneth Kwok, Senticnet 6: Ensemble application of symbolic and subsymbolic AI for sentiment analysis, 2020-10-19, CIKM '20: The 29th ACM International Conference on Information and Knowledge Management, ACM.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Erik Cambria",
                    "Yang Li",
                    "Frank Xing",
                    "Soujanya Poria",
                    "Kenneth Kwok"
                ],
                "title": "Senticnet 6: Ensemble application of symbolic and subsymbolic AI for sentiment analysis",
                "pub_date": "2020-10-19",
                "pub_title": "CIKM '20: The 29th ACM International Conference on Information and Knowledge Management",
                "pub": "ACM"
            }
        },
        {
            "ix": "115-ARR_v2_93",
            "content": "Santiago Castro, Devamanyu Hazarika, Ver\u00f3nica P\u00e9rez-Rosas, Roger Zimmermann, Rada Mihalcea, Soujanya Poria, Towards multimodal sarcasm detection (an _Obviously_ perfect paper), 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Santiago Castro",
                    "Devamanyu Hazarika",
                    "Ver\u00f3nica P\u00e9rez-Rosas",
                    "Roger Zimmermann",
                    "Rada Mihalcea",
                    "Soujanya Poria"
                ],
                "title": "Towards multimodal sarcasm detection (an _Obviously_ perfect paper)",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_94",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "115-ARR_v2_95",
            "content": "UNKNOWN, None, 1995, Muting the meaning a social function of irony. Metaphor and Symbol, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": null,
                "title": null,
                "pub_date": "1995",
                "pub_title": "Muting the meaning a social function of irony. Metaphor and Symbol",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_96",
            "content": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale, 2021, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Alexey Dosovitskiy",
                    "Lucas Beyer",
                    "Alexander Kolesnikov",
                    "Dirk Weissenborn",
                    "Xiaohua Zhai",
                    "Thomas Unterthiner",
                    "Mostafa Dehghani",
                    "Matthias Minderer",
                    "Georg Heigold",
                    "Sylvain Gelly",
                    "Jakob Uszkoreit",
                    "Neil Houlsby"
                ],
                "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
                "pub_date": "2021",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_97",
            "content": "W Raymond,  Gibbs, On the psycholinguistics of sarcasm, 1986, Journal of experimental psychology: general, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "W Raymond",
                    " Gibbs"
                ],
                "title": "On the psycholinguistics of sarcasm",
                "pub_date": "1986",
                "pub_title": "Journal of experimental psychology: general",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_98",
            "content": "UNKNOWN, None, 2007, On the psycholinguistics of sarcasm. Irony in language and thougt: A cognitive science reader, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": null,
                "title": null,
                "pub_date": "2007",
                "pub_title": "On the psycholinguistics of sarcasm. Irony in language and thougt: A cognitive science reader",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_99",
            "content": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep residual learning for image recognition, 2016-06-27, 2016 IEEE Conference on Computer Vision and Pattern Recognition, IEEE Computer Society.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Kaiming He",
                    "Xiangyu Zhang",
                    "Shaoqing Ren",
                    "Jian Sun"
                ],
                "title": "Deep residual learning for image recognition",
                "pub_date": "2016-06-27",
                "pub_title": "2016 IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": "IEEE Computer Society"
            }
        },
        {
            "ix": "115-ARR_v2_100",
            "content": "Aditya Joshi, Vinita Sharma, Pushpak Bhattacharyya, Harnessing context incongruity for sarcasm detection, 2015, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Aditya Joshi",
                    "Vinita Sharma",
                    "Pushpak Bhattacharyya"
                ],
                "title": "Harnessing context incongruity for sarcasm detection",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "115-ARR_v2_101",
            "content": "Yoon Kim, Convolutional neural networks for sentence classification, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Yoon Kim"
                ],
                "title": "Convolutional neural networks for sentence classification",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_102",
            "content": "N Thomas, Max Kipf,  Welling, Semisupervised classification with graph convolutional networks, 2017-04-24, 5th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "N Thomas",
                    "Max Kipf",
                    " Welling"
                ],
                "title": "Semisupervised classification with graph convolutional networks",
                "pub_date": "2017-04-24",
                "pub_title": "5th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_103",
            "content": "Amit Kumar Jena, Aman Sinha, Rohit Agarwal, 2020. C-net: Contextual network for sarcasm detection, , Proceedings of the Second Workshop on Figurative Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Amit Kumar Jena",
                    "Aman Sinha",
                    "Rohit Agarwal"
                ],
                "title": "2020. C-net: Contextual network for sarcasm detection",
                "pub_date": null,
                "pub_title": "Proceedings of the Second Workshop on Figurative Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "115-ARR_v2_104",
            "content": "Bin Liang, Chenwei Lou, Xiang Li, Lin Gui, Min Yang, Ruifeng Xu, Multi-modal sarcasm detection with interactive in-modal and cross-modal graphs, 2021, Proceedings of the 29th ACM International Conference on Multimedia, Association for Computing Machinery.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Bin Liang",
                    "Chenwei Lou",
                    "Xiang Li",
                    "Lin Gui",
                    "Min Yang",
                    "Ruifeng Xu"
                ],
                "title": "Multi-modal sarcasm detection with interactive in-modal and cross-modal graphs",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 29th ACM International Conference on Multimedia",
                "pub": "Association for Computing Machinery"
            }
        },
        {
            "ix": "115-ARR_v2_105",
            "content": "UNKNOWN, None, 2022, Aspect-based sentiment analysis via affective knowledge enhanced graph convolutional networks. Knowledge-Based Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2022",
                "pub_title": "Aspect-based sentiment analysis via affective knowledge enhanced graph convolutional networks. Knowledge-Based Systems",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_106",
            "content": "Bin Liang, Hang Su, Rongdi Yin, Lin Gui, Min Yang, Qin Zhao, Xiaoqi Yu, Ruifeng Xu, Beta distribution guided aspect-aware graph for aspect category sentiment analysis with affective knowledge, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Bin Liang",
                    "Hang Su",
                    "Rongdi Yin",
                    "Lin Gui",
                    "Min Yang",
                    "Qin Zhao",
                    "Xiaoqi Yu",
                    "Ruifeng Xu"
                ],
                "title": "Beta distribution guided aspect-aware graph for aspect category sentiment analysis with affective knowledge",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "115-ARR_v2_107",
            "content": "Chenwei Lou, Bin Liang, Lin Gui, Yulan He, Yixue Dang, Ruifeng Xu, Affective dependency graph for sarcasm detection, 2021, the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '21), .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Chenwei Lou",
                    "Bin Liang",
                    "Lin Gui",
                    "Yulan He",
                    "Yixue Dang",
                    "Ruifeng Xu"
                ],
                "title": "Affective dependency graph for sarcasm detection",
                "pub_date": "2021",
                "pub_title": "the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '21)",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_108",
            "content": "A George,  Miller, WordNet: A lexical database for English, 1992-02-23, Speech and Natural Language: Proceedings of a Workshop Held at, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "A George",
                    " Miller"
                ],
                "title": "WordNet: A lexical database for English",
                "pub_date": "1992-02-23",
                "pub_title": "Speech and Natural Language: Proceedings of a Workshop Held at",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_109",
            "content": "Zheng Hongliang Pan, Peng Lin, Yatao Fu, Weiping Qi,  Wang, Modeling intra and intermodality incongruity for multi-modal sarcasm detection, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Zheng Hongliang Pan",
                    "Peng Lin",
                    "Yatao Fu",
                    "Weiping Qi",
                    " Wang"
                ],
                "title": "Modeling intra and intermodality incongruity for multi-modal sarcasm detection",
                "pub_date": "2020",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "115-ARR_v2_110",
            "content": "Bo Pang, Lillian Lee, Opinion mining and sentiment analysis, 2008, Information Retrieval, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Bo Pang",
                    "Lillian Lee"
                ],
                "title": "Opinion mining and sentiment analysis",
                "pub_date": "2008",
                "pub_title": "Information Retrieval",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_111",
            "content": "Jeffrey Pennington, Richard Socher, Christopher Manning, GloVe: Global vectors for word representation, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Jeffrey Pennington",
                    "Richard Socher",
                    "Christopher Manning"
                ],
                "title": "GloVe: Global vectors for word representation",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "115-ARR_v2_112",
            "content": "Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan Gilbert, Ruihong Huang, Sarcasm as contrast between a positive sentiment and negative situation, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Ellen Riloff",
                    "Ashequl Qadir",
                    "Prafulla Surve",
                    "Lalindra De Silva",
                    "Nathan Gilbert",
                    "Ruihong Huang"
                ],
                "title": "Sarcasm as contrast between a positive sentiment and negative situation",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_113",
            "content": "Rossano Schifanella, Paloma De Juan, Joel Tetreault, Liangliang Cao, Detecting sarcasm in multimodal social platforms, 2016-10-15, Proceedings of the 2016 ACM Conference on Multimedia Conference, MM 2016, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Rossano Schifanella",
                    "Paloma De Juan",
                    "Joel Tetreault",
                    "Liangliang Cao"
                ],
                "title": "Detecting sarcasm in multimodal social platforms",
                "pub_date": "2016-10-15",
                "pub_title": "Proceedings of the 2016 ACM Conference on Multimedia Conference, MM 2016",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_114",
            "content": "Qiaoyu Tan, Ninghao Liu, Xing Zhao, Hongxia Yang, Jingren Zhou, Xia Hu, Learning to hash with graph neural networks for recommender systems, 2020-04-20, WWW '20: The Web Conference 2020, ACM / IW3C2.",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Qiaoyu Tan",
                    "Ninghao Liu",
                    "Xing Zhao",
                    "Hongxia Yang",
                    "Jingren Zhou",
                    "Xia Hu"
                ],
                "title": "Learning to hash with graph neural networks for recommender systems",
                "pub_date": "2020-04-20",
                "pub_title": "WWW '20: The Web Conference 2020",
                "pub": "ACM / IW3C2"
            }
        },
        {
            "ix": "115-ARR_v2_115",
            "content": "Yi Tay, Anh Luu, Siu Hui, Jian Su, Reasoning with sarcasm by reading inbetween, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Yi Tay",
                    "Anh Luu",
                    "Siu Hui",
                    "Jian Su"
                ],
                "title": "Reasoning with sarcasm by reading inbetween",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "115-ARR_v2_116",
            "content": "Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio, Graph attention networks, 2018-04-30, 6th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Petar Velickovic",
                    "Guillem Cucurull",
                    "Arantxa Casanova",
                    "Adriana Romero",
                    "Pietro Li\u00f2",
                    "Yoshua Bengio"
                ],
                "title": "Graph attention networks",
                "pub_date": "2018-04-30",
                "pub_title": "6th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_117",
            "content": "Jianchao Wu, Limin Wang, Li Wang, Jie Guo, Gangshan Wu, Learning actor relation graphs for group activity recognition, 2019-06-16, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Computer Vision Foundation / IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Jianchao Wu",
                    "Limin Wang",
                    "Li Wang",
                    "Jie Guo",
                    "Gangshan Wu"
                ],
                "title": "Learning actor relation graphs for group activity recognition",
                "pub_date": "2019-06-16",
                "pub_title": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019",
                "pub": "Computer Vision Foundation / IEEE"
            }
        },
        {
            "ix": "115-ARR_v2_118",
            "content": "Jie Guo-Sen Xie, Huan Liu, Ling Xiong,  Shao, Scale-aware graph neural network for fewshot semantic segmentation, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Jie Guo-Sen Xie",
                    "Huan Liu",
                    "Ling Xiong",
                    " Shao"
                ],
                "title": "Scale-aware graph neural network for fewshot semantic segmentation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_119",
            "content": "Tao Xiong, Peiran Zhang, Hongbo Zhu, Yihui Yang, Sarcasm detection with self-matching networks and low-rank bilinear pooling, 2019-05-13, The World Wide Web Conference, ACM.",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Tao Xiong",
                    "Peiran Zhang",
                    "Hongbo Zhu",
                    "Yihui Yang"
                ],
                "title": "Sarcasm detection with self-matching networks and low-rank bilinear pooling",
                "pub_date": "2019-05-13",
                "pub_title": "The World Wide Web Conference",
                "pub": "ACM"
            }
        },
        {
            "ix": "115-ARR_v2_120",
            "content": "Nan Xu, Zhixiong Zeng, Wenji Mao, 2020. Reasoning with multimodal sarcastic tweets via modeling cross-modality contrast and semantic association, , Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Nan Xu",
                    "Zhixiong Zeng",
                    "Wenji Mao"
                ],
                "title": "2020. Reasoning with multimodal sarcastic tweets via modeling cross-modality contrast and semantic association",
                "pub_date": null,
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "115-ARR_v2_121",
            "content": "Xiaocui Yang, Shi Feng, Yifei Zhang, Daling Wang, Multimodal sentiment detection based on multi-channel graph neural networks, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Xiaocui Yang",
                    "Shi Feng",
                    "Yifei Zhang",
                    "Daling Wang"
                ],
                "title": "Multimodal sentiment detection based on multi-channel graph neural networks",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "115-ARR_v2_122",
            "content": "Liang Yao, Chengsheng Mao, Yuan Luo, Graph convolutional networks for text classification, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Liang Yao",
                    "Chengsheng Mao",
                    "Yuan Luo"
                ],
                "title": "Graph convolutional networks for text classification",
                "pub_date": "2019",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_123",
            "content": "Yongjing Yin, Fandong Meng, Jinsong Su, Chulun Zhou, Zhengyuan Yang, Jie Zhou, Jiebo Luo, A novel graph-based multi-modal fusion encoder for neural machine translation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Yongjing Yin",
                    "Fandong Meng",
                    "Jinsong Su",
                    "Chulun Zhou",
                    "Zhengyuan Yang",
                    "Jie Zhou",
                    "Jiebo Luo"
                ],
                "title": "A novel graph-based multi-modal fusion encoder for neural machine translation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "115-ARR_v2_124",
            "content": "Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William Hamilton, Jure Leskovec, Graph convolutional neural networks for web-scale recommender systems, 2018-08-19, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, ACM.",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Rex Ying",
                    "Ruining He",
                    "Kaifeng Chen",
                    "Pong Eksombatchai",
                    "William Hamilton",
                    "Jure Leskovec"
                ],
                "title": "Graph convolutional neural networks for web-scale recommender systems",
                "pub_date": "2018-08-19",
                "pub_title": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
                "pub": "ACM"
            }
        },
        {
            "ix": "115-ARR_v2_125",
            "content": "Yawen Zeng, Da Cao, Xiaochi Wei, Meng Liu, Zhou Zhao, Zheng Qin, Multi-modal relational graph for cross-modal video moment retrieval, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Yawen Zeng",
                    "Da Cao",
                    "Xiaochi Wei",
                    "Meng Liu",
                    "Zhou Zhao",
                    "Zheng Qin"
                ],
                "title": "Multi-modal relational graph for cross-modal video moment retrieval",
                "pub_date": "2021",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_126",
            "content": "Chen Zhang, Qiuchi Li, Dawei Song, Aspectbased sentiment classification with aspect-specific graph convolutional networks, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Chen Zhang",
                    "Qiuchi Li",
                    "Dawei Song"
                ],
                "title": "Aspectbased sentiment classification with aspect-specific graph convolutional networks",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "115-ARR_v2_127",
            "content": "Dong Zhang, Suzhong Wei, Shoushan Li, Hanqian Wu, Qiaoming Zhu, Guodong Zhou, Multimodal graph fusion for named entity recognition with targeted visual guidance, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Dong Zhang",
                    "Suzhong Wei",
                    "Shoushan Li",
                    "Hanqian Wu",
                    "Qiaoming Zhu",
                    "Guodong Zhou"
                ],
                "title": "Multimodal graph fusion for named entity recognition with targeted visual guidance",
                "pub_date": "2021",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "115-ARR_v2_128",
            "content": "Meishan Zhang, Yue Zhang, Guohong Fu, Tweet sarcasm detection using deep neural network, 2016, Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Meishan Zhang",
                    "Yue Zhang",
                    "Guohong Fu"
                ],
                "title": "Tweet sarcasm detection using deep neural network",
                "pub_date": "2016",
                "pub_title": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "115-ARR_v2_0@0",
            "content": "Multi-Modal Sarcasm Detection via Cross-Modal Graph Convolutional Network",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_0",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_2@0",
            "content": "With the increasing popularity of posting multimodal messages online, many recent studies have been carried out utilizing both textual and visual information for multi-modal sarcasm detection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_2",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_2@1",
            "content": "In this paper, we investigate multimodal sarcasm detection from a novel perspective by constructing a cross-modal graph for each instance to explicitly draw the ironic relations between textual and visual modalities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_2",
            "start": 193,
            "end": 408,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_2@2",
            "content": "Specifically, we first detect the objects paired with descriptions of the image modality, enabling the learning of important visual information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_2",
            "start": 410,
            "end": 553,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_2@3",
            "content": "Then, the descriptions of the objects are served as a bridge to determine the importance of the association between the objects of image modality and the contextual words of text modality, so as to build a cross-modal graph for each multi-modal instance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_2",
            "start": 555,
            "end": 808,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_2@4",
            "content": "Furthermore, we devise a cross-modal graph convolutional network to make sense of the incongruity relations between modalities for multi-modal sarcasm detection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_2",
            "start": 810,
            "end": 970,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_2@5",
            "content": "Extensive experimental results and in-depth analysis show that our model achieves state-of-the-art performance in multi-modal sarcasm detection 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_2",
            "start": 972,
            "end": 1118,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_4@0",
            "content": "Sarcasm is a peculiar form of sentiment expressions, allowing individuals to express contempt sentiment or intention that is converse to the authentic/apparent sentiment information (Gibbs, 1986;Dews and Winner, 1995;Gibbs, 2007).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_4",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_4@1",
            "content": "As such, accurately detecting satirical/ironic expression could potentially improve the performance of sentiment analysis and opinion mining (Pang and Lee, 2008;Kumar Jena et al., 2020;Pan et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_4",
            "start": 231,
            "end": 433,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_5@0",
            "content": "In today's fast growing social media platforms, it is common to post multi-modal messages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_5",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_5@1",
            "content": "Therefore, in addition to developing sarcasm detection models for textual data (Riloff et al., 2013;Joshi et al., 2015), it is increasingly popular to explore sarcasm detection in multi-modal data such as text and images (Schifanella et al., 2016;Cai et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_5",
            "start": 91,
            "end": 355,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_5@2",
            "content": "Dealing with multimodal data requires an understanding of the information presented in different modalities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_5",
            "start": 357,
            "end": 464,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_5@3",
            "content": "As the sarcastic example shown in Figure 1 (a), text-only approaches may erroneously identify it as a positive sentiment expression due to the phrase \"wonderful weather\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_5",
            "start": 466,
            "end": 635,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_5@4",
            "content": "This post however contains a sarcastic expression with negative sentiment, because it is accompanied by an image with \"thunderstorm clouds\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_5",
            "start": 637,
            "end": 776,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_5@5",
            "content": "The key of effective multi-modal sarcasm detection is to accurately extract the incongruent sentiment cues from different modalities, allowing the detection of the true sentiment conveyed in the message.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_5",
            "start": 778,
            "end": 980,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_6@0",
            "content": "To perform multi-modal sarcasm detection on data composed of text and image, several related research efforts attempt to concatenate the textual and visual features to fuse sarcastic information (Schifanella et al., 2016), employ attention mechanism to implicitly fuse the features of different modalities based on external knowledge (Cai et al., 2019;Xu et al., 2020;Pan et al., 2020), or build interactive graphs to model the relations of different modalities (Liang et al., 2021a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_6",
            "start": 0,
            "end": 483,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_6@1",
            "content": "Despite promising progress made by existing models, they still suffer from the following limitations: 1) Simply considering the whole image does not produce good results, mostly due to the intricate visual information presented in an image; not to mention that only particular visual patches are related to the text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_6",
            "start": 485,
            "end": 800,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_6@2",
            "content": "As in the examples shown in Figure 1, the correct results can be easily obtained by only tracking the visual information in the bounding boxes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_6",
            "start": 802,
            "end": 944,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_6@3",
            "content": "Therefore, discriminating key visual objects from the irrelevant ones could lead to improved learning of visual information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_6",
            "start": 946,
            "end": 1069,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_6@4",
            "content": "2) Crucial visual information that relates to the sarcastic cues of text modality may be scattered in an image (Figure 1 (b)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_6",
            "start": 1071,
            "end": 1196,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_6@5",
            "content": "As such, it is essential to focus on drawing the intricate sentiment connections between text and image modalities, allowing a good exploitation of the contradictory sentiment information between modalities for learning sarcastic clues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_6",
            "start": 1198,
            "end": 1433,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_7@0",
            "content": "To this end, we propose a novel cross-modal graph convolutional networks (CMGCN) by constructing a cross-modal graph for each instance, where the important visual information and the related textual tokens are explicitly linked.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_7",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_7@1",
            "content": "This allows for the extraction of incongruous implications between two modalities in sarcasm detection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_7",
            "start": 229,
            "end": 331,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_7@2",
            "content": "Concretely, instead of trying to produce a caption of the whole image, we first detect the objects of the image to capture the important visual regions and the corresponding attribute-object pairs via the approach proposed by Anderson et al. (2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_7",
            "start": 333,
            "end": 581,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_7@3",
            "content": "Then, we explore a novel solution to assign weights to the edges of the cross-modal graph by means of computing the word similarities between the object descriptors of the attribute-object pairs and textual words based on the WordNet (Miller, 1992).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_7",
            "start": 583,
            "end": 831,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_7@4",
            "content": "Further, to introduce the multi-modal sentiment relations into the cross-modal graphs, inspired by (Lou et al., 2021), we devise a modulating factor of sentiment relation for each edge by retrieving the affective weights of attribute descriptors (usually adjectives with affective information) and textual words from external affective knowledge (SenticNet (Cambria et al., 2020)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_7",
            "start": 833,
            "end": 1213,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_7@5",
            "content": "As such, the modulating factors can be adopted to refine the edge weights of word similarities, allowing the capture of sentiment incongruities of the cross-modal nodes in the graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_7",
            "start": 1215,
            "end": 1396,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_7@6",
            "content": "Further, in the light of crossmodal graphs, we deploy a GCN architecture to make sense of the incongruous relations across the modalities for multi-modal sarcasm detection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_7",
            "start": 1398,
            "end": 1569,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_8@0",
            "content": "The main contributions of our work are summarized as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_8",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_9@0",
            "content": "\u2022 To the best of our knowledge, we are the first to explore the use of the graph model based on auxiliary object detection for modeling the contradictory sentiments between key textual and visual information in multi-modal sarcasm detection. \u2022 Using the attribute-object pairs of the image objects as the bridge, a novel approach of constructing cross-modal graphs is developed to explicitly link the two modalities by edges with the varying degree of importance. \u2022 A series of experiments on a publicly available multi-modal sarcasm detection benchmark dataset show that our proposed method achieves the state-of-the-art performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_9",
            "start": 0,
            "end": 633,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_10@0",
            "content": "2 Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_10",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_11@0",
            "content": "Multi-modal Sarcasm Detection",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_11",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_12@0",
            "content": "Previous work of sarcasm detection has been applied to textual utterances information Tay et al., 2018;Babanejad et al., 2020 the graph-based methods, Liang et al. (2021a) deployed a heterogeneous graph structure to learn the sarcastic features from both intra-and intermodality perspectives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_12",
            "start": 0,
            "end": 291,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_12@1",
            "content": "However, this method tried to grasp the visual information of the whole image, and meanwhile ignore the sentiment expression between different modalities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_12",
            "start": 293,
            "end": 446,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_12@2",
            "content": "Therefore, different from (Liang et al., 2021a), we explore a novel cross-modal GCN model based on the important visual information and sentiment cues to leverage the inconsistent implications between different modalities and thus improve the performance of multimodal sarcasm detection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_12",
            "start": 448,
            "end": 734,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_13@0",
            "content": "Graph Neural Networks",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_13",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_14@0",
            "content": "Models based on graph neural networks (GNN), including graph convolutional network (GCN) (Kipf and Welling, 2017) and graph attention network (GAT) (Velickovic et al., 2018), have achieved promising performance in many recent research studies, such as visual representation learning (Wu et al., 2019;Xie et al., 2021), text representation learning (Yao et al., 2019;Lou et al., 2021;Liang et al., 2021bLiang et al., , 2022, and recommendation systems (Ying et al., 2018;Tan et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_14",
            "start": 0,
            "end": 487,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_14@1",
            "content": "Further, there are also some research studies explored graph models to deal with the multi-modal tasks, such as multi-modal sentiment detection (Yang et al., 2021), multi-modal named entity recognition , cross-modal video moment retrieval (Zeng et al., 2021), multi-modal neural machine translation (Yin et al., 2020), and multimodal sarcasm detection (Liang et al., 2021a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_14",
            "start": 489,
            "end": 862,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_15@0",
            "content": "Methodology",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_15",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_16@0",
            "content": "In this section, we describe our proposed Cross-Modal Graph Convolutional Networks (CMGCN) model for multi-modal sarcasm detection in details.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_16",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_17@0",
            "content": "As demonstrated in Figure 2, the architecture of the proposed CMGCN contains four main components: 1) Text-modality representation, which employs the pre-trained uncased BERT-base model (Devlin et al., 2019) as the text encoder to capture the hidden representation of the text-modality; 2) Image-modality representation, which deploys the pre-trained Vision Transformer (ViT) (Dosovitskiy et al., 2021) as the image encoder to capture the hidden representation of the image-modality with respect to each bounding box (visual region); 3) Cross-modal graph, which constructs a crossmodal graph for each multi-modal example based on the external affective knowledge source and the hidden representations of text and image modalities; 4) Multi-modal fusion, which fuses the representations from image and text modalities to capture the sarcastic features by means of a GCN structure and an attention mechanism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_17",
            "start": 0,
            "end": 905,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_18@0",
            "content": "Text-modality Representation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_18",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_19@0",
            "content": "For text processing, given a sequence of words s = {w i } n i=1 , n is the length of the text s. We first adopt the pre-trained uncased BERT-base model (Devlin et al., 2019) to map each word w i into a d T -dimensional embedding:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_19",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_20@0",
            "content": "X T = [x 1 , x 2 , \u2022 \u2022 \u2022 , x n ] = BERT([CLS]s[SEP])",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_20",
            "start": 0,
            "end": 51,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_21@0",
            "content": "(1) Where X T is the embedding matrix of the input text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_21",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_21@1",
            "content": "Here, the representations of tokens [CLS] and [SEP] are not utilized in constructing the cross-modal graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_21",
            "start": 57,
            "end": 163,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_21@2",
            "content": "Subsequently, to unify the dimensions of representations between different modalities and capture the sequential relations of the context, we utilize a bidirectional LSTM (Bi-LSTM) to learn the text-modality representation of the input text:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_21",
            "start": 165,
            "end": 405,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_22@0",
            "content": "T = {t 1 , t 2 , \u2022 \u2022 \u2022 , t n } = Bi-LSTM(X T ) (2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_22",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_23@0",
            "content": "Where t j \u2208 R 2d h denotes the hidden state vector at time step j from the bidirectional LSTM, d h denotes the dimensionality of the text-modality hidden state representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_23",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_24@0",
            "content": "Image-modality Representation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_24",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_25@0",
            "content": "For image processing, given an image I, we first adopt a trained toolkit proposed by Anderson et al. (2018) to derive a series of bounding boxes (objects) paired with their attribute-object pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_25",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_25@1",
            "content": "For each visual region of the bounding box I i \u2208 R L h \u00d7Lw , following (Xu et al., 2020), we first resize it to 224 \u00d7 224, i.e. L = L h = L w = 224.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_25",
            "start": 197,
            "end": 344,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_25@2",
            "content": "Subsequently, following (Dosovitskiy et al., 2021), we reshape the region",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_25",
            "start": 346,
            "end": 418,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_26@0",
            "content": "I i \u2208 R L\u00d7L into a sequence I i = {p j \u2208 R L/p\u00d7L/p } r j=1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_26",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_27@0",
            "content": ", where r = p \u00d7 p is the number of patches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_27",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_27@1",
            "content": "Then, we flatten and map each patch to a d I -dimensional vector with a trainable linear projection:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_27",
            "start": 44,
            "end": 143,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_28@0",
            "content": "z j = p j E.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_28",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_29@0",
            "content": "For each sequence of image patches, a [class] token embedding z [class] \u2208 R d I is prepended for the sequence of embedded patches, and position embeddings are added to the patch embeddings to retain positional information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_29",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_29@1",
            "content": "The input of each visual region I i is represented as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_29",
            "start": 223,
            "end": 276,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_30@0",
            "content": "Z i = [z [class] ; z 1 ; z 2 ; \u2022 \u2022 \u2022 ; z r ] + E pos (3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_30",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_31@0",
            "content": "Where Z i \u2208 R (r+1)\u00d7d I is the input matrix of the image patches, and E pos \u2208 R (r+1)\u00d7d I is the position embedding matrix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_31",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_31@1",
            "content": "Then, we feed the input matrix Z i into the ViT encoder to acquire the representation h i of visual region I i :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_31",
            "start": 124,
            "end": 235,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_32@0",
            "content": "H i = ViT(Z i ), h i = H i,[class]",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_32",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_33@0",
            "content": "(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_33",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_34@0",
            "content": "We use the representation of the [class] token embedding to represent the visual region.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_34",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_34@1",
            "content": "Finally, the representation of the image I is defined as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_34",
            "start": 89,
            "end": 145,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_35@0",
            "content": "X I = {h 1 , h 2 , \u2022 \u2022 \u2022 , h m } (5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_35",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_36@0",
            "content": "Where m is the number of visual regions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_36",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_36@1",
            "content": "Subsequently, we employ a trainable Linear Projection to map each v i to a 2d h -dimensional vector:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_36",
            "start": 41,
            "end": 140,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_37@0",
            "content": "V = {v 1 , v 2 , \u2022 \u2022 \u2022 , v m } = X I W V (6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_37",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_38@0",
            "content": "Where W V \u2208 R d I \u00d72d h is a trainable parameter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_38",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_39@0",
            "content": "Cross-modal Graph",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_39",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_40@0",
            "content": "In this section, we describe how to construct a cross-modal graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_40",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_40@1",
            "content": "To leverage the relations between multi-modal features, we employ a graph structure to link the textual words with the associated image objects.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_40",
            "start": 67,
            "end": 210,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_40@2",
            "content": "Here, the nodes of the crossmodal graph are the representations of text and image modalities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_40",
            "start": 212,
            "end": 304,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_40@3",
            "content": "Many GCN-based approaches have demonstrated that the weights of the edges are crucial in graph information aggregation (Liang et al., 2021b;Yang et al., 2021;Lou et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_40",
            "start": 306,
            "end": 481,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_40@4",
            "content": "As such, constructing a cross-modal graph boils down to the setting of the edge weights in the graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_40",
            "start": 483,
            "end": 583,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_41@0",
            "content": "To this end, we explore a novel approach of setting the weights based on both word similarities and affective clues between textual words and the attribute-object pairs of the image regions, and the dependency tree of the text-modality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_41",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_41@1",
            "content": "The adjacency matrix A \u2208 R (n+m)\u00d7(n+m) of the crossmodal graph is defined as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_41",
            "start": 237,
            "end": 313,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_42@0",
            "content": "A i,j = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 if D i,j and i < n, j < n \u03ba i,j if i < n, j \u2265 n 0 otherwise (7) \u03ba i,j = Sim(w i , o j ) \u00d7 \u03be i,j + 1 (8) \u03be i,j = \u03b3 \u2212\u03c9(w i )\u03c9(a j ) \u00d7 |\u03c9(w i ) \u2212 \u03c9(a j )| (9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_42",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_43@0",
            "content": "Where D i,j indicates that there is a relation between w i and w j in the dependency tree of the sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_43",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_43@1",
            "content": "Sim(\u2022) represents the computation of word similarity 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_43",
            "start": 107,
            "end": 162,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_43@2",
            "content": "We set Sim(\u2022) = 0 if the return value is N one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_43",
            "start": 164,
            "end": 210,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_43@3",
            "content": "\u03be i,j is a modulating factor refers to the sentiment relation (sentiment incongruity) between an image region and a text token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_43",
            "start": 212,
            "end": 338,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_43@4",
            "content": "\u03c9(w i ) \u2208 [\u22121, 1] represents the affective weight of word w i retrieved from SenticNet (Cambria et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_43",
            "start": 340,
            "end": 449,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_43@5",
            "content": "We set \u03c9(w i ) = 0 if w i cannot be found in SenticNet.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_43",
            "start": 451,
            "end": 505,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_43@6",
            "content": "|\u2022| represents absolute value calculation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_43",
            "start": 507,
            "end": 548,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_43@7",
            "content": "a j and o j respectively denote the attribute and the object of the bounding box j. Inspired by Kipf and Welling (2017), we construct the cross-modal graph as an undirected graph, A i,j = A j,i , and set a self-loop for each node, A i,i = 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_43",
            "start": 550,
            "end": 790,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_44@0",
            "content": "The intention of the cross-modal graph construction (Equations 7 and 9) is that: 1) As in the examples shown in Figure 1, the sarcastic information of text-modality may be expressed by multiple words, such as \"wonderful weather\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_44",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_44@1",
            "content": "Therefore, we incorporate the syntax-aware relations over the dependency tree of the sentence into the cross-modal graph to advance the learning of the contextual dependencies 3 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_44",
            "start": 230,
            "end": 408,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_44@2",
            "content": "2) We devise a coefficient \u03ba i,j , which is associated with the affective weights, to modulate the influence of contrary sentiment relations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_44",
            "start": 410,
            "end": 550,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_44@3",
            "content": "Here, \u03b3 > 1 is a tuned hyper-parameter to regulate the bias of inconsistent sentiment relations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_44",
            "start": 552,
            "end": 647,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_44@4",
            "content": "That is, if the polarities of \u03c9(w i ) and \u03c9(a j ) are opposite, the value of \u03b3 is boosted, otherwise the value is shrunk.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_44",
            "start": 649,
            "end": 769,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_44@5",
            "content": "Especially, the greater the affective weights, the higher the confidence that the value of \u03b3 is boosted or shrunk.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_44",
            "start": 771,
            "end": 884,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_44@6",
            "content": "3) We add 1 to the cross-modal edges to pay more attention to the cross-modal nodes aggregation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_44",
            "start": 886,
            "end": 981,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_45@0",
            "content": "Multi-modal Fusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_45",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_46@0",
            "content": "For each instance, we explore a graph architecture to extract the crucial sarcastic clues by aggregating the correlation of nodes in the cross-modal graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_46",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_46@1",
            "content": "Concretely, we feed the adjacency matrix of the cross-modal graph A and the corresponding nodes' representations R of each multi-modal example into a multi-layers GCNs architecture to derive the graph representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_46",
            "start": 156,
            "end": 371,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_46@2",
            "content": "For each graph convolutional operation, each node in the l-th GCN layer is updated according to the hidden representations of its neighborhoods according to the adjacency matrices of the cross-modal graph, which is defined as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_46",
            "start": 373,
            "end": 598,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_47@0",
            "content": "G l = ReLU( \u00c3G l\u22121 W l + b l )(10)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_47",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_48@0",
            "content": "Where \u00c3 = D \u2212 1 2 AD \u2212 1 2 is the normalized symmetric adjacency matrix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_48",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_48@1",
            "content": "D is the degree matrix of A, where D ii = j A i,j .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_48",
            "start": 73,
            "end": 123,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_48@2",
            "content": "G l\u22121 is the hidden graph representation evolved from the preceding GCN layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_48",
            "start": 125,
            "end": 202,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_49@0",
            "content": "W l \u2208 R 2d h \u00d72d h , b l \u2208 R 2d h",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_49",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_50@0",
            "content": "are the trainable parameters of the l-th GCN layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_50",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_50@1",
            "content": "The nodes input of the first GCN layer are the concatenation of text-modality and imagemodality representations:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_50",
            "start": 52,
            "end": 163,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_51@0",
            "content": "G 0 = R. Here, R = {r 1 , r 2 , \u2022 \u2022 \u2022 , r n+m } = {t 1 , \u2022 \u2022 \u2022 , t n , v 1 , \u2022 \u2022 \u2022 , v m }.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_51",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_52@0",
            "content": "Subsequently, inspired by , we employ a retrieval-based attention mechanism to capture the graph-oriented attention information from the concatenation of text and image representations R = {r 1 , r 2 , \u2022 \u2022 \u2022 , r n+m } by means of the graph representation g derived from the final GCN layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_52",
            "start": 0,
            "end": 289,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_52@1",
            "content": "The intention is to retrieve crucially associated cross-modal features that are explicitly connected in the cross-modal graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_52",
            "start": 291,
            "end": 416,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_52@2",
            "content": "The attention weights are computed as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_52",
            "start": 418,
            "end": 455,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_53@0",
            "content": "\u03b1 t = exp(\u03b2 t ) n+m i=1 exp(\u03b2 i ) , \u03b2 t = i\u2208C r \u22a4 t g i (11)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_53",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_54@0",
            "content": "Where C denotes a set of indices in which nodes contain cross-modal edges in the graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_54",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_54@1",
            "content": "\u22a4 represents the matrix transposition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_54",
            "start": 88,
            "end": 125,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_54@2",
            "content": "The final sarcastic representation is defined as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_54",
            "start": 127,
            "end": 175,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_55@0",
            "content": "f = n+m t=1 \u03b1 t r t(12)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_55",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_56@0",
            "content": "Then, the final sarcastic representation is fed into a fully-connected layer with a softmax function to capture a probability distribution \u0177 \u2208 R dp in the sarcasm decision space:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_56",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_57@0",
            "content": "\u0177 = softmax(W o f + b o )(13)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_57",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_58@0",
            "content": "Where d p is the dimensionality of sarcasm labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_58",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_59@0",
            "content": "W o \u2208 R dp\u00d72d h and b o \u2208 R dp are trainable parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_59",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_60@0",
            "content": "Learning Objective",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_60",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_61@0",
            "content": "We minimize the cross-entropy loss via the standard gradient descent algorithm to train the model:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_61",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_62@0",
            "content": "min \u0398 L = \u2212 N i=1 dp j=1 y j i log\u0177 j i + \u03bb||\u0398|| 2 (14)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_62",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_63@0",
            "content": "where N is the training data size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_63",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_63@1",
            "content": "y i and \u0177i respectively represent the ground-truth and estimated label distribution of instance i. \u0398 denotes all trainable parameters of the model, \u03bb represents the coefficient of L 2 -regularization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_63",
            "start": 35,
            "end": 234,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_64@0",
            "content": "Experimental Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_64",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_65@0",
            "content": "For a fair comparison, the data preprocessing follows (Cai et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_65",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_65@1",
            "content": "We set the maximum number of visual regions as 10 for object detection results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_65",
            "start": 74,
            "end": 152,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_65@2",
            "content": "That is, we select the top 10 bounding boxes with highest scores if the objects are greater than 10.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_65",
            "start": 154,
            "end": 253,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_65@3",
            "content": "We utilize the pre-trained uncased BERT-base (Devlin et al., 2019) module to embed each word of text-modality as a 768-dimensional embedding and employ the pre-trained ViT 4 (Dosovitskiy et al., 2021) to embed each visual region patch as a 768dimensional embedding, i.e. d T = d I = 768.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_65",
            "start": 255,
            "end": 541,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_65@4",
            "content": "The resolution of visual region patch is set to L p = 32, correspondingly, p = 7, r = 49.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_65",
            "start": 543,
            "end": 631,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_65@5",
            "content": "5 The number of GCN layers is set to 2, which is the optimal depth in the pilot experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_65",
            "start": 633,
            "end": 724,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_65@6",
            "content": "The dimensionality of hidden representations is set to d h = 512.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_65",
            "start": 726,
            "end": 790,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_65@7",
            "content": "The coefficient \u03bb is set to 0.00001.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_65",
            "start": 792,
            "end": 827,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_65@8",
            "content": "Adam is utilized as the optimizer with a learning rate of 0.00002, and the mini-batch size is 32.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_65",
            "start": 829,
            "end": 925,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_65@9",
            "content": "The dropout rate with 0.1 is utilized to avoid overfitting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_65",
            "start": 927,
            "end": 985,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_65@10",
            "content": "We use early-stopping with patience of 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_65",
            "start": 987,
            "end": 1027,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_65@11",
            "content": "We set \u03b3 = 3 to compute the modulating factor of incongruous multi-modal sentiment relations, which is the optimal hyper-parameter in the pilot experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_65",
            "start": 1029,
            "end": 1184,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_65@12",
            "content": "Following (Cai et al., 2019), we use Accuracy, Precision, Recall, and F1-score to measure the model performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_65",
            "start": 1186,
            "end": 1297,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_65@13",
            "content": "Since the label distribution of the dataset is imbalanced, following (Pan et al., 2020), we also report Macro-average results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_65",
            "start": 1299,
            "end": 1424,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_65@14",
            "content": "The experimental results of our models are averaged over 10 runs with different random seeds to ensure the final reported results are statistically stable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_65",
            "start": 1426,
            "end": 1580,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_66@0",
            "content": "Comparison Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_66",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_67@0",
            "content": "We compare our proposed CMGCN model with a series of strong baselines, summarized as follow: 1) Image-modality methods: These models use only visual information for sarcasm detection, including Image (Cai et al., 2019), which employs ResNet (He et al., 2016) to train a sarcasm classifier; and ViT (Dosovitskiy et al., 2021), which utilizes the '[class]' token representation of the pre-trained ViT to detect the sarcasm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_67",
            "start": 0,
            "end": 420,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_68@0",
            "content": "2) Text-modality methods: These models use only textual information, including TextCNN (Kim, 2014), a deep learning model based on CNN for text classification; Bi-LSTM, a bidirectional LSTM network for text classification; SIARN (Tay et al., 2018), adopting inner-attention for textual sarcasm detection; SMSD (Xiong et al., 2019), exploring a self-matching network to capture textual incongruity information; and BERT (Devlin et al., 2019), the vanilla pre-trained uncased BERT-base taking '[CLS] text [SEP]' as input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_68",
            "start": 0,
            "end": 518,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_69@0",
            "content": "3) Multi-modal methods: These models take both text-and image-modality information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_69",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_69@1",
            "content": "Including HFM (Cai et al., 2019), a hierarchical multimodal features fusion model for multi-modal sarcasm detection; D&R Net (Xu et al., 2020), a Decomposition and Relation Network modeling both crossmodality contrast and semantic association; Res-BERT (Pan et al., 2020), concatenating image features and BERT-based text features for sarcasm prediction; Att- BERT (Pan et al., 2020), exploring an inter-modality attention and a co-attention to model the incongruity of multi-modal sarcasm detection; and InCrossMGs (Liang et al., 2021a), a graph-based model to leverage the sarcastic relations from both intra-and inter-modal perspectives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_69",
            "start": 84,
            "end": 723,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_70@0",
            "content": "We also explore several variants of CMGCN to analyze the impact of different components in the ablation study: 1) w/o G denotes without crossmodal graph, which only concatenates the representations of '[class]' and '[CLS]' tokens from ViT and BERT for sarcasm detection; 2) w/o O denotes without object detection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_70",
            "start": 0,
            "end": 312,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_70@1",
            "content": "The whole image is input into the image encoder, and the edge weights are set to 1 in the cross-modal graphs; 3) w/o S denotes without using external knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_70",
            "start": 314,
            "end": 473,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_70@2",
            "content": "All weights of edges are set to 1 in the cross-modal graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_70",
            "start": 475,
            "end": 533,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_70@3",
            "content": "Further, 4) w/o S w represents without using affective knowledge; 5) w/o D denotes without using syntax-aware information of text-modality in graph construction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_70",
            "start": 535,
            "end": 695,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_70@4",
            "content": "Further, to investigate the effectiveness of our CMGCN when used with different pre-trained models, we also set the following variants: 1) -GloVe+ResNet: We replace BERT with GloVe (Pennington et al., 2014) to initialize each word into a 300-dimensional embedding and ViT with ResNet-152 (He et al., 2016) to embed each image patch as a 2048-dimensional vector.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_70",
            "start": 697,
            "end": 1057,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_71@0",
            "content": "2) -GloVe+ViT: We use GloVe as text encoder and use ViT as image encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_71",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_72@0",
            "content": "3) -BERT+ResNet: We use BERT as text encoder and use ResNet-152 as image encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_72",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_73@0",
            "content": "Experimental Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_73",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_74@0",
            "content": "Main Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_74",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_75@0",
            "content": "We report the comparison results regarding Textmodality, Image-modality, and Text+Image modalities in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_75",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_75@1",
            "content": "From the results, we can draw the following conclusions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_75",
            "start": 111,
            "end": 166,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_75@2",
            "content": "1) Our proposed CMGCN outperforms existing baselines across all metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_75",
            "start": 168,
            "end": 239,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_75@3",
            "content": "This verifies the effectiveness of our proposed model in multi-modal sarcasm detection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_75",
            "start": 241,
            "end": 327,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_75@4",
            "content": "2) We conduct significance tests of our CMGCN over the baseline models, the results show that our CMGCN significantly outperforms the baseline models in terms of most of the evaluation metrics (with p\u2212value < 0.05).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_75",
            "start": 329,
            "end": 543,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_75@5",
            "content": "3) Our CMGCN model performs consistently better than the previous graph-based method (InCrossMGs), which demonstrates that recognizing significant visual regions and modeling sentiment relations can lead to improved performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_75",
            "start": 545,
            "end": 772,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_75@6",
            "content": "4) The methods based on text modality achieve consistently better performance than the methods based on image modality, which shows that the expression of sarcastic/nonsarcastic information primarily resides in the text modality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_75",
            "start": 774,
            "end": 1002,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_75@7",
            "content": "5) Methods based on both image and text modalities perform better than the unimodal baselines overall.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_75",
            "start": 1004,
            "end": 1105,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_75@8",
            "content": "This implies that leveraging the information of both image and text modalities is more effective for multi-modal sarcasm detection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_75",
            "start": 1107,
            "end": 1237,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_76@0",
            "content": "6) The results of macro metrics are better than other commonly used metrics overall, which indicates that models perform better in the \"negative\" class due to the imbalanced class distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_76",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_77@0",
            "content": "Ablation Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_77",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_78@0",
            "content": "To analyze the impact of different components of our proposed CMGCN, we conduct an ablation study and report the results in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_78",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_78@1",
            "content": "Note that removal of cross-modal graph (w/o G) sharply degrades the performance, which verifies the significance of cross-modal in multi-modal features fusion for learning sarcastic expressions in multimodal sarcasm detection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_78",
            "start": 133,
            "end": 358,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_78@2",
            "content": "Removal of object detection (w/o O) leads to considerable performance degradation, which demonstrates that adopting object detection to track important visual information is effective for constructing crucial relations between visual and textual information in the crossmodal graphs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_78",
            "start": 360,
            "end": 642,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_78@3",
            "content": "From the results of w/o S and S w , we conclude that exploiting the attribute-object pair as a bridge to set edge weights based on word similarity is effective when constructing cross-modal graphs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_78",
            "start": 644,
            "end": 840,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_78@4",
            "content": "Further, leveraging affective clues to capture multi-modal sentiment incongruity between text-and image-modality is effective in sarcasm detection, and thus leads to improved performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_78",
            "start": 842,
            "end": 1028,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_79@0",
            "content": "In addition, removal of syntax-aware information of text-modality leads to slight performance degradation, which indicates that incorporating syntactic information in the graph makes better learning of dependency relations of textual words and thus improves the performance of sarcasm detection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_79",
            "start": 0,
            "end": 294,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_80@0",
            "content": "Generalizability of Cross-modal Graph",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_80",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_81@0",
            "content": "To investigate the generalizability and effectiveness of our proposed cross-modal graph when used with different pre-trained methods, we conduct experiments with five variants of our proposed CMGCN by using different text and image encoders.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_81",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_81@1",
            "content": "The experimental results are shown in Figure 3 (a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_81",
            "start": 242,
            "end": 292,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_81@2",
            "content": "Note that the proposed cross-modal graph can directly work with various pre-trained models and performs consistently better than that without cross-modal graph (w/o G).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_81",
            "start": 294,
            "end": 461,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_81@3",
            "content": "This demonstrates the generalizability and effectiveness of our proposed cross-modal graph in multi-modal sarcasm detection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_81",
            "start": 463,
            "end": 586,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_81@4",
            "content": "Further, from the results, we can also conclude that superior performance is obtained when using more powerful pre-trained methods, such as ViT and BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_81",
            "start": 588,
            "end": 740,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_82@0",
            "content": "Impact of GCN Layers",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_82",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_83@0",
            "content": "In this section, we analyze the impact of the number of GCN layers on the performance of our proposed CMGCN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_83",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_83@1",
            "content": "We vary the layer number from 1 to 6 and report the results in Figure 3 (b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_83",
            "start": 109,
            "end": 184,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_83@2",
            "content": "Note that the 2-layer GCN architecture performs better than others overall, and thus the number of GCN layers is set to 2 in our model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_83",
            "start": 186,
            "end": 320,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_83@3",
            "content": "Model with one layer performs worse, which indicates that a shallow graph network structure is not able to learn sarcastic features well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_83",
            "start": 322,
            "end": 458,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_83@4",
            "content": "When the number of layers is greater than 2, the performance tends to decline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_83",
            "start": 460,
            "end": 537,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_83@5",
            "content": "This shows that further increasing the number of layers beyond 2 degrades the model performance possibly due to the sharp increase of parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_83",
            "start": 539,
            "end": 683,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_84@0",
            "content": "Visualization",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_84",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_85@0",
            "content": "To",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_85",
            "start": 0,
            "end": 1,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_86@0",
            "content": "Conclusion and Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_86",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_87@0",
            "content": "This paper has proposed a novel cross-modal graph architecture for multi-modal sarcasm detection, in which the crucial visual regions can be explicitly connected to the highly correlated textual tokens for learning the incongruity sentiment of sarcastic expression.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_87",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_87@1",
            "content": "Specifically, unlike previous research efforts that simply consider the visual information of the whole image, we attempt to recognize the important visual regions via object detection results, and further devise a novel cross-modal graph to explicitly establish the connections of scattered visual regions and the associated textual tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_87",
            "start": 266,
            "end": 606,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_87@2",
            "content": "More concretely, owing to the object detection results, the attribute-object pair descriptors of the objects are served as a bridge to track the highly related sarcastic cues between image and text modalities and their connection weights, and then deriving the cross-modal graphs based on external knowledge bases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_87",
            "start": 608,
            "end": 921,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_87@3",
            "content": "Afterwards, a GCNs architecture based on a retrieval-based attention mechanism is employed to capture the key incongruity sentiment expressions across different modalities for multi-modal sarcasm detection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_87",
            "start": 923,
            "end": 1128,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_87@4",
            "content": "To the best of our knowledge, it is the first study of utilizing a cross-modal graph to extract intricate multi-modal sarcastic relations via object detection and sentiment cues from external knowledge bases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_87",
            "start": 1130,
            "end": 1337,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_87@5",
            "content": "Extensive experiments on a public benchmark dataset show that our proposed approach significantly outperforms state-of-the-art baseline methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_87",
            "start": 1339,
            "end": 1482,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_88@0",
            "content": "As described in Section 3.3, the weights of edges in the cross-modal graph are computed based on both word similarities and affective clues between textual words and the attribute-object pairs of the image regions, and the dependency tree of the textmodality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_88",
            "start": 0,
            "end": 258,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_88@1",
            "content": "The approach can be easily generalized to other sentiment-related multi-modal learning scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_88",
            "start": 260,
            "end": 356,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_88@2",
            "content": "Nevertheless, the cross-graph solution might not be generalized well to other multi-modal tasks or data genres, if there is a lack of affective knowledge or a difficulty in deriving dependency trees in low-resource settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_88",
            "start": 358,
            "end": 581,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_88@3",
            "content": "Therefore, future research can consider exploiting alternatively approaches to automatically learn the weights of edges in the cross-modal graph without relying on external knowledge sources.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_88",
            "start": 583,
            "end": 773,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_89@0",
            "content": "Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang, Bottom-up and top-down attention for image captioning and visual question answering, 2018-06-18, 2018 IEEE Conference on Computer Vision and Pattern Recognition, IEEE Computer Society.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_89",
            "start": 0,
            "end": 281,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_90@0",
            "content": "Nastaran Babanejad, Heidar Davoudi, Affective and contextual embedding for sarcasm detection, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_90",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_91@0",
            "content": "Yitao Cai, Huiyu Cai, Xiaojun Wan, Multimodal sarcasm detection in Twitter with hierarchical fusion model, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_91",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_92@0",
            "content": "Erik Cambria, Yang Li, Frank Xing, Soujanya Poria, Kenneth Kwok, Senticnet 6: Ensemble application of symbolic and subsymbolic AI for sentiment analysis, 2020-10-19, CIKM '20: The 29th ACM International Conference on Information and Knowledge Management, ACM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_92",
            "start": 0,
            "end": 258,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_93@0",
            "content": "Santiago Castro, Devamanyu Hazarika, Ver\u00f3nica P\u00e9rez-Rosas, Roger Zimmermann, Rada Mihalcea, Soujanya Poria, Towards multimodal sarcasm detection (an _Obviously_ perfect paper), 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_93",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_94@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_94",
            "start": 0,
            "end": 335,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_95@0",
            "content": "UNKNOWN, None, 1995, Muting the meaning a social function of irony. Metaphor and Symbol, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_95",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_96@0",
            "content": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale, 2021, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_96",
            "start": 0,
            "end": 341,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_97@0",
            "content": "W Raymond,  Gibbs, On the psycholinguistics of sarcasm, 1986, Journal of experimental psychology: general, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_97",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_98@0",
            "content": "UNKNOWN, None, 2007, On the psycholinguistics of sarcasm. Irony in language and thougt: A cognitive science reader, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_98",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_99@0",
            "content": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep residual learning for image recognition, 2016-06-27, 2016 IEEE Conference on Computer Vision and Pattern Recognition, IEEE Computer Society.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_99",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_100@0",
            "content": "Aditya Joshi, Vinita Sharma, Pushpak Bhattacharyya, Harnessing context incongruity for sarcasm detection, 2015, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_100",
            "start": 0,
            "end": 316,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_101@0",
            "content": "Yoon Kim, Convolutional neural networks for sentence classification, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_101",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_102@0",
            "content": "N Thomas, Max Kipf,  Welling, Semisupervised classification with graph convolutional networks, 2017-04-24, 5th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_102",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_103@0",
            "content": "Amit Kumar Jena, Aman Sinha, Rohit Agarwal, 2020. C-net: Contextual network for sarcasm detection, , Proceedings of the Second Workshop on Figurative Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_103",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_104@0",
            "content": "Bin Liang, Chenwei Lou, Xiang Li, Lin Gui, Min Yang, Ruifeng Xu, Multi-modal sarcasm detection with interactive in-modal and cross-modal graphs, 2021, Proceedings of the 29th ACM International Conference on Multimedia, Association for Computing Machinery.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_104",
            "start": 0,
            "end": 254,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_105@0",
            "content": "UNKNOWN, None, 2022, Aspect-based sentiment analysis via affective knowledge enhanced graph convolutional networks. Knowledge-Based Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_105",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_106@0",
            "content": "Bin Liang, Hang Su, Rongdi Yin, Lin Gui, Min Yang, Qin Zhao, Xiaoqi Yu, Ruifeng Xu, Beta distribution guided aspect-aware graph for aspect category sentiment analysis with affective knowledge, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_106",
            "start": 0,
            "end": 328,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_107@0",
            "content": "Chenwei Lou, Bin Liang, Lin Gui, Yulan He, Yixue Dang, Ruifeng Xu, Affective dependency graph for sarcasm detection, 2021, the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '21), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_107",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_108@0",
            "content": "A George,  Miller, WordNet: A lexical database for English, 1992-02-23, Speech and Natural Language: Proceedings of a Workshop Held at, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_108",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_109@0",
            "content": "Zheng Hongliang Pan, Peng Lin, Yatao Fu, Weiping Qi,  Wang, Modeling intra and intermodality incongruity for multi-modal sarcasm detection, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_109",
            "start": 0,
            "end": 266,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_110@0",
            "content": "Bo Pang, Lillian Lee, Opinion mining and sentiment analysis, 2008, Information Retrieval, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_110",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_111@0",
            "content": "Jeffrey Pennington, Richard Socher, Christopher Manning, GloVe: Global vectors for word representation, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_111",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_112@0",
            "content": "Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan Gilbert, Ruihong Huang, Sarcasm as contrast between a positive sentiment and negative situation, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_112",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_113@0",
            "content": "Rossano Schifanella, Paloma De Juan, Joel Tetreault, Liangliang Cao, Detecting sarcasm in multimodal social platforms, 2016-10-15, Proceedings of the 2016 ACM Conference on Multimedia Conference, MM 2016, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_113",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_114@0",
            "content": "Qiaoyu Tan, Ninghao Liu, Xing Zhao, Hongxia Yang, Jingren Zhou, Xia Hu, Learning to hash with graph neural networks for recommender systems, 2020-04-20, WWW '20: The Web Conference 2020, ACM / IW3C2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_114",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_115@0",
            "content": "Yi Tay, Anh Luu, Siu Hui, Jian Su, Reasoning with sarcasm by reading inbetween, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_115",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_116@0",
            "content": "Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio, Graph attention networks, 2018-04-30, 6th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_116",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_117@0",
            "content": "Jianchao Wu, Limin Wang, Li Wang, Jie Guo, Gangshan Wu, Learning actor relation graphs for group activity recognition, 2019-06-16, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Computer Vision Foundation / IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_117",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_118@0",
            "content": "Jie Guo-Sen Xie, Huan Liu, Ling Xiong,  Shao, Scale-aware graph neural network for fewshot semantic segmentation, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_118",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_119@0",
            "content": "Tao Xiong, Peiran Zhang, Hongbo Zhu, Yihui Yang, Sarcasm detection with self-matching networks and low-rank bilinear pooling, 2019-05-13, The World Wide Web Conference, ACM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_119",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_120@0",
            "content": "Nan Xu, Zhixiong Zeng, Wenji Mao, 2020. Reasoning with multimodal sarcastic tweets via modeling cross-modality contrast and semantic association, , Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_120",
            "start": 0,
            "end": 286,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_121@0",
            "content": "Xiaocui Yang, Shi Feng, Yifei Zhang, Daling Wang, Multimodal sentiment detection based on multi-channel graph neural networks, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_121",
            "start": 0,
            "end": 308,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_122@0",
            "content": "Liang Yao, Chengsheng Mao, Yuan Luo, Graph convolutional networks for text classification, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_122",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_123@0",
            "content": "Yongjing Yin, Fandong Meng, Jinsong Su, Chulun Zhou, Zhengyuan Yang, Jie Zhou, Jiebo Luo, A novel graph-based multi-modal fusion encoder for neural machine translation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_123",
            "start": 0,
            "end": 305,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_124@0",
            "content": "Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William Hamilton, Jure Leskovec, Graph convolutional neural networks for web-scale recommender systems, 2018-08-19, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, ACM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_124",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_125@0",
            "content": "Yawen Zeng, Da Cao, Xiaochi Wei, Meng Liu, Zhou Zhao, Zheng Qin, Multi-modal relational graph for cross-modal video moment retrieval, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_125",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_126@0",
            "content": "Chen Zhang, Qiuchi Li, Dawei Song, Aspectbased sentiment classification with aspect-specific graph convolutional networks, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_126",
            "start": 0,
            "end": 347,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_127@0",
            "content": "Dong Zhang, Suzhong Wei, Shoushan Li, Hanqian Wu, Qiaoming Zhu, Guodong Zhou, Multimodal graph fusion for named entity recognition with targeted visual guidance, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_127",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "115-ARR_v2_128@0",
            "content": "Meishan Zhang, Yue Zhang, Guohong Fu, Tweet sarcasm detection using deep neural network, 2016, Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "115-ARR_v2_128",
            "start": 0,
            "end": 205,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "115-ARR_v2_0",
            "tgt_ix": "115-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_0",
            "tgt_ix": "115-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_1",
            "tgt_ix": "115-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_1",
            "tgt_ix": "115-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_0",
            "tgt_ix": "115-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_2",
            "tgt_ix": "115-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_4",
            "tgt_ix": "115-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_5",
            "tgt_ix": "115-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_6",
            "tgt_ix": "115-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_7",
            "tgt_ix": "115-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_8",
            "tgt_ix": "115-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_3",
            "tgt_ix": "115-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_3",
            "tgt_ix": "115-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_3",
            "tgt_ix": "115-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_3",
            "tgt_ix": "115-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_3",
            "tgt_ix": "115-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_3",
            "tgt_ix": "115-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_3",
            "tgt_ix": "115-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_3",
            "tgt_ix": "115-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_0",
            "tgt_ix": "115-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_10",
            "tgt_ix": "115-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_11",
            "tgt_ix": "115-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_11",
            "tgt_ix": "115-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_0",
            "tgt_ix": "115-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_12",
            "tgt_ix": "115-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_13",
            "tgt_ix": "115-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_13",
            "tgt_ix": "115-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_0",
            "tgt_ix": "115-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_14",
            "tgt_ix": "115-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_16",
            "tgt_ix": "115-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_15",
            "tgt_ix": "115-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_15",
            "tgt_ix": "115-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_15",
            "tgt_ix": "115-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_15",
            "tgt_ix": "115-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_17",
            "tgt_ix": "115-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_19",
            "tgt_ix": "115-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_20",
            "tgt_ix": "115-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_21",
            "tgt_ix": "115-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_22",
            "tgt_ix": "115-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_18",
            "tgt_ix": "115-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_18",
            "tgt_ix": "115-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_18",
            "tgt_ix": "115-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_18",
            "tgt_ix": "115-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_18",
            "tgt_ix": "115-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_18",
            "tgt_ix": "115-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_15",
            "tgt_ix": "115-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_23",
            "tgt_ix": "115-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_25",
            "tgt_ix": "115-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_26",
            "tgt_ix": "115-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_27",
            "tgt_ix": "115-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_28",
            "tgt_ix": "115-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_29",
            "tgt_ix": "115-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_30",
            "tgt_ix": "115-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_31",
            "tgt_ix": "115-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_32",
            "tgt_ix": "115-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_33",
            "tgt_ix": "115-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_34",
            "tgt_ix": "115-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_35",
            "tgt_ix": "115-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_36",
            "tgt_ix": "115-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_37",
            "tgt_ix": "115-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_24",
            "tgt_ix": "115-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_24",
            "tgt_ix": "115-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_24",
            "tgt_ix": "115-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_24",
            "tgt_ix": "115-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_24",
            "tgt_ix": "115-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_24",
            "tgt_ix": "115-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_24",
            "tgt_ix": "115-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_24",
            "tgt_ix": "115-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_24",
            "tgt_ix": "115-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_24",
            "tgt_ix": "115-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_24",
            "tgt_ix": "115-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_24",
            "tgt_ix": "115-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_24",
            "tgt_ix": "115-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_24",
            "tgt_ix": "115-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_24",
            "tgt_ix": "115-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_15",
            "tgt_ix": "115-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_38",
            "tgt_ix": "115-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_40",
            "tgt_ix": "115-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_41",
            "tgt_ix": "115-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_42",
            "tgt_ix": "115-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_43",
            "tgt_ix": "115-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_39",
            "tgt_ix": "115-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_39",
            "tgt_ix": "115-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_39",
            "tgt_ix": "115-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_39",
            "tgt_ix": "115-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_39",
            "tgt_ix": "115-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_39",
            "tgt_ix": "115-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_15",
            "tgt_ix": "115-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_44",
            "tgt_ix": "115-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_46",
            "tgt_ix": "115-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_47",
            "tgt_ix": "115-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_48",
            "tgt_ix": "115-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_49",
            "tgt_ix": "115-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_50",
            "tgt_ix": "115-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_51",
            "tgt_ix": "115-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_52",
            "tgt_ix": "115-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_53",
            "tgt_ix": "115-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_54",
            "tgt_ix": "115-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_55",
            "tgt_ix": "115-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_56",
            "tgt_ix": "115-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_57",
            "tgt_ix": "115-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_58",
            "tgt_ix": "115-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_45",
            "tgt_ix": "115-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_45",
            "tgt_ix": "115-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_45",
            "tgt_ix": "115-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_45",
            "tgt_ix": "115-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_45",
            "tgt_ix": "115-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_45",
            "tgt_ix": "115-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_45",
            "tgt_ix": "115-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_45",
            "tgt_ix": "115-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_45",
            "tgt_ix": "115-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_45",
            "tgt_ix": "115-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_45",
            "tgt_ix": "115-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_45",
            "tgt_ix": "115-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_45",
            "tgt_ix": "115-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_45",
            "tgt_ix": "115-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_45",
            "tgt_ix": "115-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_15",
            "tgt_ix": "115-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_59",
            "tgt_ix": "115-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_61",
            "tgt_ix": "115-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_62",
            "tgt_ix": "115-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_60",
            "tgt_ix": "115-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_60",
            "tgt_ix": "115-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_60",
            "tgt_ix": "115-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_60",
            "tgt_ix": "115-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_0",
            "tgt_ix": "115-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_63",
            "tgt_ix": "115-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_64",
            "tgt_ix": "115-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_64",
            "tgt_ix": "115-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_0",
            "tgt_ix": "115-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_65",
            "tgt_ix": "115-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_67",
            "tgt_ix": "115-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_68",
            "tgt_ix": "115-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_69",
            "tgt_ix": "115-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_70",
            "tgt_ix": "115-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_71",
            "tgt_ix": "115-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_66",
            "tgt_ix": "115-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_66",
            "tgt_ix": "115-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_66",
            "tgt_ix": "115-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_66",
            "tgt_ix": "115-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_66",
            "tgt_ix": "115-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_66",
            "tgt_ix": "115-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_66",
            "tgt_ix": "115-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_0",
            "tgt_ix": "115-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_72",
            "tgt_ix": "115-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_73",
            "tgt_ix": "115-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_73",
            "tgt_ix": "115-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_75",
            "tgt_ix": "115-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_74",
            "tgt_ix": "115-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_74",
            "tgt_ix": "115-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_74",
            "tgt_ix": "115-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_73",
            "tgt_ix": "115-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_76",
            "tgt_ix": "115-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_78",
            "tgt_ix": "115-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_77",
            "tgt_ix": "115-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_77",
            "tgt_ix": "115-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_77",
            "tgt_ix": "115-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_73",
            "tgt_ix": "115-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_79",
            "tgt_ix": "115-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_80",
            "tgt_ix": "115-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_80",
            "tgt_ix": "115-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_73",
            "tgt_ix": "115-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_81",
            "tgt_ix": "115-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_82",
            "tgt_ix": "115-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_82",
            "tgt_ix": "115-ARR_v2_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_73",
            "tgt_ix": "115-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_83",
            "tgt_ix": "115-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_84",
            "tgt_ix": "115-ARR_v2_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_84",
            "tgt_ix": "115-ARR_v2_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_0",
            "tgt_ix": "115-ARR_v2_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_85",
            "tgt_ix": "115-ARR_v2_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_87",
            "tgt_ix": "115-ARR_v2_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_86",
            "tgt_ix": "115-ARR_v2_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_86",
            "tgt_ix": "115-ARR_v2_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_86",
            "tgt_ix": "115-ARR_v2_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "115-ARR_v2_0",
            "tgt_ix": "115-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_1",
            "tgt_ix": "115-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_2",
            "tgt_ix": "115-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_2",
            "tgt_ix": "115-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_2",
            "tgt_ix": "115-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_2",
            "tgt_ix": "115-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_2",
            "tgt_ix": "115-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_2",
            "tgt_ix": "115-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_3",
            "tgt_ix": "115-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_4",
            "tgt_ix": "115-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_4",
            "tgt_ix": "115-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_5",
            "tgt_ix": "115-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_5",
            "tgt_ix": "115-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_5",
            "tgt_ix": "115-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_5",
            "tgt_ix": "115-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_5",
            "tgt_ix": "115-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_5",
            "tgt_ix": "115-ARR_v2_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_6",
            "tgt_ix": "115-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_6",
            "tgt_ix": "115-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_6",
            "tgt_ix": "115-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_6",
            "tgt_ix": "115-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_6",
            "tgt_ix": "115-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_6",
            "tgt_ix": "115-ARR_v2_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_7",
            "tgt_ix": "115-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_7",
            "tgt_ix": "115-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_7",
            "tgt_ix": "115-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_7",
            "tgt_ix": "115-ARR_v2_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_7",
            "tgt_ix": "115-ARR_v2_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_7",
            "tgt_ix": "115-ARR_v2_7@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_7",
            "tgt_ix": "115-ARR_v2_7@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_8",
            "tgt_ix": "115-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_9",
            "tgt_ix": "115-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_10",
            "tgt_ix": "115-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_11",
            "tgt_ix": "115-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_12",
            "tgt_ix": "115-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_12",
            "tgt_ix": "115-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_12",
            "tgt_ix": "115-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_13",
            "tgt_ix": "115-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_14",
            "tgt_ix": "115-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_14",
            "tgt_ix": "115-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_15",
            "tgt_ix": "115-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_16",
            "tgt_ix": "115-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_17",
            "tgt_ix": "115-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_18",
            "tgt_ix": "115-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_19",
            "tgt_ix": "115-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_20",
            "tgt_ix": "115-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_21",
            "tgt_ix": "115-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_21",
            "tgt_ix": "115-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_21",
            "tgt_ix": "115-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_22",
            "tgt_ix": "115-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_23",
            "tgt_ix": "115-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_24",
            "tgt_ix": "115-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_25",
            "tgt_ix": "115-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_25",
            "tgt_ix": "115-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_25",
            "tgt_ix": "115-ARR_v2_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_26",
            "tgt_ix": "115-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_27",
            "tgt_ix": "115-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_27",
            "tgt_ix": "115-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_28",
            "tgt_ix": "115-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_29",
            "tgt_ix": "115-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_29",
            "tgt_ix": "115-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_30",
            "tgt_ix": "115-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_31",
            "tgt_ix": "115-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_31",
            "tgt_ix": "115-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_32",
            "tgt_ix": "115-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_33",
            "tgt_ix": "115-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_34",
            "tgt_ix": "115-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_34",
            "tgt_ix": "115-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_35",
            "tgt_ix": "115-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_36",
            "tgt_ix": "115-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_36",
            "tgt_ix": "115-ARR_v2_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_37",
            "tgt_ix": "115-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_38",
            "tgt_ix": "115-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_39",
            "tgt_ix": "115-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_40",
            "tgt_ix": "115-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_40",
            "tgt_ix": "115-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_40",
            "tgt_ix": "115-ARR_v2_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_40",
            "tgt_ix": "115-ARR_v2_40@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_40",
            "tgt_ix": "115-ARR_v2_40@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_41",
            "tgt_ix": "115-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_41",
            "tgt_ix": "115-ARR_v2_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_42",
            "tgt_ix": "115-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_43",
            "tgt_ix": "115-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_43",
            "tgt_ix": "115-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_43",
            "tgt_ix": "115-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_43",
            "tgt_ix": "115-ARR_v2_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_43",
            "tgt_ix": "115-ARR_v2_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_43",
            "tgt_ix": "115-ARR_v2_43@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_43",
            "tgt_ix": "115-ARR_v2_43@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_43",
            "tgt_ix": "115-ARR_v2_43@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_44",
            "tgt_ix": "115-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_44",
            "tgt_ix": "115-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_44",
            "tgt_ix": "115-ARR_v2_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_44",
            "tgt_ix": "115-ARR_v2_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_44",
            "tgt_ix": "115-ARR_v2_44@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_44",
            "tgt_ix": "115-ARR_v2_44@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_44",
            "tgt_ix": "115-ARR_v2_44@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_45",
            "tgt_ix": "115-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_46",
            "tgt_ix": "115-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_46",
            "tgt_ix": "115-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_46",
            "tgt_ix": "115-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_47",
            "tgt_ix": "115-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_48",
            "tgt_ix": "115-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_48",
            "tgt_ix": "115-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_48",
            "tgt_ix": "115-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_49",
            "tgt_ix": "115-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_50",
            "tgt_ix": "115-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_50",
            "tgt_ix": "115-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_51",
            "tgt_ix": "115-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_52",
            "tgt_ix": "115-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_52",
            "tgt_ix": "115-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_52",
            "tgt_ix": "115-ARR_v2_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_53",
            "tgt_ix": "115-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_54",
            "tgt_ix": "115-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_54",
            "tgt_ix": "115-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_54",
            "tgt_ix": "115-ARR_v2_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_55",
            "tgt_ix": "115-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_56",
            "tgt_ix": "115-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_57",
            "tgt_ix": "115-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_58",
            "tgt_ix": "115-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_59",
            "tgt_ix": "115-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_60",
            "tgt_ix": "115-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_61",
            "tgt_ix": "115-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_62",
            "tgt_ix": "115-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_63",
            "tgt_ix": "115-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_63",
            "tgt_ix": "115-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_64",
            "tgt_ix": "115-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_65",
            "tgt_ix": "115-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_65",
            "tgt_ix": "115-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_65",
            "tgt_ix": "115-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_65",
            "tgt_ix": "115-ARR_v2_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_65",
            "tgt_ix": "115-ARR_v2_65@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_65",
            "tgt_ix": "115-ARR_v2_65@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_65",
            "tgt_ix": "115-ARR_v2_65@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_65",
            "tgt_ix": "115-ARR_v2_65@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_65",
            "tgt_ix": "115-ARR_v2_65@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_65",
            "tgt_ix": "115-ARR_v2_65@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_65",
            "tgt_ix": "115-ARR_v2_65@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_65",
            "tgt_ix": "115-ARR_v2_65@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_65",
            "tgt_ix": "115-ARR_v2_65@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_65",
            "tgt_ix": "115-ARR_v2_65@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_65",
            "tgt_ix": "115-ARR_v2_65@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_66",
            "tgt_ix": "115-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_67",
            "tgt_ix": "115-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_68",
            "tgt_ix": "115-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_69",
            "tgt_ix": "115-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_69",
            "tgt_ix": "115-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_70",
            "tgt_ix": "115-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_70",
            "tgt_ix": "115-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_70",
            "tgt_ix": "115-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_70",
            "tgt_ix": "115-ARR_v2_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_70",
            "tgt_ix": "115-ARR_v2_70@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_71",
            "tgt_ix": "115-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_72",
            "tgt_ix": "115-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_73",
            "tgt_ix": "115-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_74",
            "tgt_ix": "115-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_75",
            "tgt_ix": "115-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_75",
            "tgt_ix": "115-ARR_v2_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_75",
            "tgt_ix": "115-ARR_v2_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_75",
            "tgt_ix": "115-ARR_v2_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_75",
            "tgt_ix": "115-ARR_v2_75@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_75",
            "tgt_ix": "115-ARR_v2_75@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_75",
            "tgt_ix": "115-ARR_v2_75@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_75",
            "tgt_ix": "115-ARR_v2_75@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_75",
            "tgt_ix": "115-ARR_v2_75@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_76",
            "tgt_ix": "115-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_77",
            "tgt_ix": "115-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_78",
            "tgt_ix": "115-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_78",
            "tgt_ix": "115-ARR_v2_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_78",
            "tgt_ix": "115-ARR_v2_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_78",
            "tgt_ix": "115-ARR_v2_78@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_78",
            "tgt_ix": "115-ARR_v2_78@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_79",
            "tgt_ix": "115-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_80",
            "tgt_ix": "115-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_81",
            "tgt_ix": "115-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_81",
            "tgt_ix": "115-ARR_v2_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_81",
            "tgt_ix": "115-ARR_v2_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_81",
            "tgt_ix": "115-ARR_v2_81@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_81",
            "tgt_ix": "115-ARR_v2_81@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_82",
            "tgt_ix": "115-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_83",
            "tgt_ix": "115-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_83",
            "tgt_ix": "115-ARR_v2_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_83",
            "tgt_ix": "115-ARR_v2_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_83",
            "tgt_ix": "115-ARR_v2_83@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_83",
            "tgt_ix": "115-ARR_v2_83@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_83",
            "tgt_ix": "115-ARR_v2_83@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_84",
            "tgt_ix": "115-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_85",
            "tgt_ix": "115-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_86",
            "tgt_ix": "115-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_87",
            "tgt_ix": "115-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_87",
            "tgt_ix": "115-ARR_v2_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_87",
            "tgt_ix": "115-ARR_v2_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_87",
            "tgt_ix": "115-ARR_v2_87@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_87",
            "tgt_ix": "115-ARR_v2_87@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_87",
            "tgt_ix": "115-ARR_v2_87@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_88",
            "tgt_ix": "115-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_88",
            "tgt_ix": "115-ARR_v2_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_88",
            "tgt_ix": "115-ARR_v2_88@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_88",
            "tgt_ix": "115-ARR_v2_88@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_89",
            "tgt_ix": "115-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_90",
            "tgt_ix": "115-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_91",
            "tgt_ix": "115-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_92",
            "tgt_ix": "115-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_93",
            "tgt_ix": "115-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_94",
            "tgt_ix": "115-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_95",
            "tgt_ix": "115-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_96",
            "tgt_ix": "115-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_97",
            "tgt_ix": "115-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_98",
            "tgt_ix": "115-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_99",
            "tgt_ix": "115-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_100",
            "tgt_ix": "115-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_101",
            "tgt_ix": "115-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_102",
            "tgt_ix": "115-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_103",
            "tgt_ix": "115-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_104",
            "tgt_ix": "115-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_105",
            "tgt_ix": "115-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_106",
            "tgt_ix": "115-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_107",
            "tgt_ix": "115-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_108",
            "tgt_ix": "115-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_109",
            "tgt_ix": "115-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_110",
            "tgt_ix": "115-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_111",
            "tgt_ix": "115-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_112",
            "tgt_ix": "115-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_113",
            "tgt_ix": "115-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_114",
            "tgt_ix": "115-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_115",
            "tgt_ix": "115-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_116",
            "tgt_ix": "115-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_117",
            "tgt_ix": "115-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_118",
            "tgt_ix": "115-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_119",
            "tgt_ix": "115-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_120",
            "tgt_ix": "115-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_121",
            "tgt_ix": "115-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_122",
            "tgt_ix": "115-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_123",
            "tgt_ix": "115-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_124",
            "tgt_ix": "115-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_125",
            "tgt_ix": "115-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_126",
            "tgt_ix": "115-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_127",
            "tgt_ix": "115-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "115-ARR_v2_128",
            "tgt_ix": "115-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 788,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "115-ARR",
        "version": 2
    }
}