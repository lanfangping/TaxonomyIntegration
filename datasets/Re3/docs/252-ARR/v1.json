{
    "nodes": [
        {
            "ix": "252-ARR_v1_0",
            "content": "Learning Non-Autoregressive Models from Search for Unsupervised Sentence Summarization",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_2",
            "content": "Text summarization aims to generate a short summary for an input text. In this work, we propose a Non-Autoregressive Unsupervised Summarization (NAUS) approach, which does not require parallel data for training. Our NAUS first performs edit-based search towards a heuristically defined score, and generates a summary as pseudo-groundtruth. Then, we train an encoder-only non-autoregressive Transformer based on the search result. We also propose a dynamic programming approach for length-control decoding, which is important for the summarization task. Experiments on two datasets show that NAUS achieves state-of-the-art performance for unsupervised summarization, yet largely improving inference efficiency. Further, our algorithm is able to perform explicit length-transfer summary generation. 1",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "252-ARR_v1_4",
            "content": "Text summarization is an important natural language processing (NLP) task, aiming at generating concise summaries for given texts while preserving the key information. It has extensive real-world applications such as headline generation (Nenkova et al., 2011).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_5",
            "content": "State-of-the-art text summarization models are typically trained in a supervised way with large training corpora, comprising pairs of long texts and their summaries (Zhang et al., 2020;Aghajanyan et al., 2020Aghajanyan et al., , 2021. However, such parallel data are expensive to obtain, preventing the applications to less popular domains and less spoken languages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_6",
            "content": "Unsupervised text generation has been attracting increasing interest, because it does not require parallel data for training. One widely used approach is to compress a long text into a short one, and to reconstruct it to the long text by a cycle consistency loss (Miao and Blunsom, 2016;Wang and Lee, 2018;Baziotis et al., 2019). Due to the indifferentiability of the compressed sentence space, such an approach requires reinforcement learning (or its variants), which makes the training difficult (Kreutzer et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_7",
            "content": "Recently, Schumann et al. (2020) propose an edit-based approach for unsupervised summarization. Their model maximizes a scoring function that evaluates the quality (fluency and semantics) of the generated summary, achieving higher performance than cycle-consistency methods. However, the search approach is slow in inference because hundreds of search steps are needed for each data sample. Moreover, their approach can only select words from the input sentence with the word order preserved. Thus, it is restricted and may generate noisy summaries due to the local optimality of search algorithms.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_8",
            "content": "To address the above drawbacks, we propose a Non-Autoregressive approach to Unsupervised Summarization (NAUS). The idea is to perform search as in Schumann et al. (2020) and, inspired by Li et al. (2020), to train a machine learning model to smooth out such noise and to speed up the inference process. Different from Li et al. (2020), we propose to utilize non-autoregressive text generators, which generate all tokens in the output in parallel, based on our following observations:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_9",
            "content": "\u2022 Non-autoregressive models are several times faster than autoregressive generation, which is important when the system is deployed. \u2022 The input and output of the summarization task have a strong correspondence. Non-autoregressive generation supports encoder-only architectures, which can better utilize such input-output correspondence and even outperform autoregressive models for summarization. \u2022 For non-autoregressive models, we can design a length-control algorithm based on dynamic programming. This can satisfy the output length constraint, which is typical in summarization but can-not be easily achieved with autoregressive models.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_10",
            "content": "We conducted experiments on Gigaword headline generation (Graff et al., 2003) and DUC2004 (Over and Yen, 2004) datasets. Experiments show that our NAUS achieves state-of-the-art performance on unsupervised summarization; especially, it outperforms its teacher (i.e., the search approach), confirming that NAUS can indeed smooth out the search noise. Regarding inference efficiency, our NAUS with truncating is 1000 times more efficient than the search approach; even with dynamic programming for length control, NAUS is still 100 times more efficient than search and several times more efficient than autoregressive models. Our NAUS is also able to perform length-transfer summary generation, i.e., generating summaries of different lengths from training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_11",
            "content": "Approach",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "252-ARR_v1_12",
            "content": "In our approach, we first follow Schumann et al. (2020) and obtain a summary by discrete search towards a heuristically defined objective function ( \u00a72.1). Then, we propose a non-autoregressive model for the summarization task ( \u00a72.2). We present the training strategy and the proposed length-control algorithm in \u00a72.3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_13",
            "content": "Search-Based Summarization",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "252-ARR_v1_14",
            "content": "Consider a given source text x = (x 1 , x 2 , . . . , x n ). The goal of summarization is to find a shorter text y = (y 1 , y 2 , . . . , y m ) as the summary.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_15",
            "content": "Our work on unsupervised summarization follows the recent progress of search-based text generation. Schumann et al. (2020) formulate summarization as word-level extraction (with order preserved), and apply edit-based discrete local search to maximize a heuristically designed objective.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_16",
            "content": "Specifically, the objective function considers two aspects: (1) a language fluency score f LM (y), given by the reciprocal of a language model's perplexity; and (2) a semantic similarity score f SIM (y; x), given by the cosine embeddings. The overall objective combines the two aspects as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_17",
            "content": "f (y; x) = f LM (y) \u2022 f SIM (y; x) \u03b3 (1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_18",
            "content": "where \u03b3 is a weighting hyperparameter. Interested readers are referred to Schumann et al. (2020) for the details of the scoring function.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_19",
            "content": "Further, the desired summary length can be specified as a hard constraint, achieved by searching only among sentences of the correct length. Suppose the desired summary length is T , the approach selects T random words from the input, and maximizes the scoring function (1) by changing the selection and non-selection of two words.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_20",
            "content": "A greedy hill-climbing algorithm determines whether the change is accepted or not. In other words, a change is accepted if the score improves, or rejected otherwise. Such a process continues until a (possibly local) optimum is found.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_21",
            "content": "A pilot analysis in Schumann et al. (2020) shows that words largely overlap between a source text and its summary. This explains the high performance of such a word extraction approach, being a state-of-the-art unsupervised summarization system and outperforming strong competitors, e.g., cycle consistency (Wang and Lee, 2018;Baziotis et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_22",
            "content": "Non-Autoregressive Model for Summarization",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "252-ARR_v1_23",
            "content": "Despite the high performance, such edit-based search has several drawbacks. First, the search process is slow because hundreds of local search steps are needed to obtain a high-quality summary. Second, their approach only extracts the original words with order preserved. Therefore, the generated summary is restricted and may be noisy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_24",
            "content": "To this end, we propose a Non-Autoregressive approach to Unsupervised Summarization (NAUS) by learning from the search results. In this way, the machine learning model can smooth out the search noise and is much faster, largely alleviating the drawbacks of search-based summarization. Compared with training an autoregressive model from search (Li et al., 2020), non-autoregressive generation predicts all the words in parallel, further improving inference efficiency by several times.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_25",
            "content": "Moreover, a non-autoregressive model enables us to design an encoder-only architecture, which is more suited to the summarization task due to the strong correspondence between input and output, which cannot be fully utilized by encoder-decoder models, especially autoregressive ones.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_26",
            "content": "Specifically, we propose to use multi-layer Transformer (Vaswani et al., 2017) as the nonautoregressive architecture for summarization. Each Transformer layer is composed of a multihead attention sublayer and a feed-forward sublayer. Additionally, there is a residual connection in each sublayer, followed by layer normalization.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_27",
            "content": "Let X (n) \u2208 R T \u00d7d be representation at the nth layer, where T is the number of words and d is the dimension. Specially, the input layer X (0) is the embeddings of words. Suppose we have h attention heads. The output of the ith head in the nth attention sublayer is A",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_28",
            "content": "(n) i = softmax Q i K i \u221a d k V i",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_29",
            "content": ", where Q i , K i , and V i are matrices calculated by three distinct multi-layer perceptrons (MLPs) from X (n\u22121) ; d k is the attention dimension.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_30",
            "content": "Multiple attention heads are then concatenated:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_31",
            "content": "A (n) = Concat A (n) 1 , . . . , A (n) h W O",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_32",
            "content": "where W O \u2208 R d\u00d7d is a weight matrix. Then, we have a residual connection and layer normalization by",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_33",
            "content": "\u0100(n) = LayerNorm X (n\u22121) + A (n) (2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_34",
            "content": "Further, an MLP sublayer processes \u0100(n) , followed by residual connection and layer normalization, yielding the nth layer's representation",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_35",
            "content": "X (n) = LayerNorm \u0100(n) + MLP( \u0100(n) ) (3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_36",
            "content": "The last layer X (N ) is fed to softmax to predict the summary in a non-autoregressive manner, that is, the probability at the tth step is given by softmax",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_37",
            "content": "(W x (N ) t ), where x (N ) t",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_38",
            "content": "is the tth row of the matrix X (N ) and W is the softmax weight.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_39",
            "content": "It is emphasized that, in the vocabulary, we include a special blank token , which is handled by dynamic programming during both training and inference ( \u00a72.3). This enables us to generate a shorter summary than the input with such a multi-layer Transformer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_40",
            "content": "Our model can be thought of as an encoderonly architecture, differing from a typical encoderdecoder model with cross attention (Vaswani et al., 2017;Baziotis et al., 2019;Zhou and Rush, 2019). Previously, Su et al. (2021) propose a seemingly similar model to us, but put multiple end-of-sequence (EOS) tokens at the end of the generation; thus, they are unable to maintain the correspondence between input and output. Instead, we allow blank tokens scattering over the entire sentence; thus, the residual connections in Eqns ( 2) and (3) can better utilize such input-output correspondence for summarization.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_41",
            "content": "Training and Inference",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "252-ARR_v1_42",
            "content": "In this section, we first introduce the Connectionist Temporal Classification (CTC) training. Then, we propose a length-control decoding approach for summary generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_43",
            "content": "CTC Training. The Connectionist Temporal Classification (CTC, Graves et al., 2006) algorithm allows a special blank token in the vocabulary, and uses dynamic programming to marginalize out such blank tokens. In addition, non-autoregressive generation suffers from a common problem that words may be repeated in consecutive steps (Gu et al., 2018;; thus, CTC merges repeated words unless separated by . For example, the sequence of tokens a aabb is reduced to the text aab, denoted by \u0393(a aabb ) = aab. The CTC training is by maximum marginal likelihood estimation, treating the predictors as unobserved latent variables.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_44",
            "content": "Concretely, the likelihood is marginalized over all possible fillings of , i.e., all possible token sequences that are reduced to the groundtruth text:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_45",
            "content": "P (y|x) = w:\u0393(w)=y P (w|x)(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_46",
            "content": "where P (w|x) is the probability of generating a sequence of tokens w. Although enumerating every candidate in {w : \u0393(w) = y} is intractable, such marginalization fortunately can be computed by dynamic programming in an efficient way. Let \u03b1 s,t = w 1:s :\u0393(w 1:s )=y 1:t P (w 1:s |x) be the marginal probability of generating y 1:t up to the sth decoding slot. Moreover, \u03b1 s,0 is defined to be the probability that w 1:s is all , thus not having matched any word in y. The \u03b1 s,t variable can be further decomposed into two terms \u03b1 s,t = \u03b1 s,t + \u03b1 \u00ac s,t , where the first term is such probability with w s = , and the second term w s = . Apparently, the initialization of \u03b1 variables is",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_47",
            "content": "\u03b1 1,0 = P (w 1 = |x) (5) \u03b1 \u00ac 1,1 = P (w 1 = y 1 |x) (6) \u03b1 1,t = 0, \u2200t \u2265 1 (7) \u03b1 \u00ac 1,t = 0, \u2200t > 1 or t = 0(8)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_48",
            "content": "Eqn. ( 7) is because, at the first prediction slot, the empty token does not match any target words; Eqn. ( 8) is because the predicted non-first token must match exactly the first target word.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_49",
            "content": "The recursion formula for \u03b1 s,t is",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_50",
            "content": "\u03b1 s,t = \u03b1 s,t\u22121 P (w t = |x)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_51",
            "content": "since the newly predicted token with probability P (w t = |x) does not match any target word, inheriting \u03b1 s,t\u22121 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_52",
            "content": "The recursion formula for \u03b1 \u00ac s,t is",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_53",
            "content": "\u03b1 \u00ac s,t = \uf8f1 \uf8f2 \uf8f3 \u03b1 s\u22121,t\u22121 + \u03b1 \u00ac s\u22121,t P (w s = y t |x), if y t = y t\u22121 \u03b1 s\u22121,t\u22121 P (w s = y t |x), otherwise.",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_54",
            "content": "Here, w s is not , so we must have w s = y t , having the predicted probability P (w s = y t |x).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_55",
            "content": "If y t = y t\u22121 , then we have two sub-cases: first, w 1:s\u22121 is reduced to y 1:t\u22121 with w s\u22121 = separating two repeating words in y, having probability \u03b1 s\u22121,t\u22121 ; or second, w 1:s\u22121 is reduced to y 1:t with w s\u22121 = y t = , having probability \u03b1 \u00ac s\u22121 , which implies we are merging w s\u22121 and w s .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_56",
            "content": "If y t = y t\u22121 , then we only require w s\u22121 is reduced to y t\u22121 , where w s\u22121 can be either or non-. This is given by probability 4), as it is the probability that the entire generated sequence matches the entire target text.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_57",
            "content": "\u03b1 s\u22121,t\u22121 = \u03b1 s\u22121,t\u22121 + \u03b1 \u00ac s\u22121,t\u22121 . Finally, \u03b1 |w|,|y| is the marginal probability in Eqn. (",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_58",
            "content": "The CTC maximum likelihood estimation is to maximize the marginal probability, which is equivalent to minimizing the loss \u2212\u03b1 |w|,|y| . Since the dynamic programming formulas are differentiable, the entire model can be trained by backpropagation in an end-to-end manner with auto-differentiation tools (such as PyTorch).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_59",
            "content": "Length-Control Inference. Controlling output length is the nature of the summarization task, for example, displaying a short news headline on a mobile device. Moreover, Schumann et al. (2020) show that the main evaluation metric ROUGE (Lin, 2004) is sensitive to the summary length, and longer summaries tend to achieve higher ROUGE scores. Thus, it is crucial to control the summary length for fair comparison.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_60",
            "content": "\ud835\udc4f !\"# or \ud835\udf16 non-\ud835\udf16 or non-\ud835\udc4f !\"# Generation slot \ud835\udc60 Partial sentence length \ud835\udc61 \ud835\udc60 \u2212 1, \ud835\udc61 \u2212 1 \ud835\udc60 \u2212 1, \ud835\udc61 \ud835\udc60, \ud835\udc61",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_61",
            "content": "We propose a length-control algorithm by dynamic programming (DP), following the nature of CTC training. However, our DP is an approximate algorithm because of the dependencies introduced by removing consecutive repeated tokens. Thus, we equip our DP with a beam search mechanism.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_62",
            "content": "We define B s,t to be a set of top-B sequences with s predicted tokens that are reduced to t words. B s,t is constructed by three scenarios.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_63",
            "content": "First, the blank token is predicted for the sth generation slot, and thus the summary length t remains the same, shown by the blue arrow in Figure 2. This yields a set of candidates",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_64",
            "content": "B (1) s,t = b \u2295 | b \u2208 B s\u22121,t(9)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_65",
            "content": "where \u2295 refers to string/token concatenation. Second, a repeated word is predicted for the sth generation slot, i.e., b s\u22121 for a subsequence b of length s\u22121. In this case, the summary length t also remains the same, also shown in the blue arrow in Figure 2. This gives a candidate set",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_66",
            "content": "B (2) s,t = b \u2295 b s\u22121 | b \u2208 B s\u22121,t(10)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_67",
            "content": "Third, a non-, non-repeating word w s is generated, increasing the summary length from t \u2212 1 to t, shown by the red arrow in Figure 2. This gives",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_68",
            "content": "B (3) s,t = b \u2295 w * | b \u2208 B s\u22121,t\u22121 , w * = argmax ws = ,ws =b s\u22121 P (w s |x)(11)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_69",
            "content": "Based on the three candidates sets, we select top-B sequences to keep the beam size fixed:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_70",
            "content": "B s,t = top B (B (1) s,t \u222a B (2) s,t \u222aB (3) s,t )(12)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_71",
            "content": "where top B ranks the sequences by their predicted joint probabilities.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_72",
            "content": "Theorem 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_73",
            "content": "(1) If repeating tokens are not merged, then the proposed length-control algorithm with beam size B = 1 finds the exact optimum B S,T being the most probable length-T sentence given by S prediction slots.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_74",
            "content": "(2) If we merge repeating tokens predicted by CTC-trained models, the above algorithm may not be exact.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_75",
            "content": "Appendix A presents the proof of the theorem and provides a more detailed analysis, showing that our length-control algorithm, although being approximate inference, can generate a summary of the desired length properly. Compared with truncating an overlength output, our approach is able to generate more fluent and complete sentences. Also, our length-control algorithm is different from conventional beam search, shown in Appendix C.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_76",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "252-ARR_v1_77",
            "content": "Setup",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "252-ARR_v1_78",
            "content": "Datasets. We evaluated our NAUS model on Gigaword headline generation and DUC2004 datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_79",
            "content": "The head generation dataset (Rush et al., 2015) is constructed from the Gigaword news corpus (Graff et al., 2003), where the first sentence of a news article is considered as input text and the news title is considered as the summary. The dataset contains 3.8M/198K/1951 samples for training/validation/test. Based on the curve in Appendix B, we used 3M samples for training NAUS.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_80",
            "content": "It should be emphasized that, when NAUS learns from search, we only use the input of the training corpus: we perform search (Schumann et al., 2020) for each input, and train our NAUS from the search results. Therefore, we do not utilize any labeled parallel data, and our approach is unsupervised.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_81",
            "content": "Moreover, we considered two settings with desired summary lengths of 8 and 10, following Schumann et al. (2020). Our NAUS is trained from respective search results.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_82",
            "content": "The DUC2004 dataset (Over and Yen, 2004) is designed for testing only with 500 samples, where we also take the first sentence of an article as the input text. Our NAUS is transferred from the above headline generation corpus. Based on the length of DUC2004 summaries, we trained NAUS from search results with 13 words, also following Schumann et al. ( 2020) for fair comparison.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_83",
            "content": "Evaluation Metrics. We evaluated the quality of predicted summaries by ROUGE scores (Lin, 2004), which are the most widely used metrics in previous work (Wang and Lee, 2018;Baziotis et al., 2019;Zhou and Rush, 2019). Specifically, ROUGE-n evaluates n-gram overlap between a predicted summary and its reference summary; ROUGE-L, instead, measures the longest common sequence between the predicted and reference summaries.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_84",
            "content": "Different ROUGE variants are adopted in previous work, depending on the dataset. We followed the standard evaluation scripts and evaluated headline generation by ROUGE F1 (Wang and Lee, 2018;Baziotis et al., 2019;Schumann et al., 2020) and DUC2004 by Truncate ROUGE Recall (Dorr et al., 2003;West et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_85",
            "content": "In addition to summary quality, we also evaluated inference efficiency of different methods, as it is important for the deployment of deep learning models in real-time applications. We report the average inference time in seconds for each data sample, and compare the speedup with Schumann et al. ( 2020)'s search approach, which achieves (previous) state-of-the-art ROUGE scores. Our experiments were conducted on an i9-9940X CPU and an RTX6000 graphic card. Other implementation details are presented in Appendix B.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_86",
            "content": "Results and Analyses",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "252-ARR_v1_87",
            "content": "Main Results. Table 1 presents the performance of our model and baselines on the Gigaword headline test set. For a fair comparison, we categorize all approaches by average summary lengths of ~8 and ~10 into Groups A and B, respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_88",
            "content": "The Lead baseline extracts the first several words of the input sentence. Despite its simplicity, the Lead approach is a strong summarization baseline adopted in most previous work (F\u00e9vry and Phang, 2018;Baziotis et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_89",
            "content": "Wang and Lee (2018) utilize cycle consistency (Miao and Blunsom, 2016) for unsupervised summarization; Zhou and Rush (2019) perform beam search towards a step-by-step decomposable score of fluency and contextual matching. Both are unable to explicitly control the summary length: in a fair comparison of length 10 (Group B, Table 1), their performance is worse than the (previous) state-of-the-art approach (Schumann et al., 2020), 2 which performs edit-based local search.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_90",
            "content": "Our NAUS approach follows Schumann et al. 2020), because we do not need iterative search. Even with dynamic programming and beam search for length control, NAUS is still over 100 times faster. This shows our NAUS is extremely efficient in inference, which is important for real-time applications.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_91",
            "content": "Although the efficiency of Wang and Lee (2018) and Zhou and Rush (2019) is not available, we still expect our approach to be a few times faster (despite our higher ROUGE scores) because their models are autoregressive. By contrast, our NAUS is non-autoregressive, meaning that it predicts all words simultaneously. We will provide a controlled comparison between autoregressive and nonautoregressive models in Table 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_92",
            "content": "Table 2 shows the results on the DUC2004 dataset. The cycle-consistency approach (Baziotis et al., 2019;West et al., 2019) does not perform well on this dataset, outperformed by an early rule-based syntax tree trimming approach (Zajic et al., 2004) and the state-of-the-art edit-based search (Schumann et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_93",
            "content": "The performance of our NAUS model is consistent with Table 1, outperforming all previous methods in terms of the total ROUGE score, and being 100-1000 times faster than the search approach (Schumann et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_94",
            "content": "In general, the proposed NAUS not only achieves state-of-the-art ROUGE scores for unsupervised summarization, but also is more efficient when deployed. Results are consistent on both datasets, demonstrating the generality of our NAUS.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_95",
            "content": "In-Depth Analyses. We conduct in-depth analyses on the proposed NAUS model in Table 3. Due to the limit of time and space, we chose the Gigaword headline generation as our testbed. All the autoregressive (AR) and non-autoregressive (NAR) variants learn from the search output of our replication (Rows 2 & 11), where we achieve very close results to those reported in Schumann et al. (2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_96",
            "content": "We first tried vanilla encoder-decoder NAR Transformer (Rows 4 & 13, Gu et al., 2018), where we set the number of decoding slots as the desired summary length and thus length-control is not needed. As seen, a vanilla NAR model does not perform well, and CTC largely outperforms vanilla NAR in both groups (Rows 5-6 & 14-15). Such results are highly consistent with the translation literature Gu and Kong, 2021;Qian et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_97",
            "content": "The proposed encoder-only NAUS model outperforms encoder-decoder ones in both groups in terms of the total ROUGE score, when the summary length is controlled by either truncating or length-control decoding (Rows 8-9 & 17-18). Profoundly, our non-autoregressive NAUS is even better than the autoregressive Transformer (Rows 3 & 12) . We also experimented with previous non-autoregressive work for supervised summarization (Su et al., 2021) 3 in our learning-fromsearch setting. Although their approach appears to be encoder-only, it adds end-of-sequence (EOS) tokens at the end of the generation, and thus is unable to utilize the input-output correspondence. Their performance is higher than vanilla NAR models, but than ours. By contrast, NAUS is able to capture such correspondence with the residual connections, i.e., Eqns. ( 2) and (3), in its encoder-only architecture.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_98",
            "content": "Generally, the efficiency of encoder-only NAR 4 (without length-control decoding) is ~2 times faster than encoder-decoder NAR and ~20 times faster than the AR Transformer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_99",
            "content": "Further, our length-control decoding improves the total ROUGE score, compared with truncating, for both encoder-decoder CTC and encoder-only NAUS models (Rows 6, 9, 15, & 18), although its dynamic programming is slower. our non-autoregressive NAUS with length control is ~200 times faster than search and ~3 times faster 3 To the best of our knowledge, the other two nonautoregressive supervised summarization models are Yang et al. (2021) and . Their code and pretrained models are not available, making replication difficult. 4 The standard minimal encoder-decoder NAR model has 6 layers for the encoder and another 6 layers for the decoder (Vaswani et al., 2017). Our NAUS only has a 6-layer encoder. Our pilot study shows that more layers do not further improve performance in our encoder-only architecture. than the AR Transformer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_100",
            "content": "Human Evaluation. We also conducted human evaluation with a focus on truncating and lengthcontrol decodings. This is because truncating may generate incomplete sentences, which cannot be adequately evaluated by automatic metrics as their ROUGE scores are close. Specifically, we invited three human annotators to compare the two decoding algorithms for NAUS on 50 randomly selected samples, in the setting of Group B, Table 1 (Gigaword headline generation with a target length of 10). The annotation was conducted in a pairwise manner in terms of overall quality and fluency/completeness; average results (wins/loses/ties) are shown in Table 5. It should be mentioned that our annotation was strictly blind: the samples of two systems were presented in random order and annotators did not know which system generated a sample.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_101",
            "content": "As seen, our length-control decoding algorithm largely outperforms the truncating approach in terms of both the overall quality and fluency/completeness. The results are statistically significant (p-values< 0.01) in a one-sided binomial test. This verifies that length-control decoding is important for summarization, as truncating yields incomplete sentences, which are reflected by ROUGE scores.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_102",
            "content": "Additional results. We analyze the beam search in length-control decoding in Appendix C and present a case study in Appendix D. We also show length-transfer performance in Appendix E.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_103",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "252-ARR_v1_104",
            "content": "Summarization systems can be generally categorized into two paradigms: extractive and abstractive. Extractive systems extract certain sentences and clauses from input, for example, based on salient features (Zhou and Rush, 2019) or feature construction (He et al., 2012). Abstraction systems generate new utterances as the summary, e.g., by sequence-to-sequence models trained in a supervised way Zhang et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_105",
            "content": "Recently, unsupervised abstractive summarization is attracting increasing attention. For example, Yang et al. (2020) propose to use the Lead baseline (first several sentences) as the pseudo-groundtruth. However, such an approach only works with wellstructured articles (such as CNN/DailyMail). Wang and Lee (2018) and Baziotis et al. (2019) use cycle consistency for unsupervised summarization. Zhou and Rush (2019) propose a step-by-step decomposable scoring function and perform beam search for generate summarization. Schumann et al. (2020) propose an edit-based local search approach, which allows a more comprehensive scoring function and outperforms cycle consistency and beam search.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_106",
            "content": "Our paper follows Schumann et al. ( 2020) but trains a machine learning model to improve efficiency and smooth out search noise. Previously, fine-tune a GPT-2 model based on search results for unsupervised paraphrasing. We extend previous work in a non-trivial way by designing a non-autoregressive generator and further proposing a length-control decoding algorithm.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_107",
            "content": "Non-autoregressive generation is originally proposed for machine translation (Gu et al., 2018). Recently, Jia et al. (2021) apply non-autoregressive models to extractive document-level summarization. Su et al. (2021) stack a non-autoregressive BERT model with a conditional random field (CRF) for abstractive summarization; since the summary is shorter than the input text, their approach puts multiple end-to-sequence (EOS) tokens at the end of the sentence, and thus is unable to utilize the strong input-output correspondence in the summarization task. Yang et al. (2021) apply auxiliary part-of-speech (POS) loss and explore pretraining strategies for encoder-decoder non-autoregressive summarization; their length is given by POS tag/EOS predictions. All these studies concern supervised summarization, and none can explicitly control the output length. By contrast, our paper focuses on unsupervised summarization. We adopt CTC training in our encoderonly architecture, allowing blank tokens to better align input and output words, which is more appropriate for summarization. We further propose a dynamic programming algorithm to control the summary length.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_108",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "252-ARR_v1_109",
            "content": "In this work, we propose a non-autoregressive unsupervised summarization model (NAUS), where we further propose a length-control decoding algorithm based on dynamic programming. Experiments show that NAUS not only archives stateof-the-art unsupervised performance on Gigaword headline generation and DUC2004 datasets, but also is much more efficient than search methods and autoregressive models. Appendices present additional analyses and length-transfer experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_110",
            "content": "Limitation and Future Work. Our paper focuses on unsupervised summarization due to the importance of low-data applications. One limitation is that we have not obtained rigorous empirical results for supervised summarization, where the developed model may also work. This is because previous supervised summarization papers lack explicitly categorization of summary lengths (Yang et al., 2020;, making comparisons unfair and problematic (Schumann et al., 2020). This is also evidenced by Su et al. (2021), where the same model may differ by a few ROUGE points when generating summaries of different lengths. Nevertheless, we have compared with Su et al. (2021) in our setting and show the superiority of the NAUS under fair comparison. We plan to explore supervised summarization in future work after we establish a rigorous experimental setup, which is beyond the scope of this paper.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_111",
            "content": "Theorem 1. (1) If repeating tokens are not merged, then the proposed length-control algorithm with beam size B = 1 finds the exact optimum B S,T being the most probable length-T sentence given by S prediction slots. (2) If we merge repeating tokens predicted by CTC-trained models, the above algorithm may not be exact.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_112",
            "content": "Proof. [Part (1)] This part concerns a variant of our decoding algorithm, which only removes the blank token but does not merge consecutive repeated tokens to a single word, i.e., Eqn. ( 10) is removed. We denote this by \u0393 , for example, \u0393 (a aabb ) = aaabb, as opposed to \u0393(a aabb ) = aabb in our algorithm. We now show that, based on \u0393 , our dynamic programming algorithm in \u00a72.3 with beam size B = 1 is an exact inference algorithm.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_113",
            "content": "We define \u03b2 s,t = max b:|b|=s,|\u0393 (b)|=t P (b|x), where | \u2022 | denotes the length of a sequence. In other words, \u03b2 s,t is the maximum probability of s tokens that are reduced to t words.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_114",
            "content": "According to the definition, we have",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_115",
            "content": "\u03b2 1,0 = P (w 1 = |x) (13) \u03b2 1,1 = max w 1 = P (w 1 |x) (14) \u03b2 s,t = 0 for s > t(15)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_116",
            "content": "In ( 13), \u03b2 1,0 refers to the probability of one token that is reduced to zero words, in which case, the first predicted token can only be the blank token , corresponding to Eqn. ( 9) with s = 1 and t = 0. Likewise, \u03b2 1,1 is the maximum probability of one token that is reduced to one word. Thus, it is the probability of the most probable non-token, corresponding to Eqn. ( 11) with s = 1 and t = 0. Eqn. ( 15) asserts that fewer tokens cannot be reduced to more words; it is used for mathematical derivations, but need not to be explicitly implemented in our algorithm in \u00a72.3. The recursion variable \u03b2 s,t is computed by",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_117",
            "content": "\u03b2 s,t = max \u03b2 s\u22121,t \u2022 P (w s = |x), \u03b2 s\u22121,t\u22121 \u2022 max ws = P (w s |x)(16",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_118",
            "content": ") In other words, the variable \u03b2 s,t can inherit \u03b2 s\u22121,t with a predicted blank token , corresponding to Eqn. (9); or it can inherit \u03b2 s\u22121,t\u22121 with a predicted non-token, corresponding to Eqn. (11). Specially, if t = 0, then the second term has \u03b2 s\u22121,\u22121 undefined, and thus is ignored in the max operation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_119",
            "content": "Word P (w 1 |x) P (w 2 |x) I 0.39 0.1 like 0.4 0.9 coding 0.1 0 0.11 0 Table 5: An example of predicted probabilities of two generation slots, where we have a vocabulary of three words and a blank token .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_120",
            "content": "We need the max operator to take the higher probability in the two cases, since \u03b2 s,t is the maximum probability of s tokens being reduced to t words. This corresponds to Eqn. ( 12) with beam size B = 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_121",
            "content": "To sum up, our inductive calculation guarantees that \u03b2 S,T is the exact maximum probability of max b:|b|=S,|\u0393 (b)|=T P (b|x) for the desired length T with S generation slots; our algorithm (if not merging repeating tokens) gives the corresponding B S,T as argmax P (b|x) under the same constraints, concluding the proof of Part (1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_122",
            "content": "[Part (2)] CTC training merges consecutive repeated tokens to a single word, unless separated by the blank token (Graves et al., 2006). Since our model is trained by CTC, we should adopt this rule in inference as well. We show in this part that our algorithm, with beam size B = 1, does not yield the exact optimum with an example in Table 5.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_123",
            "content": "We consider generating a sentence of two words from the two prediction slots, i.e., S = T = 2. Apparently, the optimal sequence is \"I like\" with probability 0.39 \u2022 0.9 = 0.351. However, the algorithm would predict B 1,1 = {\"like\"} because \"like\" is the most probably token in the first slot. Then, our algorithm will give B 2,2 = {\"like I\"}, because it has to select a non-repeating token based on \u0393, yielding a non-optimal solution.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_124",
            "content": "It is noted that, if we do not merge repeating tokens as in \u0393 , our algorithm will give the exact optimum \"like like\" in the above example. This shows that merging consecutive repeated tokens requires the decoding algorithm to correct early predictions, and thus, our dynamic programming becomes an approximate inference. Nevertheless, our algorithm is able to generate a sequence of the desired length properly; its approximation happens only when the algorithm compares more repetitions with fewer s versus more s with fewer repetitions. Such approximation is further alleviated by beam search in our dynamic programming. Therefore, the proposed length-control algorithm is 1. Notice that NAUS is trained by pseudo-groundtruth given by unsupervised edit-based search (Schumann et al., 2020). Thus, our approach is indeed unsupervised.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_125",
            "content": "better than truncating a longer sentence; especially, our approach generates more fluent and complete sentences.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_126",
            "content": "Our NAUS had a Transformer encoder as the basic structure, generally following the settings in Vaswani et al. (2017): 6 encoder layers, each having 8 attention heads. The dimension was 512 for attention and 2048 for feed-forward modules.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_127",
            "content": "Our training used a batch size of 4K tokens, with a maximum of 200K updates. We used Adam with \u03b2 = (0.9, 0.98). In general, the learning rate warmed up to 5e-4 in the first 10K steps, and then decayed to 1e-9 with the inverse square-root schedule, except that we find the maximum learning rate of 1e-4 worked better for headline generation with the summary length of 8. We set the 2 weight decay to 0.01. Our length-control decoding algorithm had a beam size of 6. More details can be found in our repository (Footnote 1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_128",
            "content": "Our NAUS training is based on Schumann et al. ( 2020)'s prediction on the input of the Gigaword headline generation training set. We show performance against the number of training samples in Figure 3. As seen, NAUS outperforms its search teacher even with a small set of 0.1 million samples. The performance saturates as the number of samples increases. Based on this analysis, we used 3 million samples from the 3.8 million Gigaword training set to train our NAUS models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_129",
            "content": "As mentioned, our length-control decoding algorithm involves beam search within its dynamic programming, because the algorithm does not find the exact optimum when it merges repeating words. We analyze the effect of the beam size in our lengthcontrol algorithm.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_130",
            "content": "In addition, we compare our approach with CTC beam search (Graves et al., 2006). 5 Typically, a CTC-trained non-autoregressive model can be decoded either greedily or by beam search. The greedy decoding finds the most probable token at each step, i.e., w * i = argmax w i P (w i |x), and reduces the tokens to a sentence by \u0393(w 1 , \u2022 \u2022 \u2022 , w T ), where T is the number of decoding steps. The CTC beam search algorithm searches for the most likely sentence by marginalizing all token sequences that are reduced to y, i.e., argmax y w:\u0393(w)=y P (w|x).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_131",
            "content": "We show results in Figure 4, where we chose 10word Gigaword headline generation as the testbed with our NAUS model (Group B, Table 1). Notice that CTC beam search does not control the output length, and for fair comparison, we truncated its generated summaries. This also shows that our novel decoding approach and CTC beam search are distinct algorithms.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_132",
            "content": "As seen in Figure 4a, the beam search does play a role in our length-control algorithm. When the beam enlarges from 1 to 6, the performance (orange solid line) increases by 1.2 points in \u2206R, the difference of total ROUGE in comparison with Schumann et al. (2020) under our replication (Row 10, Table 1). However, further increasing the beam size does not yield additional performance gain. This is consistent with previous literature in autoregressive generation (Meister et al., 2020), which also suggests a beam size of 5-7 is the best in their applications. In terms of the efficiency (Figure 4b), a larger beam size monotonically increases the inference time. However, the overhead of beam search is relatively small in our dynamic programming, and thus we chose a beam size of 6 in our experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_133",
            "content": "Our length-control algorithm significantly outperforms CTC beam search (dashed blue lines) in terms of both \u2206R and efficiency. Especially, CTC beam search is three times slower, and degrades more significantly than our length-control decoding when the beam size increases.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_134",
            "content": "We show in Table 6 example summaries generated by our NAUS with truncating and length-control decoding, as well as the previous state-of-the-art method (Schumann et al., 2020). We observe that NAUS without length control generates slightly longer summaries, and if truncated, the output may be incomplete; by contrast, our length-control algorithm can generate a fluent and complete sentence of the desired length by dynamic programming. Compared with Schumann et al. (2020), our NAUS (length control) generates a more informative summary that includes the main clause (united nations condemned), which also appears in the reference summary.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_135",
            "content": "In the main paper, we present results where our NAUS is trained on search outputs (Schumann et al., 2020), which have the same length as the inference target. This follows the common assumption in machine learning that training and test samples are independently identically distributed.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_136",
            "content": "In this appendix, we show the performance of length-transfer summary generation, where the prediction has a different length from that of training. We denote such a model by NAUS i\u2192j , referring to training with i words and testing for j words.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_137",
            "content": "As seen in Groups A & B in Table 7, NAUS with length transfer is slightly worse than NAUS trained on the correct length, which is understandable. Nevertheless, length-transfer decoding still outperforms the search teacher and other baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_138",
            "content": "Moreover, we consider the third setting in Schumann et al. (2020), where the target length is 50% of the input. Since it takes time to obtain pseudogroundtruths given by the edit-based search, we would directly transfer already trained NAUS models to this setting by our length-control decoding. Results are shown in Group C, Table 7. We observe NASU 10\u219250% is better than NASU 8\u219250% , which makes much sense because the latter has a larger gap during transfer. Remarkably, both NASU 8\u219250% and NASU 10\u219250% outperform Schumann et al. (2020) and other baselines, achieving new state-of-the-art unsupervised performance on this setting as well.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_139",
            "content": "We further compare with Su et al. (2021), who use a length penalty to encourage short summaries. However, their length control works in the statistical sense but may fail for individual samples. Moreover, such a soft length penalty cannot generate longer summaries than trained. Even in the setting of 10 \u2192 8, their generates summaries are slightly longer than required, while the performance degrades much faster than NAUS.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_140",
            "content": "These results show that our novel length-control decoding algorithm is not only effective when generating summaries of similar length to the training targets, but also generalizes well to different desired summary lengths without re-training. In general, our NAUS is an effective and efficient unsupervised summarization system with the ability of explicit length control.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "252-ARR_v1_141",
            "content": "UNKNOWN, None, 2021, Muppet: Massive multi-task representations with pre-finetuning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Muppet: Massive multi-task representations with pre-finetuning",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_142",
            "content": "Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke Zettlemoyer, Sonal Gupta, Better fine-tuning by reducing representational collapse, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Armen Aghajanyan",
                    "Akshat Shrivastava",
                    "Anchit Gupta",
                    "Naman Goyal",
                    "Luke Zettlemoyer",
                    "Sonal Gupta"
                ],
                "title": "Better fine-tuning by reducing representational collapse",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_143",
            "content": "Christos Baziotis, Seq3: Differentiable sequence-to-sequence-to-sequence autoencoder for unsupervised abstractive sentence compression, 2019, Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Christos Baziotis"
                ],
                "title": "Seq3: Differentiable sequence-to-sequence-to-sequence autoencoder for unsupervised abstractive sentence compression",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_144",
            "content": "William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, Navdeep Jaitly, Imputer: Sequence modelling via imputation and dynamic programming, 2020, Proceedings of the International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "William Chan",
                    "Chitwan Saharia",
                    "Geoffrey Hinton",
                    "Mohammad Norouzi",
                    "Navdeep Jaitly"
                ],
                "title": "Imputer: Sequence modelling via imputation and dynamic programming",
                "pub_date": "2020",
                "pub_title": "Proceedings of the International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_145",
            "content": "Bonnie Dorr, David Zajic, Richard Schwartz, Hedge trimmer: A parse-and-trim approach to headline generation, 2003, Proceedings of the HLT-NAACL 03 Text Summarization Workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Bonnie Dorr",
                    "David Zajic",
                    "Richard Schwartz"
                ],
                "title": "Hedge trimmer: A parse-and-trim approach to headline generation",
                "pub_date": "2003",
                "pub_title": "Proceedings of the HLT-NAACL 03 Text Summarization Workshop",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_146",
            "content": "Thibault F\u00e9vry, Jason Phang, Unsupervised sentence compression using denoising autoencoders, 2018, Proceedings of the Conference on Computational Natural Language Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Thibault F\u00e9vry",
                    "Jason Phang"
                ],
                "title": "Unsupervised sentence compression using denoising autoencoders",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Conference on Computational Natural Language Learning",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_147",
            "content": "UNKNOWN, None, 2003, , English Gigaword. Linguistic Data Consortium.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": null,
                "title": null,
                "pub_date": "2003",
                "pub_title": null,
                "pub": "English Gigaword. Linguistic Data Consortium"
            }
        },
        {
            "ix": "252-ARR_v1_148",
            "content": "Alex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, J\u00fcrgen Schmidhuber, Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks, 2006, Proceedings of the International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Alex Graves",
                    "Santiago Fern\u00e1ndez",
                    "Faustino Gomez",
                    "J\u00fcrgen Schmidhuber"
                ],
                "title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks",
                "pub_date": "2006",
                "pub_title": "Proceedings of the International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_149",
            "content": "Jiatao Gu, James Bradbury, Caiming Xiong, O Victor, Richard Li,  Socher, Non-autoregressive neural machine translation, 2018, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Jiatao Gu",
                    "James Bradbury",
                    "Caiming Xiong",
                    "O Victor",
                    "Richard Li",
                    " Socher"
                ],
                "title": "Non-autoregressive neural machine translation",
                "pub_date": "2018",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_150",
            "content": "Jiatao Gu, Xiang Kong, Fully nonautoregressive neural machine translation: tricks of the trade, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Jiatao Gu",
                    "Xiang Kong"
                ],
                "title": "Fully nonautoregressive neural machine translation: tricks of the trade",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_151",
            "content": "Zhanying He, Chun Chen, Jiajun Bu, Can Wang, Lijun Zhang, Deng Cai, Xiaofei He, Document summarization based on data reconstruction, 2012, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Zhanying He",
                    "Chun Chen",
                    "Jiajun Bu",
                    "Can Wang",
                    "Lijun Zhang",
                    "Deng Cai",
                    "Xiaofei He"
                ],
                "title": "Document summarization based on data reconstruction",
                "pub_date": "2012",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_152",
            "content": "Ruipeng Jia, Yanan Cao, Haichao Shi, Fang Fang, Pengfei Yin, Shi Wang, Flexible nonautoregressive extractive summarization with threshold: How to extract a non-fixed number of summary sentences, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Ruipeng Jia",
                    "Yanan Cao",
                    "Haichao Shi",
                    "Fang Fang",
                    "Pengfei Yin",
                    "Shi Wang"
                ],
                "title": "Flexible nonautoregressive extractive summarization with threshold: How to extract a non-fixed number of summary sentences",
                "pub_date": "2021",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_153",
            "content": "Julia Kreutzer, Stefan Riezler, Carolin Lawrence, Offline reinforcement learning from human feedback in real-world sequence-to-sequence tasks, 2021, Proceedings of the Workshop on Structured Prediction for NLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Julia Kreutzer",
                    "Stefan Riezler",
                    "Carolin Lawrence"
                ],
                "title": "Offline reinforcement learning from human feedback in real-world sequence-to-sequence tasks",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Workshop on Structured Prediction for NLP",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_154",
            "content": "Jason Lee, Elman Mansimov, Kyunghyun Cho, Deterministic non-autoregressive neural sequence modeling by iterative refinement, 2018, Proceedings of the Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Jason Lee",
                    "Elman Mansimov",
                    "Kyunghyun Cho"
                ],
                "title": "Deterministic non-autoregressive neural sequence modeling by iterative refinement",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_155",
            "content": "Jingjing Li, Zichao Li, Lili Mou, Xin Jiang, Michael Lyu, Irwin King, Unsupervised text generation by learning from search, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Jingjing Li",
                    "Zichao Li",
                    "Lili Mou",
                    "Xin Jiang",
                    "Michael Lyu",
                    "Irwin King"
                ],
                "title": "Unsupervised text generation by learning from search",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_156",
            "content": "Chin-Yew Lin, ROUGE: A package for automatic evaluation of summaries, 2004, Text Summarization Branches Out, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Chin-Yew Lin"
                ],
                "title": "ROUGE: A package for automatic evaluation of summaries",
                "pub_date": "2004",
                "pub_title": "Text Summarization Branches Out",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_157",
            "content": "Xianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, Sen Song, Unsupervised paraphrasing by simulated annealing, 2020, Proceedings of the Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Xianggen Liu",
                    "Lili Mou",
                    "Fandong Meng",
                    "Hao Zhou",
                    "Jie Zhou",
                    "Sen Song"
                ],
                "title": "Unsupervised paraphrasing by simulated annealing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_158",
            "content": "Yixin Liu, Zi-Yi Dou, Pengfei Liu, RefSum: Refactoring neural summarization, 2021, Proceedings of the Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Yixin Liu",
                    "Zi-Yi Dou",
                    "Pengfei Liu"
                ],
                "title": "RefSum: Refactoring neural summarization",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_159",
            "content": "Clara Meister, Ryan Cotterell, Tim Vieira, If beam search is the answer, what was the question?, 2020, Proceedings of the Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Clara Meister",
                    "Ryan Cotterell",
                    "Tim Vieira"
                ],
                "title": "If beam search is the answer, what was the question?",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_160",
            "content": "Yishu Miao, Phil Blunsom, Language as a latent variable: Discrete generative models for sentence compression, 2016, Proceedings of the Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Yishu Miao",
                    "Phil Blunsom"
                ],
                "title": "Language as a latent variable: Discrete generative models for sentence compression",
                "pub_date": "2016",
                "pub_title": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_161",
            "content": "Ani Nenkova, Sameer Maskey, Yang Liu, Automatic summarization, 2011, Proceedings of the Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Ani Nenkova",
                    "Sameer Maskey",
                    "Yang Liu"
                ],
                "title": "Automatic summarization",
                "pub_date": "2011",
                "pub_title": "Proceedings of the Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_162",
            "content": "Paul Over, James Yen, An introduction to DUC-2004: Intrinsic evaluation of generic news text summarization systems, 2004, Proceedings of the Document Understanding Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Paul Over",
                    "James Yen"
                ],
                "title": "An introduction to DUC-2004: Intrinsic evaluation of generic news text summarization systems",
                "pub_date": "2004",
                "pub_title": "Proceedings of the Document Understanding Conference",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_163",
            "content": "Weizhen Qi, Yeyun Gong, Jian Jiao, Yu Yan, Weizhu Chen, Dayiheng Liu, Kewen Tang, Houqiang Li, Jiusheng Chen, Ruofei Zhang, Ming Zhou, Nan Duan, Bang: Bridging autoregressive and non-autoregressive generation with large scale pretraining, 2021, Proceedings of the International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Weizhen Qi",
                    "Yeyun Gong",
                    "Jian Jiao",
                    "Yu Yan",
                    "Weizhu Chen",
                    "Dayiheng Liu",
                    "Kewen Tang",
                    "Houqiang Li",
                    "Jiusheng Chen",
                    "Ruofei Zhang",
                    "Ming Zhou",
                    "Nan Duan"
                ],
                "title": "Bang: Bridging autoregressive and non-autoregressive generation with large scale pretraining",
                "pub_date": "2021",
                "pub_title": "Proceedings of the International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_164",
            "content": "Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, Lei Li, Glancing transformer for non-autoregressive neural machine translation, 2021, Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Lihua Qian",
                    "Hao Zhou",
                    "Yu Bao",
                    "Mingxuan Wang",
                    "Lin Qiu",
                    "Weinan Zhang",
                    "Yong Yu",
                    "Lei Li"
                ],
                "title": "Glancing transformer for non-autoregressive neural machine translation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_165",
            "content": "Alexander Rush, Sumit Chopra, Jason Weston, A neural attention model for abstractive sentence summarization, 2015, Proceedings of the Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Alexander Rush",
                    "Sumit Chopra",
                    "Jason Weston"
                ],
                "title": "A neural attention model for abstractive sentence summarization",
                "pub_date": "2015",
                "pub_title": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_166",
            "content": "Chitwan Saharia, William Chan, Saurabh Saxena, Mohammad Norouzi, Non-autoregressive machine translation with latent alignments, 2020, Proceedings of the Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Chitwan Saharia",
                    "William Chan",
                    "Saurabh Saxena",
                    "Mohammad Norouzi"
                ],
                "title": "Non-autoregressive machine translation with latent alignments",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_167",
            "content": "Raphael Schumann, Lili Mou, Yao Lu, Olga Vechtomova, and Katja Markert. 2020. Discrete optimization for unsupervised sentence summarization with word-level extraction, , Proceedings of the Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Raphael Schumann",
                    "Lili Mou",
                    "Yao Lu"
                ],
                "title": "Olga Vechtomova, and Katja Markert. 2020. Discrete optimization for unsupervised sentence summarization with word-level extraction",
                "pub_date": null,
                "pub_title": "Proceedings of the Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_168",
            "content": "Yixuan Su, Deng Cai, Yan Wang, David Vandyke, Simon Baker, Piji Li, Nigel Collier, Nonautoregressive text generation with pre-trained language models, 2021, Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Yixuan Su",
                    "Deng Cai",
                    "Yan Wang",
                    "David Vandyke",
                    "Simon Baker",
                    "Piji Li",
                    "Nigel Collier"
                ],
                "title": "Nonautoregressive text generation with pre-trained language models",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_169",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_170",
            "content": "Yaushian Wang, Hung-Yi Lee, Learning to encode text as human-readable summaries using generative adversarial networks, 2018, Proceedings of the Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Yaushian Wang",
                    "Hung-Yi Lee"
                ],
                "title": "Learning to encode text as human-readable summaries using generative adversarial networks",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_171",
            "content": "Peter West, Ari Holtzman, BottleSum: Unsupervised and selfsupervised sentence summarization using the information bottleneck principle, 2019-01, Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Peter West",
                    "Ari Holtzman"
                ],
                "title": "BottleSum: Unsupervised and selfsupervised sentence summarization using the information bottleneck principle",
                "pub_date": "2019-01",
                "pub_title": "Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_172",
            "content": "Kexin Yang, Wenqiang Lei, Dayiheng Liu, Weizhen Qi, Jiancheng Lv, POS-constrained parallel decoding for non-autoregressive generation, 2021, Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Kexin Yang",
                    "Wenqiang Lei",
                    "Dayiheng Liu",
                    "Weizhen Qi",
                    "Jiancheng Lv"
                ],
                "title": "POS-constrained parallel decoding for non-autoregressive generation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_173",
            "content": "Ziyi Yang, Chenguang Zhu, Robert Gmyr, Michael Zeng, Xuedong Huang, Eric Darve, TED: A pretrained unsupervised summarization model with theme modeling and denoising, 2020, Proceedings of the Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Ziyi Yang",
                    "Chenguang Zhu",
                    "Robert Gmyr",
                    "Michael Zeng",
                    "Xuedong Huang",
                    "Eric Darve"
                ],
                "title": "TED: A pretrained unsupervised summarization model with theme modeling and denoising",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_174",
            "content": "David Zajic, Bonnie Dorr, Richard Schwartz, BBN/UMD at DUC-2004: Topiary, 2004, Proceedings of the HLT-NAACL Document Understanding Workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "David Zajic",
                    "Bonnie Dorr",
                    "Richard Schwartz"
                ],
                "title": "BBN/UMD at DUC-2004: Topiary",
                "pub_date": "2004",
                "pub_title": "Proceedings of the HLT-NAACL Document Understanding Workshop",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_175",
            "content": "Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu, PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization, 2020, Proceedings of the International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Jingqing Zhang",
                    "Yao Zhao",
                    "Mohammad Saleh",
                    "Peter Liu"
                ],
                "title": "PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization",
                "pub_date": "2020",
                "pub_title": "Proceedings of the International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "252-ARR_v1_176",
            "content": "Jiawei Zhou, Alexander Rush, Simple unsupervised summarization by contextual matching, 2019, Proceedings of the Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Jiawei Zhou",
                    "Alexander Rush"
                ],
                "title": "Simple unsupervised summarization by contextual matching",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "252-ARR_v1_0@0",
            "content": "Learning Non-Autoregressive Models from Search for Unsupervised Sentence Summarization",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_0",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_2@0",
            "content": "Text summarization aims to generate a short summary for an input text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_2",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_2@1",
            "content": "In this work, we propose a Non-Autoregressive Unsupervised Summarization (NAUS) approach, which does not require parallel data for training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_2",
            "start": 71,
            "end": 210,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_2@2",
            "content": "Our NAUS first performs edit-based search towards a heuristically defined score, and generates a summary as pseudo-groundtruth.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_2",
            "start": 212,
            "end": 338,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_2@3",
            "content": "Then, we train an encoder-only non-autoregressive Transformer based on the search result.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_2",
            "start": 340,
            "end": 428,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_2@4",
            "content": "We also propose a dynamic programming approach for length-control decoding, which is important for the summarization task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_2",
            "start": 430,
            "end": 551,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_2@5",
            "content": "Experiments on two datasets show that NAUS achieves state-of-the-art performance for unsupervised summarization, yet largely improving inference efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_2",
            "start": 553,
            "end": 708,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_2@6",
            "content": "Further, our algorithm is able to perform explicit length-transfer summary generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_2",
            "start": 710,
            "end": 795,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_2@7",
            "content": "1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_2",
            "start": 797,
            "end": 797,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_4@0",
            "content": "Text summarization is an important natural language processing (NLP) task, aiming at generating concise summaries for given texts while preserving the key information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_4",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_4@1",
            "content": "It has extensive real-world applications such as headline generation (Nenkova et al., 2011).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_4",
            "start": 168,
            "end": 259,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_5@0",
            "content": "State-of-the-art text summarization models are typically trained in a supervised way with large training corpora, comprising pairs of long texts and their summaries (Zhang et al., 2020;Aghajanyan et al., 2020Aghajanyan et al., , 2021.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_5",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_5@1",
            "content": "However, such parallel data are expensive to obtain, preventing the applications to less popular domains and less spoken languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_5",
            "start": 235,
            "end": 365,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_6@0",
            "content": "Unsupervised text generation has been attracting increasing interest, because it does not require parallel data for training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_6",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_6@1",
            "content": "One widely used approach is to compress a long text into a short one, and to reconstruct it to the long text by a cycle consistency loss (Miao and Blunsom, 2016;Wang and Lee, 2018;Baziotis et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_6",
            "start": 126,
            "end": 328,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_6@2",
            "content": "Due to the indifferentiability of the compressed sentence space, such an approach requires reinforcement learning (or its variants), which makes the training difficult (Kreutzer et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_6",
            "start": 330,
            "end": 521,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_7@0",
            "content": "Recently, Schumann et al. (2020) propose an edit-based approach for unsupervised summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_7",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_7@1",
            "content": "Their model maximizes a scoring function that evaluates the quality (fluency and semantics) of the generated summary, achieving higher performance than cycle-consistency methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_7",
            "start": 96,
            "end": 273,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_7@2",
            "content": "However, the search approach is slow in inference because hundreds of search steps are needed for each data sample.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_7",
            "start": 275,
            "end": 389,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_7@3",
            "content": "Moreover, their approach can only select words from the input sentence with the word order preserved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_7",
            "start": 391,
            "end": 491,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_7@4",
            "content": "Thus, it is restricted and may generate noisy summaries due to the local optimality of search algorithms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_7",
            "start": 493,
            "end": 597,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_8@0",
            "content": "To address the above drawbacks, we propose a Non-Autoregressive approach to Unsupervised Summarization (NAUS).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_8",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_8@1",
            "content": "The idea is to perform search as in Schumann et al. (2020) and, inspired by Li et al. (2020), to train a machine learning model to smooth out such noise and to speed up the inference process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_8",
            "start": 111,
            "end": 301,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_8@2",
            "content": "Different from Li et al. (2020), we propose to utilize non-autoregressive text generators, which generate all tokens in the output in parallel, based on our following observations:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_8",
            "start": 303,
            "end": 482,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_9@0",
            "content": "\u2022 Non-autoregressive models are several times faster than autoregressive generation, which is important when the system is deployed. \u2022 The input and output of the summarization task have a strong correspondence. Non-autoregressive generation supports encoder-only architectures, which can better utilize such input-output correspondence and even outperform autoregressive models for summarization. \u2022 For non-autoregressive models, we can design a length-control algorithm based on dynamic programming. This can satisfy the output length constraint, which is typical in summarization but can-not be easily achieved with autoregressive models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_9",
            "start": 0,
            "end": 640,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_10@0",
            "content": "We conducted experiments on Gigaword headline generation (Graff et al., 2003) and DUC2004 (Over and Yen, 2004) datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_10",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_10@1",
            "content": "Experiments show that our NAUS achieves state-of-the-art performance on unsupervised summarization; especially, it outperforms its teacher (i.e., the search approach), confirming that NAUS can indeed smooth out the search noise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_10",
            "start": 121,
            "end": 348,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_10@2",
            "content": "Regarding inference efficiency, our NAUS with truncating is 1000 times more efficient than the search approach; even with dynamic programming for length control, NAUS is still 100 times more efficient than search and several times more efficient than autoregressive models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_10",
            "start": 350,
            "end": 622,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_10@3",
            "content": "Our NAUS is also able to perform length-transfer summary generation, i.e., generating summaries of different lengths from training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_10",
            "start": 624,
            "end": 754,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_11@0",
            "content": "Approach",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_11",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_12@0",
            "content": "In our approach, we first follow Schumann et al. (2020) and obtain a summary by discrete search towards a heuristically defined objective function ( \u00a72.1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_12",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_12@1",
            "content": "Then, we propose a non-autoregressive model for the summarization task ( \u00a72.2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_12",
            "start": 156,
            "end": 234,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_12@2",
            "content": "We present the training strategy and the proposed length-control algorithm in \u00a72.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_12",
            "start": 236,
            "end": 318,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_13@0",
            "content": "Search-Based Summarization",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_13",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_14@0",
            "content": "Consider a given source text x = (x 1 , x 2 , . . . , x n ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_14",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_14@1",
            "content": "The goal of summarization is to find a shorter text y = (y 1 , y 2 , . . . , y m ) as the summary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_14",
            "start": 61,
            "end": 158,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_15@0",
            "content": "Our work on unsupervised summarization follows the recent progress of search-based text generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_15",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_15@1",
            "content": "Schumann et al. (2020) formulate summarization as word-level extraction (with order preserved), and apply edit-based discrete local search to maximize a heuristically designed objective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_15",
            "start": 100,
            "end": 285,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_16@0",
            "content": "Specifically, the objective function considers two aspects: (1) a language fluency score f LM (y), given by the reciprocal of a language model's perplexity; and (2) a semantic similarity score f SIM (y; x), given by the cosine embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_16",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_16@1",
            "content": "The overall objective combines the two aspects as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_16",
            "start": 239,
            "end": 287,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_17@0",
            "content": "f (y; x) = f LM (y) \u2022 f SIM (y; x) \u03b3 (1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_17",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_18@0",
            "content": "where \u03b3 is a weighting hyperparameter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_18",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_18@1",
            "content": "Interested readers are referred to Schumann et al. (2020) for the details of the scoring function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_18",
            "start": 39,
            "end": 136,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_19@0",
            "content": "Further, the desired summary length can be specified as a hard constraint, achieved by searching only among sentences of the correct length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_19",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_19@1",
            "content": "Suppose the desired summary length is T , the approach selects T random words from the input, and maximizes the scoring function (1) by changing the selection and non-selection of two words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_19",
            "start": 141,
            "end": 330,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_20@0",
            "content": "A greedy hill-climbing algorithm determines whether the change is accepted or not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_20",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_20@1",
            "content": "In other words, a change is accepted if the score improves, or rejected otherwise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_20",
            "start": 83,
            "end": 164,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_20@2",
            "content": "Such a process continues until a (possibly local) optimum is found.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_20",
            "start": 166,
            "end": 232,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_21@0",
            "content": "A pilot analysis in Schumann et al. (2020) shows that words largely overlap between a source text and its summary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_21",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_21@1",
            "content": "This explains the high performance of such a word extraction approach, being a state-of-the-art unsupervised summarization system and outperforming strong competitors, e.g., cycle consistency (Wang and Lee, 2018;Baziotis et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_21",
            "start": 115,
            "end": 349,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_22@0",
            "content": "Non-Autoregressive Model for Summarization",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_22",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_23@0",
            "content": "Despite the high performance, such edit-based search has several drawbacks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_23",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_23@1",
            "content": "First, the search process is slow because hundreds of local search steps are needed to obtain a high-quality summary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_23",
            "start": 76,
            "end": 192,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_23@2",
            "content": "Second, their approach only extracts the original words with order preserved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_23",
            "start": 194,
            "end": 270,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_23@3",
            "content": "Therefore, the generated summary is restricted and may be noisy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_23",
            "start": 272,
            "end": 335,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_24@0",
            "content": "To this end, we propose a Non-Autoregressive approach to Unsupervised Summarization (NAUS) by learning from the search results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_24",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_24@1",
            "content": "In this way, the machine learning model can smooth out the search noise and is much faster, largely alleviating the drawbacks of search-based summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_24",
            "start": 128,
            "end": 283,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_24@2",
            "content": "Compared with training an autoregressive model from search (Li et al., 2020), non-autoregressive generation predicts all the words in parallel, further improving inference efficiency by several times.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_24",
            "start": 285,
            "end": 484,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_25@0",
            "content": "Moreover, a non-autoregressive model enables us to design an encoder-only architecture, which is more suited to the summarization task due to the strong correspondence between input and output, which cannot be fully utilized by encoder-decoder models, especially autoregressive ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_25",
            "start": 0,
            "end": 282,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_26@0",
            "content": "Specifically, we propose to use multi-layer Transformer (Vaswani et al., 2017) as the nonautoregressive architecture for summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_26",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_26@1",
            "content": "Each Transformer layer is composed of a multihead attention sublayer and a feed-forward sublayer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_26",
            "start": 136,
            "end": 232,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_26@2",
            "content": "Additionally, there is a residual connection in each sublayer, followed by layer normalization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_26",
            "start": 234,
            "end": 328,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_27@0",
            "content": "Let X (n) \u2208 R T \u00d7d be representation at the nth layer, where T is the number of words and d is the dimension.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_27",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_27@1",
            "content": "Specially, the input layer X (0) is the embeddings of words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_27",
            "start": 110,
            "end": 169,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_27@2",
            "content": "Suppose we have h attention heads.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_27",
            "start": 171,
            "end": 204,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_27@3",
            "content": "The output of the ith head in the nth attention sublayer is A",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_27",
            "start": 206,
            "end": 266,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_28@0",
            "content": "(n) i = softmax Q i K i \u221a d k V i",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_28",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_29@0",
            "content": ", where Q i , K i , and V i are matrices calculated by three distinct multi-layer perceptrons (MLPs) from X (n\u22121) ; d k is the attention dimension.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_29",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_30@0",
            "content": "Multiple attention heads are then concatenated:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_30",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_31@0",
            "content": "A (n) = Concat A (n) 1 , . . . , A (n) h W O",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_31",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_32@0",
            "content": "where W O \u2208 R d\u00d7d is a weight matrix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_32",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_32@1",
            "content": "Then, we have a residual connection and layer normalization by",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_32",
            "start": 38,
            "end": 99,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_33@0",
            "content": "\u0100(n) = LayerNorm X (n\u22121) + A (n) (2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_33",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_34@0",
            "content": "Further, an MLP sublayer processes \u0100(n) , followed by residual connection and layer normalization, yielding the nth layer's representation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_34",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_35@0",
            "content": "X (n) = LayerNorm \u0100(n) + MLP( \u0100(n) ) (3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_35",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_36@0",
            "content": "The last layer X (N ) is fed to softmax to predict the summary in a non-autoregressive manner, that is, the probability at the tth step is given by softmax",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_36",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_37@0",
            "content": "(W x (N ) t ), where x (N ) t",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_37",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_38@0",
            "content": "is the tth row of the matrix X (N ) and W is the softmax weight.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_38",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_39@0",
            "content": "It is emphasized that, in the vocabulary, we include a special blank token , which is handled by dynamic programming during both training and inference ( \u00a72.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_39",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_39@1",
            "content": "This enables us to generate a shorter summary than the input with such a multi-layer Transformer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_39",
            "start": 161,
            "end": 257,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_40@0",
            "content": "Our model can be thought of as an encoderonly architecture, differing from a typical encoderdecoder model with cross attention (Vaswani et al., 2017;Baziotis et al., 2019;Zhou and Rush, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_40",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_40@1",
            "content": "Previously, Su et al. (2021) propose a seemingly similar model to us, but put multiple end-of-sequence (EOS) tokens at the end of the generation; thus, they are unable to maintain the correspondence between input and output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_40",
            "start": 193,
            "end": 416,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_40@2",
            "content": "Instead, we allow blank tokens scattering over the entire sentence; thus, the residual connections in Eqns ( 2) and (3) can better utilize such input-output correspondence for summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_40",
            "start": 418,
            "end": 607,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_41@0",
            "content": "Training and Inference",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_41",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_42@0",
            "content": "In this section, we first introduce the Connectionist Temporal Classification (CTC) training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_42",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_42@1",
            "content": "Then, we propose a length-control decoding approach for summary generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_42",
            "start": 94,
            "end": 168,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_43@0",
            "content": "CTC Training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_43",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_43@1",
            "content": "The Connectionist Temporal Classification (CTC, Graves et al., 2006) algorithm allows a special blank token in the vocabulary, and uses dynamic programming to marginalize out such blank tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_43",
            "start": 14,
            "end": 206,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_43@2",
            "content": "In addition, non-autoregressive generation suffers from a common problem that words may be repeated in consecutive steps (Gu et al., 2018;; thus, CTC merges repeated words unless separated by .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_43",
            "start": 208,
            "end": 400,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_43@3",
            "content": "For example, the sequence of tokens a aabb is reduced to the text aab, denoted by \u0393(a aabb ) = aab.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_43",
            "start": 402,
            "end": 500,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_43@4",
            "content": "The CTC training is by maximum marginal likelihood estimation, treating the predictors as unobserved latent variables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_43",
            "start": 502,
            "end": 619,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_44@0",
            "content": "Concretely, the likelihood is marginalized over all possible fillings of , i.e., all possible token sequences that are reduced to the groundtruth text:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_44",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_45@0",
            "content": "P (y|x) = w:\u0393(w)=y P (w|x)(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_45",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_46@0",
            "content": "where P (w|x) is the probability of generating a sequence of tokens w. Although enumerating every candidate in {w : \u0393(w) = y} is intractable, such marginalization fortunately can be computed by dynamic programming in an efficient way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_46",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_46@1",
            "content": "Let \u03b1 s,t = w 1:s :\u0393(w 1:s )=y 1:t P (w 1:s |x) be the marginal probability of generating y 1:t up to the sth decoding slot.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_46",
            "start": 235,
            "end": 358,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_46@2",
            "content": "Moreover, \u03b1 s,0 is defined to be the probability that w 1:s is all , thus not having matched any word in y. The \u03b1 s,t variable can be further decomposed into two terms \u03b1 s,t = \u03b1 s,t + \u03b1 \u00ac s,t , where the first term is such probability with w s = , and the second term w s = .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_46",
            "start": 360,
            "end": 634,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_46@3",
            "content": "Apparently, the initialization of \u03b1 variables is",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_46",
            "start": 636,
            "end": 683,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_47@0",
            "content": "\u03b1 1,0 = P (w 1 = |x) (5) \u03b1 \u00ac 1,1 = P (w 1 = y 1 |x) (6) \u03b1 1,t = 0, \u2200t \u2265 1 (7) \u03b1 \u00ac 1,t = 0, \u2200t > 1 or t = 0(8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_47",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_48@0",
            "content": "Eqn. ( 7) is because, at the first prediction slot, the empty token does not match any target words; Eqn. ( 8) is because the predicted non-first token must match exactly the first target word.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_48",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_49@0",
            "content": "The recursion formula for \u03b1 s,t is",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_49",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_50@0",
            "content": "\u03b1 s,t = \u03b1 s,t\u22121 P (w t = |x)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_50",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_51@0",
            "content": "since the newly predicted token with probability P (w t = |x) does not match any target word, inheriting \u03b1 s,t\u22121 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_51",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_52@0",
            "content": "The recursion formula for \u03b1 \u00ac s,t is",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_52",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_53@0",
            "content": "\u03b1 \u00ac s,t = \uf8f1 \uf8f2 \uf8f3 \u03b1 s\u22121,t\u22121 + \u03b1 \u00ac s\u22121,t P (w s = y t |x), if y t = y t\u22121 \u03b1 s\u22121,t\u22121 P (w s = y t |x), otherwise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_53",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_54@0",
            "content": "Here, w s is not , so we must have w s = y t , having the predicted probability P (w s = y t |x).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_54",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_55@0",
            "content": "If y t = y t\u22121 , then we have two sub-cases: first, w 1:s\u22121 is reduced to y 1:t\u22121 with w s\u22121 = separating two repeating words in y, having probability \u03b1 s\u22121,t\u22121 ; or second, w 1:s\u22121 is reduced to y 1:t with w s\u22121 = y t = , having probability \u03b1 \u00ac s\u22121 , which implies we are merging w s\u22121 and w s .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_55",
            "start": 0,
            "end": 295,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_56@0",
            "content": "If y t = y t\u22121 , then we only require w s\u22121 is reduced to y t\u22121 , where w s\u22121 can be either or non-.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_56",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_56@1",
            "content": "This is given by probability 4), as it is the probability that the entire generated sequence matches the entire target text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_56",
            "start": 101,
            "end": 224,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_57@0",
            "content": "\u03b1 s\u22121,t\u22121 = \u03b1 s\u22121,t\u22121 + \u03b1 \u00ac s\u22121,t\u22121 . Finally, \u03b1 |w|,|y| is the marginal probability in Eqn. (",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_57",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_58@0",
            "content": "The CTC maximum likelihood estimation is to maximize the marginal probability, which is equivalent to minimizing the loss \u2212\u03b1 |w|,|y| .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_58",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_58@1",
            "content": "Since the dynamic programming formulas are differentiable, the entire model can be trained by backpropagation in an end-to-end manner with auto-differentiation tools (such as PyTorch).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_58",
            "start": 135,
            "end": 318,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_59@0",
            "content": "Length-Control Inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_59",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_59@1",
            "content": "Controlling output length is the nature of the summarization task, for example, displaying a short news headline on a mobile device.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_59",
            "start": 26,
            "end": 157,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_59@2",
            "content": "Moreover, Schumann et al. (2020) show that the main evaluation metric ROUGE (Lin, 2004) is sensitive to the summary length, and longer summaries tend to achieve higher ROUGE scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_59",
            "start": 159,
            "end": 339,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_59@3",
            "content": "Thus, it is crucial to control the summary length for fair comparison.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_59",
            "start": 341,
            "end": 410,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_60@0",
            "content": "\ud835\udc4f !\"# or \ud835\udf16 non-\ud835\udf16 or non-\ud835\udc4f !\"# Generation slot \ud835\udc60 Partial sentence length \ud835\udc61 \ud835\udc60 \u2212 1, \ud835\udc61 \u2212 1 \ud835\udc60 \u2212 1, \ud835\udc61 \ud835\udc60, \ud835\udc61",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_60",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_61@0",
            "content": "We propose a length-control algorithm by dynamic programming (DP), following the nature of CTC training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_61",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_61@1",
            "content": "However, our DP is an approximate algorithm because of the dependencies introduced by removing consecutive repeated tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_61",
            "start": 105,
            "end": 227,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_61@2",
            "content": "Thus, we equip our DP with a beam search mechanism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_61",
            "start": 229,
            "end": 279,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_62@0",
            "content": "We define B s,t to be a set of top-B sequences with s predicted tokens that are reduced to t words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_62",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_62@1",
            "content": "B s,t is constructed by three scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_62",
            "start": 100,
            "end": 139,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_63@0",
            "content": "First, the blank token is predicted for the sth generation slot, and thus the summary length t remains the same, shown by the blue arrow in Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_63",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_63@1",
            "content": "This yields a set of candidates",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_63",
            "start": 150,
            "end": 180,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_64@0",
            "content": "B (1) s,t = b \u2295 | b \u2208 B s\u22121,t(9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_64",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_65@0",
            "content": "where \u2295 refers to string/token concatenation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_65",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_65@1",
            "content": "Second, a repeated word is predicted for the sth generation slot, i.e., b s\u22121 for a subsequence b of length s\u22121.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_65",
            "start": 46,
            "end": 157,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_65@2",
            "content": "In this case, the summary length t also remains the same, also shown in the blue arrow in Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_65",
            "start": 159,
            "end": 257,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_65@3",
            "content": "This gives a candidate set",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_65",
            "start": 259,
            "end": 284,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_66@0",
            "content": "B (2) s,t = b \u2295 b s\u22121 | b \u2208 B s\u22121,t(10)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_66",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_67@0",
            "content": "Third, a non-, non-repeating word w s is generated, increasing the summary length from t \u2212 1 to t, shown by the red arrow in Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_67",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_67@1",
            "content": "This gives",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_67",
            "start": 135,
            "end": 144,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_68@0",
            "content": "B (3) s,t = b \u2295 w * | b \u2208 B s\u22121,t\u22121 , w * = argmax ws = ,ws =b s\u22121 P (w s |x)(11)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_68",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_69@0",
            "content": "Based on the three candidates sets, we select top-B sequences to keep the beam size fixed:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_69",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_70@0",
            "content": "B s,t = top B (B (1) s,t \u222a B (2) s,t \u222aB (3) s,t )(12)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_70",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_71@0",
            "content": "where top B ranks the sequences by their predicted joint probabilities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_71",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_72@0",
            "content": "Theorem 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_72",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_73@0",
            "content": "(1) If repeating tokens are not merged, then the proposed length-control algorithm with beam size B = 1 finds the exact optimum B S,T being the most probable length-T sentence given by S prediction slots.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_73",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_74@0",
            "content": "(2) If we merge repeating tokens predicted by CTC-trained models, the above algorithm may not be exact.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_74",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_75@0",
            "content": "Appendix A presents the proof of the theorem and provides a more detailed analysis, showing that our length-control algorithm, although being approximate inference, can generate a summary of the desired length properly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_75",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_75@1",
            "content": "Compared with truncating an overlength output, our approach is able to generate more fluent and complete sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_75",
            "start": 220,
            "end": 334,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_75@2",
            "content": "Also, our length-control algorithm is different from conventional beam search, shown in Appendix C.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_75",
            "start": 336,
            "end": 434,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_76@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_76",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_77@0",
            "content": "Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_77",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_78@0",
            "content": "Datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_78",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_78@1",
            "content": "We evaluated our NAUS model on Gigaword headline generation and DUC2004 datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_78",
            "start": 10,
            "end": 90,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_79@0",
            "content": "The head generation dataset (Rush et al., 2015) is constructed from the Gigaword news corpus (Graff et al., 2003), where the first sentence of a news article is considered as input text and the news title is considered as the summary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_79",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_79@1",
            "content": "The dataset contains 3.8M/198K/1951 samples for training/validation/test.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_79",
            "start": 235,
            "end": 307,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_79@2",
            "content": "Based on the curve in Appendix B, we used 3M samples for training NAUS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_79",
            "start": 309,
            "end": 379,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_80@0",
            "content": "It should be emphasized that, when NAUS learns from search, we only use the input of the training corpus: we perform search (Schumann et al., 2020) for each input, and train our NAUS from the search results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_80",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_80@1",
            "content": "Therefore, we do not utilize any labeled parallel data, and our approach is unsupervised.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_80",
            "start": 208,
            "end": 296,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_81@0",
            "content": "Moreover, we considered two settings with desired summary lengths of 8 and 10, following Schumann et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_81",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_81@1",
            "content": "Our NAUS is trained from respective search results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_81",
            "start": 113,
            "end": 163,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_82@0",
            "content": "The DUC2004 dataset (Over and Yen, 2004) is designed for testing only with 500 samples, where we also take the first sentence of an article as the input text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_82",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_82@1",
            "content": "Our NAUS is transferred from the above headline generation corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_82",
            "start": 159,
            "end": 224,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_82@2",
            "content": "Based on the length of DUC2004 summaries, we trained NAUS from search results with 13 words, also following Schumann et al. ( 2020) for fair comparison.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_82",
            "start": 226,
            "end": 377,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_83@0",
            "content": "Evaluation Metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_83",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_83@1",
            "content": "We evaluated the quality of predicted summaries by ROUGE scores (Lin, 2004), which are the most widely used metrics in previous work (Wang and Lee, 2018;Baziotis et al., 2019;Zhou and Rush, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_83",
            "start": 20,
            "end": 215,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_83@2",
            "content": "Specifically, ROUGE-n evaluates n-gram overlap between a predicted summary and its reference summary; ROUGE-L, instead, measures the longest common sequence between the predicted and reference summaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_83",
            "start": 217,
            "end": 419,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_84@0",
            "content": "Different ROUGE variants are adopted in previous work, depending on the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_84",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_84@1",
            "content": "We followed the standard evaluation scripts and evaluated headline generation by ROUGE F1 (Wang and Lee, 2018;Baziotis et al., 2019;Schumann et al., 2020) and DUC2004 by Truncate ROUGE Recall (Dorr et al., 2003;West et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_84",
            "start": 81,
            "end": 310,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_85@0",
            "content": "In addition to summary quality, we also evaluated inference efficiency of different methods, as it is important for the deployment of deep learning models in real-time applications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_85",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_85@1",
            "content": "We report the average inference time in seconds for each data sample, and compare the speedup with Schumann et al. ( 2020)'s search approach, which achieves (previous) state-of-the-art ROUGE scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_85",
            "start": 182,
            "end": 379,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_85@2",
            "content": "Our experiments were conducted on an i9-9940X CPU and an RTX6000 graphic card.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_85",
            "start": 381,
            "end": 458,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_85@3",
            "content": "Other implementation details are presented in Appendix B.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_85",
            "start": 460,
            "end": 516,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_86@0",
            "content": "Results and Analyses",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_86",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_87@0",
            "content": "Main Results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_87",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_87@1",
            "content": "Table 1 presents the performance of our model and baselines on the Gigaword headline test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_87",
            "start": 14,
            "end": 107,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_87@2",
            "content": "For a fair comparison, we categorize all approaches by average summary lengths of ~8 and ~10 into Groups A and B, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_87",
            "start": 109,
            "end": 235,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_88@0",
            "content": "The Lead baseline extracts the first several words of the input sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_88",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_88@1",
            "content": "Despite its simplicity, the Lead approach is a strong summarization baseline adopted in most previous work (F\u00e9vry and Phang, 2018;Baziotis et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_88",
            "start": 74,
            "end": 226,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_89@0",
            "content": "Wang and Lee (2018) utilize cycle consistency (Miao and Blunsom, 2016) for unsupervised summarization; Zhou and Rush (2019) perform beam search towards a step-by-step decomposable score of fluency and contextual matching.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_89",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_89@1",
            "content": "Both are unable to explicitly control the summary length: in a fair comparison of length 10 (Group B, Table 1), their performance is worse than the (previous) state-of-the-art approach (Schumann et al., 2020), 2 which performs edit-based local search.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_89",
            "start": 222,
            "end": 472,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_90@0",
            "content": "Our NAUS approach follows Schumann et al. 2020), because we do not need iterative search.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_90",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_90@1",
            "content": "Even with dynamic programming and beam search for length control, NAUS is still over 100 times faster.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_90",
            "start": 90,
            "end": 191,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_90@2",
            "content": "This shows our NAUS is extremely efficient in inference, which is important for real-time applications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_90",
            "start": 193,
            "end": 295,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_91@0",
            "content": "Although the efficiency of Wang and Lee (2018) and Zhou and Rush (2019) is not available, we still expect our approach to be a few times faster (despite our higher ROUGE scores) because their models are autoregressive.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_91",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_91@1",
            "content": "By contrast, our NAUS is non-autoregressive, meaning that it predicts all words simultaneously.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_91",
            "start": 219,
            "end": 313,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_91@2",
            "content": "We will provide a controlled comparison between autoregressive and nonautoregressive models in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_91",
            "start": 315,
            "end": 417,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_92@0",
            "content": "Table 2 shows the results on the DUC2004 dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_92",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_92@1",
            "content": "The cycle-consistency approach (Baziotis et al., 2019;West et al., 2019) does not perform well on this dataset, outperformed by an early rule-based syntax tree trimming approach (Zajic et al., 2004) and the state-of-the-art edit-based search (Schumann et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_92",
            "start": 50,
            "end": 315,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_93@0",
            "content": "The performance of our NAUS model is consistent with Table 1, outperforming all previous methods in terms of the total ROUGE score, and being 100-1000 times faster than the search approach (Schumann et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_93",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_94@0",
            "content": "In general, the proposed NAUS not only achieves state-of-the-art ROUGE scores for unsupervised summarization, but also is more efficient when deployed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_94",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_94@1",
            "content": "Results are consistent on both datasets, demonstrating the generality of our NAUS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_94",
            "start": 152,
            "end": 233,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_95@0",
            "content": "In-Depth Analyses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_95",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_95@1",
            "content": "We conduct in-depth analyses on the proposed NAUS model in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_95",
            "start": 19,
            "end": 85,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_95@2",
            "content": "Due to the limit of time and space, we chose the Gigaword headline generation as our testbed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_95",
            "start": 87,
            "end": 179,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_95@3",
            "content": "All the autoregressive (AR) and non-autoregressive (NAR) variants learn from the search output of our replication (Rows 2 & 11), where we achieve very close results to those reported in Schumann et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_95",
            "start": 181,
            "end": 389,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_96@0",
            "content": "We first tried vanilla encoder-decoder NAR Transformer (Rows 4 & 13, Gu et al., 2018), where we set the number of decoding slots as the desired summary length and thus length-control is not needed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_96",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_96@1",
            "content": "As seen, a vanilla NAR model does not perform well, and CTC largely outperforms vanilla NAR in both groups (Rows 5-6 & 14-15).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_96",
            "start": 198,
            "end": 323,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_96@2",
            "content": "Such results are highly consistent with the translation literature Gu and Kong, 2021;Qian et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_96",
            "start": 325,
            "end": 428,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_97@0",
            "content": "The proposed encoder-only NAUS model outperforms encoder-decoder ones in both groups in terms of the total ROUGE score, when the summary length is controlled by either truncating or length-control decoding (Rows 8-9 & 17-18).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_97",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_97@1",
            "content": "Profoundly, our non-autoregressive NAUS is even better than the autoregressive Transformer (Rows 3 & 12) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_97",
            "start": 226,
            "end": 331,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_97@2",
            "content": "We also experimented with previous non-autoregressive work for supervised summarization (Su et al., 2021) 3 in our learning-fromsearch setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_97",
            "start": 333,
            "end": 475,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_97@3",
            "content": "Although their approach appears to be encoder-only, it adds end-of-sequence (EOS) tokens at the end of the generation, and thus is unable to utilize the input-output correspondence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_97",
            "start": 477,
            "end": 657,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_97@4",
            "content": "Their performance is higher than vanilla NAR models, but than ours.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_97",
            "start": 659,
            "end": 725,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_97@5",
            "content": "By contrast, NAUS is able to capture such correspondence with the residual connections, i.e., Eqns. ( 2) and (3), in its encoder-only architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_97",
            "start": 727,
            "end": 873,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_98@0",
            "content": "Generally, the efficiency of encoder-only NAR 4 (without length-control decoding) is ~2 times faster than encoder-decoder NAR and ~20 times faster than the AR Transformer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_98",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_99@0",
            "content": "Further, our length-control decoding improves the total ROUGE score, compared with truncating, for both encoder-decoder CTC and encoder-only NAUS models (Rows 6, 9, 15, & 18), although its dynamic programming is slower.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_99",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_99@1",
            "content": "our non-autoregressive NAUS with length control is ~200 times faster than search and ~3 times faster 3 To the best of our knowledge, the other two nonautoregressive supervised summarization models are Yang et al. (2021) and .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_99",
            "start": 220,
            "end": 444,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_99@2",
            "content": "Their code and pretrained models are not available, making replication difficult.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_99",
            "start": 446,
            "end": 526,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_99@3",
            "content": "4 The standard minimal encoder-decoder NAR model has 6 layers for the encoder and another 6 layers for the decoder (Vaswani et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_99",
            "start": 528,
            "end": 665,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_99@4",
            "content": "Our NAUS only has a 6-layer encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_99",
            "start": 667,
            "end": 702,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_99@5",
            "content": "Our pilot study shows that more layers do not further improve performance in our encoder-only architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_99",
            "start": 704,
            "end": 810,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_99@6",
            "content": "than the AR Transformer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_99",
            "start": 812,
            "end": 835,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_100@0",
            "content": "Human Evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_100",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_100@1",
            "content": "We also conducted human evaluation with a focus on truncating and lengthcontrol decodings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_100",
            "start": 18,
            "end": 107,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_100@2",
            "content": "This is because truncating may generate incomplete sentences, which cannot be adequately evaluated by automatic metrics as their ROUGE scores are close.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_100",
            "start": 109,
            "end": 260,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_100@3",
            "content": "Specifically, we invited three human annotators to compare the two decoding algorithms for NAUS on 50 randomly selected samples, in the setting of Group B, Table 1 (Gigaword headline generation with a target length of 10).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_100",
            "start": 262,
            "end": 483,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_100@4",
            "content": "The annotation was conducted in a pairwise manner in terms of overall quality and fluency/completeness; average results (wins/loses/ties) are shown in Table 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_100",
            "start": 485,
            "end": 643,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_100@5",
            "content": "It should be mentioned that our annotation was strictly blind: the samples of two systems were presented in random order and annotators did not know which system generated a sample.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_100",
            "start": 645,
            "end": 825,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_101@0",
            "content": "As seen, our length-control decoding algorithm largely outperforms the truncating approach in terms of both the overall quality and fluency/completeness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_101",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_101@1",
            "content": "The results are statistically significant (p-values< 0.01) in a one-sided binomial test.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_101",
            "start": 154,
            "end": 241,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_101@2",
            "content": "This verifies that length-control decoding is important for summarization, as truncating yields incomplete sentences, which are reflected by ROUGE scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_101",
            "start": 243,
            "end": 396,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_102@0",
            "content": "Additional results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_102",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_102@1",
            "content": "We analyze the beam search in length-control decoding in Appendix C and present a case study in Appendix D. We also show length-transfer performance in Appendix E.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_102",
            "start": 20,
            "end": 182,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_103@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_103",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_104@0",
            "content": "Summarization systems can be generally categorized into two paradigms: extractive and abstractive.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_104",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_104@1",
            "content": "Extractive systems extract certain sentences and clauses from input, for example, based on salient features (Zhou and Rush, 2019) or feature construction (He et al., 2012).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_104",
            "start": 99,
            "end": 270,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_104@2",
            "content": "Abstraction systems generate new utterances as the summary, e.g., by sequence-to-sequence models trained in a supervised way Zhang et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_104",
            "start": 272,
            "end": 416,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_105@0",
            "content": "Recently, unsupervised abstractive summarization is attracting increasing attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_105",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_105@1",
            "content": "For example, Yang et al. (2020) propose to use the Lead baseline (first several sentences) as the pseudo-groundtruth.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_105",
            "start": 85,
            "end": 201,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_105@2",
            "content": "However, such an approach only works with wellstructured articles (such as CNN/DailyMail).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_105",
            "start": 203,
            "end": 292,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_105@3",
            "content": "Wang and Lee (2018) and Baziotis et al. (2019) use cycle consistency for unsupervised summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_105",
            "start": 294,
            "end": 393,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_105@4",
            "content": "Zhou and Rush (2019) propose a step-by-step decomposable scoring function and perform beam search for generate summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_105",
            "start": 395,
            "end": 519,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_105@5",
            "content": "Schumann et al. (2020) propose an edit-based local search approach, which allows a more comprehensive scoring function and outperforms cycle consistency and beam search.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_105",
            "start": 521,
            "end": 689,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_106@0",
            "content": "Our paper follows Schumann et al. ( 2020) but trains a machine learning model to improve efficiency and smooth out search noise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_106",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_106@1",
            "content": "Previously, fine-tune a GPT-2 model based on search results for unsupervised paraphrasing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_106",
            "start": 129,
            "end": 218,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_106@2",
            "content": "We extend previous work in a non-trivial way by designing a non-autoregressive generator and further proposing a length-control decoding algorithm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_106",
            "start": 220,
            "end": 366,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_107@0",
            "content": "Non-autoregressive generation is originally proposed for machine translation (Gu et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_107",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_107@1",
            "content": "Recently, Jia et al. (2021) apply non-autoregressive models to extractive document-level summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_107",
            "start": 96,
            "end": 198,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_107@2",
            "content": "Su et al. (2021) stack a non-autoregressive BERT model with a conditional random field (CRF) for abstractive summarization; since the summary is shorter than the input text, their approach puts multiple end-to-sequence (EOS) tokens at the end of the sentence, and thus is unable to utilize the strong input-output correspondence in the summarization task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_107",
            "start": 200,
            "end": 554,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_107@3",
            "content": "Yang et al. (2021) apply auxiliary part-of-speech (POS) loss and explore pretraining strategies for encoder-decoder non-autoregressive summarization; their length is given by POS tag/EOS predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_107",
            "start": 556,
            "end": 754,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_107@4",
            "content": "All these studies concern supervised summarization, and none can explicitly control the output length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_107",
            "start": 756,
            "end": 857,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_107@5",
            "content": "By contrast, our paper focuses on unsupervised summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_107",
            "start": 859,
            "end": 919,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_107@6",
            "content": "We adopt CTC training in our encoderonly architecture, allowing blank tokens to better align input and output words, which is more appropriate for summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_107",
            "start": 921,
            "end": 1081,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_107@7",
            "content": "We further propose a dynamic programming algorithm to control the summary length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_107",
            "start": 1083,
            "end": 1163,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_108@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_108",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_109@0",
            "content": "In this work, we propose a non-autoregressive unsupervised summarization model (NAUS), where we further propose a length-control decoding algorithm based on dynamic programming.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_109",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_109@1",
            "content": "Experiments show that NAUS not only archives stateof-the-art unsupervised performance on Gigaword headline generation and DUC2004 datasets, but also is much more efficient than search methods and autoregressive models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_109",
            "start": 178,
            "end": 395,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_109@2",
            "content": "Appendices present additional analyses and length-transfer experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_109",
            "start": 397,
            "end": 467,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_110@0",
            "content": "Limitation and Future Work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_110",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_110@1",
            "content": "Our paper focuses on unsupervised summarization due to the importance of low-data applications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_110",
            "start": 28,
            "end": 122,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_110@2",
            "content": "One limitation is that we have not obtained rigorous empirical results for supervised summarization, where the developed model may also work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_110",
            "start": 124,
            "end": 264,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_110@3",
            "content": "This is because previous supervised summarization papers lack explicitly categorization of summary lengths (Yang et al., 2020;, making comparisons unfair and problematic (Schumann et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_110",
            "start": 266,
            "end": 459,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_110@4",
            "content": "This is also evidenced by Su et al. (2021), where the same model may differ by a few ROUGE points when generating summaries of different lengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_110",
            "start": 461,
            "end": 605,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_110@5",
            "content": "Nevertheless, we have compared with Su et al. (2021) in our setting and show the superiority of the NAUS under fair comparison.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_110",
            "start": 607,
            "end": 733,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_110@6",
            "content": "We plan to explore supervised summarization in future work after we establish a rigorous experimental setup, which is beyond the scope of this paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_110",
            "start": 735,
            "end": 883,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_111@0",
            "content": "Theorem 1. (1) If repeating tokens are not merged, then the proposed length-control algorithm with beam size B = 1 finds the exact optimum B S,T being the most probable length-T sentence given by S prediction slots.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_111",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_111@1",
            "content": "(2) If we merge repeating tokens predicted by CTC-trained models, the above algorithm may not be exact.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_111",
            "start": 216,
            "end": 318,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_112@0",
            "content": "Proof. [Part (1)] This part concerns a variant of our decoding algorithm, which only removes the blank token but does not merge consecutive repeated tokens to a single word, i.e., Eqn. ( 10) is removed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_112",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_112@1",
            "content": "We denote this by \u0393 , for example, \u0393 (a aabb ) = aaabb, as opposed to \u0393(a aabb ) = aabb in our algorithm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_112",
            "start": 203,
            "end": 307,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_112@2",
            "content": "We now show that, based on \u0393 , our dynamic programming algorithm in \u00a72.3 with beam size B = 1 is an exact inference algorithm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_112",
            "start": 309,
            "end": 434,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_113@0",
            "content": "We define \u03b2 s,t = max b:|b|=s,|\u0393 (b)|=t P (b|x), where | \u2022 | denotes the length of a sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_113",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_113@1",
            "content": "In other words, \u03b2 s,t is the maximum probability of s tokens that are reduced to t words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_113",
            "start": 95,
            "end": 183,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_114@0",
            "content": "According to the definition, we have",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_114",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_115@0",
            "content": "\u03b2 1,0 = P (w 1 = |x) (13) \u03b2 1,1 = max w 1 = P (w 1 |x) (14) \u03b2 s,t = 0 for s > t(15)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_115",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_116@0",
            "content": "In ( 13), \u03b2 1,0 refers to the probability of one token that is reduced to zero words, in which case, the first predicted token can only be the blank token , corresponding to Eqn. ( 9) with s = 1 and t = 0.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_116",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_116@1",
            "content": "Likewise, \u03b2 1,1 is the maximum probability of one token that is reduced to one word.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_116",
            "start": 206,
            "end": 289,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_116@2",
            "content": "Thus, it is the probability of the most probable non-token, corresponding to Eqn. ( 11) with s = 1 and t = 0.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_116",
            "start": 291,
            "end": 399,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_116@3",
            "content": "Eqn. ( 15) asserts that fewer tokens cannot be reduced to more words; it is used for mathematical derivations, but need not to be explicitly implemented in our algorithm in \u00a72.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_116",
            "start": 401,
            "end": 578,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_116@4",
            "content": "The recursion variable \u03b2 s,t is computed by",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_116",
            "start": 580,
            "end": 622,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_117@0",
            "content": "\u03b2 s,t = max \u03b2 s\u22121,t \u2022 P (w s = |x), \u03b2 s\u22121,t\u22121 \u2022 max ws = P (w s |x)(16",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_117",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_118@0",
            "content": ") In other words, the variable \u03b2 s,t can inherit \u03b2 s\u22121,t with a predicted blank token , corresponding to Eqn. (9); or it can inherit \u03b2 s\u22121,t\u22121 with a predicted non-token, corresponding to Eqn. (11).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_118",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_118@1",
            "content": "Specially, if t = 0, then the second term has \u03b2 s\u22121,\u22121 undefined, and thus is ignored in the max operation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_118",
            "start": 199,
            "end": 305,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_119@0",
            "content": "Word P (w 1 |x) P (w 2 |x) I 0.39 0.1 like 0.4 0.9 coding 0.1 0 0.11 0 Table 5: An example of predicted probabilities of two generation slots, where we have a vocabulary of three words and a blank token .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_119",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_120@0",
            "content": "We need the max operator to take the higher probability in the two cases, since \u03b2 s,t is the maximum probability of s tokens being reduced to t words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_120",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_120@1",
            "content": "This corresponds to Eqn. ( 12) with beam size B = 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_120",
            "start": 151,
            "end": 202,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_121@0",
            "content": "To sum up, our inductive calculation guarantees that \u03b2 S,T is the exact maximum probability of max b:|b|=S,|\u0393 (b)|=T P (b|x) for the desired length T with S generation slots; our algorithm (if not merging repeating tokens) gives the corresponding B S,T as argmax P (b|x) under the same constraints, concluding the proof of Part (1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_121",
            "start": 0,
            "end": 331,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_122@0",
            "content": "[Part (2)] CTC training merges consecutive repeated tokens to a single word, unless separated by the blank token (Graves et al., 2006).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_122",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_122@1",
            "content": "Since our model is trained by CTC, we should adopt this rule in inference as well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_122",
            "start": 136,
            "end": 217,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_122@2",
            "content": "We show in this part that our algorithm, with beam size B = 1, does not yield the exact optimum with an example in Table 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_122",
            "start": 219,
            "end": 341,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_123@0",
            "content": "We consider generating a sentence of two words from the two prediction slots, i.e., S = T = 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_123",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_123@1",
            "content": "Apparently, the optimal sequence is \"I like\" with probability 0.39 \u2022 0.9 = 0.351.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_123",
            "start": 95,
            "end": 175,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_123@2",
            "content": "However, the algorithm would predict B 1,1 = {\"like\"} because \"like\" is the most probably token in the first slot.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_123",
            "start": 177,
            "end": 290,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_123@3",
            "content": "Then, our algorithm will give B 2,2 = {\"like I\"}, because it has to select a non-repeating token based on \u0393, yielding a non-optimal solution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_123",
            "start": 292,
            "end": 432,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_124@0",
            "content": "It is noted that, if we do not merge repeating tokens as in \u0393 , our algorithm will give the exact optimum \"like like\" in the above example.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_124",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_124@1",
            "content": "This shows that merging consecutive repeated tokens requires the decoding algorithm to correct early predictions, and thus, our dynamic programming becomes an approximate inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_124",
            "start": 140,
            "end": 320,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_124@2",
            "content": "Nevertheless, our algorithm is able to generate a sequence of the desired length properly; its approximation happens only when the algorithm compares more repetitions with fewer s versus more s with fewer repetitions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_124",
            "start": 322,
            "end": 538,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_124@3",
            "content": "Such approximation is further alleviated by beam search in our dynamic programming.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_124",
            "start": 540,
            "end": 622,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_124@4",
            "content": "Therefore, the proposed length-control algorithm is 1. Notice that NAUS is trained by pseudo-groundtruth given by unsupervised edit-based search (Schumann et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_124",
            "start": 624,
            "end": 792,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_124@5",
            "content": "Thus, our approach is indeed unsupervised.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_124",
            "start": 794,
            "end": 835,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_125@0",
            "content": "better than truncating a longer sentence; especially, our approach generates more fluent and complete sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_125",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_126@0",
            "content": "Our NAUS had a Transformer encoder as the basic structure, generally following the settings in Vaswani et al. (2017): 6 encoder layers, each having 8 attention heads.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_126",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_126@1",
            "content": "The dimension was 512 for attention and 2048 for feed-forward modules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_126",
            "start": 167,
            "end": 236,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_127@0",
            "content": "Our training used a batch size of 4K tokens, with a maximum of 200K updates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_127",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_127@1",
            "content": "We used Adam with \u03b2 = (0.9, 0.98).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_127",
            "start": 77,
            "end": 110,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_127@2",
            "content": "In general, the learning rate warmed up to 5e-4 in the first 10K steps, and then decayed to 1e-9 with the inverse square-root schedule, except that we find the maximum learning rate of 1e-4 worked better for headline generation with the summary length of 8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_127",
            "start": 112,
            "end": 368,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_127@3",
            "content": "We set the 2 weight decay to 0.01.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_127",
            "start": 370,
            "end": 403,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_127@4",
            "content": "Our length-control decoding algorithm had a beam size of 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_127",
            "start": 405,
            "end": 463,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_127@5",
            "content": "More details can be found in our repository (Footnote 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_127",
            "start": 465,
            "end": 521,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_128@0",
            "content": "Our NAUS training is based on Schumann et al. ( 2020)'s prediction on the input of the Gigaword headline generation training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_128",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_128@1",
            "content": "We show performance against the number of training samples in Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_128",
            "start": 130,
            "end": 200,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_128@2",
            "content": "As seen, NAUS outperforms its search teacher even with a small set of 0.1 million samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_128",
            "start": 202,
            "end": 291,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_128@3",
            "content": "The performance saturates as the number of samples increases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_128",
            "start": 293,
            "end": 353,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_128@4",
            "content": "Based on this analysis, we used 3 million samples from the 3.8 million Gigaword training set to train our NAUS models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_128",
            "start": 355,
            "end": 472,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_129@0",
            "content": "As mentioned, our length-control decoding algorithm involves beam search within its dynamic programming, because the algorithm does not find the exact optimum when it merges repeating words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_129",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_129@1",
            "content": "We analyze the effect of the beam size in our lengthcontrol algorithm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_129",
            "start": 191,
            "end": 260,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_130@0",
            "content": "In addition, we compare our approach with CTC beam search (Graves et al., 2006).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_130",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_130@1",
            "content": "5 Typically, a CTC-trained non-autoregressive model can be decoded either greedily or by beam search.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_130",
            "start": 81,
            "end": 181,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_130@2",
            "content": "The greedy decoding finds the most probable token at each step, i.e., w * i = argmax w i P (w i |x), and reduces the tokens to a sentence by \u0393(w 1 , \u2022 \u2022 \u2022 , w T ), where T is the number of decoding steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_130",
            "start": 183,
            "end": 386,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_130@3",
            "content": "The CTC beam search algorithm searches for the most likely sentence by marginalizing all token sequences that are reduced to y, i.e., argmax y w:\u0393(w)=y P (w|x).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_130",
            "start": 388,
            "end": 547,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_131@0",
            "content": "We show results in Figure 4, where we chose 10word Gigaword headline generation as the testbed with our NAUS model (Group B, Table 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_131",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_131@1",
            "content": "Notice that CTC beam search does not control the output length, and for fair comparison, we truncated its generated summaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_131",
            "start": 135,
            "end": 260,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_131@2",
            "content": "This also shows that our novel decoding approach and CTC beam search are distinct algorithms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_131",
            "start": 262,
            "end": 354,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_132@0",
            "content": "As seen in Figure 4a, the beam search does play a role in our length-control algorithm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_132",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_132@1",
            "content": "When the beam enlarges from 1 to 6, the performance (orange solid line) increases by 1.2 points in \u2206R, the difference of total ROUGE in comparison with Schumann et al. (2020) under our replication (Row 10, Table 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_132",
            "start": 88,
            "end": 302,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_132@2",
            "content": "However, further increasing the beam size does not yield additional performance gain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_132",
            "start": 304,
            "end": 388,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_132@3",
            "content": "This is consistent with previous literature in autoregressive generation (Meister et al., 2020), which also suggests a beam size of 5-7 is the best in their applications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_132",
            "start": 390,
            "end": 559,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_132@4",
            "content": "In terms of the efficiency (Figure 4b), a larger beam size monotonically increases the inference time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_132",
            "start": 561,
            "end": 662,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_132@5",
            "content": "However, the overhead of beam search is relatively small in our dynamic programming, and thus we chose a beam size of 6 in our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_132",
            "start": 664,
            "end": 802,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_133@0",
            "content": "Our length-control algorithm significantly outperforms CTC beam search (dashed blue lines) in terms of both \u2206R and efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_133",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_133@1",
            "content": "Especially, CTC beam search is three times slower, and degrades more significantly than our length-control decoding when the beam size increases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_133",
            "start": 127,
            "end": 271,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_134@0",
            "content": "We show in Table 6 example summaries generated by our NAUS with truncating and length-control decoding, as well as the previous state-of-the-art method (Schumann et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_134",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_134@1",
            "content": "We observe that NAUS without length control generates slightly longer summaries, and if truncated, the output may be incomplete; by contrast, our length-control algorithm can generate a fluent and complete sentence of the desired length by dynamic programming.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_134",
            "start": 177,
            "end": 436,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_134@2",
            "content": "Compared with Schumann et al. (2020), our NAUS (length control) generates a more informative summary that includes the main clause (united nations condemned), which also appears in the reference summary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_134",
            "start": 438,
            "end": 640,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_135@0",
            "content": "In the main paper, we present results where our NAUS is trained on search outputs (Schumann et al., 2020), which have the same length as the inference target.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_135",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_135@1",
            "content": "This follows the common assumption in machine learning that training and test samples are independently identically distributed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_135",
            "start": 159,
            "end": 286,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_136@0",
            "content": "In this appendix, we show the performance of length-transfer summary generation, where the prediction has a different length from that of training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_136",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_136@1",
            "content": "We denote such a model by NAUS i\u2192j , referring to training with i words and testing for j words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_136",
            "start": 148,
            "end": 243,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_137@0",
            "content": "As seen in Groups A & B in Table 7, NAUS with length transfer is slightly worse than NAUS trained on the correct length, which is understandable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_137",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_137@1",
            "content": "Nevertheless, length-transfer decoding still outperforms the search teacher and other baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_137",
            "start": 146,
            "end": 241,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_138@0",
            "content": "Moreover, we consider the third setting in Schumann et al. (2020), where the target length is 50% of the input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_138",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_138@1",
            "content": "Since it takes time to obtain pseudogroundtruths given by the edit-based search, we would directly transfer already trained NAUS models to this setting by our length-control decoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_138",
            "start": 112,
            "end": 294,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_138@2",
            "content": "Results are shown in Group C, Table 7.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_138",
            "start": 296,
            "end": 333,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_138@3",
            "content": "We observe NASU 10\u219250% is better than NASU 8\u219250% , which makes much sense because the latter has a larger gap during transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_138",
            "start": 335,
            "end": 460,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_138@4",
            "content": "Remarkably, both NASU 8\u219250% and NASU 10\u219250% outperform Schumann et al. (2020) and other baselines, achieving new state-of-the-art unsupervised performance on this setting as well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_138",
            "start": 462,
            "end": 640,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_139@0",
            "content": "We further compare with Su et al. (2021), who use a length penalty to encourage short summaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_139",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_139@1",
            "content": "However, their length control works in the statistical sense but may fail for individual samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_139",
            "start": 97,
            "end": 193,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_139@2",
            "content": "Moreover, such a soft length penalty cannot generate longer summaries than trained.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_139",
            "start": 195,
            "end": 277,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_139@3",
            "content": "Even in the setting of 10 \u2192 8, their generates summaries are slightly longer than required, while the performance degrades much faster than NAUS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_139",
            "start": 279,
            "end": 423,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_140@0",
            "content": "These results show that our novel length-control decoding algorithm is not only effective when generating summaries of similar length to the training targets, but also generalizes well to different desired summary lengths without re-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_140",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_140@1",
            "content": "In general, our NAUS is an effective and efficient unsupervised summarization system with the ability of explicit length control.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_140",
            "start": 243,
            "end": 371,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_141@0",
            "content": "UNKNOWN, None, 2021, Muppet: Massive multi-task representations with pre-finetuning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_141",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_142@0",
            "content": "Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke Zettlemoyer, Sonal Gupta, Better fine-tuning by reducing representational collapse, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_142",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_143@0",
            "content": "Christos Baziotis, Seq3: Differentiable sequence-to-sequence-to-sequence autoencoder for unsupervised abstractive sentence compression, 2019, Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_143",
            "start": 0,
            "end": 281,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_144@0",
            "content": "William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, Navdeep Jaitly, Imputer: Sequence modelling via imputation and dynamic programming, 2020, Proceedings of the International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_144",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_145@0",
            "content": "Bonnie Dorr, David Zajic, Richard Schwartz, Hedge trimmer: A parse-and-trim approach to headline generation, 2003, Proceedings of the HLT-NAACL 03 Text Summarization Workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_145",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_146@0",
            "content": "Thibault F\u00e9vry, Jason Phang, Unsupervised sentence compression using denoising autoencoders, 2018, Proceedings of the Conference on Computational Natural Language Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_146",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_147@0",
            "content": "UNKNOWN, None, 2003, , English Gigaword. Linguistic Data Consortium.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_147",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_148@0",
            "content": "Alex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, J\u00fcrgen Schmidhuber, Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks, 2006, Proceedings of the International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_148",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_149@0",
            "content": "Jiatao Gu, James Bradbury, Caiming Xiong, O Victor, Richard Li,  Socher, Non-autoregressive neural machine translation, 2018, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_149",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_150@0",
            "content": "Jiatao Gu, Xiang Kong, Fully nonautoregressive neural machine translation: tricks of the trade, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_150",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_151@0",
            "content": "Zhanying He, Chun Chen, Jiajun Bu, Can Wang, Lijun Zhang, Deng Cai, Xiaofei He, Document summarization based on data reconstruction, 2012, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_151",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_152@0",
            "content": "Ruipeng Jia, Yanan Cao, Haichao Shi, Fang Fang, Pengfei Yin, Shi Wang, Flexible nonautoregressive extractive summarization with threshold: How to extract a non-fixed number of summary sentences, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_152",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_153@0",
            "content": "Julia Kreutzer, Stefan Riezler, Carolin Lawrence, Offline reinforcement learning from human feedback in real-world sequence-to-sequence tasks, 2021, Proceedings of the Workshop on Structured Prediction for NLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_153",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_154@0",
            "content": "Jason Lee, Elman Mansimov, Kyunghyun Cho, Deterministic non-autoregressive neural sequence modeling by iterative refinement, 2018, Proceedings of the Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_154",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_155@0",
            "content": "Jingjing Li, Zichao Li, Lili Mou, Xin Jiang, Michael Lyu, Irwin King, Unsupervised text generation by learning from search, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_155",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_156@0",
            "content": "Chin-Yew Lin, ROUGE: A package for automatic evaluation of summaries, 2004, Text Summarization Branches Out, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_156",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_157@0",
            "content": "Xianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, Sen Song, Unsupervised paraphrasing by simulated annealing, 2020, Proceedings of the Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_157",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_158@0",
            "content": "Yixin Liu, Zi-Yi Dou, Pengfei Liu, RefSum: Refactoring neural summarization, 2021, Proceedings of the Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_158",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_159@0",
            "content": "Clara Meister, Ryan Cotterell, Tim Vieira, If beam search is the answer, what was the question?, 2020, Proceedings of the Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_159",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_160@0",
            "content": "Yishu Miao, Phil Blunsom, Language as a latent variable: Discrete generative models for sentence compression, 2016, Proceedings of the Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_160",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_161@0",
            "content": "Ani Nenkova, Sameer Maskey, Yang Liu, Automatic summarization, 2011, Proceedings of the Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_161",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_162@0",
            "content": "Paul Over, James Yen, An introduction to DUC-2004: Intrinsic evaluation of generic news text summarization systems, 2004, Proceedings of the Document Understanding Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_162",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_163@0",
            "content": "Weizhen Qi, Yeyun Gong, Jian Jiao, Yu Yan, Weizhu Chen, Dayiheng Liu, Kewen Tang, Houqiang Li, Jiusheng Chen, Ruofei Zhang, Ming Zhou, Nan Duan, Bang: Bridging autoregressive and non-autoregressive generation with large scale pretraining, 2021, Proceedings of the International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_163",
            "start": 0,
            "end": 310,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_164@0",
            "content": "Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, Lei Li, Glancing transformer for non-autoregressive neural machine translation, 2021, Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_164",
            "start": 0,
            "end": 317,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_165@0",
            "content": "Alexander Rush, Sumit Chopra, Jason Weston, A neural attention model for abstractive sentence summarization, 2015, Proceedings of the Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_165",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_166@0",
            "content": "Chitwan Saharia, William Chan, Saurabh Saxena, Mohammad Norouzi, Non-autoregressive machine translation with latent alignments, 2020, Proceedings of the Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_166",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_167@0",
            "content": "Raphael Schumann, Lili Mou, Yao Lu, Olga Vechtomova, and Katja Markert. 2020. Discrete optimization for unsupervised sentence summarization with word-level extraction, , Proceedings of the Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_167",
            "start": 0,
            "end": 254,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_168@0",
            "content": "Yixuan Su, Deng Cai, Yan Wang, David Vandyke, Simon Baker, Piji Li, Nigel Collier, Nonautoregressive text generation with pre-trained language models, 2021, Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_168",
            "start": 0,
            "end": 261,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_169@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_169",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_170@0",
            "content": "Yaushian Wang, Hung-Yi Lee, Learning to encode text as human-readable summaries using generative adversarial networks, 2018, Proceedings of the Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_170",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_171@0",
            "content": "Peter West, Ari Holtzman, BottleSum: Unsupervised and selfsupervised sentence summarization using the information bottleneck principle, 2019-01, Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_171",
            "start": 0,
            "end": 298,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_172@0",
            "content": "Kexin Yang, Wenqiang Lei, Dayiheng Liu, Weizhen Qi, Jiancheng Lv, POS-constrained parallel decoding for non-autoregressive generation, 2021, Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_172",
            "start": 0,
            "end": 295,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_173@0",
            "content": "Ziyi Yang, Chenguang Zhu, Robert Gmyr, Michael Zeng, Xuedong Huang, Eric Darve, TED: A pretrained unsupervised summarization model with theme modeling and denoising, 2020, Proceedings of the Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_173",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_174@0",
            "content": "David Zajic, Bonnie Dorr, Richard Schwartz, BBN/UMD at DUC-2004: Topiary, 2004, Proceedings of the HLT-NAACL Document Understanding Workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_174",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_175@0",
            "content": "Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu, PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization, 2020, Proceedings of the International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_175",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "252-ARR_v1_176@0",
            "content": "Jiawei Zhou, Alexander Rush, Simple unsupervised summarization by contextual matching, 2019, Proceedings of the Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "252-ARR_v1_176",
            "start": 0,
            "end": 177,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "252-ARR_v1_0",
            "tgt_ix": "252-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_0",
            "tgt_ix": "252-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_1",
            "tgt_ix": "252-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_1",
            "tgt_ix": "252-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_0",
            "tgt_ix": "252-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_2",
            "tgt_ix": "252-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_4",
            "tgt_ix": "252-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_5",
            "tgt_ix": "252-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_6",
            "tgt_ix": "252-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_7",
            "tgt_ix": "252-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_8",
            "tgt_ix": "252-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_3",
            "tgt_ix": "252-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_3",
            "tgt_ix": "252-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_3",
            "tgt_ix": "252-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_3",
            "tgt_ix": "252-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_3",
            "tgt_ix": "252-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_3",
            "tgt_ix": "252-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_3",
            "tgt_ix": "252-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_3",
            "tgt_ix": "252-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_0",
            "tgt_ix": "252-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_10",
            "tgt_ix": "252-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_11",
            "tgt_ix": "252-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_11",
            "tgt_ix": "252-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_11",
            "tgt_ix": "252-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_12",
            "tgt_ix": "252-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_14",
            "tgt_ix": "252-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_15",
            "tgt_ix": "252-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_16",
            "tgt_ix": "252-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_17",
            "tgt_ix": "252-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_18",
            "tgt_ix": "252-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_19",
            "tgt_ix": "252-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_20",
            "tgt_ix": "252-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_13",
            "tgt_ix": "252-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_13",
            "tgt_ix": "252-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_13",
            "tgt_ix": "252-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_13",
            "tgt_ix": "252-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_13",
            "tgt_ix": "252-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_13",
            "tgt_ix": "252-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_13",
            "tgt_ix": "252-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_13",
            "tgt_ix": "252-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_13",
            "tgt_ix": "252-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_11",
            "tgt_ix": "252-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_21",
            "tgt_ix": "252-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_23",
            "tgt_ix": "252-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_24",
            "tgt_ix": "252-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_25",
            "tgt_ix": "252-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_26",
            "tgt_ix": "252-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_27",
            "tgt_ix": "252-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_28",
            "tgt_ix": "252-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_29",
            "tgt_ix": "252-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_30",
            "tgt_ix": "252-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_31",
            "tgt_ix": "252-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_32",
            "tgt_ix": "252-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_33",
            "tgt_ix": "252-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_34",
            "tgt_ix": "252-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_35",
            "tgt_ix": "252-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_36",
            "tgt_ix": "252-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_37",
            "tgt_ix": "252-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_38",
            "tgt_ix": "252-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_39",
            "tgt_ix": "252-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_11",
            "tgt_ix": "252-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_40",
            "tgt_ix": "252-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_42",
            "tgt_ix": "252-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_43",
            "tgt_ix": "252-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_44",
            "tgt_ix": "252-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_45",
            "tgt_ix": "252-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_46",
            "tgt_ix": "252-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_47",
            "tgt_ix": "252-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_48",
            "tgt_ix": "252-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_49",
            "tgt_ix": "252-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_50",
            "tgt_ix": "252-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_51",
            "tgt_ix": "252-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_52",
            "tgt_ix": "252-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_53",
            "tgt_ix": "252-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_54",
            "tgt_ix": "252-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_55",
            "tgt_ix": "252-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_56",
            "tgt_ix": "252-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_57",
            "tgt_ix": "252-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_58",
            "tgt_ix": "252-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_59",
            "tgt_ix": "252-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_60",
            "tgt_ix": "252-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_61",
            "tgt_ix": "252-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_62",
            "tgt_ix": "252-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_63",
            "tgt_ix": "252-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_64",
            "tgt_ix": "252-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_65",
            "tgt_ix": "252-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_66",
            "tgt_ix": "252-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_67",
            "tgt_ix": "252-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_68",
            "tgt_ix": "252-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_69",
            "tgt_ix": "252-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_70",
            "tgt_ix": "252-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_71",
            "tgt_ix": "252-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_72",
            "tgt_ix": "252-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_73",
            "tgt_ix": "252-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_74",
            "tgt_ix": "252-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_0",
            "tgt_ix": "252-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_75",
            "tgt_ix": "252-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_76",
            "tgt_ix": "252-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_76",
            "tgt_ix": "252-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_78",
            "tgt_ix": "252-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_79",
            "tgt_ix": "252-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_80",
            "tgt_ix": "252-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_81",
            "tgt_ix": "252-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_82",
            "tgt_ix": "252-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_83",
            "tgt_ix": "252-ARR_v1_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_84",
            "tgt_ix": "252-ARR_v1_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_77",
            "tgt_ix": "252-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_77",
            "tgt_ix": "252-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_77",
            "tgt_ix": "252-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_77",
            "tgt_ix": "252-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_77",
            "tgt_ix": "252-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_77",
            "tgt_ix": "252-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_77",
            "tgt_ix": "252-ARR_v1_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_77",
            "tgt_ix": "252-ARR_v1_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_77",
            "tgt_ix": "252-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_76",
            "tgt_ix": "252-ARR_v1_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_85",
            "tgt_ix": "252-ARR_v1_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_87",
            "tgt_ix": "252-ARR_v1_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_88",
            "tgt_ix": "252-ARR_v1_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_89",
            "tgt_ix": "252-ARR_v1_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_90",
            "tgt_ix": "252-ARR_v1_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_91",
            "tgt_ix": "252-ARR_v1_92",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_92",
            "tgt_ix": "252-ARR_v1_93",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_93",
            "tgt_ix": "252-ARR_v1_94",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_94",
            "tgt_ix": "252-ARR_v1_95",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_95",
            "tgt_ix": "252-ARR_v1_96",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_96",
            "tgt_ix": "252-ARR_v1_97",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_97",
            "tgt_ix": "252-ARR_v1_98",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_98",
            "tgt_ix": "252-ARR_v1_99",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_99",
            "tgt_ix": "252-ARR_v1_100",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_100",
            "tgt_ix": "252-ARR_v1_101",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_101",
            "tgt_ix": "252-ARR_v1_102",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_86",
            "tgt_ix": "252-ARR_v1_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_86",
            "tgt_ix": "252-ARR_v1_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_86",
            "tgt_ix": "252-ARR_v1_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_86",
            "tgt_ix": "252-ARR_v1_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_86",
            "tgt_ix": "252-ARR_v1_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_86",
            "tgt_ix": "252-ARR_v1_92",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_86",
            "tgt_ix": "252-ARR_v1_93",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_86",
            "tgt_ix": "252-ARR_v1_94",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_86",
            "tgt_ix": "252-ARR_v1_95",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_86",
            "tgt_ix": "252-ARR_v1_96",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_86",
            "tgt_ix": "252-ARR_v1_97",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_86",
            "tgt_ix": "252-ARR_v1_98",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_86",
            "tgt_ix": "252-ARR_v1_99",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_86",
            "tgt_ix": "252-ARR_v1_100",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_86",
            "tgt_ix": "252-ARR_v1_101",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_86",
            "tgt_ix": "252-ARR_v1_102",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_86",
            "tgt_ix": "252-ARR_v1_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_0",
            "tgt_ix": "252-ARR_v1_103",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_102",
            "tgt_ix": "252-ARR_v1_103",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_104",
            "tgt_ix": "252-ARR_v1_105",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_105",
            "tgt_ix": "252-ARR_v1_106",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_106",
            "tgt_ix": "252-ARR_v1_107",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_103",
            "tgt_ix": "252-ARR_v1_104",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_103",
            "tgt_ix": "252-ARR_v1_105",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_103",
            "tgt_ix": "252-ARR_v1_106",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_103",
            "tgt_ix": "252-ARR_v1_107",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_103",
            "tgt_ix": "252-ARR_v1_104",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_0",
            "tgt_ix": "252-ARR_v1_108",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_107",
            "tgt_ix": "252-ARR_v1_108",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_109",
            "tgt_ix": "252-ARR_v1_110",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_109",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_110",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_109",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_111",
            "tgt_ix": "252-ARR_v1_112",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_112",
            "tgt_ix": "252-ARR_v1_113",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_113",
            "tgt_ix": "252-ARR_v1_114",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_114",
            "tgt_ix": "252-ARR_v1_115",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_115",
            "tgt_ix": "252-ARR_v1_116",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_116",
            "tgt_ix": "252-ARR_v1_117",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_117",
            "tgt_ix": "252-ARR_v1_118",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_118",
            "tgt_ix": "252-ARR_v1_119",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_119",
            "tgt_ix": "252-ARR_v1_120",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_120",
            "tgt_ix": "252-ARR_v1_121",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_121",
            "tgt_ix": "252-ARR_v1_122",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_122",
            "tgt_ix": "252-ARR_v1_123",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_123",
            "tgt_ix": "252-ARR_v1_124",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_124",
            "tgt_ix": "252-ARR_v1_125",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_111",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_112",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_113",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_114",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_115",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_116",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_117",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_118",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_119",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_120",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_121",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_122",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_123",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_124",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_125",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_110",
            "tgt_ix": "252-ARR_v1_111",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_126",
            "tgt_ix": "252-ARR_v1_127",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_127",
            "tgt_ix": "252-ARR_v1_128",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_126",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_127",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_128",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_125",
            "tgt_ix": "252-ARR_v1_126",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_129",
            "tgt_ix": "252-ARR_v1_130",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_130",
            "tgt_ix": "252-ARR_v1_131",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_131",
            "tgt_ix": "252-ARR_v1_132",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_132",
            "tgt_ix": "252-ARR_v1_133",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_129",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_130",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_131",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_132",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_133",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_128",
            "tgt_ix": "252-ARR_v1_129",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_134",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_133",
            "tgt_ix": "252-ARR_v1_134",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_135",
            "tgt_ix": "252-ARR_v1_136",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_136",
            "tgt_ix": "252-ARR_v1_137",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_137",
            "tgt_ix": "252-ARR_v1_138",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_138",
            "tgt_ix": "252-ARR_v1_139",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_139",
            "tgt_ix": "252-ARR_v1_140",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_135",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_136",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_137",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_138",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_139",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_140",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_134",
            "tgt_ix": "252-ARR_v1_135",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "252-ARR_v1_0",
            "tgt_ix": "252-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_1",
            "tgt_ix": "252-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_2",
            "tgt_ix": "252-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_2",
            "tgt_ix": "252-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_2",
            "tgt_ix": "252-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_2",
            "tgt_ix": "252-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_2",
            "tgt_ix": "252-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_2",
            "tgt_ix": "252-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_2",
            "tgt_ix": "252-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_2",
            "tgt_ix": "252-ARR_v1_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_3",
            "tgt_ix": "252-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_4",
            "tgt_ix": "252-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_4",
            "tgt_ix": "252-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_5",
            "tgt_ix": "252-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_5",
            "tgt_ix": "252-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_6",
            "tgt_ix": "252-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_6",
            "tgt_ix": "252-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_6",
            "tgt_ix": "252-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_7",
            "tgt_ix": "252-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_7",
            "tgt_ix": "252-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_7",
            "tgt_ix": "252-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_7",
            "tgt_ix": "252-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_7",
            "tgt_ix": "252-ARR_v1_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_8",
            "tgt_ix": "252-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_8",
            "tgt_ix": "252-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_8",
            "tgt_ix": "252-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_9",
            "tgt_ix": "252-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_10",
            "tgt_ix": "252-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_10",
            "tgt_ix": "252-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_10",
            "tgt_ix": "252-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_10",
            "tgt_ix": "252-ARR_v1_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_11",
            "tgt_ix": "252-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_12",
            "tgt_ix": "252-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_12",
            "tgt_ix": "252-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_12",
            "tgt_ix": "252-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_13",
            "tgt_ix": "252-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_14",
            "tgt_ix": "252-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_14",
            "tgt_ix": "252-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_15",
            "tgt_ix": "252-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_15",
            "tgt_ix": "252-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_16",
            "tgt_ix": "252-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_16",
            "tgt_ix": "252-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_17",
            "tgt_ix": "252-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_18",
            "tgt_ix": "252-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_18",
            "tgt_ix": "252-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_19",
            "tgt_ix": "252-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_19",
            "tgt_ix": "252-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_20",
            "tgt_ix": "252-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_20",
            "tgt_ix": "252-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_20",
            "tgt_ix": "252-ARR_v1_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_21",
            "tgt_ix": "252-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_21",
            "tgt_ix": "252-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_22",
            "tgt_ix": "252-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_23",
            "tgt_ix": "252-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_23",
            "tgt_ix": "252-ARR_v1_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_23",
            "tgt_ix": "252-ARR_v1_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_23",
            "tgt_ix": "252-ARR_v1_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_24",
            "tgt_ix": "252-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_24",
            "tgt_ix": "252-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_24",
            "tgt_ix": "252-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_25",
            "tgt_ix": "252-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_26",
            "tgt_ix": "252-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_26",
            "tgt_ix": "252-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_26",
            "tgt_ix": "252-ARR_v1_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_27",
            "tgt_ix": "252-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_27",
            "tgt_ix": "252-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_27",
            "tgt_ix": "252-ARR_v1_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_27",
            "tgt_ix": "252-ARR_v1_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_28",
            "tgt_ix": "252-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_29",
            "tgt_ix": "252-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_30",
            "tgt_ix": "252-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_31",
            "tgt_ix": "252-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_32",
            "tgt_ix": "252-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_32",
            "tgt_ix": "252-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_33",
            "tgt_ix": "252-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_34",
            "tgt_ix": "252-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_35",
            "tgt_ix": "252-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_36",
            "tgt_ix": "252-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_37",
            "tgt_ix": "252-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_38",
            "tgt_ix": "252-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_39",
            "tgt_ix": "252-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_39",
            "tgt_ix": "252-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_40",
            "tgt_ix": "252-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_40",
            "tgt_ix": "252-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_40",
            "tgt_ix": "252-ARR_v1_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_41",
            "tgt_ix": "252-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_42",
            "tgt_ix": "252-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_42",
            "tgt_ix": "252-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_43",
            "tgt_ix": "252-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_43",
            "tgt_ix": "252-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_43",
            "tgt_ix": "252-ARR_v1_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_43",
            "tgt_ix": "252-ARR_v1_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_43",
            "tgt_ix": "252-ARR_v1_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_44",
            "tgt_ix": "252-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_45",
            "tgt_ix": "252-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_46",
            "tgt_ix": "252-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_46",
            "tgt_ix": "252-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_46",
            "tgt_ix": "252-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_46",
            "tgt_ix": "252-ARR_v1_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_47",
            "tgt_ix": "252-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_48",
            "tgt_ix": "252-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_49",
            "tgt_ix": "252-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_50",
            "tgt_ix": "252-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_51",
            "tgt_ix": "252-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_52",
            "tgt_ix": "252-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_53",
            "tgt_ix": "252-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_54",
            "tgt_ix": "252-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_55",
            "tgt_ix": "252-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_56",
            "tgt_ix": "252-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_56",
            "tgt_ix": "252-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_57",
            "tgt_ix": "252-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_58",
            "tgt_ix": "252-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_58",
            "tgt_ix": "252-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_59",
            "tgt_ix": "252-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_59",
            "tgt_ix": "252-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_59",
            "tgt_ix": "252-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_59",
            "tgt_ix": "252-ARR_v1_59@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_60",
            "tgt_ix": "252-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_61",
            "tgt_ix": "252-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_61",
            "tgt_ix": "252-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_61",
            "tgt_ix": "252-ARR_v1_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_62",
            "tgt_ix": "252-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_62",
            "tgt_ix": "252-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_63",
            "tgt_ix": "252-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_63",
            "tgt_ix": "252-ARR_v1_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_64",
            "tgt_ix": "252-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_65",
            "tgt_ix": "252-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_65",
            "tgt_ix": "252-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_65",
            "tgt_ix": "252-ARR_v1_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_65",
            "tgt_ix": "252-ARR_v1_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_66",
            "tgt_ix": "252-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_67",
            "tgt_ix": "252-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_67",
            "tgt_ix": "252-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_68",
            "tgt_ix": "252-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_69",
            "tgt_ix": "252-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_70",
            "tgt_ix": "252-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_71",
            "tgt_ix": "252-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_72",
            "tgt_ix": "252-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_73",
            "tgt_ix": "252-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_74",
            "tgt_ix": "252-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_75",
            "tgt_ix": "252-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_75",
            "tgt_ix": "252-ARR_v1_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_75",
            "tgt_ix": "252-ARR_v1_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_76",
            "tgt_ix": "252-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_77",
            "tgt_ix": "252-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_78",
            "tgt_ix": "252-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_78",
            "tgt_ix": "252-ARR_v1_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_79",
            "tgt_ix": "252-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_79",
            "tgt_ix": "252-ARR_v1_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_79",
            "tgt_ix": "252-ARR_v1_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_80",
            "tgt_ix": "252-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_80",
            "tgt_ix": "252-ARR_v1_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_81",
            "tgt_ix": "252-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_81",
            "tgt_ix": "252-ARR_v1_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_82",
            "tgt_ix": "252-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_82",
            "tgt_ix": "252-ARR_v1_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_82",
            "tgt_ix": "252-ARR_v1_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_83",
            "tgt_ix": "252-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_83",
            "tgt_ix": "252-ARR_v1_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_83",
            "tgt_ix": "252-ARR_v1_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_84",
            "tgt_ix": "252-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_84",
            "tgt_ix": "252-ARR_v1_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_85",
            "tgt_ix": "252-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_85",
            "tgt_ix": "252-ARR_v1_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_85",
            "tgt_ix": "252-ARR_v1_85@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_85",
            "tgt_ix": "252-ARR_v1_85@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_86",
            "tgt_ix": "252-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_87",
            "tgt_ix": "252-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_87",
            "tgt_ix": "252-ARR_v1_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_87",
            "tgt_ix": "252-ARR_v1_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_88",
            "tgt_ix": "252-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_88",
            "tgt_ix": "252-ARR_v1_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_89",
            "tgt_ix": "252-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_89",
            "tgt_ix": "252-ARR_v1_89@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_90",
            "tgt_ix": "252-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_90",
            "tgt_ix": "252-ARR_v1_90@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_90",
            "tgt_ix": "252-ARR_v1_90@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_91",
            "tgt_ix": "252-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_91",
            "tgt_ix": "252-ARR_v1_91@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_91",
            "tgt_ix": "252-ARR_v1_91@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_92",
            "tgt_ix": "252-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_92",
            "tgt_ix": "252-ARR_v1_92@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_93",
            "tgt_ix": "252-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_94",
            "tgt_ix": "252-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_94",
            "tgt_ix": "252-ARR_v1_94@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_95",
            "tgt_ix": "252-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_95",
            "tgt_ix": "252-ARR_v1_95@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_95",
            "tgt_ix": "252-ARR_v1_95@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_95",
            "tgt_ix": "252-ARR_v1_95@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_96",
            "tgt_ix": "252-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_96",
            "tgt_ix": "252-ARR_v1_96@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_96",
            "tgt_ix": "252-ARR_v1_96@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_97",
            "tgt_ix": "252-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_97",
            "tgt_ix": "252-ARR_v1_97@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_97",
            "tgt_ix": "252-ARR_v1_97@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_97",
            "tgt_ix": "252-ARR_v1_97@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_97",
            "tgt_ix": "252-ARR_v1_97@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_97",
            "tgt_ix": "252-ARR_v1_97@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_98",
            "tgt_ix": "252-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_99",
            "tgt_ix": "252-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_99",
            "tgt_ix": "252-ARR_v1_99@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_99",
            "tgt_ix": "252-ARR_v1_99@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_99",
            "tgt_ix": "252-ARR_v1_99@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_99",
            "tgt_ix": "252-ARR_v1_99@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_99",
            "tgt_ix": "252-ARR_v1_99@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_99",
            "tgt_ix": "252-ARR_v1_99@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_100",
            "tgt_ix": "252-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_100",
            "tgt_ix": "252-ARR_v1_100@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_100",
            "tgt_ix": "252-ARR_v1_100@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_100",
            "tgt_ix": "252-ARR_v1_100@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_100",
            "tgt_ix": "252-ARR_v1_100@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_100",
            "tgt_ix": "252-ARR_v1_100@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_101",
            "tgt_ix": "252-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_101",
            "tgt_ix": "252-ARR_v1_101@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_101",
            "tgt_ix": "252-ARR_v1_101@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_102",
            "tgt_ix": "252-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_102",
            "tgt_ix": "252-ARR_v1_102@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_103",
            "tgt_ix": "252-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_104",
            "tgt_ix": "252-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_104",
            "tgt_ix": "252-ARR_v1_104@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_104",
            "tgt_ix": "252-ARR_v1_104@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_105",
            "tgt_ix": "252-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_105",
            "tgt_ix": "252-ARR_v1_105@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_105",
            "tgt_ix": "252-ARR_v1_105@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_105",
            "tgt_ix": "252-ARR_v1_105@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_105",
            "tgt_ix": "252-ARR_v1_105@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_105",
            "tgt_ix": "252-ARR_v1_105@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_106",
            "tgt_ix": "252-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_106",
            "tgt_ix": "252-ARR_v1_106@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_106",
            "tgt_ix": "252-ARR_v1_106@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_107",
            "tgt_ix": "252-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_107",
            "tgt_ix": "252-ARR_v1_107@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_107",
            "tgt_ix": "252-ARR_v1_107@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_107",
            "tgt_ix": "252-ARR_v1_107@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_107",
            "tgt_ix": "252-ARR_v1_107@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_107",
            "tgt_ix": "252-ARR_v1_107@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_107",
            "tgt_ix": "252-ARR_v1_107@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_107",
            "tgt_ix": "252-ARR_v1_107@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_108",
            "tgt_ix": "252-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_109",
            "tgt_ix": "252-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_109",
            "tgt_ix": "252-ARR_v1_109@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_109",
            "tgt_ix": "252-ARR_v1_109@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_110",
            "tgt_ix": "252-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_110",
            "tgt_ix": "252-ARR_v1_110@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_110",
            "tgt_ix": "252-ARR_v1_110@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_110",
            "tgt_ix": "252-ARR_v1_110@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_110",
            "tgt_ix": "252-ARR_v1_110@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_110",
            "tgt_ix": "252-ARR_v1_110@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_110",
            "tgt_ix": "252-ARR_v1_110@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_111",
            "tgt_ix": "252-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_111",
            "tgt_ix": "252-ARR_v1_111@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_112",
            "tgt_ix": "252-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_112",
            "tgt_ix": "252-ARR_v1_112@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_112",
            "tgt_ix": "252-ARR_v1_112@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_113",
            "tgt_ix": "252-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_113",
            "tgt_ix": "252-ARR_v1_113@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_114",
            "tgt_ix": "252-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_115",
            "tgt_ix": "252-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_116",
            "tgt_ix": "252-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_116",
            "tgt_ix": "252-ARR_v1_116@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_116",
            "tgt_ix": "252-ARR_v1_116@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_116",
            "tgt_ix": "252-ARR_v1_116@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_116",
            "tgt_ix": "252-ARR_v1_116@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_117",
            "tgt_ix": "252-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_118",
            "tgt_ix": "252-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_118",
            "tgt_ix": "252-ARR_v1_118@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_119",
            "tgt_ix": "252-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_120",
            "tgt_ix": "252-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_120",
            "tgt_ix": "252-ARR_v1_120@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_121",
            "tgt_ix": "252-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_122",
            "tgt_ix": "252-ARR_v1_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_122",
            "tgt_ix": "252-ARR_v1_122@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_122",
            "tgt_ix": "252-ARR_v1_122@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_123",
            "tgt_ix": "252-ARR_v1_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_123",
            "tgt_ix": "252-ARR_v1_123@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_123",
            "tgt_ix": "252-ARR_v1_123@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_123",
            "tgt_ix": "252-ARR_v1_123@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_124",
            "tgt_ix": "252-ARR_v1_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_124",
            "tgt_ix": "252-ARR_v1_124@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_124",
            "tgt_ix": "252-ARR_v1_124@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_124",
            "tgt_ix": "252-ARR_v1_124@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_124",
            "tgt_ix": "252-ARR_v1_124@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_124",
            "tgt_ix": "252-ARR_v1_124@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_125",
            "tgt_ix": "252-ARR_v1_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_126",
            "tgt_ix": "252-ARR_v1_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_126",
            "tgt_ix": "252-ARR_v1_126@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_127",
            "tgt_ix": "252-ARR_v1_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_127",
            "tgt_ix": "252-ARR_v1_127@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_127",
            "tgt_ix": "252-ARR_v1_127@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_127",
            "tgt_ix": "252-ARR_v1_127@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_127",
            "tgt_ix": "252-ARR_v1_127@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_127",
            "tgt_ix": "252-ARR_v1_127@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_128",
            "tgt_ix": "252-ARR_v1_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_128",
            "tgt_ix": "252-ARR_v1_128@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_128",
            "tgt_ix": "252-ARR_v1_128@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_128",
            "tgt_ix": "252-ARR_v1_128@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_128",
            "tgt_ix": "252-ARR_v1_128@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_129",
            "tgt_ix": "252-ARR_v1_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_129",
            "tgt_ix": "252-ARR_v1_129@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_130",
            "tgt_ix": "252-ARR_v1_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_130",
            "tgt_ix": "252-ARR_v1_130@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_130",
            "tgt_ix": "252-ARR_v1_130@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_130",
            "tgt_ix": "252-ARR_v1_130@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_131",
            "tgt_ix": "252-ARR_v1_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_131",
            "tgt_ix": "252-ARR_v1_131@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_131",
            "tgt_ix": "252-ARR_v1_131@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_132",
            "tgt_ix": "252-ARR_v1_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_132",
            "tgt_ix": "252-ARR_v1_132@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_132",
            "tgt_ix": "252-ARR_v1_132@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_132",
            "tgt_ix": "252-ARR_v1_132@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_132",
            "tgt_ix": "252-ARR_v1_132@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_132",
            "tgt_ix": "252-ARR_v1_132@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_133",
            "tgt_ix": "252-ARR_v1_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_133",
            "tgt_ix": "252-ARR_v1_133@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_134",
            "tgt_ix": "252-ARR_v1_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_134",
            "tgt_ix": "252-ARR_v1_134@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_134",
            "tgt_ix": "252-ARR_v1_134@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_135",
            "tgt_ix": "252-ARR_v1_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_135",
            "tgt_ix": "252-ARR_v1_135@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_136",
            "tgt_ix": "252-ARR_v1_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_136",
            "tgt_ix": "252-ARR_v1_136@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_137",
            "tgt_ix": "252-ARR_v1_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_137",
            "tgt_ix": "252-ARR_v1_137@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_138",
            "tgt_ix": "252-ARR_v1_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_138",
            "tgt_ix": "252-ARR_v1_138@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_138",
            "tgt_ix": "252-ARR_v1_138@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_138",
            "tgt_ix": "252-ARR_v1_138@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_138",
            "tgt_ix": "252-ARR_v1_138@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_139",
            "tgt_ix": "252-ARR_v1_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_139",
            "tgt_ix": "252-ARR_v1_139@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_139",
            "tgt_ix": "252-ARR_v1_139@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_139",
            "tgt_ix": "252-ARR_v1_139@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_140",
            "tgt_ix": "252-ARR_v1_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_140",
            "tgt_ix": "252-ARR_v1_140@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_141",
            "tgt_ix": "252-ARR_v1_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_142",
            "tgt_ix": "252-ARR_v1_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_143",
            "tgt_ix": "252-ARR_v1_143@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_144",
            "tgt_ix": "252-ARR_v1_144@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_145",
            "tgt_ix": "252-ARR_v1_145@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_146",
            "tgt_ix": "252-ARR_v1_146@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_147",
            "tgt_ix": "252-ARR_v1_147@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_148",
            "tgt_ix": "252-ARR_v1_148@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_149",
            "tgt_ix": "252-ARR_v1_149@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_150",
            "tgt_ix": "252-ARR_v1_150@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_151",
            "tgt_ix": "252-ARR_v1_151@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_152",
            "tgt_ix": "252-ARR_v1_152@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_153",
            "tgt_ix": "252-ARR_v1_153@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_154",
            "tgt_ix": "252-ARR_v1_154@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_155",
            "tgt_ix": "252-ARR_v1_155@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_156",
            "tgt_ix": "252-ARR_v1_156@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_157",
            "tgt_ix": "252-ARR_v1_157@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_158",
            "tgt_ix": "252-ARR_v1_158@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_159",
            "tgt_ix": "252-ARR_v1_159@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_160",
            "tgt_ix": "252-ARR_v1_160@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_161",
            "tgt_ix": "252-ARR_v1_161@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_162",
            "tgt_ix": "252-ARR_v1_162@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_163",
            "tgt_ix": "252-ARR_v1_163@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_164",
            "tgt_ix": "252-ARR_v1_164@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_165",
            "tgt_ix": "252-ARR_v1_165@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_166",
            "tgt_ix": "252-ARR_v1_166@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_167",
            "tgt_ix": "252-ARR_v1_167@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_168",
            "tgt_ix": "252-ARR_v1_168@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_169",
            "tgt_ix": "252-ARR_v1_169@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_170",
            "tgt_ix": "252-ARR_v1_170@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_171",
            "tgt_ix": "252-ARR_v1_171@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_172",
            "tgt_ix": "252-ARR_v1_172@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_173",
            "tgt_ix": "252-ARR_v1_173@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_174",
            "tgt_ix": "252-ARR_v1_174@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_175",
            "tgt_ix": "252-ARR_v1_175@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "252-ARR_v1_176",
            "tgt_ix": "252-ARR_v1_176@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1850,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "252-ARR",
        "version": 1
    }
}