{
    "nodes": [
        {
            "ix": "412-ARR_v2_0",
            "content": "Domain Adaptation in Multilingual and Multi-Domain Monolingual Settings for Complex Word Identification",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_2",
            "content": "Complex word identification (CWI) is a cornerstone process towards proper text simplification. CWI is highly dependent on context, whereas its difficulty is augmented by the scarcity of available datasets which vary greatly in terms of domains and languages. As such, it becomes increasingly more difficult to develop a robust model that generalizes across a wide array of input examples. In this paper, we propose a novel training technique for the CWI task based on domain adaptation to improve the target character and context representations. This technique addresses the problem of working with multiple domains, inasmuch as it creates a way of smoothing the differences between the explored datasets. Moreover, we also propose a similar auxiliary task, namely text simplification, that can be used to complement lexical complexity prediction. Our model obtains a boost of up to 2.42% in terms of Pearson Correlation Coefficients in contrast to vanilla training techniques, when considering the CompLex from the Lexical Complexity Prediction 2021 dataset. At the same time, we obtain an increase of 3% in Pearson scores, while considering a cross-lingual setup relying on the Complex Word Identification 2018 dataset. In addition, our model yields state-ofthe-art results in terms of Mean Absolute Error.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "412-ARR_v2_4",
            "content": "The overarching goal of the complex word identification (CWI) task is to find words that can be simplified in a given text (Paetzold and Specia, 2016b). Evaluating word difficulty represents one step towards achieving simplified, which in return facilitates access to knowledge to a wider audience texts (Maddela and Xu, 2018). However, complex word identification is a highly contextualized task, far from being trivial. The datasets are scarce and, most of the time, the input entries are limited or cover different domains/areas of expertise. There-",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_5",
            "content": "CompLex LCP Dataset Bible But let each man test his own work , and then he will take pride in himself and not in his neighbor.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_6",
            "content": "A genome database search revealed orthologs of ADAM11, ADAM22 and ADAM23 genes to exist in vertebrates such as mammals, fish, and amphibians, but not in invertebrates.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_7",
            "content": "They also allow for easy compensation for the thousands of accidents involving vehicles from more than one Member State.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_8",
            "content": "English CWI Dataset Wikipedia Normally , the land will be passed down to future generations in a way that recognizes the community's traditional connection to that country .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_9",
            "content": "The JAS 39C Gripen crashed onto a runway at around 9:30 am local time (02:30 UTC) and exploded , closing the airport to commercial flights.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_10",
            "content": "The car has been removed from the scene for forensic technical examination .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_11",
            "content": "Table 1: Examples of complex words annotated for each of the domains from CompLex LCP and CWI datasets. The shades indicate the complexity; the darker the shade, the more complex the sequence of words. Best viewed in color.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_12",
            "content": "fore, developing a robust and reliable model that can be used to properly evaluate the complexity of tokens is a challenging task. Table 1 showcases examples of complex words annotations from the CompLex LCP (Shardlow et al., 2020(Shardlow et al., , 2021b and English CWI (Yimam et al., 2018) datasets employed in this work. Nevertheless, certain training techniques and auxiliary tasks help the model improve its generalization abilities, forcing it to focus only on the most relevant, general features (Schrom et al., 2021). Techniques like domain adaptation (Ganin et al., 2016) can be used for various tasks, with the purpose of selecting relevant features for follow-up processes. At the same time, the cross-domain scenario can be transposed to a cross-lingual setup, where the input entries are part of multiple available languages. Performance can be improved by also employing the power of domain adaptation, where the domain is the language; as such, the task of identifying complex tokens can be approached even for low resource languages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_13",
            "content": "We propose several solutions to improve the performance of a model for CWI in a cross-domain or a cross-lingual setting, by adding auxiliary components (i.e., Transformer (Vaswani et al., 2017) decoders, Variational Auto Encoders -VAEs (Kingma and Welling, 2014)), as well as a domain adaptation training technique (Farahani et al., 2021). Moreover, we use the domain adaptation intuition and we apply it in a multi-task adversarial training scenario, where the main task is trained alongside an auxiliary one, and a task discriminator has the purpose of generalizing task-specific features.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_14",
            "content": "We summarize our main contributions as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_15",
            "content": "\u2022 Applying the concept of domain adaptation in a monolingual, cross-domain scenario for complex word identification; \u2022 Introducing the domain adaptation technique in a cross-lingual setup, where the discriminator has the purpose to support the model extract only the most relevant features across all languages; \u2022 Proposing additional components (i.e., Transformer decoders and Variational Auto Encoders) trained alongside the main CWI task to provide more meaningful representations of the inputs and to ensure robustness, while generating new representations or by tuning the existing ones; \u2022 Experimenting with an additional text simplification task alongside domain/language adaptation, with the purpose of extracting cross-task features and improving performance.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_16",
            "content": "2 Related Work Domain Adaptation. Several works employed domain adaptation to improve performance. For example, Du et al. (2020) approached the sentiment analysis task by using a BERT-based (Devlin et al., 2019) (Klimaszewski and Andruszkiewicz, 2019), mixup synthesis training (Tang et al., 2020), and effective regularization (Vernikos et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_17",
            "content": "Cross-Lingual Domain Adaptation. Chen et al. (2018) proposed ADAN, an architecture based on a feed-forward neural network with three main components, namely: a feature extractor, a sentiment classifier, and a language discriminator. The latter had the purpose of supporting the adversarial training setup, thus covering the scenario where the model was unable to detect whether the input language was from the source dataset or the target one. A similar cross-lingual approach was adopted by Zhang et al. (2020), who developed a system to classify entries from the target language, while only labels from the source language were provided. Keung et al. (2019) employed the usage of multilingual BERT (Pires et al., 2019) and argued that a language-adversarial task can improve the performance of zero-resource cross-lingual transfers. Moreover, training under an adversarial technique helps the Transformer model align the representations of the English inputs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_18",
            "content": "Under a Named Entity Recognition training scenario, Kim et al. (2017) used features on two levels (i.e., word and characters), together with Recurrent Neural Networks and a language discriminator used for the domain-adversarial setup. Similarly, Huang et al. (2019) used target language discriminators during the process of training models for low-resource name tagging.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_19",
            "content": "Word Complexity Prediction. Gooding and Kochmar (2019) based their implementation for CWI as a sequence labeling task on Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks, inasmuch as the context helps towards proper identification of complex tokens. The authors used 300-dimensional pretrained word embeddings as inputs for the LSTMs. Also adopting a sequence labeling approach, Finnimore et al. (2019) considered handcrafted features, including punctuation or syllables, that can properly identify complex structures.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_20",
            "content": "The same sequence labeling approach can be applied under a plurality voting technique (Polikar, 2006), or even using an Oracle (Kuncheva et al., 2001). The Oracle functions best when applied to multiple solutions, by jointly using them to obtain a final prediction. At the same time, Zaharia et al. (2020) explored the power of Transformer-based models (Vaswani et al., 2017) in cross-lingual environments by using different training scenarios, depending on the scarcity of the resources: zero-shot, one-shot, as well as few-shot learning. Moreover, CWI can be also approached as a probabilistic task. For example, De Hertog and Tack (2018) introduced a series of architectures that combine deep learning features, as well as handcrafted features to address CWI as a regression problem.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_21",
            "content": "Method",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "412-ARR_v2_22",
            "content": "Datasets",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "412-ARR_v2_23",
            "content": "We experimented with two datasets, one monolingual -CompLex LCP 2021 (Shardlow et al., 2020(Shardlow et al., , 2021b) -and one cross-lingual -the CWI Shared Dataset (Yimam et al., 2018). The entries of Com-pLex consist of a sentence in English and a target token, alongside the complexity of the token, given its context. The complexities are continuous values between 0 and 1, annotated by various individuals on an initial 5-point Likert scale; the annotations were then normalized.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_24",
            "content": "The CompLex dataset contains two types of entries, each with its corresponding subset of entries: a) single, where the target token is represented by a single word, and b) multiple, where the target token is represented by a group of words. While the single-word dataset contains 7,662 training entries, 421 trial entries, and 917 test entries, the multiword dataset has lower counts, with 1,517 training entries, 99 trial entries, and 184 for testing. At the same time, the entries correspond to three different domains (i.e., biblical, biomedical, and political), therefore displaying different characteristics and challenging the models towards generalization.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_25",
            "content": "The CWI dataset was introduced in the CWI Shared Task 2018 (Yimam et al., 2018). It is a multilingual dataset, containing entries in English, German, Spanish, and French. Moreover, the English entries are split into three categories, depending on their proficiency levels: professional (News), non-professional (WikiNews), and Wikipedia articles. Most entries are for the English language (27,299 training and 3,328 validation), while the fewest training entries are for German (6,151 training and 795 validation). The French language does not contain training or validation entries.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_26",
            "content": "The Domain Adaption Model",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "412-ARR_v2_27",
            "content": "The overarching architecture of our method is introduced in Figure 1. All underlying components are presented in detail in the following subsections. Our model combines character-level BiLSTM features (i.e., F t ) with Transformer-based features for the context sentence (i.e., F c ). The concatenated features (F c +F t ) are then passed through three linear layers, with a dropout separating the first and second. The output is a value representing the complexity of the target word.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_28",
            "content": "Three configurations were experimented. Within Basic Domain Adaptation, the previous features are passed through an additional component, the domain discriminator, composed of a linear layer followed by a softmax activation function. A gradient reversal layer (Ganin and Lempitsky, 2015) is added between the feature concatenation and the discriminator to reverse the gradients through the backpropagation phase and support extracting general features. The loss function is determined by Equation 1 as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_29",
            "content": "L = L r \u2212 \u03b2\u03bbL d (1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_30",
            "content": "where L r is the regression loss, L d is the general domain loss, \u03b2 is a hyperparameter used for controlling the importance of L d , and \u03bb is another hyperparameter that varies as the training process progresses.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_31",
            "content": "The following setups also include the Basic Domain Adaptation training setting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_32",
            "content": "VAE and Domain Adaptation considers the previous configuration, plus the VAE encoder, that yields the F v features, and the VAE decoder, which aims to reconstruct the input. The concatenation layer now contains the BiLSTM and Transformer features, along with the VAE encoder features (F v ), namely F t +F c +F v . The loss function is depicted by Equation 2 as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_33",
            "content": "L = L r \u2212 \u03b2\u03bbL d + \u03b1L v (2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_34",
            "content": "where, additionally, L v represents the VAE loss described in Equation 6.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_35",
            "content": "Transformer Decoder and Domain Adaptation adds a Transformer Decoder with the purpose of reconstructing the original input, for a more robust context feature extraction. The loss is denoted by Equation 3 as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_36",
            "content": "L = L r \u2212 \u03b2\u03bbL d + \u03b1L dec (3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_37",
            "content": "where L dec represents the decoder loss described in Equation 9.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_38",
            "content": "Character-level BiLSTM for Target Word Representation",
            "ntype": "title",
            "meta": {
                "section": "3.2.1"
            }
        },
        {
            "ix": "412-ARR_v2_39",
            "content": "The purpose of this component is to determine the complexity of the target token, given only its constituent characters. A character-level Bidirectional Long Short-Term Memory (BiLSTM) network receives as input an array of characters corresponding to the target word (or group of words), and yields a representation that is afterwards concatenated to the previously mentioned Transformer-based representations. Each character c is mapped to a certain value obtained from the character vocabulary V, containing all the characters present in the input dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_40",
            "content": "The character sequence is represented as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_41",
            "content": "C i = [c 1 , c 2 , . . . , c n ],",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_42",
            "content": "where n is the maximum length of a target token. C i is then passed through a character embedding layer, thus yielding the output Emb target . Emb target is then fed to the BiLSTM, followed by a dropout layer, thus obtaining the final target word representation, F t .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_43",
            "content": "Transformer-based Context Representation",
            "ntype": "title",
            "meta": {
                "section": "3.2.2"
            }
        },
        {
            "ix": "412-ARR_v2_44",
            "content": "We rely on a Transformer-based model as the main feature extractor for the context of the target word (i.e., the full sentence), considering their superior performance on most natural language processing tasks. The selected model for the first dataset is RoBERTa (Liu et al., 2019), as it yields better results when compared to its counterpart, BERT. RoBERTa is trained with higher learning rates and larger mini-batches, and it modifies the key hyper-parameters of BERT. We employed the usage of XLM-RoBERTa (Conneau et al., 2020), the multilingual counterpart of RoBERTa, now trained on a very large corpus of multilingual texts, for the second cross-lingual task. The features used for our task are represented by the pooled output of the Transformer model. The feature vector F c of 768 elements captures information about the context of the target word.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_45",
            "content": "Variational AutoEncoders",
            "ntype": "title",
            "meta": {
                "section": "3.2.3"
            }
        },
        {
            "ix": "412-ARR_v2_46",
            "content": "We aim to further improve performance by adding extra features via Variational AutoEncoders (VAEs) (Kingma and Welling, 2014) to the context representation for a target word. More specifically for the CWI task, we use the latent vector z, alongside the Transformer and the Char BiLSTM features. Moreover, we also need to ensure that the Encoder representation is accurate; therefore, we consider the VAE encoding and decoding as an additional task having the purpose of minimizing the reconstruction loss.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_47",
            "content": "The VAE consists of two parts, namely the encoder and the decoder. The encoder g(x) produces the approximation q(z|x) of the posterior distribution p(z|x), thus mapping the input x to the latent space z. The process is presented in Equation 4. We use as features the representation z, denoted as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_48",
            "content": "F v . p(z|x) \u2248 q(z|x) = N (\u00b5(x), \u03c3(x))(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_49",
            "content": "The decoder f(z) maps the latent space to the input space (i.e., p(z) to p(x)), by using Equation 5. Furthermore, E q represents the expectation with relation to the distribution q.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_50",
            "content": "L(f, g) = i {\u2212D KL [q(z|x i )||p(z)] + E q(z|x i ) [ln p(x i |z)]} (6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_51",
            "content": "Discriminators",
            "ntype": "title",
            "meta": {
                "section": "3.2.4"
            }
        },
        {
            "ix": "412-ARR_v2_52",
            "content": "The features extracted by our architecture can vary greatly as the input entries can originate from different domains or languages. Consequently, we introduced a generalization technique to extract only cross-domain features that do not present a bias towards a certain domain. We thus employ an adversarial training technique based on domain adaptation, forcing the model to only extract relevant cross-domain features.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_53",
            "content": "A discriminator acts as a classifier, containing three linear layers with corresponding activation functions. The discriminator classifies the input sentence into one of the available domains. Unlike traditional classification approaches, our purpose is not to minimize the loss, but to maximize it. We want our model to become incapable of distinguishing between different categories of input entries, therefore extracting the most relevant, cross-domain features.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_54",
            "content": "Our architecture is encouraged to generalize in terms of extracted features by the gradient reversal layer that reverses the gradients during the backpropagation phase; as such, the parameters are updated towards the direction that maximizes the loss instead of minimizing it.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_55",
            "content": "Three scenarios were considered, each one targeting a different approach towards domain adaptation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_56",
            "content": "Domain Discriminator. The first scenario is applied on the first dataset, CompLex, with entries only in English, but covering multiple domains. The discriminator has the purpose of identifying the domain of the entry, namely biblical, biomedical or political. The intuition is that, by grasping only cross-domain features, the performance of the model increases on all three domains, instead of performing well only on one, while poorer on the others.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_57",
            "content": "Language Discriminator. The intuition is similar to the previous scenario, except that we experimented with the second multilingual dataset. Therefore, our interest was that our model extracts cross-lingual features, such that the performance is equal on all the target languages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_58",
            "content": "Task Discriminator. In this scenario, we trained a similar, auxiliary task, represented by text simplification. A task discriminator is implemented to detect the origin of the input entry: either the main task or the auxiliary task (i.e., simplified version). The dataset used for text simplification is represented by BenchLS (Paetzold and Specia, 2016a) 1 . The employed simplification process consists of masking the word considered to be complex and then using a Transformer for Masked Language Modeling to predict the best candidate. The corresponding flow is described in Algorithm 1, while the loss function is presented in Equation 7:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_59",
            "content": "L = L r \u2212 \u03b2\u03bbL task_id + L M L (7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_60",
            "content": "where L ML is the Sparse Categorical Cross Entropy loss.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_61",
            "content": "All previous discriminators use the same loss, namely Categorical Cross Entropy (Zhang and Sabuncu, 2018).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_62",
            "content": "The overall loss consists of the difference between the task loss and the domain/language loss. Moreover, the importance of the latter can be controlled by multiplication with a \u03bb hyperparameter, that changes over time, and a fixed \u03b2 hyperparameter. The network parameters, \u03b8 p are updated according to Equation 8, where \u03b7 is the learning rate, L d is the domain loss, L r is the task loss and \u03b2 is the weight for the domain loss. A similar equation for language loss (L l ) is in place for the second dataset, where instead of the domain loss L d we used the language identification loss L l , having the same formula.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_63",
            "content": "\u03b8 p = \u03b8 p \u2212 \u03b7( \u2202L r \u2202\u03b8 p \u2212 \u03b2\u03bb \u2202L d \u2202\u03b8 p ) (8)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_64",
            "content": "Transformer Decoder",
            "ntype": "title",
            "meta": {
                "section": "3.2.5"
            }
        },
        {
            "ix": "412-ARR_v2_65",
            "content": "Our model also considers a decoder to reconstruct the original input, starting from the Transformer representation. The intuition behind introducing The decoder receives as input the outputs of the hidden Transformer layer alongside an embedding of the original input, which are passed through a Gated Recurrent Unit (GRU) (Chung et al., 2014) layer for obtaining the final representation of the initial input. Additionally, two linear layers separated by a dropout are introduced before obtaining the final representation, y = F d . The loss is computed by using the Negative Log Likelihood loss between the outputs of the decoder and the original Transformer input id representation of the entries (see Equations 9 and 10).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_66",
            "content": "L(x, y) = N n=1 l n (9) l n = \u2212w yn x n,yn , w c = weight[c] \u2022 1{c = ignore_index} (10)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_67",
            "content": "Experimental Setup",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "412-ARR_v2_68",
            "content": "The optimizer used for our models is represented by AdamW (Kingma and Ba, 2014). The learning rate is set to 2e-5, while the loss functions used for the complexity task are the L1 loss (Janocha and Czarnecki, 2016) for the CompLex LCP dataset and the Mean Squared Error (MSE) loss (Kline and Berardi, 2005) for the CWI dataset. The auxiliary losses are summed to the main loss (i.e., complexity prediction) and are scaled according to their priority, with a factor of \u03b1, where \u03b1 is set to 0.1 for the VAE loss, and 0.01 for the Transformer decoder and task discriminator losses. The \u03bb parameter used for domain adaptation was updated according to Equation 11:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_69",
            "content": "\u03bb = 2 1 + e \u2212\u03b3 \u2212 1 (11)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_70",
            "content": "where is the number of epochs the model was trained; \u03b3 was set to 0.1, while \u03b2 was set to 0.2. Moreover, each model was trained for 8 epochs, except for the one including the VAE features, which was trained for 12 epochs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_71",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "412-ARR_v2_72",
            "content": "LCP 2021 CompLex Dataset",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "412-ARR_v2_73",
            "content": "We consider as baselines two models used for the LCP 2021 competition (Shardlow et al., 2021a), as well as the best-registered score. Almeida et al. (2021) employed the usage of neural network solutions; more specifically, they used chunks of the sentences obtained with Sent2Vec as input features. Zaharia et al. (2021) created models that are based on target and context feature extractors, alongside features resulted from Graph Convolutional Networks, Capsule Networks, and pre-trained word embeddings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_74",
            "content": "Table 2 depicts the results obtained for the English dataset using domain adaptation and various configurations. \"Base\" denotes the initial model (RoBERTa + Char BiLSTM) on which we apply domain adaptation, as well as the auxiliary tasks. The domain adaptation technique offers improved performance when applied on top of an architecture, considering that the model learns cross-domain features. The only exception is represented by a slightly lower Pearson score on the model that uses domain adaptation alongside the Transformer decoding auxiliary task (Base + Decoder + DA), with a value of .7969 on the trial dataset, when compared",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_75",
            "content": "CWI 2018 Dataset",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "412-ARR_v2_76",
            "content": "We also experimented with a multilingual dataset, where the discriminant is considered to be the language. The baseline consists of three models used from the CWI 2018 competition. The performance is evaluated in terms of MAE; however, we also report the Pearson Correlation Coefficient. First, Kajiwara and Komachi (2018) based their models on regressors, alongside features represented by the number of characters or words and the frequency of the target word in certain corpora. Second, the approach of Bingel and Bjerva ( 2018) is based on Random Forest Regressors, as well as feed-forward neural networks alongside specific features, such as log-probability, inflectional complexity, or target-sentence similarity; the authors focused on non-English entries. Third, Gooding and Kochmar (2018) approach the English section of the dataset by employing linear regressions. The authors used several types of handcrafted features, including word n-grams, POS tags, dependency parse relations, and psycholinguistic features.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_77",
            "content": "Table 3 presents the results obtained on the multilingual validation dataset and compares the performance of different configurations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_78",
            "content": "Discussions",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "412-ARR_v2_79",
            "content": "The domain adaptation technique supports our model to learn general cross-domain or crosslanguage features, while achieving higher performance. Moreover, jointly training on two different tasks (i.e., lexical complexity prediction and text simplification), coupled with domain adaptation to generalize the features from the two tasks, can lead to improved results. However, there are entries for which our models were unable to properly predict the complexity score, namely: a) entries with a different level of complexity (i.e. biomedical), and b) entries part of a language that was not present in the training dataset (i.e., French). For the former, scientific terms (e.g., \"sitosterolemia\"), abbreviations (e.g., \"ES\"), or complex elements (e.g., \"H3-2meK9\") impose a series of difficulties for our feature extractors, considering the absence of these tokens from the Transformer vocabulary. The latter category of problematic entries creates new challenges in the sense that it represents a completely new language on which the architecture is tested. However, as seen in the results section, the cross-lingual domain adaptation technique offers good improvements, helping the model achieve better performance on French, even though the initial architecture was not exposed to any French example.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_80",
            "content": "Conclusions and Future Work",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "412-ARR_v2_81",
            "content": "This work proposes a series of training techniques, including domain adaptation, as well as multi-task adversarial learning, that can be used for improving the overall performance of the models for CWI. Domain adaptation improves results by encouraging the models to extract more general features, that can be further used for the lexical complexity prediction task. Moreover, by jointly training the model on the CWI tasks and an auxiliary similar task (i.e., text simplification), the overall performance is improved. The task discriminator also ensures the extraction of general features, thus making the model more robust on the CWI dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_82",
            "content": "For future work, we intend to experiment with meta-learning (Finn et al., 2017) alongside domain adaptation (Wang et al., 2019), considering the scope of the previously applied training techniques. This would enable us to initialize the model's weights in the best manner, thus ensuring optimal results during the training phase.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "412-ARR_v2_83",
            "content": "Raul Almeida, Hegler Tissot, Marcos Didonet Del Fabro, C3sl at semeval-2021 task 1: Predicting lexical complexity of words in specific contexts with sentence embeddings, 2021, Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Raul Almeida",
                    "Hegler Tissot",
                    "Marcos Didonet Del Fabro"
                ],
                "title": "C3sl at semeval-2021 task 1: Predicting lexical complexity of words in specific contexts with sentence embeddings",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_84",
            "content": "Joachim Bingel, Johannes Bjerva, Crosslingual complex word identification with multitask learning, 2018, Proceedings of the thirteenth workshop on innovative use of NLP for building educational applications, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Joachim Bingel",
                    "Johannes Bjerva"
                ],
                "title": "Crosslingual complex word identification with multitask learning",
                "pub_date": "2018",
                "pub_title": "Proceedings of the thirteenth workshop on innovative use of NLP for building educational applications",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_85",
            "content": "Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie, Kilian Weinberger, Adversarial deep averaging networks for cross-lingual sentiment classification, 2018, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Xilun Chen",
                    "Yu Sun",
                    "Ben Athiwaratkun",
                    "Claire Cardie",
                    "Kilian Weinberger"
                ],
                "title": "Adversarial deep averaging networks for cross-lingual sentiment classification",
                "pub_date": "2018",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_86",
            "content": "Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio, Empirical evaluation of gated recurrent neural networks on sequence modeling, 2014, NIPS 2014 Deep Learning and Representation Learning Workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Junyoung Chung",
                    "Caglar Gulcehre",
                    "Kyunghyun Cho",
                    "Yoshua Bengio"
                ],
                "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
                "pub_date": "2014",
                "pub_title": "NIPS 2014 Deep Learning and Representation Learning Workshop",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_87",
            "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, \u00c9douard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Unsupervised cross-lingual representation learning at scale, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Alexis Conneau",
                    "Kartikay Khandelwal",
                    "Naman Goyal",
                    "Vishrav Chaudhary",
                    "Guillaume Wenzek",
                    "Francisco Guzm\u00e1n",
                    "\u00c9douard Grave",
                    "Myle Ott",
                    "Luke Zettlemoyer",
                    "Veselin Stoyanov"
                ],
                "title": "Unsupervised cross-lingual representation learning at scale",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_88",
            "content": "Erenay Dayanik, Sebastian Pad\u00f3, Masking actor information leads to fairer political claims detection, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Erenay Dayanik",
                    "Sebastian Pad\u00f3"
                ],
                "title": "Masking actor information leads to fairer political claims detection",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_89",
            "content": "Dirk De Hertog, Ana\u00efs Tack, Deep learning architecture for complex word identification, 2018, Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Dirk De Hertog",
                    "Ana\u00efs Tack"
                ],
                "title": "Deep learning architecture for complex word identification",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_90",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_91",
            "content": "Chunning Du, Haifeng Sun, Jingyu Wang, Qi Qi, Jianxin Liao, Adversarial and domain-aware bert for cross-domain sentiment analysis, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Chunning Du",
                    "Haifeng Sun",
                    "Jingyu Wang",
                    "Qi Qi",
                    "Jianxin Liao"
                ],
                "title": "Adversarial and domain-aware bert for cross-domain sentiment analysis",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_92",
            "content": "Abolfazl Farahani, Sahar Voghoei, Khaled Rasheed, Hamid Arabnia, A brief review of domain adaptation, 2021, Advances in Data Science and Information Engineering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Abolfazl Farahani",
                    "Sahar Voghoei",
                    "Khaled Rasheed",
                    "Hamid Arabnia"
                ],
                "title": "A brief review of domain adaptation",
                "pub_date": "2021",
                "pub_title": "Advances in Data Science and Information Engineering",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_93",
            "content": "Chelsea Finn, Pieter Abbeel, Sergey Levine, Model-agnostic meta-learning for fast adaptation of deep networks, 2017, Proceedings of the 34th International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Chelsea Finn",
                    "Pieter Abbeel",
                    "Sergey Levine"
                ],
                "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 34th International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_94",
            "content": "Pierre Finnimore, Elisabeth Fritzsch, Daniel King, Alison Sneyd, Aneeq Ur Rehman, Fernando Alva-Manchego, Andreas Vlachos, Strong baselines for complex word identification across multiple languages, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Pierre Finnimore",
                    "Elisabeth Fritzsch",
                    "Daniel King",
                    "Alison Sneyd",
                    "Aneeq Ur Rehman",
                    "Fernando Alva-Manchego",
                    "Andreas Vlachos"
                ],
                "title": "Strong baselines for complex word identification across multiple languages",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_95",
            "content": "Yaroslav Ganin, Victor Lempitsky, Unsupervised domain adaptation by backpropagation, 2015, International conference on machine learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Yaroslav Ganin",
                    "Victor Lempitsky"
                ],
                "title": "Unsupervised domain adaptation by backpropagation",
                "pub_date": "2015",
                "pub_title": "International conference on machine learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "412-ARR_v2_96",
            "content": "UNKNOWN, None, 2016, Domain-adversarial training of neural networks. The journal of machine learning research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Domain-adversarial training of neural networks. The journal of machine learning research",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_97",
            "content": "Sian Gooding, Ekaterina Kochmar, Camb at cwi shared task 2018: Complex word identification with ensemble-based voting, 2018, Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Sian Gooding",
                    "Ekaterina Kochmar"
                ],
                "title": "Camb at cwi shared task 2018: Complex word identification with ensemble-based voting",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_98",
            "content": "Sian Gooding, Ekaterina Kochmar, Complex word identification as a sequence labelling task, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Sian Gooding",
                    "Ekaterina Kochmar"
                ],
                "title": "Complex word identification as a sequence labelling task",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_99",
            "content": "Sepp Hochreiter, J\u00fcrgen Schmidhuber, Long short-term memory, 1997, Neural computation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Sepp Hochreiter",
                    "J\u00fcrgen Schmidhuber"
                ],
                "title": "Long short-term memory",
                "pub_date": "1997",
                "pub_title": "Neural computation",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_100",
            "content": "Lifu Huang, Ji Heng, Jonathan , Crosslingual multi-level adversarial transfer to enhance low-resource name tagging, 2019-05, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Lifu Huang",
                    "Ji Heng",
                    "Jonathan "
                ],
                "title": "Crosslingual multi-level adversarial transfer to enhance low-resource name tagging",
                "pub_date": "2019-05",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_101",
            "content": "Katarzyna Janocha, Wojciech Czarnecki, On loss functions for deep neural networks in classification, 2016, Schedae Informaticae, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Katarzyna Janocha",
                    "Wojciech Czarnecki"
                ],
                "title": "On loss functions for deep neural networks in classification",
                "pub_date": "2016",
                "pub_title": "Schedae Informaticae",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_102",
            "content": "Tomoyuki Kajiwara, Mamoru Komachi, Complex word identification based on frequency in a learner corpus, 2018, Proceedings of the thirteenth workshop on innovative use of NLP for building educational applications, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Tomoyuki Kajiwara",
                    "Mamoru Komachi"
                ],
                "title": "Complex word identification based on frequency in a learner corpus",
                "pub_date": "2018",
                "pub_title": "Proceedings of the thirteenth workshop on innovative use of NLP for building educational applications",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_103",
            "content": "Phillip Keung, Vikas Bhardwaj, Adversarial learning with contextual embeddings for zeroresource cross-lingual classification and ner, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Phillip Keung",
                    "Vikas Bhardwaj"
                ],
                "title": "Adversarial learning with contextual embeddings for zeroresource cross-lingual classification and ner",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_104",
            "content": "Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, Eric Fosler-Lussier, Cross-lingual transfer learning for pos tagging without cross-lingual resources, 2017, Proceedings of the 2017 conference on empirical methods in natural language processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Joo-Kyung Kim",
                    "Young-Bum Kim",
                    "Ruhi Sarikaya",
                    "Eric Fosler-Lussier"
                ],
                "title": "Cross-lingual transfer learning for pos tagging without cross-lingual resources",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 2017 conference on empirical methods in natural language processing",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_105",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A method for stochastic optimization, 2014, Proceedings of the 3rd International Conference for Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "P Diederik",
                    "Jimmy Kingma",
                    " Ba"
                ],
                "title": "Adam: A method for stochastic optimization",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 3rd International Conference for Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_106",
            "content": "P Diederik, Max Kingma,  Welling, Autoencoding variational bayes, 2014, Proceedings of the 2nd International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "P Diederik",
                    "Max Kingma",
                    " Welling"
                ],
                "title": "Autoencoding variational bayes",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 2nd International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_107",
            "content": "Mateusz Klimaszewski, Piotr Andruszkiewicz, Wut at semeval-2019 task 9: Domainadversarial neural networks for domain adaptation in suggestion mining, 2019, Proceedings of the 13th International Workshop on Semantic Evaluation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Mateusz Klimaszewski",
                    "Piotr Andruszkiewicz"
                ],
                "title": "Wut at semeval-2019 task 9: Domainadversarial neural networks for domain adaptation in suggestion mining",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 13th International Workshop on Semantic Evaluation",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_108",
            "content": "M Douglas,  Kline, L Victor,  Berardi, Revisiting squared-error and cross-entropy functions for training neural network classifiers, 2005, Neural Computing & Applications, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "M Douglas",
                    " Kline",
                    "L Victor",
                    " Berardi"
                ],
                "title": "Revisiting squared-error and cross-entropy functions for training neural network classifiers",
                "pub_date": "2005",
                "pub_title": "Neural Computing & Applications",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_109",
            "content": "Ludmila I Kuncheva, C James, Robert Pw Bezdek,  Duin, Decision templates for multiple classifier fusion: an experimental comparison, 2001, Pattern recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    " Ludmila I Kuncheva",
                    "C James",
                    "Robert Pw Bezdek",
                    " Duin"
                ],
                "title": "Decision templates for multiple classifier fusion: an experimental comparison",
                "pub_date": "2001",
                "pub_title": "Pattern recognition",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_110",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_111",
            "content": "Mounica Maddela, Wei Xu, A wordcomplexity lexicon and a neural readability ranking model for lexical simplification, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Mounica Maddela",
                    "Wei Xu"
                ],
                "title": "A wordcomplexity lexicon and a neural readability ranking model for lexical simplification",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_112",
            "content": "Robert Mchardy, Heike Adel, Roman Klinger, Adversarial training for satire detection: Controlling for confounding variables, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Robert Mchardy",
                    "Heike Adel",
                    "Roman Klinger"
                ],
                "title": "Adversarial training for satire detection: Controlling for confounding variables",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_113",
            "content": "Gustavo Paetzold, Lucia Specia, Benchmarking lexical simplification systems, 2016, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Gustavo Paetzold",
                    "Lucia Specia"
                ],
                "title": "Benchmarking lexical simplification systems",
                "pub_date": "2016",
                "pub_title": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_114",
            "content": "Gustavo Paetzold, Lucia Specia, Semeval 2016 task 11: Complex word identification, 2016, Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Gustavo Paetzold",
                    "Lucia Specia"
                ],
                "title": "Semeval 2016 task 11: Complex word identification",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_115",
            "content": "Telmo Pires, Eva Schlinger, Dan Garrette, How multilingual is multilingual bert?, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Telmo Pires",
                    "Eva Schlinger",
                    "Dan Garrette"
                ],
                "title": "How multilingual is multilingual bert?",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_116",
            "content": "UNKNOWN, None, 2006, Ensemble based systems in decision making. IEEE Circuits and systems magazine, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": null,
                "title": null,
                "pub_date": "2006",
                "pub_title": "Ensemble based systems in decision making. IEEE Circuits and systems magazine",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_117",
            "content": "UNKNOWN, None, 2021, Improved multi-source domain adaptation by preservation of factors. Image and Vision Computing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Improved multi-source domain adaptation by preservation of factors. Image and Vision Computing",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_118",
            "content": "Matthew Shardlow, Richard Evans, Gustavo Paetzold, Marcos Zampieri, Semeval-2021 task 1: Lexical complexity prediction, 2021, Proceedings of the 14th International Workshop on Semantic Evaluation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Matthew Shardlow",
                    "Richard Evans",
                    "Gustavo Paetzold",
                    "Marcos Zampieri"
                ],
                "title": "Semeval-2021 task 1: Lexical complexity prediction",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 14th International Workshop on Semantic Evaluation",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_119",
            "content": "UNKNOWN, None, 2021, Predicting lexical complexity in english texts, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Predicting lexical complexity in english texts",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_120",
            "content": "Matthew Shardlow, Marcos Zampieri, Michael Cooper, Complex-a new corpus for lexical complexity predicition from likertscale data, 2020, Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI), .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Matthew Shardlow",
                    "Marcos Zampieri",
                    "Michael Cooper"
                ],
                "title": "Complex-a new corpus for lexical complexity predicition from likertscale data",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI)",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_121",
            "content": "Yuhua Tang, Zhipeng Lin, Haotian Wang, Liyang Xu, Adversarial mixup synthesis training for unsupervised domain adaptation, 2020, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Yuhua Tang",
                    "Zhipeng Lin",
                    "Haotian Wang",
                    "Liyang Xu"
                ],
                "title": "Adversarial mixup synthesis training for unsupervised domain adaptation",
                "pub_date": "2020",
                "pub_title": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "412-ARR_v2_122",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Proceedings of the 31st International Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 31st International Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_123",
            "content": "Giorgos Vernikos, Katerina Margatina, Alexandra Chronopoulou, Ion Androutsopoulos, Domain adversarial fine-tuning as an effective regularizer, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Giorgos Vernikos",
                    "Katerina Margatina",
                    "Alexandra Chronopoulou",
                    "Ion Androutsopoulos"
                ],
                "title": "Domain adversarial fine-tuning as an effective regularizer",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_124",
            "content": "Ke Wang, Gong Zhang, Henry Leung, Sar target recognition based on cross-domain and crosstask transfer learning, 2019, IEEE Access, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Ke Wang",
                    "Gong Zhang",
                    "Henry Leung"
                ],
                "title": "Sar target recognition based on cross-domain and crosstask transfer learning",
                "pub_date": "2019",
                "pub_title": "IEEE Access",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_125",
            "content": "Chris Seid Muhie Yimam, Shervin Biemann, Gustavo Malmasi, Lucia Paetzold, Sanja Specia,  \u0160tajner, A report on the complex word identification shared task, 2018, Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Chris Seid Muhie Yimam",
                    "Shervin Biemann",
                    "Gustavo Malmasi",
                    "Lucia Paetzold",
                    "Sanja Specia",
                    " \u0160tajner"
                ],
                "title": "A report on the complex word identification shared task",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_126",
            "content": "George-Eduard Zaharia, Dumitru-Clementin Cercel, Mihai Dascalu, Cross-lingual transfer learning for complex word identification, 2020, 2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "George-Eduard Zaharia",
                    "Dumitru-Clementin Cercel",
                    "Mihai Dascalu"
                ],
                "title": "Cross-lingual transfer learning for complex word identification",
                "pub_date": "2020",
                "pub_title": "2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "412-ARR_v2_127",
            "content": "George-Eduard Zaharia, Dumitru-Clementin Cercel, Mihai Dascalu, Upb at semeval-2021 task 1: Combining deep learning and hand-crafted features for lexical complexity prediction, 2021, Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "George-Eduard Zaharia",
                    "Dumitru-Clementin Cercel",
                    "Mihai Dascalu"
                ],
                "title": "Upb at semeval-2021 task 1: Combining deep learning and hand-crafted features for lexical complexity prediction",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_128",
            "content": "Dejiao Zhang, Ramesh Nallapati, Henghui Zhu, Feng Nan, Kathleen Cicero Dos Santos, Bing Mckeown,  Xiang, Unsupervised domain adaptation for cross-lingual text labeling, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Dejiao Zhang",
                    "Ramesh Nallapati",
                    "Henghui Zhu",
                    "Feng Nan",
                    "Kathleen Cicero Dos Santos",
                    "Bing Mckeown",
                    " Xiang"
                ],
                "title": "Unsupervised domain adaptation for cross-lingual text labeling",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings",
                "pub": null
            }
        },
        {
            "ix": "412-ARR_v2_129",
            "content": "Zhilu Zhang, R Mert,  Sabuncu, Generalized cross entropy loss for training deep neural networks with noisy labels, 2018, Proceedings of the 32nd International Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Zhilu Zhang",
                    "R Mert",
                    " Sabuncu"
                ],
                "title": "Generalized cross entropy loss for training deep neural networks with noisy labels",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 32nd International Conference on Neural Information Processing Systems",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "412-ARR_v2_0@0",
            "content": "Domain Adaptation in Multilingual and Multi-Domain Monolingual Settings for Complex Word Identification",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_0",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_2@0",
            "content": "Complex word identification (CWI) is a cornerstone process towards proper text simplification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_2",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_2@1",
            "content": "CWI is highly dependent on context, whereas its difficulty is augmented by the scarcity of available datasets which vary greatly in terms of domains and languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_2",
            "start": 95,
            "end": 257,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_2@2",
            "content": "As such, it becomes increasingly more difficult to develop a robust model that generalizes across a wide array of input examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_2",
            "start": 259,
            "end": 387,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_2@3",
            "content": "In this paper, we propose a novel training technique for the CWI task based on domain adaptation to improve the target character and context representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_2",
            "start": 389,
            "end": 545,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_2@4",
            "content": "This technique addresses the problem of working with multiple domains, inasmuch as it creates a way of smoothing the differences between the explored datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_2",
            "start": 547,
            "end": 705,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_2@5",
            "content": "Moreover, we also propose a similar auxiliary task, namely text simplification, that can be used to complement lexical complexity prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_2",
            "start": 707,
            "end": 847,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_2@6",
            "content": "Our model obtains a boost of up to 2.42% in terms of Pearson Correlation Coefficients in contrast to vanilla training techniques, when considering the CompLex from the Lexical Complexity Prediction 2021 dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_2",
            "start": 849,
            "end": 1059,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_2@7",
            "content": "At the same time, we obtain an increase of 3% in Pearson scores, while considering a cross-lingual setup relying on the Complex Word Identification 2018 dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_2",
            "start": 1061,
            "end": 1221,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_2@8",
            "content": "In addition, our model yields state-ofthe-art results in terms of Mean Absolute Error.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_2",
            "start": 1223,
            "end": 1308,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_4@0",
            "content": "The overarching goal of the complex word identification (CWI) task is to find words that can be simplified in a given text (Paetzold and Specia, 2016b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_4",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_4@1",
            "content": "Evaluating word difficulty represents one step towards achieving simplified, which in return facilitates access to knowledge to a wider audience texts (Maddela and Xu, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_4",
            "start": 153,
            "end": 326,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_4@2",
            "content": "However, complex word identification is a highly contextualized task, far from being trivial.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_4",
            "start": 328,
            "end": 420,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_4@3",
            "content": "The datasets are scarce and, most of the time, the input entries are limited or cover different domains/areas of expertise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_4",
            "start": 422,
            "end": 544,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_4@4",
            "content": "There-",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_4",
            "start": 546,
            "end": 551,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_5@0",
            "content": "CompLex LCP Dataset Bible But let each man test his own work , and then he will take pride in himself and not in his neighbor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_5",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_6@0",
            "content": "A genome database search revealed orthologs of ADAM11, ADAM22 and ADAM23 genes to exist in vertebrates such as mammals, fish, and amphibians, but not in invertebrates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_6",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_7@0",
            "content": "They also allow for easy compensation for the thousands of accidents involving vehicles from more than one Member State.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_7",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_8@0",
            "content": "English CWI Dataset Wikipedia Normally , the land will be passed down to future generations in a way that recognizes the community's traditional connection to that country .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_8",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_9@0",
            "content": "The JAS 39C Gripen crashed onto a runway at around 9:30 am local time (02:30 UTC) and exploded , closing the airport to commercial flights.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_9",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_10@0",
            "content": "The car has been removed from the scene for forensic technical examination .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_10",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_11@0",
            "content": "Table 1: Examples of complex words annotated for each of the domains from CompLex LCP and CWI datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_11",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_11@1",
            "content": "The shades indicate the complexity; the darker the shade, the more complex the sequence of words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_11",
            "start": 104,
            "end": 200,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_11@2",
            "content": "Best viewed in color.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_11",
            "start": 202,
            "end": 222,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_12@0",
            "content": "fore, developing a robust and reliable model that can be used to properly evaluate the complexity of tokens is a challenging task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_12",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_12@1",
            "content": "Table 1 showcases examples of complex words annotations from the CompLex LCP (Shardlow et al., 2020(Shardlow et al., , 2021b and English CWI (Yimam et al., 2018) datasets employed in this work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_12",
            "start": 131,
            "end": 323,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_12@2",
            "content": "Nevertheless, certain training techniques and auxiliary tasks help the model improve its generalization abilities, forcing it to focus only on the most relevant, general features (Schrom et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_12",
            "start": 325,
            "end": 525,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_12@3",
            "content": "Techniques like domain adaptation (Ganin et al., 2016) can be used for various tasks, with the purpose of selecting relevant features for follow-up processes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_12",
            "start": 527,
            "end": 684,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_12@4",
            "content": "At the same time, the cross-domain scenario can be transposed to a cross-lingual setup, where the input entries are part of multiple available languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_12",
            "start": 686,
            "end": 838,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_12@5",
            "content": "Performance can be improved by also employing the power of domain adaptation, where the domain is the language; as such, the task of identifying complex tokens can be approached even for low resource languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_12",
            "start": 840,
            "end": 1049,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_13@0",
            "content": "We propose several solutions to improve the performance of a model for CWI in a cross-domain or a cross-lingual setting, by adding auxiliary components (i.e., Transformer (Vaswani et al., 2017) decoders, Variational Auto Encoders -VAEs (Kingma and Welling, 2014)), as well as a domain adaptation training technique (Farahani et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_13",
            "start": 0,
            "end": 338,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_13@1",
            "content": "Moreover, we use the domain adaptation intuition and we apply it in a multi-task adversarial training scenario, where the main task is trained alongside an auxiliary one, and a task discriminator has the purpose of generalizing task-specific features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_13",
            "start": 340,
            "end": 590,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_14@0",
            "content": "We summarize our main contributions as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_14",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_15@0",
            "content": "\u2022 Applying the concept of domain adaptation in a monolingual, cross-domain scenario for complex word identification; \u2022 Introducing the domain adaptation technique in a cross-lingual setup, where the discriminator has the purpose to support the model extract only the most relevant features across all languages; \u2022 Proposing additional components (i.e., Transformer decoders and Variational Auto Encoders) trained alongside the main CWI task to provide more meaningful representations of the inputs and to ensure robustness, while generating new representations or by tuning the existing ones; \u2022 Experimenting with an additional text simplification task alongside domain/language adaptation, with the purpose of extracting cross-task features and improving performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_15",
            "start": 0,
            "end": 767,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_16@0",
            "content": "2 Related Work Domain Adaptation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_16",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_16@1",
            "content": "Several works employed domain adaptation to improve performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_16",
            "start": 34,
            "end": 97,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_16@2",
            "content": "For example, Du et al. (2020) approached the sentiment analysis task by using a BERT-based (Devlin et al., 2019) (Klimaszewski and Andruszkiewicz, 2019), mixup synthesis training (Tang et al., 2020), and effective regularization (Vernikos et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_16",
            "start": 99,
            "end": 351,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_17@0",
            "content": "Cross-Lingual Domain Adaptation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_17",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_17@1",
            "content": "Chen et al. (2018) proposed ADAN, an architecture based on a feed-forward neural network with three main components, namely: a feature extractor, a sentiment classifier, and a language discriminator.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_17",
            "start": 33,
            "end": 231,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_17@2",
            "content": "The latter had the purpose of supporting the adversarial training setup, thus covering the scenario where the model was unable to detect whether the input language was from the source dataset or the target one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_17",
            "start": 233,
            "end": 442,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_17@3",
            "content": "A similar cross-lingual approach was adopted by Zhang et al. (2020), who developed a system to classify entries from the target language, while only labels from the source language were provided.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_17",
            "start": 444,
            "end": 638,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_17@4",
            "content": "Keung et al. (2019) employed the usage of multilingual BERT (Pires et al., 2019) and argued that a language-adversarial task can improve the performance of zero-resource cross-lingual transfers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_17",
            "start": 640,
            "end": 833,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_17@5",
            "content": "Moreover, training under an adversarial technique helps the Transformer model align the representations of the English inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_17",
            "start": 835,
            "end": 960,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_18@0",
            "content": "Under a Named Entity Recognition training scenario, Kim et al. (2017) used features on two levels (i.e., word and characters), together with Recurrent Neural Networks and a language discriminator used for the domain-adversarial setup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_18",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_18@1",
            "content": "Similarly, Huang et al. (2019) used target language discriminators during the process of training models for low-resource name tagging.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_18",
            "start": 235,
            "end": 369,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_19@0",
            "content": "Word Complexity Prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_19",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_19@1",
            "content": "Gooding and Kochmar (2019) based their implementation for CWI as a sequence labeling task on Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks, inasmuch as the context helps towards proper identification of complex tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_19",
            "start": 28,
            "end": 273,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_19@2",
            "content": "The authors used 300-dimensional pretrained word embeddings as inputs for the LSTMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_19",
            "start": 275,
            "end": 358,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_19@3",
            "content": "Also adopting a sequence labeling approach, Finnimore et al. (2019) considered handcrafted features, including punctuation or syllables, that can properly identify complex structures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_19",
            "start": 360,
            "end": 542,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_20@0",
            "content": "The same sequence labeling approach can be applied under a plurality voting technique (Polikar, 2006), or even using an Oracle (Kuncheva et al., 2001).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_20",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_20@1",
            "content": "The Oracle functions best when applied to multiple solutions, by jointly using them to obtain a final prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_20",
            "start": 152,
            "end": 264,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_20@2",
            "content": "At the same time, Zaharia et al. (2020) explored the power of Transformer-based models (Vaswani et al., 2017) in cross-lingual environments by using different training scenarios, depending on the scarcity of the resources: zero-shot, one-shot, as well as few-shot learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_20",
            "start": 266,
            "end": 538,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_20@3",
            "content": "Moreover, CWI can be also approached as a probabilistic task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_20",
            "start": 540,
            "end": 600,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_20@4",
            "content": "For example, De Hertog and Tack (2018) introduced a series of architectures that combine deep learning features, as well as handcrafted features to address CWI as a regression problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_20",
            "start": 602,
            "end": 785,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_21@0",
            "content": "Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_21",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_22@0",
            "content": "Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_22",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_23@0",
            "content": "We experimented with two datasets, one monolingual -CompLex LCP 2021 (Shardlow et al., 2020(Shardlow et al., , 2021b) -and one cross-lingual -the CWI Shared Dataset (Yimam et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_23",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_23@1",
            "content": "The entries of Com-pLex consist of a sentence in English and a target token, alongside the complexity of the token, given its context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_23",
            "start": 187,
            "end": 320,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_23@2",
            "content": "The complexities are continuous values between 0 and 1, annotated by various individuals on an initial 5-point Likert scale; the annotations were then normalized.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_23",
            "start": 322,
            "end": 483,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_24@0",
            "content": "The CompLex dataset contains two types of entries, each with its corresponding subset of entries: a) single, where the target token is represented by a single word, and b) multiple, where the target token is represented by a group of words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_24",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_24@1",
            "content": "While the single-word dataset contains 7,662 training entries, 421 trial entries, and 917 test entries, the multiword dataset has lower counts, with 1,517 training entries, 99 trial entries, and 184 for testing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_24",
            "start": 241,
            "end": 451,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_24@2",
            "content": "At the same time, the entries correspond to three different domains (i.e., biblical, biomedical, and political), therefore displaying different characteristics and challenging the models towards generalization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_24",
            "start": 453,
            "end": 662,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_25@0",
            "content": "The CWI dataset was introduced in the CWI Shared Task 2018 (Yimam et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_25",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_25@1",
            "content": "It is a multilingual dataset, containing entries in English, German, Spanish, and French.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_25",
            "start": 81,
            "end": 169,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_25@2",
            "content": "Moreover, the English entries are split into three categories, depending on their proficiency levels: professional (News), non-professional (WikiNews), and Wikipedia articles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_25",
            "start": 171,
            "end": 345,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_25@3",
            "content": "Most entries are for the English language (27,299 training and 3,328 validation), while the fewest training entries are for German (6,151 training and 795 validation).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_25",
            "start": 347,
            "end": 513,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_25@4",
            "content": "The French language does not contain training or validation entries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_25",
            "start": 515,
            "end": 582,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_26@0",
            "content": "The Domain Adaption Model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_26",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_27@0",
            "content": "The overarching architecture of our method is introduced in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_27",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_27@1",
            "content": "All underlying components are presented in detail in the following subsections.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_27",
            "start": 70,
            "end": 148,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_27@2",
            "content": "Our model combines character-level BiLSTM features (i.e., F t ) with Transformer-based features for the context sentence (i.e., F c ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_27",
            "start": 150,
            "end": 283,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_27@3",
            "content": "The concatenated features (F c +F t ) are then passed through three linear layers, with a dropout separating the first and second.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_27",
            "start": 285,
            "end": 414,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_27@4",
            "content": "The output is a value representing the complexity of the target word.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_27",
            "start": 416,
            "end": 484,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_28@0",
            "content": "Three configurations were experimented.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_28",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_28@1",
            "content": "Within Basic Domain Adaptation, the previous features are passed through an additional component, the domain discriminator, composed of a linear layer followed by a softmax activation function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_28",
            "start": 40,
            "end": 232,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_28@2",
            "content": "A gradient reversal layer (Ganin and Lempitsky, 2015) is added between the feature concatenation and the discriminator to reverse the gradients through the backpropagation phase and support extracting general features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_28",
            "start": 234,
            "end": 451,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_28@3",
            "content": "The loss function is determined by Equation 1 as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_28",
            "start": 453,
            "end": 501,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_29@0",
            "content": "L = L r \u2212 \u03b2\u03bbL d (1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_29",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_30@0",
            "content": "where L r is the regression loss, L d is the general domain loss, \u03b2 is a hyperparameter used for controlling the importance of L d , and \u03bb is another hyperparameter that varies as the training process progresses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_30",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_31@0",
            "content": "The following setups also include the Basic Domain Adaptation training setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_31",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_32@0",
            "content": "VAE and Domain Adaptation considers the previous configuration, plus the VAE encoder, that yields the F v features, and the VAE decoder, which aims to reconstruct the input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_32",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_32@1",
            "content": "The concatenation layer now contains the BiLSTM and Transformer features, along with the VAE encoder features (F v ), namely F t +F c +F v .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_32",
            "start": 174,
            "end": 313,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_32@2",
            "content": "The loss function is depicted by Equation 2 as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_32",
            "start": 315,
            "end": 361,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_33@0",
            "content": "L = L r \u2212 \u03b2\u03bbL d + \u03b1L v (2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_33",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_34@0",
            "content": "where, additionally, L v represents the VAE loss described in Equation 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_34",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_35@0",
            "content": "Transformer Decoder and Domain Adaptation adds a Transformer Decoder with the purpose of reconstructing the original input, for a more robust context feature extraction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_35",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_35@1",
            "content": "The loss is denoted by Equation 3 as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_35",
            "start": 170,
            "end": 206,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_36@0",
            "content": "L = L r \u2212 \u03b2\u03bbL d + \u03b1L dec (3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_36",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_37@0",
            "content": "where L dec represents the decoder loss described in Equation 9.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_37",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_38@0",
            "content": "Character-level BiLSTM for Target Word Representation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_38",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_39@0",
            "content": "The purpose of this component is to determine the complexity of the target token, given only its constituent characters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_39",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_39@1",
            "content": "A character-level Bidirectional Long Short-Term Memory (BiLSTM) network receives as input an array of characters corresponding to the target word (or group of words), and yields a representation that is afterwards concatenated to the previously mentioned Transformer-based representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_39",
            "start": 121,
            "end": 409,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_39@2",
            "content": "Each character c is mapped to a certain value obtained from the character vocabulary V, containing all the characters present in the input dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_39",
            "start": 411,
            "end": 557,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_40@0",
            "content": "The character sequence is represented as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_40",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_41@0",
            "content": "C i = [c 1 , c 2 , . . . , c n ],",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_41",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_42@0",
            "content": "where n is the maximum length of a target token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_42",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_42@1",
            "content": "C i is then passed through a character embedding layer, thus yielding the output Emb target .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_42",
            "start": 49,
            "end": 141,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_42@2",
            "content": "Emb target is then fed to the BiLSTM, followed by a dropout layer, thus obtaining the final target word representation, F t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_42",
            "start": 143,
            "end": 267,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_43@0",
            "content": "Transformer-based Context Representation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_43",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_44@0",
            "content": "We rely on a Transformer-based model as the main feature extractor for the context of the target word (i.e., the full sentence), considering their superior performance on most natural language processing tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_44",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_44@1",
            "content": "The selected model for the first dataset is RoBERTa (Liu et al., 2019), as it yields better results when compared to its counterpart, BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_44",
            "start": 211,
            "end": 349,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_44@2",
            "content": "RoBERTa is trained with higher learning rates and larger mini-batches, and it modifies the key hyper-parameters of BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_44",
            "start": 351,
            "end": 470,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_44@3",
            "content": "We employed the usage of XLM-RoBERTa (Conneau et al., 2020), the multilingual counterpart of RoBERTa, now trained on a very large corpus of multilingual texts, for the second cross-lingual task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_44",
            "start": 472,
            "end": 665,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_44@4",
            "content": "The features used for our task are represented by the pooled output of the Transformer model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_44",
            "start": 667,
            "end": 759,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_44@5",
            "content": "The feature vector F c of 768 elements captures information about the context of the target word.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_44",
            "start": 761,
            "end": 857,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_45@0",
            "content": "Variational AutoEncoders",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_45",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_46@0",
            "content": "We aim to further improve performance by adding extra features via Variational AutoEncoders (VAEs) (Kingma and Welling, 2014) to the context representation for a target word.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_46",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_46@1",
            "content": "More specifically for the CWI task, we use the latent vector z, alongside the Transformer and the Char BiLSTM features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_46",
            "start": 175,
            "end": 293,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_46@2",
            "content": "Moreover, we also need to ensure that the Encoder representation is accurate; therefore, we consider the VAE encoding and decoding as an additional task having the purpose of minimizing the reconstruction loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_46",
            "start": 295,
            "end": 504,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_47@0",
            "content": "The VAE consists of two parts, namely the encoder and the decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_47",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_47@1",
            "content": "The encoder g(x) produces the approximation q(z|x) of the posterior distribution p(z|x), thus mapping the input x to the latent space z. The process is presented in Equation 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_47",
            "start": 67,
            "end": 242,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_47@2",
            "content": "We use as features the representation z, denoted as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_47",
            "start": 244,
            "end": 294,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_48@0",
            "content": "F v . p(z|x) \u2248 q(z|x) = N (\u00b5(x), \u03c3(x))(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_48",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_49@0",
            "content": "The decoder f(z) maps the latent space to the input space (i.e., p(z) to p(x)), by using Equation 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_49",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_49@1",
            "content": "Furthermore, E q represents the expectation with relation to the distribution q.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_49",
            "start": 101,
            "end": 180,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_50@0",
            "content": "L(f, g) = i {\u2212D KL [q(z|x i )||p(z)] + E q(z|x i ) [ln p(x i |z)]} (6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_50",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_51@0",
            "content": "Discriminators",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_51",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_52@0",
            "content": "The features extracted by our architecture can vary greatly as the input entries can originate from different domains or languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_52",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_52@1",
            "content": "Consequently, we introduced a generalization technique to extract only cross-domain features that do not present a bias towards a certain domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_52",
            "start": 132,
            "end": 276,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_52@2",
            "content": "We thus employ an adversarial training technique based on domain adaptation, forcing the model to only extract relevant cross-domain features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_52",
            "start": 278,
            "end": 419,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_53@0",
            "content": "A discriminator acts as a classifier, containing three linear layers with corresponding activation functions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_53",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_53@1",
            "content": "The discriminator classifies the input sentence into one of the available domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_53",
            "start": 110,
            "end": 191,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_53@2",
            "content": "Unlike traditional classification approaches, our purpose is not to minimize the loss, but to maximize it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_53",
            "start": 193,
            "end": 298,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_53@3",
            "content": "We want our model to become incapable of distinguishing between different categories of input entries, therefore extracting the most relevant, cross-domain features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_53",
            "start": 300,
            "end": 464,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_54@0",
            "content": "Our architecture is encouraged to generalize in terms of extracted features by the gradient reversal layer that reverses the gradients during the backpropagation phase; as such, the parameters are updated towards the direction that maximizes the loss instead of minimizing it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_54",
            "start": 0,
            "end": 275,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_55@0",
            "content": "Three scenarios were considered, each one targeting a different approach towards domain adaptation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_55",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_56@0",
            "content": "Domain Discriminator.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_56",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_56@1",
            "content": "The first scenario is applied on the first dataset, CompLex, with entries only in English, but covering multiple domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_56",
            "start": 22,
            "end": 142,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_56@2",
            "content": "The discriminator has the purpose of identifying the domain of the entry, namely biblical, biomedical or political.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_56",
            "start": 144,
            "end": 258,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_56@3",
            "content": "The intuition is that, by grasping only cross-domain features, the performance of the model increases on all three domains, instead of performing well only on one, while poorer on the others.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_56",
            "start": 260,
            "end": 450,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_57@0",
            "content": "Language Discriminator.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_57",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_57@1",
            "content": "The intuition is similar to the previous scenario, except that we experimented with the second multilingual dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_57",
            "start": 24,
            "end": 139,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_57@2",
            "content": "Therefore, our interest was that our model extracts cross-lingual features, such that the performance is equal on all the target languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_57",
            "start": 141,
            "end": 279,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_58@0",
            "content": "Task Discriminator.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_58",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_58@1",
            "content": "In this scenario, we trained a similar, auxiliary task, represented by text simplification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_58",
            "start": 20,
            "end": 110,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_58@2",
            "content": "A task discriminator is implemented to detect the origin of the input entry: either the main task or the auxiliary task (i.e., simplified version).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_58",
            "start": 112,
            "end": 258,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_58@3",
            "content": "The dataset used for text simplification is represented by BenchLS (Paetzold and Specia, 2016a) 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_58",
            "start": 260,
            "end": 358,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_58@4",
            "content": "The employed simplification process consists of masking the word considered to be complex and then using a Transformer for Masked Language Modeling to predict the best candidate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_58",
            "start": 360,
            "end": 537,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_58@5",
            "content": "The corresponding flow is described in Algorithm 1, while the loss function is presented in Equation 7:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_58",
            "start": 539,
            "end": 641,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_59@0",
            "content": "L = L r \u2212 \u03b2\u03bbL task_id + L M L (7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_59",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_60@0",
            "content": "where L ML is the Sparse Categorical Cross Entropy loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_60",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_61@0",
            "content": "All previous discriminators use the same loss, namely Categorical Cross Entropy (Zhang and Sabuncu, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_61",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_62@0",
            "content": "The overall loss consists of the difference between the task loss and the domain/language loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_62",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_62@1",
            "content": "Moreover, the importance of the latter can be controlled by multiplication with a \u03bb hyperparameter, that changes over time, and a fixed \u03b2 hyperparameter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_62",
            "start": 96,
            "end": 248,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_62@2",
            "content": "The network parameters, \u03b8 p are updated according to Equation 8, where \u03b7 is the learning rate, L d is the domain loss, L r is the task loss and \u03b2 is the weight for the domain loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_62",
            "start": 250,
            "end": 429,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_62@3",
            "content": "A similar equation for language loss (L l ) is in place for the second dataset, where instead of the domain loss L d we used the language identification loss L l , having the same formula.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_62",
            "start": 431,
            "end": 618,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_63@0",
            "content": "\u03b8 p = \u03b8 p \u2212 \u03b7( \u2202L r \u2202\u03b8 p \u2212 \u03b2\u03bb \u2202L d \u2202\u03b8 p ) (8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_63",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_64@0",
            "content": "Transformer Decoder",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_64",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_65@0",
            "content": "Our model also considers a decoder to reconstruct the original input, starting from the Transformer representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_65",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_65@1",
            "content": "The intuition behind introducing The decoder receives as input the outputs of the hidden Transformer layer alongside an embedding of the original input, which are passed through a Gated Recurrent Unit (GRU) (Chung et al., 2014) layer for obtaining the final representation of the initial input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_65",
            "start": 116,
            "end": 409,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_65@2",
            "content": "Additionally, two linear layers separated by a dropout are introduced before obtaining the final representation, y = F d .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_65",
            "start": 411,
            "end": 532,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_65@3",
            "content": "The loss is computed by using the Negative Log Likelihood loss between the outputs of the decoder and the original Transformer input id representation of the entries (see Equations 9 and 10).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_65",
            "start": 534,
            "end": 724,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_66@0",
            "content": "L(x, y) = N n=1 l n (9) l n = \u2212w yn x n,yn , w c = weight[c] \u2022 1{c = ignore_index} (10)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_66",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_67@0",
            "content": "Experimental Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_67",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_68@0",
            "content": "The optimizer used for our models is represented by AdamW (Kingma and Ba, 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_68",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_68@1",
            "content": "The learning rate is set to 2e-5, while the loss functions used for the complexity task are the L1 loss (Janocha and Czarnecki, 2016) for the CompLex LCP dataset and the Mean Squared Error (MSE) loss (Kline and Berardi, 2005) for the CWI dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_68",
            "start": 81,
            "end": 326,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_68@2",
            "content": "The auxiliary losses are summed to the main loss (i.e., complexity prediction) and are scaled according to their priority, with a factor of \u03b1, where \u03b1 is set to 0.1 for the VAE loss, and 0.01 for the Transformer decoder and task discriminator losses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_68",
            "start": 328,
            "end": 577,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_68@3",
            "content": "The \u03bb parameter used for domain adaptation was updated according to Equation 11:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_68",
            "start": 579,
            "end": 658,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_69@0",
            "content": "\u03bb = 2 1 + e \u2212\u03b3 \u2212 1 (11)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_69",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_70@0",
            "content": "where is the number of epochs the model was trained; \u03b3 was set to 0.1, while \u03b2 was set to 0.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_70",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_70@1",
            "content": "Moreover, each model was trained for 8 epochs, except for the one including the VAE features, which was trained for 12 epochs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_70",
            "start": 95,
            "end": 220,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_71@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_71",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_72@0",
            "content": "LCP 2021 CompLex Dataset",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_72",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_73@0",
            "content": "We consider as baselines two models used for the LCP 2021 competition (Shardlow et al., 2021a), as well as the best-registered score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_73",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_73@1",
            "content": "Almeida et al. (2021) employed the usage of neural network solutions; more specifically, they used chunks of the sentences obtained with Sent2Vec as input features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_73",
            "start": 134,
            "end": 297,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_73@2",
            "content": "Zaharia et al. (2021) created models that are based on target and context feature extractors, alongside features resulted from Graph Convolutional Networks, Capsule Networks, and pre-trained word embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_73",
            "start": 299,
            "end": 505,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_74@0",
            "content": "Table 2 depicts the results obtained for the English dataset using domain adaptation and various configurations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_74",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_74@1",
            "content": "\"Base\" denotes the initial model (RoBERTa + Char BiLSTM) on which we apply domain adaptation, as well as the auxiliary tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_74",
            "start": 113,
            "end": 237,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_74@2",
            "content": "The domain adaptation technique offers improved performance when applied on top of an architecture, considering that the model learns cross-domain features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_74",
            "start": 239,
            "end": 394,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_74@3",
            "content": "The only exception is represented by a slightly lower Pearson score on the model that uses domain adaptation alongside the Transformer decoding auxiliary task (Base + Decoder + DA), with a value of .7969 on the trial dataset, when compared",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_74",
            "start": 396,
            "end": 634,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_75@0",
            "content": "CWI 2018 Dataset",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_75",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_76@0",
            "content": "We also experimented with a multilingual dataset, where the discriminant is considered to be the language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_76",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_76@1",
            "content": "The baseline consists of three models used from the CWI 2018 competition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_76",
            "start": 107,
            "end": 179,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_76@2",
            "content": "The performance is evaluated in terms of MAE; however, we also report the Pearson Correlation Coefficient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_76",
            "start": 181,
            "end": 286,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_76@3",
            "content": "First, Kajiwara and Komachi (2018) based their models on regressors, alongside features represented by the number of characters or words and the frequency of the target word in certain corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_76",
            "start": 288,
            "end": 480,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_76@4",
            "content": "Second, the approach of Bingel and Bjerva ( 2018) is based on Random Forest Regressors, as well as feed-forward neural networks alongside specific features, such as log-probability, inflectional complexity, or target-sentence similarity; the authors focused on non-English entries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_76",
            "start": 482,
            "end": 762,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_76@5",
            "content": "Third, Gooding and Kochmar (2018) approach the English section of the dataset by employing linear regressions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_76",
            "start": 764,
            "end": 873,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_76@6",
            "content": "The authors used several types of handcrafted features, including word n-grams, POS tags, dependency parse relations, and psycholinguistic features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_76",
            "start": 875,
            "end": 1022,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_77@0",
            "content": "Table 3 presents the results obtained on the multilingual validation dataset and compares the performance of different configurations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_77",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_78@0",
            "content": "Discussions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_78",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_79@0",
            "content": "The domain adaptation technique supports our model to learn general cross-domain or crosslanguage features, while achieving higher performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_79",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_79@1",
            "content": "Moreover, jointly training on two different tasks (i.e., lexical complexity prediction and text simplification), coupled with domain adaptation to generalize the features from the two tasks, can lead to improved results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_79",
            "start": 144,
            "end": 363,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_79@2",
            "content": "However, there are entries for which our models were unable to properly predict the complexity score, namely: a) entries with a different level of complexity (i.e. biomedical), and b) entries part of a language that was not present in the training dataset (i.e., French).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_79",
            "start": 365,
            "end": 635,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_79@3",
            "content": "For the former, scientific terms (e.g., \"sitosterolemia\"), abbreviations (e.g., \"ES\"), or complex elements (e.g., \"H3-2meK9\") impose a series of difficulties for our feature extractors, considering the absence of these tokens from the Transformer vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_79",
            "start": 637,
            "end": 894,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_79@4",
            "content": "The latter category of problematic entries creates new challenges in the sense that it represents a completely new language on which the architecture is tested.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_79",
            "start": 896,
            "end": 1055,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_79@5",
            "content": "However, as seen in the results section, the cross-lingual domain adaptation technique offers good improvements, helping the model achieve better performance on French, even though the initial architecture was not exposed to any French example.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_79",
            "start": 1057,
            "end": 1300,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_80@0",
            "content": "Conclusions and Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_80",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_81@0",
            "content": "This work proposes a series of training techniques, including domain adaptation, as well as multi-task adversarial learning, that can be used for improving the overall performance of the models for CWI.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_81",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_81@1",
            "content": "Domain adaptation improves results by encouraging the models to extract more general features, that can be further used for the lexical complexity prediction task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_81",
            "start": 203,
            "end": 365,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_81@2",
            "content": "Moreover, by jointly training the model on the CWI tasks and an auxiliary similar task (i.e., text simplification), the overall performance is improved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_81",
            "start": 367,
            "end": 518,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_81@3",
            "content": "The task discriminator also ensures the extraction of general features, thus making the model more robust on the CWI dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_81",
            "start": 520,
            "end": 644,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_82@0",
            "content": "For future work, we intend to experiment with meta-learning (Finn et al., 2017) alongside domain adaptation (Wang et al., 2019), considering the scope of the previously applied training techniques.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_82",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_82@1",
            "content": "This would enable us to initialize the model's weights in the best manner, thus ensuring optimal results during the training phase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_82",
            "start": 198,
            "end": 328,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_83@0",
            "content": "Raul Almeida, Hegler Tissot, Marcos Didonet Del Fabro, C3sl at semeval-2021 task 1: Predicting lexical complexity of words in specific contexts with sentence embeddings, 2021, Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_83",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_84@0",
            "content": "Joachim Bingel, Johannes Bjerva, Crosslingual complex word identification with multitask learning, 2018, Proceedings of the thirteenth workshop on innovative use of NLP for building educational applications, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_84",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_85@0",
            "content": "Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie, Kilian Weinberger, Adversarial deep averaging networks for cross-lingual sentiment classification, 2018, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_85",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_86@0",
            "content": "Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio, Empirical evaluation of gated recurrent neural networks on sequence modeling, 2014, NIPS 2014 Deep Learning and Representation Learning Workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_86",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_87@0",
            "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, \u00c9douard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Unsupervised cross-lingual representation learning at scale, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_87",
            "start": 0,
            "end": 322,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_88@0",
            "content": "Erenay Dayanik, Sebastian Pad\u00f3, Masking actor information leads to fairer political claims detection, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_88",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_89@0",
            "content": "Dirk De Hertog, Ana\u00efs Tack, Deep learning architecture for complex word identification, 2018, Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_89",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_90@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_90",
            "start": 0,
            "end": 294,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_91@0",
            "content": "Chunning Du, Haifeng Sun, Jingyu Wang, Qi Qi, Jianxin Liao, Adversarial and domain-aware bert for cross-domain sentiment analysis, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_91",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_92@0",
            "content": "Abolfazl Farahani, Sahar Voghoei, Khaled Rasheed, Hamid Arabnia, A brief review of domain adaptation, 2021, Advances in Data Science and Information Engineering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_92",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_93@0",
            "content": "Chelsea Finn, Pieter Abbeel, Sergey Levine, Model-agnostic meta-learning for fast adaptation of deep networks, 2017, Proceedings of the 34th International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_93",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_94@0",
            "content": "Pierre Finnimore, Elisabeth Fritzsch, Daniel King, Alison Sneyd, Aneeq Ur Rehman, Fernando Alva-Manchego, Andreas Vlachos, Strong baselines for complex word identification across multiple languages, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_94",
            "start": 0,
            "end": 349,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_95@0",
            "content": "Yaroslav Ganin, Victor Lempitsky, Unsupervised domain adaptation by backpropagation, 2015, International conference on machine learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_95",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_96@0",
            "content": "UNKNOWN, None, 2016, Domain-adversarial training of neural networks. The journal of machine learning research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_96",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_97@0",
            "content": "Sian Gooding, Ekaterina Kochmar, Camb at cwi shared task 2018: Complex word identification with ensemble-based voting, 2018, Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_97",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_98@0",
            "content": "Sian Gooding, Ekaterina Kochmar, Complex word identification as a sequence labelling task, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_98",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_99@0",
            "content": "Sepp Hochreiter, J\u00fcrgen Schmidhuber, Long short-term memory, 1997, Neural computation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_99",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_100@0",
            "content": "Lifu Huang, Ji Heng, Jonathan , Crosslingual multi-level adversarial transfer to enhance low-resource name tagging, 2019-05, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_100",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_101@0",
            "content": "Katarzyna Janocha, Wojciech Czarnecki, On loss functions for deep neural networks in classification, 2016, Schedae Informaticae, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_101",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_102@0",
            "content": "Tomoyuki Kajiwara, Mamoru Komachi, Complex word identification based on frequency in a learner corpus, 2018, Proceedings of the thirteenth workshop on innovative use of NLP for building educational applications, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_102",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_103@0",
            "content": "Phillip Keung, Vikas Bhardwaj, Adversarial learning with contextual embeddings for zeroresource cross-lingual classification and ner, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_103",
            "start": 0,
            "end": 317,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_104@0",
            "content": "Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, Eric Fosler-Lussier, Cross-lingual transfer learning for pos tagging without cross-lingual resources, 2017, Proceedings of the 2017 conference on empirical methods in natural language processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_104",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_105@0",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A method for stochastic optimization, 2014, Proceedings of the 3rd International Conference for Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_105",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_106@0",
            "content": "P Diederik, Max Kingma,  Welling, Autoencoding variational bayes, 2014, Proceedings of the 2nd International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_106",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_107@0",
            "content": "Mateusz Klimaszewski, Piotr Andruszkiewicz, Wut at semeval-2019 task 9: Domainadversarial neural networks for domain adaptation in suggestion mining, 2019, Proceedings of the 13th International Workshop on Semantic Evaluation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_107",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_108@0",
            "content": "M Douglas,  Kline, L Victor,  Berardi, Revisiting squared-error and cross-entropy functions for training neural network classifiers, 2005, Neural Computing & Applications, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_108",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_109@0",
            "content": "Ludmila I Kuncheva, C James, Robert Pw Bezdek,  Duin, Decision templates for multiple classifier fusion: an experimental comparison, 2001, Pattern recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_109",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_110@0",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_110",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_111@0",
            "content": "Mounica Maddela, Wei Xu, A wordcomplexity lexicon and a neural readability ranking model for lexical simplification, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_111",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_112@0",
            "content": "Robert Mchardy, Heike Adel, Roman Klinger, Adversarial training for satire detection: Controlling for confounding variables, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_112",
            "start": 0,
            "end": 275,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_113@0",
            "content": "Gustavo Paetzold, Lucia Specia, Benchmarking lexical simplification systems, 2016, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_113",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_114@0",
            "content": "Gustavo Paetzold, Lucia Specia, Semeval 2016 task 11: Complex word identification, 2016, Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_114",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_115@0",
            "content": "Telmo Pires, Eva Schlinger, Dan Garrette, How multilingual is multilingual bert?, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_115",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_116@0",
            "content": "UNKNOWN, None, 2006, Ensemble based systems in decision making. IEEE Circuits and systems magazine, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_116",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_117@0",
            "content": "UNKNOWN, None, 2021, Improved multi-source domain adaptation by preservation of factors. Image and Vision Computing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_117",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_118@0",
            "content": "Matthew Shardlow, Richard Evans, Gustavo Paetzold, Marcos Zampieri, Semeval-2021 task 1: Lexical complexity prediction, 2021, Proceedings of the 14th International Workshop on Semantic Evaluation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_118",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_119@0",
            "content": "UNKNOWN, None, 2021, Predicting lexical complexity in english texts, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_119",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_120@0",
            "content": "Matthew Shardlow, Marcos Zampieri, Michael Cooper, Complex-a new corpus for lexical complexity predicition from likertscale data, 2020, Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_120",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_121@0",
            "content": "Yuhua Tang, Zhipeng Lin, Haotian Wang, Liyang Xu, Adversarial mixup synthesis training for unsupervised domain adaptation, 2020, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_121",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_122@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Proceedings of the 31st International Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_122",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_123@0",
            "content": "Giorgos Vernikos, Katerina Margatina, Alexandra Chronopoulou, Ion Androutsopoulos, Domain adversarial fine-tuning as an effective regularizer, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_123",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_124@0",
            "content": "Ke Wang, Gong Zhang, Henry Leung, Sar target recognition based on cross-domain and crosstask transfer learning, 2019, IEEE Access, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_124",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_125@0",
            "content": "Chris Seid Muhie Yimam, Shervin Biemann, Gustavo Malmasi, Lucia Paetzold, Sanja Specia,  \u0160tajner, A report on the complex word identification shared task, 2018, Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_125",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_126@0",
            "content": "George-Eduard Zaharia, Dumitru-Clementin Cercel, Mihai Dascalu, Cross-lingual transfer learning for complex word identification, 2020, 2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_126",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_127@0",
            "content": "George-Eduard Zaharia, Dumitru-Clementin Cercel, Mihai Dascalu, Upb at semeval-2021 task 1: Combining deep learning and hand-crafted features for lexical complexity prediction, 2021, Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_127",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_128@0",
            "content": "Dejiao Zhang, Ramesh Nallapati, Henghui Zhu, Feng Nan, Kathleen Cicero Dos Santos, Bing Mckeown,  Xiang, Unsupervised domain adaptation for cross-lingual text labeling, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_128",
            "start": 0,
            "end": 273,
            "label": {}
        },
        {
            "ix": "412-ARR_v2_129@0",
            "content": "Zhilu Zhang, R Mert,  Sabuncu, Generalized cross entropy loss for training deep neural networks with noisy labels, 2018, Proceedings of the 32nd International Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "412-ARR_v2_129",
            "start": 0,
            "end": 212,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "412-ARR_v2_0",
            "tgt_ix": "412-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_0",
            "tgt_ix": "412-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_1",
            "tgt_ix": "412-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_1",
            "tgt_ix": "412-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_0",
            "tgt_ix": "412-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_2",
            "tgt_ix": "412-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_4",
            "tgt_ix": "412-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_5",
            "tgt_ix": "412-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_7",
            "tgt_ix": "412-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_6",
            "tgt_ix": "412-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_8",
            "tgt_ix": "412-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_10",
            "tgt_ix": "412-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_11",
            "tgt_ix": "412-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_12",
            "tgt_ix": "412-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_13",
            "tgt_ix": "412-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_14",
            "tgt_ix": "412-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_16",
            "tgt_ix": "412-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_17",
            "tgt_ix": "412-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_18",
            "tgt_ix": "412-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_19",
            "tgt_ix": "412-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_9",
            "tgt_ix": "412-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_0",
            "tgt_ix": "412-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_20",
            "tgt_ix": "412-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_21",
            "tgt_ix": "412-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_21",
            "tgt_ix": "412-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_23",
            "tgt_ix": "412-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_24",
            "tgt_ix": "412-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_22",
            "tgt_ix": "412-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_22",
            "tgt_ix": "412-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_22",
            "tgt_ix": "412-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_22",
            "tgt_ix": "412-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_21",
            "tgt_ix": "412-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_25",
            "tgt_ix": "412-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_27",
            "tgt_ix": "412-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_28",
            "tgt_ix": "412-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_29",
            "tgt_ix": "412-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_30",
            "tgt_ix": "412-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_31",
            "tgt_ix": "412-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_32",
            "tgt_ix": "412-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_33",
            "tgt_ix": "412-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_34",
            "tgt_ix": "412-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_35",
            "tgt_ix": "412-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_36",
            "tgt_ix": "412-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_26",
            "tgt_ix": "412-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_26",
            "tgt_ix": "412-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_26",
            "tgt_ix": "412-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_26",
            "tgt_ix": "412-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_26",
            "tgt_ix": "412-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_26",
            "tgt_ix": "412-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_26",
            "tgt_ix": "412-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_26",
            "tgt_ix": "412-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_26",
            "tgt_ix": "412-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_26",
            "tgt_ix": "412-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_26",
            "tgt_ix": "412-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_26",
            "tgt_ix": "412-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_21",
            "tgt_ix": "412-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_37",
            "tgt_ix": "412-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_39",
            "tgt_ix": "412-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_40",
            "tgt_ix": "412-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_41",
            "tgt_ix": "412-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_38",
            "tgt_ix": "412-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_38",
            "tgt_ix": "412-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_38",
            "tgt_ix": "412-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_38",
            "tgt_ix": "412-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_38",
            "tgt_ix": "412-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_21",
            "tgt_ix": "412-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_42",
            "tgt_ix": "412-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_43",
            "tgt_ix": "412-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_43",
            "tgt_ix": "412-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_21",
            "tgt_ix": "412-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_44",
            "tgt_ix": "412-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_46",
            "tgt_ix": "412-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_47",
            "tgt_ix": "412-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_48",
            "tgt_ix": "412-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_49",
            "tgt_ix": "412-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_45",
            "tgt_ix": "412-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_45",
            "tgt_ix": "412-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_45",
            "tgt_ix": "412-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_45",
            "tgt_ix": "412-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_45",
            "tgt_ix": "412-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_45",
            "tgt_ix": "412-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_21",
            "tgt_ix": "412-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_50",
            "tgt_ix": "412-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_52",
            "tgt_ix": "412-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_53",
            "tgt_ix": "412-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_54",
            "tgt_ix": "412-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_55",
            "tgt_ix": "412-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_56",
            "tgt_ix": "412-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_57",
            "tgt_ix": "412-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_58",
            "tgt_ix": "412-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_59",
            "tgt_ix": "412-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_60",
            "tgt_ix": "412-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_61",
            "tgt_ix": "412-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_62",
            "tgt_ix": "412-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_51",
            "tgt_ix": "412-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_51",
            "tgt_ix": "412-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_51",
            "tgt_ix": "412-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_51",
            "tgt_ix": "412-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_51",
            "tgt_ix": "412-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_51",
            "tgt_ix": "412-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_51",
            "tgt_ix": "412-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_51",
            "tgt_ix": "412-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_51",
            "tgt_ix": "412-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_51",
            "tgt_ix": "412-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_51",
            "tgt_ix": "412-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_51",
            "tgt_ix": "412-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_51",
            "tgt_ix": "412-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_21",
            "tgt_ix": "412-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_63",
            "tgt_ix": "412-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_65",
            "tgt_ix": "412-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_64",
            "tgt_ix": "412-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_64",
            "tgt_ix": "412-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_64",
            "tgt_ix": "412-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_21",
            "tgt_ix": "412-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_66",
            "tgt_ix": "412-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_68",
            "tgt_ix": "412-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_69",
            "tgt_ix": "412-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_67",
            "tgt_ix": "412-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_67",
            "tgt_ix": "412-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_67",
            "tgt_ix": "412-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_67",
            "tgt_ix": "412-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_0",
            "tgt_ix": "412-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_70",
            "tgt_ix": "412-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_71",
            "tgt_ix": "412-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_71",
            "tgt_ix": "412-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_73",
            "tgt_ix": "412-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_72",
            "tgt_ix": "412-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_72",
            "tgt_ix": "412-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_72",
            "tgt_ix": "412-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_71",
            "tgt_ix": "412-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_74",
            "tgt_ix": "412-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_76",
            "tgt_ix": "412-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_75",
            "tgt_ix": "412-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_75",
            "tgt_ix": "412-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_75",
            "tgt_ix": "412-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_0",
            "tgt_ix": "412-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_77",
            "tgt_ix": "412-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_78",
            "tgt_ix": "412-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_78",
            "tgt_ix": "412-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_0",
            "tgt_ix": "412-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_79",
            "tgt_ix": "412-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_81",
            "tgt_ix": "412-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_80",
            "tgt_ix": "412-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_80",
            "tgt_ix": "412-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_80",
            "tgt_ix": "412-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "412-ARR_v2_0",
            "tgt_ix": "412-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_1",
            "tgt_ix": "412-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_2",
            "tgt_ix": "412-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_2",
            "tgt_ix": "412-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_2",
            "tgt_ix": "412-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_2",
            "tgt_ix": "412-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_2",
            "tgt_ix": "412-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_2",
            "tgt_ix": "412-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_2",
            "tgt_ix": "412-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_2",
            "tgt_ix": "412-ARR_v2_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_2",
            "tgt_ix": "412-ARR_v2_2@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_3",
            "tgt_ix": "412-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_4",
            "tgt_ix": "412-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_4",
            "tgt_ix": "412-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_4",
            "tgt_ix": "412-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_4",
            "tgt_ix": "412-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_4",
            "tgt_ix": "412-ARR_v2_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_5",
            "tgt_ix": "412-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_6",
            "tgt_ix": "412-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_7",
            "tgt_ix": "412-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_8",
            "tgt_ix": "412-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_9",
            "tgt_ix": "412-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_10",
            "tgt_ix": "412-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_11",
            "tgt_ix": "412-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_11",
            "tgt_ix": "412-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_11",
            "tgt_ix": "412-ARR_v2_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_12",
            "tgt_ix": "412-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_12",
            "tgt_ix": "412-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_12",
            "tgt_ix": "412-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_12",
            "tgt_ix": "412-ARR_v2_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_12",
            "tgt_ix": "412-ARR_v2_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_12",
            "tgt_ix": "412-ARR_v2_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_13",
            "tgt_ix": "412-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_13",
            "tgt_ix": "412-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_14",
            "tgt_ix": "412-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_15",
            "tgt_ix": "412-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_16",
            "tgt_ix": "412-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_16",
            "tgt_ix": "412-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_16",
            "tgt_ix": "412-ARR_v2_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_17",
            "tgt_ix": "412-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_17",
            "tgt_ix": "412-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_17",
            "tgt_ix": "412-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_17",
            "tgt_ix": "412-ARR_v2_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_17",
            "tgt_ix": "412-ARR_v2_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_17",
            "tgt_ix": "412-ARR_v2_17@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_18",
            "tgt_ix": "412-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_18",
            "tgt_ix": "412-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_19",
            "tgt_ix": "412-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_19",
            "tgt_ix": "412-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_19",
            "tgt_ix": "412-ARR_v2_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_19",
            "tgt_ix": "412-ARR_v2_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_20",
            "tgt_ix": "412-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_20",
            "tgt_ix": "412-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_20",
            "tgt_ix": "412-ARR_v2_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_20",
            "tgt_ix": "412-ARR_v2_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_20",
            "tgt_ix": "412-ARR_v2_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_21",
            "tgt_ix": "412-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_22",
            "tgt_ix": "412-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_23",
            "tgt_ix": "412-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_23",
            "tgt_ix": "412-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_23",
            "tgt_ix": "412-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_24",
            "tgt_ix": "412-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_24",
            "tgt_ix": "412-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_24",
            "tgt_ix": "412-ARR_v2_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_25",
            "tgt_ix": "412-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_25",
            "tgt_ix": "412-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_25",
            "tgt_ix": "412-ARR_v2_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_25",
            "tgt_ix": "412-ARR_v2_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_25",
            "tgt_ix": "412-ARR_v2_25@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_26",
            "tgt_ix": "412-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_27",
            "tgt_ix": "412-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_27",
            "tgt_ix": "412-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_27",
            "tgt_ix": "412-ARR_v2_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_27",
            "tgt_ix": "412-ARR_v2_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_27",
            "tgt_ix": "412-ARR_v2_27@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_28",
            "tgt_ix": "412-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_28",
            "tgt_ix": "412-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_28",
            "tgt_ix": "412-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_28",
            "tgt_ix": "412-ARR_v2_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_29",
            "tgt_ix": "412-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_30",
            "tgt_ix": "412-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_31",
            "tgt_ix": "412-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_32",
            "tgt_ix": "412-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_32",
            "tgt_ix": "412-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_32",
            "tgt_ix": "412-ARR_v2_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_33",
            "tgt_ix": "412-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_34",
            "tgt_ix": "412-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_35",
            "tgt_ix": "412-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_35",
            "tgt_ix": "412-ARR_v2_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_36",
            "tgt_ix": "412-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_37",
            "tgt_ix": "412-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_38",
            "tgt_ix": "412-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_39",
            "tgt_ix": "412-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_39",
            "tgt_ix": "412-ARR_v2_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_39",
            "tgt_ix": "412-ARR_v2_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_40",
            "tgt_ix": "412-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_41",
            "tgt_ix": "412-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_42",
            "tgt_ix": "412-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_42",
            "tgt_ix": "412-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_42",
            "tgt_ix": "412-ARR_v2_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_43",
            "tgt_ix": "412-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_44",
            "tgt_ix": "412-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_44",
            "tgt_ix": "412-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_44",
            "tgt_ix": "412-ARR_v2_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_44",
            "tgt_ix": "412-ARR_v2_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_44",
            "tgt_ix": "412-ARR_v2_44@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_44",
            "tgt_ix": "412-ARR_v2_44@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_45",
            "tgt_ix": "412-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_46",
            "tgt_ix": "412-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_46",
            "tgt_ix": "412-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_46",
            "tgt_ix": "412-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_47",
            "tgt_ix": "412-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_47",
            "tgt_ix": "412-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_47",
            "tgt_ix": "412-ARR_v2_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_48",
            "tgt_ix": "412-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_49",
            "tgt_ix": "412-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_49",
            "tgt_ix": "412-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_50",
            "tgt_ix": "412-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_51",
            "tgt_ix": "412-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_52",
            "tgt_ix": "412-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_52",
            "tgt_ix": "412-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_52",
            "tgt_ix": "412-ARR_v2_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_53",
            "tgt_ix": "412-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_53",
            "tgt_ix": "412-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_53",
            "tgt_ix": "412-ARR_v2_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_53",
            "tgt_ix": "412-ARR_v2_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_54",
            "tgt_ix": "412-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_55",
            "tgt_ix": "412-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_56",
            "tgt_ix": "412-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_56",
            "tgt_ix": "412-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_56",
            "tgt_ix": "412-ARR_v2_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_56",
            "tgt_ix": "412-ARR_v2_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_57",
            "tgt_ix": "412-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_57",
            "tgt_ix": "412-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_57",
            "tgt_ix": "412-ARR_v2_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_58",
            "tgt_ix": "412-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_58",
            "tgt_ix": "412-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_58",
            "tgt_ix": "412-ARR_v2_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_58",
            "tgt_ix": "412-ARR_v2_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_58",
            "tgt_ix": "412-ARR_v2_58@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_58",
            "tgt_ix": "412-ARR_v2_58@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_59",
            "tgt_ix": "412-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_60",
            "tgt_ix": "412-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_61",
            "tgt_ix": "412-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_62",
            "tgt_ix": "412-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_62",
            "tgt_ix": "412-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_62",
            "tgt_ix": "412-ARR_v2_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_62",
            "tgt_ix": "412-ARR_v2_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_63",
            "tgt_ix": "412-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_64",
            "tgt_ix": "412-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_65",
            "tgt_ix": "412-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_65",
            "tgt_ix": "412-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_65",
            "tgt_ix": "412-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_65",
            "tgt_ix": "412-ARR_v2_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_66",
            "tgt_ix": "412-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_67",
            "tgt_ix": "412-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_68",
            "tgt_ix": "412-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_68",
            "tgt_ix": "412-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_68",
            "tgt_ix": "412-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_68",
            "tgt_ix": "412-ARR_v2_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_69",
            "tgt_ix": "412-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_70",
            "tgt_ix": "412-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_70",
            "tgt_ix": "412-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_71",
            "tgt_ix": "412-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_72",
            "tgt_ix": "412-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_73",
            "tgt_ix": "412-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_73",
            "tgt_ix": "412-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_73",
            "tgt_ix": "412-ARR_v2_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_74",
            "tgt_ix": "412-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_74",
            "tgt_ix": "412-ARR_v2_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_74",
            "tgt_ix": "412-ARR_v2_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_74",
            "tgt_ix": "412-ARR_v2_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_75",
            "tgt_ix": "412-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_76",
            "tgt_ix": "412-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_76",
            "tgt_ix": "412-ARR_v2_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_76",
            "tgt_ix": "412-ARR_v2_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_76",
            "tgt_ix": "412-ARR_v2_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_76",
            "tgt_ix": "412-ARR_v2_76@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_76",
            "tgt_ix": "412-ARR_v2_76@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_76",
            "tgt_ix": "412-ARR_v2_76@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_77",
            "tgt_ix": "412-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_78",
            "tgt_ix": "412-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_79",
            "tgt_ix": "412-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_79",
            "tgt_ix": "412-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_79",
            "tgt_ix": "412-ARR_v2_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_79",
            "tgt_ix": "412-ARR_v2_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_79",
            "tgt_ix": "412-ARR_v2_79@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_79",
            "tgt_ix": "412-ARR_v2_79@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_80",
            "tgt_ix": "412-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_81",
            "tgt_ix": "412-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_81",
            "tgt_ix": "412-ARR_v2_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_81",
            "tgt_ix": "412-ARR_v2_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_81",
            "tgt_ix": "412-ARR_v2_81@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_82",
            "tgt_ix": "412-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_82",
            "tgt_ix": "412-ARR_v2_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_83",
            "tgt_ix": "412-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_84",
            "tgt_ix": "412-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_85",
            "tgt_ix": "412-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_86",
            "tgt_ix": "412-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_87",
            "tgt_ix": "412-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_88",
            "tgt_ix": "412-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_89",
            "tgt_ix": "412-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_90",
            "tgt_ix": "412-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_91",
            "tgt_ix": "412-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_92",
            "tgt_ix": "412-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_93",
            "tgt_ix": "412-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_94",
            "tgt_ix": "412-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_95",
            "tgt_ix": "412-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_96",
            "tgt_ix": "412-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_97",
            "tgt_ix": "412-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_98",
            "tgt_ix": "412-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_99",
            "tgt_ix": "412-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_100",
            "tgt_ix": "412-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_101",
            "tgt_ix": "412-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_102",
            "tgt_ix": "412-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_103",
            "tgt_ix": "412-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_104",
            "tgt_ix": "412-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_105",
            "tgt_ix": "412-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_106",
            "tgt_ix": "412-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_107",
            "tgt_ix": "412-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_108",
            "tgt_ix": "412-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_109",
            "tgt_ix": "412-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_110",
            "tgt_ix": "412-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_111",
            "tgt_ix": "412-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_112",
            "tgt_ix": "412-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_113",
            "tgt_ix": "412-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_114",
            "tgt_ix": "412-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_115",
            "tgt_ix": "412-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_116",
            "tgt_ix": "412-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_117",
            "tgt_ix": "412-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_118",
            "tgt_ix": "412-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_119",
            "tgt_ix": "412-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_120",
            "tgt_ix": "412-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_121",
            "tgt_ix": "412-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_122",
            "tgt_ix": "412-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_123",
            "tgt_ix": "412-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_124",
            "tgt_ix": "412-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_125",
            "tgt_ix": "412-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_126",
            "tgt_ix": "412-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_127",
            "tgt_ix": "412-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_128",
            "tgt_ix": "412-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "412-ARR_v2_129",
            "tgt_ix": "412-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 775,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "412-ARR",
        "version": 2
    }
}