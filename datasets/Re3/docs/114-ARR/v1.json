{
    "nodes": [
        {
            "ix": "114-ARR_v1_0",
            "content": "Saliency as Evidence: Event Detection with Trigger Saliency Attribution",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_2",
            "content": "Event detection (ED) is a critical subtask of event extraction that seeks to identify event triggers of certain types in texts. Despite significant advances in ED, existing methods typically follow a \"one model fits all types\" approach, which sees no differences between event types and often results in a quite skewed performance. Finding the causes of skewed performance is crucial for the robustness of an ED model, but to date there has been little exploration of this problem. This research examines the issue in depth and presents a new concept termed trigger salience attribution, which can explicitly quantify the underlying patterns of events. On this foundation, we develop a new training mechanism for ED, which can distinguish between triggerdependent and context-dependent types and achieve promising performance on two benchmarks. Finally, by highlighting many distinct characteristics of trigger-dependent and context-dependent types, our work may promote more research into this problem.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "114-ARR_v1_4",
            "content": "Event detection (ED), the first and a crucial step of event extraction, aims to identify events of certain types in texts (Ahn, 2006;Nguyen and Grishman, 2015;Mitamura et al., 2017). Previous methods to ED typically see no difference between event types and devise a single model to address them all (Ji and Grishman, 2008;Li et al., 2013;Chen et al., 2015;. However, such approaches indeed produce quite skewed performance on different types. Tasking the ACE benchmark as an example, we note the state-of-the-art ED model (Wadden et al., 2019) can strike 90% in F1 for the type DIVORCE, yet only 50% for the type START-POSITION; it is more surprising that the training set of DIVORCE is 8 times smaller than that of START-POSITION. Finding the causes underlying the skewed performance is crucial to the robust-S1: The couple divorced four years later. S2: He became the first US minister to England. ness of an ED model; however, this problem is still understudied in current research.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_5",
            "content": "This study takes a fresh look at the problem by attributing the skewed performance to the contextual patterns of events. Consider two typical cases of DIVORCE and START-POSITION shown in Figure 1. Intuitively, they have distinct patterns: the DI-VORCE event is more trigger-dependent, because the trigger word (divorced) is very indicative of the event's occurrence; by contrast, the START-POSITION event is more context-dependent -the event semantic is primarily expressed by contexts rather than the trigger (become), which is a merely light verb. We hypothesize an ED model performs poorly on context-dependent types because capturing context semantics is challenging (Lu et al., 2019;. With the above intuitions, two questions rise: (i) Can we estimate an event's pattern quantitatively? (ii)) How to robustify an ED model by characterizing such patterns?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_6",
            "content": "We introduce a brandy new concept called trigger saliency attribution that can explicitly quantify an event's contextual pattern. As shown in Figure 2, to determine how much an event depends on triggers/contexts, the key notion is to measure the trigger's contribution to expressing overall the event semantic. To this end, we first assign each sentence a global event label that represents the overall event semantic. Then, inspired by the feature attribution method (Simonyan et al., 2014;Sundararajan et al., 2017), we regard each word as a feature and compute its saliency value (i.e., contribution) for predicting the global event label. Finally, by examining the ground-truth trigger's saliency value, we can determine how much an event depends on triggers or contexts: a higher value, for example, indicates that the trigger contributes more to the event, implying the event is more trigger-dependent.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_7",
            "content": "We also develop a new training mechanism based on trigger saliency attribution, which uses saliency as evidence to enhance learning. Our method is simple yet effective -instead of using a single model to detect all event types, we group event types with similar patterns together (assessed by trigger saliency attribution) and develop separate models for each group. This strategy enables different models to capture distinct patterns. The model for context-dependent types, for example, can focus on mining contextual information for learning. Furthermore, we augment the above framework with two saliency-exploration strategy, which can explicitly integrate saliency information into learning and produce improved performance particularly for context-dependent types ( \u00a7 6.2).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_8",
            "content": "We have conducted extensive experiments on two ED benchmarks (i.e., ACE 2005(LDC, 2005 and MAVEN ) to verify the effectiveness of our approach. From the results: (i) Our trigger saliency attribution method does capture the underlying pattern and can well explain the skewed performance, obtaining Spearman's correlation coefficients of 0.72 and 0.61 with per-type F1 on ACE 2005 and MAVEN respectively; (ii) Our new training regime based on saliency demonstrates improved results on the two benchmarks. On ACE 2005, for example, it produces a 2% absolute gain in F1 over methods training different event types jointly. Finally, we compare and emphasize several significant aspects (e.g., linguistic and lexical patterns) of trigger-dependent and contextdependent event types, and our work may inspire future research into their differences.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_9",
            "content": "To summarize, our contributions are three-fold:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_10",
            "content": "\u2022 We investigate the causes of an ED model's skewed performance and develop a new concept called trigger saliency attribution, which can explicitly assess the underlying pattern of events. As a seminal study, our results may inspire further research into this problem. \u2022 We propose a new training mechanism for ED based on trigger saliency attribution, which achieves promising results on two bench-S1: The couple divorced four years later.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_11",
            "content": "S2: He became the first minister to England.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_12",
            "content": "[Divorce]",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_13",
            "content": "Step 1",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_14",
            "content": "High Contribution i / J w \uf0b6 \uf0b6",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_15",
            "content": "Step 2",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_16",
            "content": "Step 1",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_17",
            "content": "Step 2 marks, particularly when dealing with contextdependent event types.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_18",
            "content": "Low Contribution i / J w \uf0b6 \uf0b6",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_19",
            "content": "\u2022 We highlight many distinct patterns of triggerdependent and context-dependent event types, and our findings suggest that the traditional \"one model fits all types\" paradigm may need to be revised.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_20",
            "content": "Background and Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "114-ARR_v1_21",
            "content": "Event Detection. ED is a critical subtask of event extraction that seeks to locate event instances in text, which has received a lot of attention from researchers. Traditional methods for ED typically use fine-grained features (Ahn, 2006;Ji and Grishman, 2008;Liao and Grishman, 2010;Hong et al., 2011;Li et al., 2013), whereas newer methods rely on neural networks (Chen et al., 2015;Nguyen and Grishman, 2015;Feng et al., 2016;Nguyen and Nguyen, 2019), which have investigated the use of syntactic information (Liu et al., 2018;Lai et al., 2020), document-level cues (Wadden et al., 2019;Du and Cardie, 2020;Lai et al., 2021;Pouran Ben Veyseh et al., 2021;Li et al., 2021;Chen et al., 2021), and external supervision (Tong et al., 2020) to boost learning. However, most methods recognize no distinction between event types and train a single model to identify all event types, resulting in rather skewed performance on different event types. Two seminal works (Lu et al., 2019; have observed the comparatively poor performance on context-dependent texts and offered a better context-exploration strategy to improve training. Nonetheless, they are in a position to improve performance rather than investigate the root causes.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_22",
            "content": "Our approach, on the other hand, takes a fresh look at the issue and aims to define the underlying patterns of events for learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_23",
            "content": "Feature Attribution. The goal of feature attribution (FA) is to assess how important an input feature for model prediction, which has sparked a lot of interest in interpreting model decisions (Simonyan et al., 2014;Sundararajan et al., 2017).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_24",
            "content": "Formally, suppose we have an input vector x = (x 1 , x 2 , ..., x n ) \u2208 R n and a function F: R n \u2192 [0, 1] representing a model. The attribution value of x, with respect to the output F(x), is defined as a vector",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_25",
            "content": "A F (x) = (a 1 , a 2 , ..., a n ) \u2208 R n",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_26",
            "content": ", where a i measures the contribution of x i to F(x). The existing FA methods are classified as gradient-based methods, which consider the gradient of the output to the input as the attribution value (Simonyan et al., 2014;Springenberg et al., 2015), and reference-based methods, which consider the difference between the model's output and some \"reference\" output, in terms of the difference between the input and some \"reference\" input, as the attribution value (Ribeiro et al., 2016;Sundararajan et al., 2017).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_27",
            "content": "FA have been used to interpret model predictions in applications including image classification (Simonyan et al., 2014), machine translation (Ding et al., 2017), text classification (Chen et al., 2018), and others (Bastings and Filippova, 2020). To the best of our knowledge, this is the first work introducing FA to ED for quantifying the underlying event patterns.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_28",
            "content": "Integrated Gradient. Integrated Gradient (Sundararajan et al., 2017) is a specific (referencebased) FA method that views the feature attribution value as the accumulated gradient along the line between the model's input x and a reference input x , which denotes the lack of a feature 1 . Particularly, the attribution value of x i (i.e., the i th dimension of x) with respect to an output F(x) is defined as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_29",
            "content": "ai = (xi \u2212 x i ) \u00d7 1 \u03b1=0 \u2202F(x + \u03b1 \u00d7 (x \u2212 x )) \u2202xi d\u03b1 (1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_30",
            "content": "where \u2202F (x) \u2202x i indicates the gradient of F(x) to x i . In our approach, we prefer Integrated Gradient to other FA methods due to its computing efficiency and effectiveness in addressing a wide range of text based tasks (Sundararajan et al., 2017;Liu and Avci, 2019;Bastings and Filippova, 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_31",
            "content": "1 In text related tasks, x is usually set as a sequence of embedding vectors with all zero values . Evaluate type-level saliency with Eq. ( 5); 10 end for",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_32",
            "content": "Trigger Saliency Attribution",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "114-ARR_v1_33",
            "content": "Algorithm 1 provides an overview of our trigger saliency attribution method, which consists of three major steps: (i) sentence-level event classification, (ii) word-level saliency estimation, and (iii) typelevel saliency estimation. Let s = [w 1 , w 2 , \u2022 \u2022 \u2022 , w N ] be a sentence of N words, and the ED task corresponds to predicting an event label sequence",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_34",
            "content": "Y s = [y 1 , y 2 , \u2022 \u2022 \u2022 , y N ],",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_35",
            "content": "where y i \u2208 T \u222a {O} indicates the event label of w i , T is a set containing all pre-defined event types, and O is a \"null type\" denoting no-trigger words.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_36",
            "content": "Sentence-Level Event Classification. We start by giving s a sentence-level event label G s , which represents the overall event semantic. Let the label be",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_37",
            "content": "G s = [g 1 , g 2 , ..., g |T | ] \u2208 R |T | , where g i \u2208 {0, 1}",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_38",
            "content": "indicates whether a trigger of the i th event type is contained by s (g i =1) or not (g i =0). Following that, we construct a sentence-level event classifier and aim to learn a mapping from s to G s . Particularly, we devise a BERT based sentence classifier (Devlin et al., 2019) and adopt a multi-label binary crossentropy loss for optimization:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_39",
            "content": "L(Gs; Xs) = \u2212 1 |T | |T | i=1 gi \u2022 log(o s i ) + (1 \u2212 gi) \u2022 log(1 \u2212 o s i ) (2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_40",
            "content": "where X s is the input embedding of s in BERT, o s \u2208 R |T | indicates the logits vector computed by the classier, and o s i denotes the i th element of o s .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_41",
            "content": "Word-Level Saliency Estimation. Based on the sentence-level classifier, we next use Integrated Gradient (Sundararajan et al., 2017) to calculate the contribution (i.e., saliency value) of each word to the prediction. We utilize the loss function as the desired model , and calculate the saliency of w i , more accurately, its BERT representation x i \u2208 X s , regarding the loss by:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_42",
            "content": "\u03b1w i = (xi \u2212 x i )\u00d7 1 \u03b1=0 \u2202L(Gs; X + \u03b1 \u00d7 (Xs \u2212 X )) \u2202xi d\u03b1 (3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_43",
            "content": "where X is a sequence of all-zero vectors (serving as a reference input), and x i denotes the i th element in X . We then normalize \u03b1 w i as a scalar value \u03b1 w i with a sentence-wise normalization:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_44",
            "content": "\u03b1 w i = e \u03b1w i 2 / N n=1 e \u03b1w n 2 (4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_45",
            "content": "where denotes the L 2 norm. In actuality, we may not be concerned with a word's saliency to the general event semantic G s , but rather with a specific event type T \u2208 T . To this end, we replace G s with the one-hot representation of T in Equation (3) for evaluation. Finally, we represent the word-level saliency of w i with respect to the event type T by \u03b1 (T ) w i , and we suppose \u03b1 (T ) w i = 0 if the sentence does not describe any event of type T .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_46",
            "content": "Type-Level Saliency Estimation. Based on the word-level saliency, we measure the type-level trigger saliency value (regarding an event type T ) as: (i) Word Saliency Embeddings. Given that trigger-dependent types often have indicative triggers, we build a mechanism called word saliency embeddings (WSEs) in the model for T trigger to capture such regularities. Specifically, we first quantify each word's saliency value 3 as 0 or 1 based on \u03bb, i.e., the threshold we used previously for distinguishing event types, and then use a separate embedding vector to distinguish 0 and 1, similar to word embeddings. Such embeddings are incorporated into the model 4 to capture a regularity that words with high saliency values are more likely to be triggers. Note WSEs are also incorporated in the model for the T context , which on the other hand seeks to learn the opposite regularity that words with high saliency values may not be triggers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_47",
            "content": "SL(T ) = (s,Ys) w\u2208{w i |y i =T } \u03b1 (T ) w # of",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_48",
            "content": "(ii) Saliency as Context Evidence. In the event detector for T context , we also devise a regime for interpreting salient information as context evidence for reasoning. Consider the previous example S2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_49",
            "content": "Our method identifies the context words \"US minister\" as the most salient words (with saliency values larger than \u03bb) expressing the overall event semantic.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_50",
            "content": "Here we regard salient contexts as supplementary evidence and concatenate them with the sentence for learning, as shown in the bottom of Figure 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_51",
            "content": "Compared with WSEs, this method can additional capture the lexical semantics of the salient words, which has been shown to considerably aid in the recognition of context-dependent event types ( \u00a7 7).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_52",
            "content": "Model Ensemble. In the testing stage, we combine the results of two models to make a final prediction. If ambiguous cases occur, i.e., the two ED models predict different event types for the same word, we use the type with a higher probability as the result. We use cross-entropy loss for optimization. For example, the model for T trigger is trained by minimizing the following loss:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_53",
            "content": "L = \u2212 (s,Ys) (w i ,y i )\u2208(s,Ys) log P (y i |w i )(6",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_54",
            "content": ") where (s, Y s ) refers to each training instance; (w i , y i 5 ) ranges over each pair of word and its groundtruth event label; P (y i |w i ) denotes the conditional probability that the model predicts y i for w i . We use Adam (Kingma and Ba, 2015) with default hyper-parameters for parameter update.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_55",
            "content": "Experimental Setups",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "114-ARR_v1_56",
            "content": "Datasets. We conduct experiments on ACE 2005 (LDC, 2005) and MAVEN documents. We adopt a common split for evaluation following previous works (Li et al., 2013;Wadden et al., 2019). MAVEN is a newly released corpus defining 168 more fine-grained event types . Because the MAVEN test set is not publicly available and our study is concerned with per-type performance, we instead use the MAVEN development set for assessment and divide the original MAVEN training set as 9:1 for training and validating. Table 1 displays the comprehensive data statistics for the two datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_57",
            "content": "Evaluation Metrics. We adopt the following metrics to evaluate our model: (i) Spearman's rank correlation coefficient, which can determine the statistical dependency between two ranked variable sequences. The metric is defined as \u03c1 = 1 \u2212",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_58",
            "content": "6 d 2 i n(n 2 \u22121)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_59",
            "content": ", where d i is the difference between the i th pair of ranked variables, and n is the sequence length. We use it to measure how well our trigger saliency attribution results correlate with per-type model performance. (ii) Precision (P), Recall (R) and (Micro) F1, which are widely used to assess the overall performance of an ED model. (iii) Macro F1, the arithmetic mean of class-wise F1-scores, which will be low for models that only perform well on common types but badly on rare types.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_60",
            "content": "Implementations. In our trigger saliency attribution method, the sentence-level classifier is built on the BERT-base. The batch size is set to 20, and the learning rate is set to 1e-5. After 5 epochs, it achieves 74.8% in F1 on the ACE 2005 development set, matching the state-of-the-art performance . As for the two ED models, we consider BERT-base architectures. The batch size is set to 20, chosen from [1,5,10,20,30]. The learning rate is set to 1e-5, chosen from a range from 1e-3 to 1e-6. The dimension of word saliency embeddings is empirically set to 100. To allow for further investigation, we have made our code publicly available at http://anomynous.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_61",
            "content": "6 Experimental Results",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_62",
            "content": "Results of Correlation Measurement",
            "ntype": "title",
            "meta": {
                "section": "6.1"
            }
        },
        {
            "ix": "114-ARR_v1_63",
            "content": "Table 2 shows the Spearman's rank correlation between per-type F1 and four criteria: 1) the number of training instances (regarding an event type); 2) trigger variance, defined as the ratio of the number of unique event triggers to the total number of event triggers (regarding an event type); 3) trigger attention value, which corresponds to the groundtruth trigger's attention value in the BERT model; 4) trigger saliency attribution (our method). We use a state-of-the-art ED model (Wadden et al., 2019) and perform a 5-run average on the development set to obtain the per-type F1 score.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_64",
            "content": "According to the results, our trigger saliency attribution approach correlates the best with model performance, yielding a score as high as 0.72 and 0.61 in Spearman's \u03c1 correlation. This suggests that our method can well explain the skewed performance. Our other findings are interesting: (i) Surprisingly, the number of training examples shows a negligible correlation (\u03c1 = 0.06 and 0.09) with per-type F1. This implies that simply collecting more training data may not be an effective way to improve an ED model. (ii) The trigger variance metric demonstrates a moderate association (\u03c1 = 0.25 and 0,26), indicating that the diversity of event triggers is a factor influencing model performance. (iii) The trigger attention value also shows a poor association, which may be another proof that attention is not explainable (Jain and Wallace, 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_65",
            "content": "Lastly, Figure 4 visualizes correlations between per-type F1 and the number of training instances and our trigger saliency attribution method. In addition to noting that our method adequately explains the per-type F1-score, we find that \u03bb = 0.25 may be a good threshold for distinguishing between triggerdependent and context-dependent event types.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_66",
            "content": "Results of Saliency Enhanced ED",
            "ntype": "title",
            "meta": {
                "section": "6.2"
            }
        },
        {
            "ix": "114-ARR_v1_67",
            "content": "To test the efficacy of our saliency enhanced ED model: 1) For ACE 2005, we compare our model with (i) DYGIE++ (Wadden et al., 2019), which uses a graph view to learn context features; (ii) Trig-gerQA (Du and Cardie, 2020), which uses a question answering formulation for the task; (iii) OneIE , which adopts cross-sentence features for the task. Because pre-processing has a significant impact on the results (Orr et al., 2018), to ensure a fair comparison, we only consider models using the same pre-processing steps as in (Wadden et al., 2019). 2) For MAVEN, we use the BERT+CRF proposed in the original work for comparison. As a baseline, we also construct a model called BERTEns, which ensembles two BERT models similar to ours but does not differentiate event types. We refer to our approach that merely separates event types for learning (without saliency-exploration strategies) as SaliencyED (SL), and our full approach as SaliencyED (Full). (SL), which only differentiates event types for training, outperforms BERTEns by 1.6% in F1. This emphasizes the significance of identifying event patterns for ED. (iii) Our method gives the best Macro F1 on two datasets, indicating that it performs well on both common and rare event types.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_68",
            "content": "Table 4 shows the performance breakdown for trigger-dependent (TD) and context-dependent (CD) types. According to the results, different models consistently produce good performance on TD types but low performance on CD types, implying that the patterns found by our trigger saliency attribution method are reasonable. When comparing SaliencyED (SL) and SaliencyED (Full), we see that the saliency-exploring method is more effective on CD types (+2.3% in F1) than on TD types (+0.3% in F1). This makes sense because detecting context-dependent events relies significantly on context reasoning, and our method can just use important contexts as evidence to improve learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_69",
            "content": "Discussion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "114-ARR_v1_70",
            "content": "Ablation Study. We undertake an ablation study in Table 5 to investigate different model components, using the more challenging contextdependent (CD) types as an example. In the variant models, +WSE and +Evidence denote supplementing SaliencyED (SL) with word saliency embeddings and context evidence, respectively. +MaskAtt is an approach for calculating attention that masks the word itself, which can drive the model to focus more on contexts for learning; +Gold Argument is an oracle method that uses gold event arguments as evidence for learning. Based on the results, +Evidence outperforms +WSE and +MaskAtt, indicating its efficacy. Interestingly, +MaskAtt also boosts performance, implying that the contexts of CD events do carry important information for asserting the event. Finally, the superior performance of +Gold Arguments implies that finding indicative evidence (e.g., event arguments) is the key factor boosting learning on CD types.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_71",
            "content": "Impact of Event Type Division. We use our event type division method as a baseline and compare it to three other event type division strategies: 1) at random; 2) based on the amount of training instances; 3) based on development set performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_72",
            "content": "According to the results, the first two strategies decrease performance by 1.27% and 1.41% in Micro F1 on ACE, and 1.53% and 1.40% on MAVEN, which suggests that an inappropriate separation of event types impairs learning. The third strategy based on development performance improves learning (+0.8%/+1.1% on ACE/MAVEN), but it is still inferior to our approach. An explanation is that the final model performance is the product of a combination of factors, and thus categorizing event types based on development set performance may not assure that event types with similar patterns are grouped together, resulting in inferior results. Distinctions in TD/CD Types. We use ACE 2005 as a case to highlight the distinct characteristics between TD and CD types. Figure 5 (Left) depicts the top k accuracy (hit@k) in the case where the most salient word in a sentence appears to be an event trigger; Figure 5 (Right) depicts the performance drop in an adversarial attack in which the gold event triggers are masked for sentencelevel event type classification. The CD and TD types exhibit opposing behaviors: TD types display excellent H@k accuracy but a significant performance loss in adversarial attack, whereas CD types exhibit the opposite tendency. This implies that the CD and TD types respectively rely on triggers and contexts. Figure 6 shows a comparison of the number of event arguments for TD and CD types.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_73",
            "content": "Clearly, CD types have a larger number of event arguments than TD types. This is also another indication that CD types rely on contexts -they require more arguments to convey an event.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_74",
            "content": "Linguistic/Lexical Insights. 1994). Accordingly, triggers of TD types are at the lower level of WordNet, with an average of 5.6 hypernyms; yet CD type triggers are at a higher level of WordNet, with 2.3 hypernyms. This finding supports our intuition that TD types are more concrete whereas CD types are more abstract.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_75",
            "content": "Case Visualization. Figure 7 depicts the saliency map of several cases. Accordingly, event triggers of TD types do usually have large saliency values. For example, case 2) is the instance of DIVORCE with the lowest trigger saliency value, which is still as high as 0.34. In contrast, event triggers of CD types typically have low saliency values. For example, case 4) and 6) show random instances of TRANSFER-MONEY and TRANSPORT, where the trigger saliency values are only 0.01.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_76",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "8"
            }
        },
        {
            "ix": "114-ARR_v1_77",
            "content": "In this study, we analyze the origins of an ED model's skewed performance and introduce a new notion called trigger saliency attribution to quantify the pattern of events. We devise a new training paradigm for ED that can distinguish between trigger-dependent and context-dependent types for learning, yielding promising results on two benchmarks. We also examine the differences between the two types extensively, and our work may promote future research on this problem. In the future, we would apply our method to other tasks (e.g., relation extraction) where contextual patterns matter.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_78",
            "content": "We provide the full set of event types in ACE (LDC, 2005) and MAVEN and their saliency values evaluated by our method.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "114-ARR_v1_79",
            "content": "David Ahn, The stages of event extraction, 2006, Proceedings of the Workshop on Annotating and Reasoning about Time and Events, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "David Ahn"
                ],
                "title": "The stages of event extraction",
                "pub_date": "2006",
                "pub_title": "Proceedings of the Workshop on Annotating and Reasoning about Time and Events",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_80",
            "content": "Jasmijn Bastings, Katja Filippova, The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?, 2020, Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Jasmijn Bastings",
                    "Katja Filippova"
                ],
                "title": "The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_81",
            "content": "Jianbo Chen, Le Song, Martin Wainwright, Michael Jordan, Learning to explain: An information-theoretic perspective on model interpretation, 2018, Proceedings of the 35th International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Jianbo Chen",
                    "Le Song",
                    "Martin Wainwright",
                    "Michael Jordan"
                ],
                "title": "Learning to explain: An information-theoretic perspective on model interpretation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 35th International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_82",
            "content": "Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun, Honey or poison? solving the trigger curse in few-shot event detection via causal intervention, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Jiawei Chen",
                    "Hongyu Lin",
                    "Xianpei Han",
                    "Le Sun"
                ],
                "title": "Honey or poison? solving the trigger curse in few-shot event detection via causal intervention",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "114-ARR_v1_83",
            "content": "Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, Jun Zhao, Event extraction via dynamic multipooling convolutional neural networks, 2015, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Yubo Chen",
                    "Liheng Xu",
                    "Kang Liu",
                    "Daojian Zeng",
                    "Jun Zhao"
                ],
                "title": "Event extraction via dynamic multipooling convolutional neural networks",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "114-ARR_v1_84",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "114-ARR_v1_85",
            "content": "Yanzhuo Ding, Yang Liu, Huanbo Luan, Maosong Sun, Visualizing and understanding neural machine translation, 2017, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Yanzhuo Ding",
                    "Yang Liu",
                    "Huanbo Luan",
                    "Maosong Sun"
                ],
                "title": "Visualizing and understanding neural machine translation",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "114-ARR_v1_86",
            "content": "Xinya Du, Claire Cardie, Event extraction by answering (almost) natural questions, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Xinya Du",
                    "Claire Cardie"
                ],
                "title": "Event extraction by answering (almost) natural questions",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_87",
            "content": "Xiaocheng Feng, Lifu Huang, Duyu Tang, Heng Ji, Bing Qin, Ting Liu, A languageindependent neural network for event detection, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Xiaocheng Feng",
                    "Lifu Huang",
                    "Duyu Tang",
                    "Heng Ji",
                    "Bing Qin",
                    "Ting Liu"
                ],
                "title": "A languageindependent neural network for event detection",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "114-ARR_v1_88",
            "content": "Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao, Guodong Zhou, Qiaoming Zhu, Using cross-entity inference to improve event extraction, 2011, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Yu Hong",
                    "Jianfeng Zhang",
                    "Bin Ma",
                    "Jianmin Yao",
                    "Guodong Zhou",
                    "Qiaoming Zhu"
                ],
                "title": "Using cross-entity inference to improve event extraction",
                "pub_date": "2011",
                "pub_title": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_89",
            "content": "Sarthak Jain, Byron Wallace, Attention is not Explanation, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Sarthak Jain",
                    "Byron Wallace"
                ],
                "title": "Attention is not Explanation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "114-ARR_v1_90",
            "content": "Heng Ji, Ralph Grishman, Refining event extraction through cross-document inference, 2008, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Heng Ji",
                    "Ralph Grishman"
                ],
                "title": "Refining event extraction through cross-document inference",
                "pub_date": "2008",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_91",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A method for stochastic optimization, 2015-05-07, 3rd International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "P Diederik",
                    "Jimmy Kingma",
                    " Ba"
                ],
                "title": "Adam: A method for stochastic optimization",
                "pub_date": "2015-05-07",
                "pub_title": "3rd International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_92",
            "content": "Viet Lai, Franck Dernoncourt, Thien Huu Nguyen, Learning prototype representations across few-shot tasks for event detection, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Viet Lai",
                    "Franck Dernoncourt",
                    "Thien Huu Nguyen"
                ],
                "title": "Learning prototype representations across few-shot tasks for event detection",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "114-ARR_v1_93",
            "content": "Tuan Viet Dac Lai, Thien Huu Ngo Nguyen,  Nguyen, Event detection: Gate diversity and syntactic importance scores for graph convolution neural networks, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Tuan Viet Dac Lai",
                    "Thien Huu Ngo Nguyen",
                    " Nguyen"
                ],
                "title": "Event detection: Gate diversity and syntactic importance scores for graph convolution neural networks",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_94",
            "content": "UNKNOWN, None, , Automatic Content Extraction) English Annotation Guidelines for Events, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Automatic Content Extraction) English Annotation Guidelines for Events",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_95",
            "content": "Qi Li, Ji Heng, Liang Huang, Joint event extraction via structured prediction with global features, 2013, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Qi Li",
                    "Ji Heng",
                    "Liang Huang"
                ],
                "title": "Joint event extraction via structured prediction with global features",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "114-ARR_v1_96",
            "content": "Rui Li, Wenlin Zhao, Cheng Yang, Sen Su, Treasures outside contexts: Improving event detection via global statistics, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Rui Li",
                    "Wenlin Zhao",
                    "Cheng Yang",
                    "Sen Su"
                ],
                "title": "Treasures outside contexts: Improving event detection via global statistics",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "114-ARR_v1_97",
            "content": "Shasha Liao, Ralph Grishman, Using document level cross-event inference to improve event extraction, 2010, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Shasha Liao",
                    "Ralph Grishman"
                ],
                "title": "Using document level cross-event inference to improve event extraction",
                "pub_date": "2010",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_98",
            "content": "Ying Lin, Heng Ji, Fei Huang, Lingfei Wu, A joint neural model for information extraction with global features, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Ying Lin",
                    "Heng Ji",
                    "Fei Huang",
                    "Lingfei Wu"
                ],
                "title": "A joint neural model for information extraction with global features",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "114-ARR_v1_99",
            "content": "Frederick Liu, Besim Avci, Incorporating priors with feature attribution on text classification, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Frederick Liu",
                    "Besim Avci"
                ],
                "title": "Incorporating priors with feature attribution on text classification",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_100",
            "content": "Jian Liu, Yubo Chen, Kang Liu, Yantao Jia, Zhicheng Sheng, How does context matter? on the robustness of event detection with contextselective mask generalization, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Jian Liu",
                    "Yubo Chen",
                    "Kang Liu",
                    "Yantao Jia",
                    "Zhicheng Sheng"
                ],
                "title": "How does context matter? on the robustness of event detection with contextselective mask generalization",
                "pub_date": "2020",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "114-ARR_v1_101",
            "content": "Shulin Liu, Yang Li, Feng Zhang, Tao Yang, Xinpeng Zhou, Event detection without triggers, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Shulin Liu",
                    "Yang Li",
                    "Feng Zhang",
                    "Tao Yang",
                    "Xinpeng Zhou"
                ],
                "title": "Event detection without triggers",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "114-ARR_v1_102",
            "content": "Xiao Liu, Zhunchen Luo, Heyan Huang, Jointly multiple events extraction via attentionbased graph information aggregation, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Xiao Liu",
                    "Zhunchen Luo",
                    "Heyan Huang"
                ],
                "title": "Jointly multiple events extraction via attentionbased graph information aggregation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "114-ARR_v1_103",
            "content": "Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, Distilling discrimination and generalization knowledge for event detection via deltarepresentation learning, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Yaojie Lu",
                    "Hongyu Lin",
                    "Xianpei Han",
                    "Le Sun"
                ],
                "title": "Distilling discrimination and generalization knowledge for event detection via deltarepresentation learning",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "114-ARR_v1_104",
            "content": "George Miller, WordNet: A lexical database for English, 1994-03-08, Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "George Miller"
                ],
                "title": "WordNet: A lexical database for English",
                "pub_date": "1994-03-08",
                "pub_title": "Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_105",
            "content": "T Mitamura, Zhengzhong Liu, E Hovy, Events detection, coreference and sequencing: What's next? overview of the tac kbp 2017 event track, 2017, Theory and Applications of Categories, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "T Mitamura",
                    "Zhengzhong Liu",
                    "E Hovy"
                ],
                "title": "Events detection, coreference and sequencing: What's next? overview of the tac kbp 2017 event track",
                "pub_date": "2017",
                "pub_title": "Theory and Applications of Categories",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_106",
            "content": "Huu Thien, Ralph Nguyen,  Grishman, Event detection and domain adaptation with convolutional neural networks, 2015, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Huu Thien",
                    "Ralph Nguyen",
                    " Grishman"
                ],
                "title": "Event detection and domain adaptation with convolutional neural networks",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "114-ARR_v1_107",
            "content": "Minh Trung, Thien Nguyen,  Huu Nguyen, One for all: Neural joint modeling of entities and events, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Minh Trung",
                    "Thien Nguyen",
                    " Huu Nguyen"
                ],
                "title": "One for all: Neural joint modeling of entities and events",
                "pub_date": "2019",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_108",
            "content": "Walker Orr, Prasad Tadepalli, Xiaoli Fern, Event detection with neural networks: A rigorous empirical evaluation, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Walker Orr",
                    "Prasad Tadepalli",
                    "Xiaoli Fern"
                ],
                "title": "Event detection with neural networks: A rigorous empirical evaluation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "114-ARR_v1_109",
            "content": "Ben Amir Pouran, Minh Veyseh, Nghia Van Nguyen, Bonan Ngo Trung, Thien Huu Min,  Nguyen, Modeling document-level context for event detection via important context selection, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Ben Amir Pouran",
                    "Minh Veyseh",
                    "Nghia Van Nguyen",
                    "Bonan Ngo Trung",
                    "Thien Huu Min",
                    " Nguyen"
                ],
                "title": "Modeling document-level context for event detection via important context selection",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "114-ARR_v1_110",
            "content": "Sameer Marco Tulio Ribeiro, Carlos Singh,  Guestrin, why should i trust you?\": Explaining the predictions of any classifier, 2016, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16, Association for Computing Machinery.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Sameer Marco Tulio Ribeiro",
                    "Carlos Singh",
                    " Guestrin"
                ],
                "title": "why should i trust you?\": Explaining the predictions of any classifier",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16",
                "pub": "Association for Computing Machinery"
            }
        },
        {
            "ix": "114-ARR_v1_111",
            "content": "Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, Deep inside convolutional networks: Visualising image classification models and saliency maps, 2014, Workshop at International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Karen Simonyan",
                    "Andrea Vedaldi",
                    "Andrew Zisserman"
                ],
                "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
                "pub_date": "2014",
                "pub_title": "Workshop at International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_112",
            "content": "J Springenberg, A Dosovitskiy, T Brox, M Riedmiller, Striving for simplicity: The all convolutional net, 2015, ICLR (workshop track, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "J Springenberg",
                    "A Dosovitskiy",
                    "T Brox",
                    "M Riedmiller"
                ],
                "title": "Striving for simplicity: The all convolutional net",
                "pub_date": "2015",
                "pub_title": "ICLR (workshop track",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_113",
            "content": "Mukund Sundararajan, Ankur Taly, Qiqi Yan, Axiomatic attribution for deep networks, 2017, Proceedings of the 34th International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Mukund Sundararajan",
                    "Ankur Taly",
                    "Qiqi Yan"
                ],
                "title": "Axiomatic attribution for deep networks",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 34th International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_114",
            "content": "Meihan Tong, Bin Xu, Shuai Wang, Yixin Cao, Lei Hou, Juanzi Li, Improving event detection via open-domain trigger knowledge, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Meihan Tong",
                    "Bin Xu",
                    "Shuai Wang",
                    "Yixin Cao",
                    "Lei Hou",
                    "Juanzi Li"
                ],
                "title": "Improving event detection via open-domain trigger knowledge",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_115",
            "content": "David Wadden, Ulme Wennberg, Yi Luan, Hannaneh Hajishirzi, Entity, relation, and event extraction with contextualized span representations, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "David Wadden",
                    "Ulme Wennberg",
                    "Yi Luan",
                    "Hannaneh Hajishirzi"
                ],
                "title": "Entity, relation, and event extraction with contextualized span representations",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_116",
            "content": "Eric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subramanian, Matt Gardner, Sameer Singh, AllenNLP interpret: A framework for explaining predictions of NLP models, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Eric Wallace",
                    "Jens Tuyls",
                    "Junlin Wang",
                    "Sanjay Subramanian",
                    "Matt Gardner",
                    "Sameer Singh"
                ],
                "title": "AllenNLP interpret: A framework for explaining predictions of NLP models",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_117",
            "content": "Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong Han, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin, Jie Zhou, MAVEN: A Massive General Domain Event Detection Dataset, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Xiaozhi Wang",
                    "Ziqi Wang",
                    "Xu Han",
                    "Wangyi Jiang",
                    "Rong Han",
                    "Zhiyuan Liu",
                    "Juanzi Li",
                    "Peng Li",
                    "Yankai Lin",
                    "Jie Zhou"
                ],
                "title": "MAVEN: A Massive General Domain Event Detection Dataset",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "114-ARR_v1_118",
            "content": "Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han, Songlin Hu, Conditional bert contextual augmentation, 2019, International Conference on Computational Science, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Xing Wu",
                    "Shangwen Lv",
                    "Liangjun Zang",
                    "Jizhong Han",
                    "Songlin Hu"
                ],
                "title": "Conditional bert contextual augmentation",
                "pub_date": "2019",
                "pub_title": "International Conference on Computational Science",
                "pub": "Springer"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "114-ARR_v1_0@0",
            "content": "Saliency as Evidence: Event Detection with Trigger Saliency Attribution",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_0",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_2@0",
            "content": "Event detection (ED) is a critical subtask of event extraction that seeks to identify event triggers of certain types in texts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_2",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_2@1",
            "content": "Despite significant advances in ED, existing methods typically follow a \"one model fits all types\" approach, which sees no differences between event types and often results in a quite skewed performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_2",
            "start": 128,
            "end": 330,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_2@2",
            "content": "Finding the causes of skewed performance is crucial for the robustness of an ED model, but to date there has been little exploration of this problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_2",
            "start": 332,
            "end": 480,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_2@3",
            "content": "This research examines the issue in depth and presents a new concept termed trigger salience attribution, which can explicitly quantify the underlying patterns of events.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_2",
            "start": 482,
            "end": 651,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_2@4",
            "content": "On this foundation, we develop a new training mechanism for ED, which can distinguish between triggerdependent and context-dependent types and achieve promising performance on two benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_2",
            "start": 653,
            "end": 843,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_2@5",
            "content": "Finally, by highlighting many distinct characteristics of trigger-dependent and context-dependent types, our work may promote more research into this problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_2",
            "start": 845,
            "end": 1002,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_4@0",
            "content": "Event detection (ED), the first and a crucial step of event extraction, aims to identify events of certain types in texts (Ahn, 2006;Nguyen and Grishman, 2015;Mitamura et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_4",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_4@1",
            "content": "Previous methods to ED typically see no difference between event types and devise a single model to address them all (Ji and Grishman, 2008;Li et al., 2013;Chen et al., 2015;. However, such approaches indeed produce quite skewed performance on different types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_4",
            "start": 183,
            "end": 442,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_4@2",
            "content": "Tasking the ACE benchmark as an example, we note the state-of-the-art ED model (Wadden et al., 2019) can strike 90% in F1 for the type DIVORCE, yet only 50% for the type START-POSITION; it is more surprising that the training set of DIVORCE is 8 times smaller than that of START-POSITION.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_4",
            "start": 444,
            "end": 731,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_4@3",
            "content": "Finding the causes underlying the skewed performance is crucial to the robust-S1: The couple divorced four years later.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_4",
            "start": 733,
            "end": 851,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_4@4",
            "content": "S2: He became the first US minister to England.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_4",
            "start": 853,
            "end": 899,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_4@5",
            "content": "ness of an ED model; however, this problem is still understudied in current research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_4",
            "start": 901,
            "end": 985,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_5@0",
            "content": "This study takes a fresh look at the problem by attributing the skewed performance to the contextual patterns of events.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_5",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_5@1",
            "content": "Consider two typical cases of DIVORCE and START-POSITION shown in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_5",
            "start": 121,
            "end": 195,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_5@2",
            "content": "Intuitively, they have distinct patterns: the DI-VORCE event is more trigger-dependent, because the trigger word (divorced) is very indicative of the event's occurrence; by contrast, the START-POSITION event is more context-dependent -the event semantic is primarily expressed by contexts rather than the trigger (become), which is a merely light verb.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_5",
            "start": 197,
            "end": 548,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_5@3",
            "content": "We hypothesize an ED model performs poorly on context-dependent types because capturing context semantics is challenging (Lu et al., 2019;. With the above intuitions, two questions rise: (i) Can we estimate an event's pattern quantitatively? (ii)) How to robustify an ED model by characterizing such patterns?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_5",
            "start": 550,
            "end": 858,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_6@0",
            "content": "We introduce a brandy new concept called trigger saliency attribution that can explicitly quantify an event's contextual pattern.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_6",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_6@1",
            "content": "As shown in Figure 2, to determine how much an event depends on triggers/contexts, the key notion is to measure the trigger's contribution to expressing overall the event semantic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_6",
            "start": 130,
            "end": 309,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_6@2",
            "content": "To this end, we first assign each sentence a global event label that represents the overall event semantic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_6",
            "start": 311,
            "end": 417,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_6@3",
            "content": "Then, inspired by the feature attribution method (Simonyan et al., 2014;Sundararajan et al., 2017), we regard each word as a feature and compute its saliency value (i.e., contribution) for predicting the global event label.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_6",
            "start": 419,
            "end": 641,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_6@4",
            "content": "Finally, by examining the ground-truth trigger's saliency value, we can determine how much an event depends on triggers or contexts: a higher value, for example, indicates that the trigger contributes more to the event, implying the event is more trigger-dependent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_6",
            "start": 643,
            "end": 907,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_7@0",
            "content": "We also develop a new training mechanism based on trigger saliency attribution, which uses saliency as evidence to enhance learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_7",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_7@1",
            "content": "Our method is simple yet effective -instead of using a single model to detect all event types, we group event types with similar patterns together (assessed by trigger saliency attribution) and develop separate models for each group.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_7",
            "start": 133,
            "end": 365,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_7@2",
            "content": "This strategy enables different models to capture distinct patterns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_7",
            "start": 367,
            "end": 434,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_7@3",
            "content": "The model for context-dependent types, for example, can focus on mining contextual information for learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_7",
            "start": 436,
            "end": 543,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_7@4",
            "content": "Furthermore, we augment the above framework with two saliency-exploration strategy, which can explicitly integrate saliency information into learning and produce improved performance particularly for context-dependent types ( \u00a7 6.2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_7",
            "start": 545,
            "end": 777,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_8@0",
            "content": "We have conducted extensive experiments on two ED benchmarks (i.e., ACE 2005(LDC, 2005 and MAVEN ) to verify the effectiveness of our approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_8",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_8@1",
            "content": "From the results: (i) Our trigger saliency attribution method does capture the underlying pattern and can well explain the skewed performance, obtaining Spearman's correlation coefficients of 0.72 and 0.61 with per-type F1 on ACE 2005 and MAVEN respectively; (ii) Our new training regime based on saliency demonstrates improved results on the two benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_8",
            "start": 144,
            "end": 501,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_8@2",
            "content": "On ACE 2005, for example, it produces a 2% absolute gain in F1 over methods training different event types jointly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_8",
            "start": 503,
            "end": 617,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_8@3",
            "content": "Finally, we compare and emphasize several significant aspects (e.g., linguistic and lexical patterns) of trigger-dependent and contextdependent event types, and our work may inspire future research into their differences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_8",
            "start": 619,
            "end": 839,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_9@0",
            "content": "To summarize, our contributions are three-fold:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_9",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_10@0",
            "content": "\u2022 We investigate the causes of an ED model's skewed performance and develop a new concept called trigger saliency attribution, which can explicitly assess the underlying pattern of events. As a seminal study, our results may inspire further research into this problem. \u2022 We propose a new training mechanism for ED based on trigger saliency attribution, which achieves promising results on two bench-S1: The couple divorced four years later.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_10",
            "start": 0,
            "end": 439,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_11@0",
            "content": "S2: He became the first minister to England.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_11",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_12@0",
            "content": "[Divorce]",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_12",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_13@0",
            "content": "Step 1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_13",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_14@0",
            "content": "High Contribution i / J w \uf0b6 \uf0b6",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_14",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_15@0",
            "content": "Step 2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_15",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_16@0",
            "content": "Step 1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_16",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_17@0",
            "content": "Step 2 marks, particularly when dealing with contextdependent event types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_17",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_18@0",
            "content": "Low Contribution i / J w \uf0b6 \uf0b6",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_18",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_19@0",
            "content": "\u2022 We highlight many distinct patterns of triggerdependent and context-dependent event types, and our findings suggest that the traditional \"one model fits all types\" paradigm may need to be revised.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_19",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_20@0",
            "content": "Background and Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_20",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_21@0",
            "content": "Event Detection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_21",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_21@1",
            "content": "ED is a critical subtask of event extraction that seeks to locate event instances in text, which has received a lot of attention from researchers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_21",
            "start": 17,
            "end": 162,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_21@2",
            "content": "Traditional methods for ED typically use fine-grained features (Ahn, 2006;Ji and Grishman, 2008;Liao and Grishman, 2010;Hong et al., 2011;Li et al., 2013), whereas newer methods rely on neural networks (Chen et al., 2015;Nguyen and Grishman, 2015;Feng et al., 2016;Nguyen and Nguyen, 2019), which have investigated the use of syntactic information (Liu et al., 2018;Lai et al., 2020), document-level cues (Wadden et al., 2019;Du and Cardie, 2020;Lai et al., 2021;Pouran Ben Veyseh et al., 2021;Li et al., 2021;Chen et al., 2021), and external supervision (Tong et al., 2020) to boost learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_21",
            "start": 164,
            "end": 756,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_21@3",
            "content": "However, most methods recognize no distinction between event types and train a single model to identify all event types, resulting in rather skewed performance on different event types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_21",
            "start": 758,
            "end": 942,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_21@4",
            "content": "Two seminal works (Lu et al., 2019; have observed the comparatively poor performance on context-dependent texts and offered a better context-exploration strategy to improve training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_21",
            "start": 944,
            "end": 1125,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_21@5",
            "content": "Nonetheless, they are in a position to improve performance rather than investigate the root causes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_21",
            "start": 1127,
            "end": 1225,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_22@0",
            "content": "Our approach, on the other hand, takes a fresh look at the issue and aims to define the underlying patterns of events for learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_22",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_23@0",
            "content": "Feature Attribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_23",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_23@1",
            "content": "The goal of feature attribution (FA) is to assess how important an input feature for model prediction, which has sparked a lot of interest in interpreting model decisions (Simonyan et al., 2014;Sundararajan et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_23",
            "start": 21,
            "end": 241,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_24@0",
            "content": "Formally, suppose we have an input vector x = (x 1 , x 2 , ..., x n ) \u2208 R n and a function F: R n \u2192 [0, 1] representing a model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_24",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_24@1",
            "content": "The attribution value of x, with respect to the output F(x), is defined as a vector",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_24",
            "start": 129,
            "end": 211,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_25@0",
            "content": "A F (x) = (a 1 , a 2 , ..., a n ) \u2208 R n",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_25",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_26@0",
            "content": ", where a i measures the contribution of x i to F(x).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_26",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_26@1",
            "content": "The existing FA methods are classified as gradient-based methods, which consider the gradient of the output to the input as the attribution value (Simonyan et al., 2014;Springenberg et al., 2015), and reference-based methods, which consider the difference between the model's output and some \"reference\" output, in terms of the difference between the input and some \"reference\" input, as the attribution value (Ribeiro et al., 2016;Sundararajan et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_26",
            "start": 54,
            "end": 512,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_27@0",
            "content": "FA have been used to interpret model predictions in applications including image classification (Simonyan et al., 2014), machine translation (Ding et al., 2017), text classification (Chen et al., 2018), and others (Bastings and Filippova, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_27",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_27@1",
            "content": "To the best of our knowledge, this is the first work introducing FA to ED for quantifying the underlying event patterns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_27",
            "start": 246,
            "end": 365,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_28@0",
            "content": "Integrated Gradient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_28",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_28@1",
            "content": "Integrated Gradient (Sundararajan et al., 2017) is a specific (referencebased) FA method that views the feature attribution value as the accumulated gradient along the line between the model's input x and a reference input x , which denotes the lack of a feature 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_28",
            "start": 21,
            "end": 286,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_28@2",
            "content": "Particularly, the attribution value of x i (i.e., the i th dimension of x) with respect to an output F(x) is defined as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_28",
            "start": 288,
            "end": 407,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_29@0",
            "content": "ai = (xi \u2212 x i ) \u00d7 1 \u03b1=0 \u2202F(x + \u03b1 \u00d7 (x \u2212 x )) \u2202xi d\u03b1 (1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_29",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_30@0",
            "content": "where \u2202F (x) \u2202x i indicates the gradient of F(x) to x i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_30",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_30@1",
            "content": "In our approach, we prefer Integrated Gradient to other FA methods due to its computing efficiency and effectiveness in addressing a wide range of text based tasks (Sundararajan et al., 2017;Liu and Avci, 2019;Bastings and Filippova, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_30",
            "start": 58,
            "end": 297,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_31@0",
            "content": "1 In text related tasks, x is usually set as a sequence of embedding vectors with all zero values .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_31",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_31@1",
            "content": "Evaluate type-level saliency with Eq. ( 5); 10 end for",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_31",
            "start": 100,
            "end": 153,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_32@0",
            "content": "Trigger Saliency Attribution",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_32",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_33@0",
            "content": "Algorithm 1 provides an overview of our trigger saliency attribution method, which consists of three major steps: (i) sentence-level event classification, (ii) word-level saliency estimation, and (iii) typelevel saliency estimation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_33",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_33@1",
            "content": "Let s = [w 1 , w 2 , \u2022 \u2022 \u2022 , w N ] be a sentence of N words, and the ED task corresponds to predicting an event label sequence",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_33",
            "start": 233,
            "end": 358,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_34@0",
            "content": "Y s = [y 1 , y 2 , \u2022 \u2022 \u2022 , y N ],",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_34",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_35@0",
            "content": "where y i \u2208 T \u222a {O} indicates the event label of w i , T is a set containing all pre-defined event types, and O is a \"null type\" denoting no-trigger words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_35",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_36@0",
            "content": "Sentence-Level Event Classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_36",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_36@1",
            "content": "We start by giving s a sentence-level event label G s , which represents the overall event semantic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_36",
            "start": 37,
            "end": 136,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_36@2",
            "content": "Let the label be",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_36",
            "start": 138,
            "end": 153,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_37@0",
            "content": "G s = [g 1 , g 2 , ..., g |T | ] \u2208 R |T | , where g i \u2208 {0, 1}",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_37",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_38@0",
            "content": "indicates whether a trigger of the i th event type is contained by s (g i =1) or not (g i =0).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_38",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_38@1",
            "content": "Following that, we construct a sentence-level event classifier and aim to learn a mapping from s to G s .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_38",
            "start": 95,
            "end": 199,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_38@2",
            "content": "Particularly, we devise a BERT based sentence classifier (Devlin et al., 2019) and adopt a multi-label binary crossentropy loss for optimization:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_38",
            "start": 201,
            "end": 345,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_39@0",
            "content": "L(Gs; Xs) = \u2212 1 |T | |T | i=1 gi \u2022 log(o s i ) + (1 \u2212 gi) \u2022 log(1 \u2212 o s i ) (2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_39",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_40@0",
            "content": "where X s is the input embedding of s in BERT, o s \u2208 R |T | indicates the logits vector computed by the classier, and o s i denotes the i th element of o s .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_40",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_41@0",
            "content": "Word-Level Saliency Estimation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_41",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_41@1",
            "content": "Based on the sentence-level classifier, we next use Integrated Gradient (Sundararajan et al., 2017) to calculate the contribution (i.e., saliency value) of each word to the prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_41",
            "start": 32,
            "end": 215,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_41@2",
            "content": "We utilize the loss function as the desired model , and calculate the saliency of w i , more accurately, its BERT representation x i \u2208 X s , regarding the loss by:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_41",
            "start": 217,
            "end": 379,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_42@0",
            "content": "\u03b1w i = (xi \u2212 x i )\u00d7 1 \u03b1=0 \u2202L(Gs; X + \u03b1 \u00d7 (Xs \u2212 X )) \u2202xi d\u03b1 (3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_42",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_43@0",
            "content": "where X is a sequence of all-zero vectors (serving as a reference input), and x i denotes the i th element in X .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_43",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_43@1",
            "content": "We then normalize \u03b1 w i as a scalar value \u03b1 w i with a sentence-wise normalization:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_43",
            "start": 114,
            "end": 196,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_44@0",
            "content": "\u03b1 w i = e \u03b1w i 2 / N n=1 e \u03b1w n 2 (4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_44",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_45@0",
            "content": "where denotes the L 2 norm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_45",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_45@1",
            "content": "In actuality, we may not be concerned with a word's saliency to the general event semantic G s , but rather with a specific event type T \u2208 T .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_45",
            "start": 28,
            "end": 169,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_45@2",
            "content": "To this end, we replace G s with the one-hot representation of T in Equation (3) for evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_45",
            "start": 171,
            "end": 266,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_45@3",
            "content": "Finally, we represent the word-level saliency of w i with respect to the event type T by \u03b1 (T ) w i , and we suppose \u03b1 (T ) w i = 0 if the sentence does not describe any event of type T .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_45",
            "start": 268,
            "end": 454,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_46@0",
            "content": "Type-Level Saliency Estimation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_46",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_46@1",
            "content": "Based on the word-level saliency, we measure the type-level trigger saliency value (regarding an event type T ) as: (i) Word Saliency Embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_46",
            "start": 32,
            "end": 176,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_46@2",
            "content": "Given that trigger-dependent types often have indicative triggers, we build a mechanism called word saliency embeddings (WSEs) in the model for T trigger to capture such regularities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_46",
            "start": 178,
            "end": 360,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_46@3",
            "content": "Specifically, we first quantify each word's saliency value 3 as 0 or 1 based on \u03bb, i.e., the threshold we used previously for distinguishing event types, and then use a separate embedding vector to distinguish 0 and 1, similar to word embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_46",
            "start": 362,
            "end": 607,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_46@4",
            "content": "Such embeddings are incorporated into the model 4 to capture a regularity that words with high saliency values are more likely to be triggers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_46",
            "start": 609,
            "end": 750,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_46@5",
            "content": "Note WSEs are also incorporated in the model for the T context , which on the other hand seeks to learn the opposite regularity that words with high saliency values may not be triggers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_46",
            "start": 752,
            "end": 936,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_47@0",
            "content": "SL(T ) = (s,Ys) w\u2208{w i |y i =T } \u03b1 (T ) w # of",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_47",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_48@0",
            "content": "(ii) Saliency as Context Evidence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_48",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_48@1",
            "content": "In the event detector for T context , we also devise a regime for interpreting salient information as context evidence for reasoning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_48",
            "start": 35,
            "end": 167,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_48@2",
            "content": "Consider the previous example S2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_48",
            "start": 169,
            "end": 201,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_49@0",
            "content": "Our method identifies the context words \"US minister\" as the most salient words (with saliency values larger than \u03bb) expressing the overall event semantic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_49",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_50@0",
            "content": "Here we regard salient contexts as supplementary evidence and concatenate them with the sentence for learning, as shown in the bottom of Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_50",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_51@0",
            "content": "Compared with WSEs, this method can additional capture the lexical semantics of the salient words, which has been shown to considerably aid in the recognition of context-dependent event types ( \u00a7 7).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_51",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_52@0",
            "content": "Model Ensemble.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_52",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_52@1",
            "content": "In the testing stage, we combine the results of two models to make a final prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_52",
            "start": 16,
            "end": 101,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_52@2",
            "content": "If ambiguous cases occur, i.e., the two ED models predict different event types for the same word, we use the type with a higher probability as the result.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_52",
            "start": 103,
            "end": 257,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_52@3",
            "content": "We use cross-entropy loss for optimization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_52",
            "start": 259,
            "end": 301,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_52@4",
            "content": "For example, the model for T trigger is trained by minimizing the following loss:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_52",
            "start": 303,
            "end": 383,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_53@0",
            "content": "L = \u2212 (s,Ys) (w i ,y i )\u2208(s,Ys) log P (y i |w i )(6",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_53",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_54@0",
            "content": ") where (s, Y s ) refers to each training instance; (w i , y i 5 ) ranges over each pair of word and its groundtruth event label; P (y i |w i ) denotes the conditional probability that the model predicts y i for w i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_54",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_54@1",
            "content": "We use Adam (Kingma and Ba, 2015) with default hyper-parameters for parameter update.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_54",
            "start": 218,
            "end": 302,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_55@0",
            "content": "Experimental Setups",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_55",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_56@0",
            "content": "Datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_56",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_56@1",
            "content": "We conduct experiments on ACE 2005 (LDC, 2005) and MAVEN documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_56",
            "start": 10,
            "end": 76,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_56@2",
            "content": "We adopt a common split for evaluation following previous works (Li et al., 2013;Wadden et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_56",
            "start": 78,
            "end": 179,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_56@3",
            "content": "MAVEN is a newly released corpus defining 168 more fine-grained event types .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_56",
            "start": 181,
            "end": 257,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_56@4",
            "content": "Because the MAVEN test set is not publicly available and our study is concerned with per-type performance, we instead use the MAVEN development set for assessment and divide the original MAVEN training set as 9:1 for training and validating.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_56",
            "start": 259,
            "end": 499,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_56@5",
            "content": "Table 1 displays the comprehensive data statistics for the two datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_56",
            "start": 501,
            "end": 572,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_57@0",
            "content": "Evaluation Metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_57",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_57@1",
            "content": "We adopt the following metrics to evaluate our model: (i) Spearman's rank correlation coefficient, which can determine the statistical dependency between two ranked variable sequences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_57",
            "start": 20,
            "end": 203,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_57@2",
            "content": "The metric is defined as \u03c1 = 1 \u2212",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_57",
            "start": 205,
            "end": 236,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_58@0",
            "content": "6 d 2 i n(n 2 \u22121)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_58",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_59@0",
            "content": ", where d i is the difference between the i th pair of ranked variables, and n is the sequence length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_59",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_59@1",
            "content": "We use it to measure how well our trigger saliency attribution results correlate with per-type model performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_59",
            "start": 103,
            "end": 215,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_59@2",
            "content": "(ii) Precision (P), Recall (R) and (Micro) F1, which are widely used to assess the overall performance of an ED model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_59",
            "start": 217,
            "end": 334,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_59@3",
            "content": "(iii) Macro F1, the arithmetic mean of class-wise F1-scores, which will be low for models that only perform well on common types but badly on rare types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_59",
            "start": 336,
            "end": 488,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_60@0",
            "content": "Implementations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_60",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_60@1",
            "content": "In our trigger saliency attribution method, the sentence-level classifier is built on the BERT-base.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_60",
            "start": 17,
            "end": 116,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_60@2",
            "content": "The batch size is set to 20, and the learning rate is set to 1e-5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_60",
            "start": 118,
            "end": 183,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_60@3",
            "content": "After 5 epochs, it achieves 74.8% in F1 on the ACE 2005 development set, matching the state-of-the-art performance .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_60",
            "start": 185,
            "end": 300,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_60@4",
            "content": "As for the two ED models, we consider BERT-base architectures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_60",
            "start": 302,
            "end": 363,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_60@5",
            "content": "The batch size is set to 20, chosen from [1,5,10,20,30].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_60",
            "start": 365,
            "end": 420,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_60@6",
            "content": "The learning rate is set to 1e-5, chosen from a range from 1e-3 to 1e-6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_60",
            "start": 422,
            "end": 493,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_60@7",
            "content": "The dimension of word saliency embeddings is empirically set to 100.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_60",
            "start": 495,
            "end": 562,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_60@8",
            "content": "To allow for further investigation, we have made our code publicly available at http://anomynous.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_60",
            "start": 564,
            "end": 660,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_61@0",
            "content": "6 Experimental Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_61",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_62@0",
            "content": "Results of Correlation Measurement",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_62",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_63@0",
            "content": "Table 2 shows the Spearman's rank correlation between per-type F1 and four criteria: 1) the number of training instances (regarding an event type); 2) trigger variance, defined as the ratio of the number of unique event triggers to the total number of event triggers (regarding an event type); 3) trigger attention value, which corresponds to the groundtruth trigger's attention value in the BERT model; 4) trigger saliency attribution (our method).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_63",
            "start": 0,
            "end": 448,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_63@1",
            "content": "We use a state-of-the-art ED model (Wadden et al., 2019) and perform a 5-run average on the development set to obtain the per-type F1 score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_63",
            "start": 450,
            "end": 589,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_64@0",
            "content": "According to the results, our trigger saliency attribution approach correlates the best with model performance, yielding a score as high as 0.72 and 0.61 in Spearman's \u03c1 correlation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_64",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_64@1",
            "content": "This suggests that our method can well explain the skewed performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_64",
            "start": 183,
            "end": 252,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_64@2",
            "content": "Our other findings are interesting: (i) Surprisingly, the number of training examples shows a negligible correlation (\u03c1 = 0.06 and 0.09) with per-type F1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_64",
            "start": 254,
            "end": 407,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_64@3",
            "content": "This implies that simply collecting more training data may not be an effective way to improve an ED model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_64",
            "start": 409,
            "end": 514,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_64@4",
            "content": "(ii) The trigger variance metric demonstrates a moderate association (\u03c1 = 0.25 and 0,26), indicating that the diversity of event triggers is a factor influencing model performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_64",
            "start": 516,
            "end": 695,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_64@5",
            "content": "(iii) The trigger attention value also shows a poor association, which may be another proof that attention is not explainable (Jain and Wallace, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_64",
            "start": 697,
            "end": 847,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_65@0",
            "content": "Lastly, Figure 4 visualizes correlations between per-type F1 and the number of training instances and our trigger saliency attribution method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_65",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_65@1",
            "content": "In addition to noting that our method adequately explains the per-type F1-score, we find that \u03bb = 0.25 may be a good threshold for distinguishing between triggerdependent and context-dependent event types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_65",
            "start": 143,
            "end": 347,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_66@0",
            "content": "Results of Saliency Enhanced ED",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_66",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_67@0",
            "content": "To test the efficacy of our saliency enhanced ED model: 1) For ACE 2005, we compare our model with (i) DYGIE++ (Wadden et al., 2019), which uses a graph view to learn context features; (ii) Trig-gerQA (Du and Cardie, 2020), which uses a question answering formulation for the task; (iii) OneIE , which adopts cross-sentence features for the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_67",
            "start": 0,
            "end": 345,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_67@1",
            "content": "Because pre-processing has a significant impact on the results (Orr et al., 2018), to ensure a fair comparison, we only consider models using the same pre-processing steps as in (Wadden et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_67",
            "start": 347,
            "end": 546,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_67@2",
            "content": "2) For MAVEN, we use the BERT+CRF proposed in the original work for comparison.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_67",
            "start": 548,
            "end": 626,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_67@3",
            "content": "As a baseline, we also construct a model called BERTEns, which ensembles two BERT models similar to ours but does not differentiate event types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_67",
            "start": 628,
            "end": 771,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_67@4",
            "content": "We refer to our approach that merely separates event types for learning (without saliency-exploration strategies) as SaliencyED (SL), and our full approach as SaliencyED (Full).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_67",
            "start": 773,
            "end": 949,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_67@5",
            "content": "(SL), which only differentiates event types for training, outperforms BERTEns by 1.6% in F1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_67",
            "start": 951,
            "end": 1042,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_67@6",
            "content": "This emphasizes the significance of identifying event patterns for ED.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_67",
            "start": 1044,
            "end": 1113,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_67@7",
            "content": "(iii) Our method gives the best Macro F1 on two datasets, indicating that it performs well on both common and rare event types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_67",
            "start": 1115,
            "end": 1241,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_68@0",
            "content": "Table 4 shows the performance breakdown for trigger-dependent (TD) and context-dependent (CD) types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_68",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_68@1",
            "content": "According to the results, different models consistently produce good performance on TD types but low performance on CD types, implying that the patterns found by our trigger saliency attribution method are reasonable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_68",
            "start": 101,
            "end": 317,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_68@2",
            "content": "When comparing SaliencyED (SL) and SaliencyED (Full), we see that the saliency-exploring method is more effective on CD types (+2.3% in F1) than on TD types (+0.3% in F1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_68",
            "start": 319,
            "end": 489,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_68@3",
            "content": "This makes sense because detecting context-dependent events relies significantly on context reasoning, and our method can just use important contexts as evidence to improve learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_68",
            "start": 491,
            "end": 672,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_69@0",
            "content": "Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_69",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_70@0",
            "content": "Ablation Study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_70",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_70@1",
            "content": "We undertake an ablation study in Table 5 to investigate different model components, using the more challenging contextdependent (CD) types as an example.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_70",
            "start": 16,
            "end": 169,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_70@2",
            "content": "In the variant models, +WSE and +Evidence denote supplementing SaliencyED (SL) with word saliency embeddings and context evidence, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_70",
            "start": 171,
            "end": 314,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_70@3",
            "content": "+MaskAtt is an approach for calculating attention that masks the word itself, which can drive the model to focus more on contexts for learning; +Gold Argument is an oracle method that uses gold event arguments as evidence for learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_70",
            "start": 316,
            "end": 550,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_70@4",
            "content": "Based on the results, +Evidence outperforms +WSE and +MaskAtt, indicating its efficacy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_70",
            "start": 552,
            "end": 638,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_70@5",
            "content": "Interestingly, +MaskAtt also boosts performance, implying that the contexts of CD events do carry important information for asserting the event.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_70",
            "start": 640,
            "end": 783,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_70@6",
            "content": "Finally, the superior performance of +Gold Arguments implies that finding indicative evidence (e.g., event arguments) is the key factor boosting learning on CD types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_70",
            "start": 785,
            "end": 950,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_71@0",
            "content": "Impact of Event Type Division.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_71",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_71@1",
            "content": "We use our event type division method as a baseline and compare it to three other event type division strategies: 1) at random; 2) based on the amount of training instances; 3) based on development set performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_71",
            "start": 31,
            "end": 244,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_72@0",
            "content": "According to the results, the first two strategies decrease performance by 1.27% and 1.41% in Micro F1 on ACE, and 1.53% and 1.40% on MAVEN, which suggests that an inappropriate separation of event types impairs learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_72",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_72@1",
            "content": "The third strategy based on development performance improves learning (+0.8%/+1.1% on ACE/MAVEN), but it is still inferior to our approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_72",
            "start": 222,
            "end": 360,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_72@2",
            "content": "An explanation is that the final model performance is the product of a combination of factors, and thus categorizing event types based on development set performance may not assure that event types with similar patterns are grouped together, resulting in inferior results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_72",
            "start": 362,
            "end": 633,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_72@3",
            "content": "Distinctions in TD/CD Types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_72",
            "start": 635,
            "end": 662,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_72@4",
            "content": "We use ACE 2005 as a case to highlight the distinct characteristics between TD and CD types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_72",
            "start": 664,
            "end": 755,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_72@5",
            "content": "Figure 5 (Left) depicts the top k accuracy (hit@k) in the case where the most salient word in a sentence appears to be an event trigger; Figure 5 (Right) depicts the performance drop in an adversarial attack in which the gold event triggers are masked for sentencelevel event type classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_72",
            "start": 757,
            "end": 1052,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_72@6",
            "content": "The CD and TD types exhibit opposing behaviors: TD types display excellent H@k accuracy but a significant performance loss in adversarial attack, whereas CD types exhibit the opposite tendency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_72",
            "start": 1054,
            "end": 1246,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_72@7",
            "content": "This implies that the CD and TD types respectively rely on triggers and contexts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_72",
            "start": 1248,
            "end": 1328,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_72@8",
            "content": "Figure 6 shows a comparison of the number of event arguments for TD and CD types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_72",
            "start": 1330,
            "end": 1410,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_73@0",
            "content": "Clearly, CD types have a larger number of event arguments than TD types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_73",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_73@1",
            "content": "This is also another indication that CD types rely on contexts -they require more arguments to convey an event.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_73",
            "start": 73,
            "end": 183,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_74@0",
            "content": "Linguistic/Lexical Insights.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_74",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_74@1",
            "content": "1994).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_74",
            "start": 29,
            "end": 34,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_74@2",
            "content": "Accordingly, triggers of TD types are at the lower level of WordNet, with an average of 5.6 hypernyms; yet CD type triggers are at a higher level of WordNet, with 2.3 hypernyms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_74",
            "start": 36,
            "end": 212,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_74@3",
            "content": "This finding supports our intuition that TD types are more concrete whereas CD types are more abstract.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_74",
            "start": 214,
            "end": 316,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_75@0",
            "content": "Case Visualization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_75",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_75@1",
            "content": "Figure 7 depicts the saliency map of several cases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_75",
            "start": 20,
            "end": 70,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_75@2",
            "content": "Accordingly, event triggers of TD types do usually have large saliency values.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_75",
            "start": 72,
            "end": 149,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_75@3",
            "content": "For example, case 2) is the instance of DIVORCE with the lowest trigger saliency value, which is still as high as 0.34.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_75",
            "start": 151,
            "end": 269,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_75@4",
            "content": "In contrast, event triggers of CD types typically have low saliency values.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_75",
            "start": 271,
            "end": 345,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_75@5",
            "content": "For example, case 4) and 6) show random instances of TRANSFER-MONEY and TRANSPORT, where the trigger saliency values are only 0.01.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_75",
            "start": 347,
            "end": 477,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_76@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_76",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_77@0",
            "content": "In this study, we analyze the origins of an ED model's skewed performance and introduce a new notion called trigger saliency attribution to quantify the pattern of events.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_77",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_77@1",
            "content": "We devise a new training paradigm for ED that can distinguish between trigger-dependent and context-dependent types for learning, yielding promising results on two benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_77",
            "start": 172,
            "end": 346,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_77@2",
            "content": "We also examine the differences between the two types extensively, and our work may promote future research on this problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_77",
            "start": 348,
            "end": 471,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_77@3",
            "content": "In the future, we would apply our method to other tasks (e.g., relation extraction) where contextual patterns matter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_77",
            "start": 473,
            "end": 589,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_78@0",
            "content": "We provide the full set of event types in ACE (LDC, 2005) and MAVEN and their saliency values evaluated by our method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_78",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_79@0",
            "content": "David Ahn, The stages of event extraction, 2006, Proceedings of the Workshop on Annotating and Reasoning about Time and Events, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_79",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_80@0",
            "content": "Jasmijn Bastings, Katja Filippova, The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?, 2020, Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_80",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_81@0",
            "content": "Jianbo Chen, Le Song, Martin Wainwright, Michael Jordan, Learning to explain: An information-theoretic perspective on model interpretation, 2018, Proceedings of the 35th International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_81",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_82@0",
            "content": "Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun, Honey or poison? solving the trigger curse in few-shot event detection via causal intervention, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_82",
            "start": 0,
            "end": 277,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_83@0",
            "content": "Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, Jun Zhao, Event extraction via dynamic multipooling convolutional neural networks, 2015, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_83",
            "start": 0,
            "end": 309,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_84@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_84",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_85@0",
            "content": "Yanzhuo Ding, Yang Liu, Huanbo Luan, Maosong Sun, Visualizing and understanding neural machine translation, 2017, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_85",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_86@0",
            "content": "Xinya Du, Claire Cardie, Event extraction by answering (almost) natural questions, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_86",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_87@0",
            "content": "Xiaocheng Feng, Lifu Huang, Duyu Tang, Heng Ji, Bing Qin, Ting Liu, A languageindependent neural network for event detection, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_87",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_88@0",
            "content": "Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao, Guodong Zhou, Qiaoming Zhu, Using cross-entity inference to improve event extraction, 2011, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_88",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_89@0",
            "content": "Sarthak Jain, Byron Wallace, Attention is not Explanation, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_89",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_90@0",
            "content": "Heng Ji, Ralph Grishman, Refining event extraction through cross-document inference, 2008, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_90",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_91@0",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A method for stochastic optimization, 2015-05-07, 3rd International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_91",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_92@0",
            "content": "Viet Lai, Franck Dernoncourt, Thien Huu Nguyen, Learning prototype representations across few-shot tasks for event detection, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_92",
            "start": 0,
            "end": 261,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_93@0",
            "content": "Tuan Viet Dac Lai, Thien Huu Ngo Nguyen,  Nguyen, Event detection: Gate diversity and syntactic importance scores for graph convolution neural networks, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_93",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_94@0",
            "content": "UNKNOWN, None, , Automatic Content Extraction) English Annotation Guidelines for Events, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_94",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_95@0",
            "content": "Qi Li, Ji Heng, Liang Huang, Joint event extraction via structured prediction with global features, 2013, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_95",
            "start": 0,
            "end": 236,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_96@0",
            "content": "Rui Li, Wenlin Zhao, Cheng Yang, Sen Su, Treasures outside contexts: Improving event detection via global statistics, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_96",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_97@0",
            "content": "Shasha Liao, Ralph Grishman, Using document level cross-event inference to improve event extraction, 2010, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_97",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_98@0",
            "content": "Ying Lin, Heng Ji, Fei Huang, Lingfei Wu, A joint neural model for information extraction with global features, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_98",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_99@0",
            "content": "Frederick Liu, Besim Avci, Incorporating priors with feature attribution on text classification, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_99",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_100@0",
            "content": "Jian Liu, Yubo Chen, Kang Liu, Yantao Jia, Zhicheng Sheng, How does context matter? on the robustness of event detection with contextselective mask generalization, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_100",
            "start": 0,
            "end": 290,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_101@0",
            "content": "Shulin Liu, Yang Li, Feng Zhang, Tao Yang, Xinpeng Zhou, Event detection without triggers, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_101",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_102@0",
            "content": "Xiao Liu, Zhunchen Luo, Heyan Huang, Jointly multiple events extraction via attentionbased graph information aggregation, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_102",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_103@0",
            "content": "Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, Distilling discrimination and generalization knowledge for event detection via deltarepresentation learning, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_103",
            "start": 0,
            "end": 289,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_104@0",
            "content": "George Miller, WordNet: A lexical database for English, 1994-03-08, Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_104",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_105@0",
            "content": "T Mitamura, Zhengzhong Liu, E Hovy, Events detection, coreference and sequencing: What's next? overview of the tac kbp 2017 event track, 2017, Theory and Applications of Categories, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_105",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_106@0",
            "content": "Huu Thien, Ralph Nguyen,  Grishman, Event detection and domain adaptation with convolutional neural networks, 2015, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_106",
            "start": 0,
            "end": 291,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_107@0",
            "content": "Minh Trung, Thien Nguyen,  Huu Nguyen, One for all: Neural joint modeling of entities and events, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_107",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_108@0",
            "content": "Walker Orr, Prasad Tadepalli, Xiaoli Fern, Event detection with neural networks: A rigorous empirical evaluation, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_108",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_109@0",
            "content": "Ben Amir Pouran, Minh Veyseh, Nghia Van Nguyen, Bonan Ngo Trung, Thien Huu Min,  Nguyen, Modeling document-level context for event detection via important context selection, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_109",
            "start": 0,
            "end": 309,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_110@0",
            "content": "Sameer Marco Tulio Ribeiro, Carlos Singh,  Guestrin, why should i trust you?\": Explaining the predictions of any classifier, 2016, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16, Association for Computing Machinery.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_110",
            "start": 0,
            "end": 275,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_111@0",
            "content": "Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, Deep inside convolutional networks: Visualising image classification models and saliency maps, 2014, Workshop at International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_111",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_112@0",
            "content": "J Springenberg, A Dosovitskiy, T Brox, M Riedmiller, Striving for simplicity: The all convolutional net, 2015, ICLR (workshop track, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_112",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_113@0",
            "content": "Mukund Sundararajan, Ankur Taly, Qiqi Yan, Axiomatic attribution for deep networks, 2017, Proceedings of the 34th International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_113",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_114@0",
            "content": "Meihan Tong, Bin Xu, Shuai Wang, Yixin Cao, Lei Hou, Juanzi Li, Improving event detection via open-domain trigger knowledge, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_114",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_115@0",
            "content": "David Wadden, Ulme Wennberg, Yi Luan, Hannaneh Hajishirzi, Entity, relation, and event extraction with contextualized span representations, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_115",
            "start": 0,
            "end": 323,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_116@0",
            "content": "Eric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subramanian, Matt Gardner, Sameer Singh, AllenNLP interpret: A framework for explaining predictions of NLP models, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_116",
            "start": 0,
            "end": 367,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_117@0",
            "content": "Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong Han, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin, Jie Zhou, MAVEN: A Massive General Domain Event Detection Dataset, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_117",
            "start": 0,
            "end": 271,
            "label": {}
        },
        {
            "ix": "114-ARR_v1_118@0",
            "content": "Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han, Songlin Hu, Conditional bert contextual augmentation, 2019, International Conference on Computational Science, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "114-ARR_v1_118",
            "start": 0,
            "end": 169,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "114-ARR_v1_0",
            "tgt_ix": "114-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_0",
            "tgt_ix": "114-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_1",
            "tgt_ix": "114-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_1",
            "tgt_ix": "114-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_0",
            "tgt_ix": "114-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_2",
            "tgt_ix": "114-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_4",
            "tgt_ix": "114-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_5",
            "tgt_ix": "114-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_6",
            "tgt_ix": "114-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_7",
            "tgt_ix": "114-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_8",
            "tgt_ix": "114-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_9",
            "tgt_ix": "114-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_11",
            "tgt_ix": "114-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_3",
            "tgt_ix": "114-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_3",
            "tgt_ix": "114-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_3",
            "tgt_ix": "114-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_3",
            "tgt_ix": "114-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_3",
            "tgt_ix": "114-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_3",
            "tgt_ix": "114-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_3",
            "tgt_ix": "114-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_3",
            "tgt_ix": "114-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_3",
            "tgt_ix": "114-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_3",
            "tgt_ix": "114-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_13",
            "tgt_ix": "114-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_14",
            "tgt_ix": "114-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_15",
            "tgt_ix": "114-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_16",
            "tgt_ix": "114-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_17",
            "tgt_ix": "114-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_18",
            "tgt_ix": "114-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_3",
            "tgt_ix": "114-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_3",
            "tgt_ix": "114-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_3",
            "tgt_ix": "114-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_3",
            "tgt_ix": "114-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_3",
            "tgt_ix": "114-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_3",
            "tgt_ix": "114-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_3",
            "tgt_ix": "114-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_12",
            "tgt_ix": "114-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_0",
            "tgt_ix": "114-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_21",
            "tgt_ix": "114-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_22",
            "tgt_ix": "114-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_23",
            "tgt_ix": "114-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_24",
            "tgt_ix": "114-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_25",
            "tgt_ix": "114-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_26",
            "tgt_ix": "114-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_27",
            "tgt_ix": "114-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_28",
            "tgt_ix": "114-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_29",
            "tgt_ix": "114-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_30",
            "tgt_ix": "114-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_20",
            "tgt_ix": "114-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_20",
            "tgt_ix": "114-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_20",
            "tgt_ix": "114-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_20",
            "tgt_ix": "114-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_20",
            "tgt_ix": "114-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_20",
            "tgt_ix": "114-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_20",
            "tgt_ix": "114-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_20",
            "tgt_ix": "114-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_20",
            "tgt_ix": "114-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_20",
            "tgt_ix": "114-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_20",
            "tgt_ix": "114-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_20",
            "tgt_ix": "114-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_0",
            "tgt_ix": "114-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_31",
            "tgt_ix": "114-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_33",
            "tgt_ix": "114-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_34",
            "tgt_ix": "114-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_35",
            "tgt_ix": "114-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_36",
            "tgt_ix": "114-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_37",
            "tgt_ix": "114-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_38",
            "tgt_ix": "114-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_39",
            "tgt_ix": "114-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_40",
            "tgt_ix": "114-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_41",
            "tgt_ix": "114-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_42",
            "tgt_ix": "114-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_43",
            "tgt_ix": "114-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_44",
            "tgt_ix": "114-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_45",
            "tgt_ix": "114-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_46",
            "tgt_ix": "114-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_47",
            "tgt_ix": "114-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_48",
            "tgt_ix": "114-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_49",
            "tgt_ix": "114-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_50",
            "tgt_ix": "114-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_51",
            "tgt_ix": "114-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_52",
            "tgt_ix": "114-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_53",
            "tgt_ix": "114-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_0",
            "tgt_ix": "114-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_54",
            "tgt_ix": "114-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_56",
            "tgt_ix": "114-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_57",
            "tgt_ix": "114-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_58",
            "tgt_ix": "114-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_59",
            "tgt_ix": "114-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_60",
            "tgt_ix": "114-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_55",
            "tgt_ix": "114-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_55",
            "tgt_ix": "114-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_55",
            "tgt_ix": "114-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_55",
            "tgt_ix": "114-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_55",
            "tgt_ix": "114-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_55",
            "tgt_ix": "114-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_55",
            "tgt_ix": "114-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_0",
            "tgt_ix": "114-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_61",
            "tgt_ix": "114-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_63",
            "tgt_ix": "114-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_64",
            "tgt_ix": "114-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_62",
            "tgt_ix": "114-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_62",
            "tgt_ix": "114-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_62",
            "tgt_ix": "114-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_62",
            "tgt_ix": "114-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_0",
            "tgt_ix": "114-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_65",
            "tgt_ix": "114-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_67",
            "tgt_ix": "114-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_66",
            "tgt_ix": "114-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_66",
            "tgt_ix": "114-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_66",
            "tgt_ix": "114-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_0",
            "tgt_ix": "114-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_68",
            "tgt_ix": "114-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_70",
            "tgt_ix": "114-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_71",
            "tgt_ix": "114-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_72",
            "tgt_ix": "114-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_73",
            "tgt_ix": "114-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_74",
            "tgt_ix": "114-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_69",
            "tgt_ix": "114-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_69",
            "tgt_ix": "114-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_69",
            "tgt_ix": "114-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_69",
            "tgt_ix": "114-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_69",
            "tgt_ix": "114-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_69",
            "tgt_ix": "114-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_69",
            "tgt_ix": "114-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_0",
            "tgt_ix": "114-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_75",
            "tgt_ix": "114-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_76",
            "tgt_ix": "114-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_76",
            "tgt_ix": "114-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_76",
            "tgt_ix": "114-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_77",
            "tgt_ix": "114-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "114-ARR_v1_0",
            "tgt_ix": "114-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_1",
            "tgt_ix": "114-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_2",
            "tgt_ix": "114-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_2",
            "tgt_ix": "114-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_2",
            "tgt_ix": "114-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_2",
            "tgt_ix": "114-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_2",
            "tgt_ix": "114-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_2",
            "tgt_ix": "114-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_3",
            "tgt_ix": "114-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_4",
            "tgt_ix": "114-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_4",
            "tgt_ix": "114-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_4",
            "tgt_ix": "114-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_4",
            "tgt_ix": "114-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_4",
            "tgt_ix": "114-ARR_v1_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_4",
            "tgt_ix": "114-ARR_v1_4@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_5",
            "tgt_ix": "114-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_5",
            "tgt_ix": "114-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_5",
            "tgt_ix": "114-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_5",
            "tgt_ix": "114-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_6",
            "tgt_ix": "114-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_6",
            "tgt_ix": "114-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_6",
            "tgt_ix": "114-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_6",
            "tgt_ix": "114-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_6",
            "tgt_ix": "114-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_7",
            "tgt_ix": "114-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_7",
            "tgt_ix": "114-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_7",
            "tgt_ix": "114-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_7",
            "tgt_ix": "114-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_7",
            "tgt_ix": "114-ARR_v1_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_8",
            "tgt_ix": "114-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_8",
            "tgt_ix": "114-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_8",
            "tgt_ix": "114-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_8",
            "tgt_ix": "114-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_9",
            "tgt_ix": "114-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_10",
            "tgt_ix": "114-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_11",
            "tgt_ix": "114-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_12",
            "tgt_ix": "114-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_13",
            "tgt_ix": "114-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_14",
            "tgt_ix": "114-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_15",
            "tgt_ix": "114-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_16",
            "tgt_ix": "114-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_17",
            "tgt_ix": "114-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_18",
            "tgt_ix": "114-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_19",
            "tgt_ix": "114-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_20",
            "tgt_ix": "114-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_21",
            "tgt_ix": "114-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_21",
            "tgt_ix": "114-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_21",
            "tgt_ix": "114-ARR_v1_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_21",
            "tgt_ix": "114-ARR_v1_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_21",
            "tgt_ix": "114-ARR_v1_21@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_21",
            "tgt_ix": "114-ARR_v1_21@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_22",
            "tgt_ix": "114-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_23",
            "tgt_ix": "114-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_23",
            "tgt_ix": "114-ARR_v1_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_24",
            "tgt_ix": "114-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_24",
            "tgt_ix": "114-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_25",
            "tgt_ix": "114-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_26",
            "tgt_ix": "114-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_26",
            "tgt_ix": "114-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_27",
            "tgt_ix": "114-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_27",
            "tgt_ix": "114-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_28",
            "tgt_ix": "114-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_28",
            "tgt_ix": "114-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_28",
            "tgt_ix": "114-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_29",
            "tgt_ix": "114-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_30",
            "tgt_ix": "114-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_30",
            "tgt_ix": "114-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_31",
            "tgt_ix": "114-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_31",
            "tgt_ix": "114-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_32",
            "tgt_ix": "114-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_33",
            "tgt_ix": "114-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_33",
            "tgt_ix": "114-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_34",
            "tgt_ix": "114-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_35",
            "tgt_ix": "114-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_36",
            "tgt_ix": "114-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_36",
            "tgt_ix": "114-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_36",
            "tgt_ix": "114-ARR_v1_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_37",
            "tgt_ix": "114-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_38",
            "tgt_ix": "114-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_38",
            "tgt_ix": "114-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_38",
            "tgt_ix": "114-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_39",
            "tgt_ix": "114-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_40",
            "tgt_ix": "114-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_41",
            "tgt_ix": "114-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_41",
            "tgt_ix": "114-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_41",
            "tgt_ix": "114-ARR_v1_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_42",
            "tgt_ix": "114-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_43",
            "tgt_ix": "114-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_43",
            "tgt_ix": "114-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_44",
            "tgt_ix": "114-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_45",
            "tgt_ix": "114-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_45",
            "tgt_ix": "114-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_45",
            "tgt_ix": "114-ARR_v1_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_45",
            "tgt_ix": "114-ARR_v1_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_46",
            "tgt_ix": "114-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_46",
            "tgt_ix": "114-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_46",
            "tgt_ix": "114-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_46",
            "tgt_ix": "114-ARR_v1_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_46",
            "tgt_ix": "114-ARR_v1_46@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_46",
            "tgt_ix": "114-ARR_v1_46@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_47",
            "tgt_ix": "114-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_48",
            "tgt_ix": "114-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_48",
            "tgt_ix": "114-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_48",
            "tgt_ix": "114-ARR_v1_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_49",
            "tgt_ix": "114-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_50",
            "tgt_ix": "114-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_51",
            "tgt_ix": "114-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_52",
            "tgt_ix": "114-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_52",
            "tgt_ix": "114-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_52",
            "tgt_ix": "114-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_52",
            "tgt_ix": "114-ARR_v1_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_52",
            "tgt_ix": "114-ARR_v1_52@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_53",
            "tgt_ix": "114-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_54",
            "tgt_ix": "114-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_54",
            "tgt_ix": "114-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_55",
            "tgt_ix": "114-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_56",
            "tgt_ix": "114-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_56",
            "tgt_ix": "114-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_56",
            "tgt_ix": "114-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_56",
            "tgt_ix": "114-ARR_v1_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_56",
            "tgt_ix": "114-ARR_v1_56@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_56",
            "tgt_ix": "114-ARR_v1_56@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_57",
            "tgt_ix": "114-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_57",
            "tgt_ix": "114-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_57",
            "tgt_ix": "114-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_58",
            "tgt_ix": "114-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_59",
            "tgt_ix": "114-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_59",
            "tgt_ix": "114-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_59",
            "tgt_ix": "114-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_59",
            "tgt_ix": "114-ARR_v1_59@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_60",
            "tgt_ix": "114-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_60",
            "tgt_ix": "114-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_60",
            "tgt_ix": "114-ARR_v1_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_60",
            "tgt_ix": "114-ARR_v1_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_60",
            "tgt_ix": "114-ARR_v1_60@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_60",
            "tgt_ix": "114-ARR_v1_60@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_60",
            "tgt_ix": "114-ARR_v1_60@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_60",
            "tgt_ix": "114-ARR_v1_60@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_60",
            "tgt_ix": "114-ARR_v1_60@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_61",
            "tgt_ix": "114-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_62",
            "tgt_ix": "114-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_63",
            "tgt_ix": "114-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_63",
            "tgt_ix": "114-ARR_v1_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_64",
            "tgt_ix": "114-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_64",
            "tgt_ix": "114-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_64",
            "tgt_ix": "114-ARR_v1_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_64",
            "tgt_ix": "114-ARR_v1_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_64",
            "tgt_ix": "114-ARR_v1_64@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_64",
            "tgt_ix": "114-ARR_v1_64@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_65",
            "tgt_ix": "114-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_65",
            "tgt_ix": "114-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_66",
            "tgt_ix": "114-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_67",
            "tgt_ix": "114-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_67",
            "tgt_ix": "114-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_67",
            "tgt_ix": "114-ARR_v1_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_67",
            "tgt_ix": "114-ARR_v1_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_67",
            "tgt_ix": "114-ARR_v1_67@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_67",
            "tgt_ix": "114-ARR_v1_67@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_67",
            "tgt_ix": "114-ARR_v1_67@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_67",
            "tgt_ix": "114-ARR_v1_67@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_68",
            "tgt_ix": "114-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_68",
            "tgt_ix": "114-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_68",
            "tgt_ix": "114-ARR_v1_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_68",
            "tgt_ix": "114-ARR_v1_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_69",
            "tgt_ix": "114-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_70",
            "tgt_ix": "114-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_70",
            "tgt_ix": "114-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_70",
            "tgt_ix": "114-ARR_v1_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_70",
            "tgt_ix": "114-ARR_v1_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_70",
            "tgt_ix": "114-ARR_v1_70@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_70",
            "tgt_ix": "114-ARR_v1_70@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_70",
            "tgt_ix": "114-ARR_v1_70@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_71",
            "tgt_ix": "114-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_71",
            "tgt_ix": "114-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_72",
            "tgt_ix": "114-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_72",
            "tgt_ix": "114-ARR_v1_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_72",
            "tgt_ix": "114-ARR_v1_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_72",
            "tgt_ix": "114-ARR_v1_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_72",
            "tgt_ix": "114-ARR_v1_72@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_72",
            "tgt_ix": "114-ARR_v1_72@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_72",
            "tgt_ix": "114-ARR_v1_72@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_72",
            "tgt_ix": "114-ARR_v1_72@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_72",
            "tgt_ix": "114-ARR_v1_72@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_73",
            "tgt_ix": "114-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_73",
            "tgt_ix": "114-ARR_v1_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_74",
            "tgt_ix": "114-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_74",
            "tgt_ix": "114-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_74",
            "tgt_ix": "114-ARR_v1_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_74",
            "tgt_ix": "114-ARR_v1_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_75",
            "tgt_ix": "114-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_75",
            "tgt_ix": "114-ARR_v1_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_75",
            "tgt_ix": "114-ARR_v1_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_75",
            "tgt_ix": "114-ARR_v1_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_75",
            "tgt_ix": "114-ARR_v1_75@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_75",
            "tgt_ix": "114-ARR_v1_75@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_76",
            "tgt_ix": "114-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_77",
            "tgt_ix": "114-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_77",
            "tgt_ix": "114-ARR_v1_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_77",
            "tgt_ix": "114-ARR_v1_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_77",
            "tgt_ix": "114-ARR_v1_77@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_78",
            "tgt_ix": "114-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_79",
            "tgt_ix": "114-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_80",
            "tgt_ix": "114-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_81",
            "tgt_ix": "114-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_82",
            "tgt_ix": "114-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_83",
            "tgt_ix": "114-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_84",
            "tgt_ix": "114-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_85",
            "tgt_ix": "114-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_86",
            "tgt_ix": "114-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_87",
            "tgt_ix": "114-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_88",
            "tgt_ix": "114-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_89",
            "tgt_ix": "114-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_90",
            "tgt_ix": "114-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_91",
            "tgt_ix": "114-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_92",
            "tgt_ix": "114-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_93",
            "tgt_ix": "114-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_94",
            "tgt_ix": "114-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_95",
            "tgt_ix": "114-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_96",
            "tgt_ix": "114-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_97",
            "tgt_ix": "114-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_98",
            "tgt_ix": "114-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_99",
            "tgt_ix": "114-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_100",
            "tgt_ix": "114-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_101",
            "tgt_ix": "114-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_102",
            "tgt_ix": "114-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_103",
            "tgt_ix": "114-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_104",
            "tgt_ix": "114-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_105",
            "tgt_ix": "114-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_106",
            "tgt_ix": "114-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_107",
            "tgt_ix": "114-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_108",
            "tgt_ix": "114-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_109",
            "tgt_ix": "114-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_110",
            "tgt_ix": "114-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_111",
            "tgt_ix": "114-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_112",
            "tgt_ix": "114-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_113",
            "tgt_ix": "114-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_114",
            "tgt_ix": "114-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_115",
            "tgt_ix": "114-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_116",
            "tgt_ix": "114-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_117",
            "tgt_ix": "114-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "114-ARR_v1_118",
            "tgt_ix": "114-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1154,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "114-ARR",
        "version": 1
    }
}