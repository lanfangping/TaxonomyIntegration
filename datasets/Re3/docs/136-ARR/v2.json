{
    "nodes": [
        {
            "ix": "136-ARR_v2_0",
            "content": "Neural Pipeline for Zero-Shot Data-to-Text Generation",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_2",
            "content": "In data-to-text (D2T) generation, training on in-domain data leads to overfitting to the data representation and repeating training data noise. We examine how to avoid finetuning pretrained language models (PLMs) on D2T generation datasets while still taking advantage of surface realization capabilities of PLMs. Inspired by pipeline approaches, we propose to generate text by transforming single-item descriptions with a sequence of modules trained on generaldomain text-based operations: ordering, aggregation, and paragraph compression. We train PLMs for performing these operations on a synthetic corpus WIKIFLUENT which we build from English Wikipedia. Our experiments on two major triple-to-text datasets-WebNLG and E2E-show that our approach enables D2T generation from RDF triples in zero-shot settings. 1",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "136-ARR_v2_4",
            "content": "The aim of data-to-text (D2T) generation is to produce natural language descriptions of structured data (Gatt and Krahmer, 2018;Reiter and Dale, 1997). Although pipelines of rule-based D2T generation modules are still used in practice (Dale, 2020), end-to-end approaches based on PLMs recently showed superior benchmark performance (Ke et al., 2021;Chen et al., 2020a;Ferreira et al., 2020;Kale and Rastogi, 2020b;Ribeiro et al., 2020), surpassing pipeline systems (Ferreira et al., 2019) in both automatic and human evaluation metrics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_5",
            "content": "Finetuning PLMs on human-written references is widely accepted as a standard approach for adapting PLMs to the D2T generation objective and achieving good performance on a given benchmark (Agarwal et al., 2021;Ke et al., 2021). However, finetuning a model on the domain-specific data leads to overfitting to the particular benchmark, decreasing performance on out-of-domain 1 Our code and data is available at https://github. com/kasnerz/zeroshot-d2t-pipeline. In-domain knowledge is included only in the simple hand-crafted templates for each predicate.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_6",
            "content": "data (Laha et al., 2019). Gathering a large set of references for a particular domain is also costly and time-consuming as it usually requires collecting human-written references through crowdsourcing . These problems can be partially mitigated using few-shot approaches (Chen et al., 2020b;Ke et al., 2021;Su et al., 2021a), which operate with only several dozens or hundreds of annotated examples, but the robustness of these approaches is questionable-selecting a representative set of examples which would improve performance is difficult (Chang et al., 2021a), and the limited sample is often noisy, increasing the chance of hallucinations and omissions (Du\u0161ek et al., 2019;Harkous et al., 2020;Rebuffel et al., 2022).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_7",
            "content": "In this paper, we present a zero-shot alternative to the traditional finetuning paradigm by formulating the D2T generation from RDF triples as a sequence of general-domain operations over text in natural language. We start by transforming individual triples to text using trivial templates, which we subsequently order, aggregate, and compress on the paragraph level to produce the resulting description of the data. In constrast to traditional pipeline systems, all our pipeline modules are built upon PLMs and operate over sentences in natural language. The modules are trained on our new WIKI-FLUENT corpus, which contains 934k examples of first paragraphs from the English Wikipedia, each supplied with a synthesized set of simple templatelike sentences which together convey the meaning of the original paragraph. Our approach allows generating natural language descriptions from RDF triples with a minimum amount of domain-specific rules or knowledge and without using training data from the D2T datasets. Although our approach is primarily a probe into the territory of zero-shot approaches and cannot yet match the quality of stateof-the-art models, we show that it can yield large improvements upon simple baselines and match older supervised systems on automatic metrics for text fluency. Moreover, the semantic accuracy metrics and our manual error analysis suggest that our approach offers a way to prevent omissions and hallucinations common in few-shot approaches.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_8",
            "content": "Our contributions are the following: (1) We propose an alternative D2T generation approach based on general-domain text-to-text operations (ordering, aggregation, and paragraph compression).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_9",
            "content": "(2) We introduce a synthetic WIKIFLUENT corpus containing 934k sentences based on English Wikipedia, providing training data for the operations in (1). (3) We apply our system on two D2T datasets and evaluate its performance both automatically and manually, including the contribution of individual pipeline modules. (4) We release our code, data, pretrained models, and system outputs to ease future research. 1",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_10",
            "content": "2 Related Work D2T Generation with PLMs Large neural language models pretrained on self-supervised tasks (Lewis et al., 2020;Liu et al., 2019;Devlin et al., 2019) have recently gained a lot of traction in D2T generation research (Ferreira et al., 2020;Kasner and Du\u0161ek, 2020b). Following Chen et al. (2020b), other works adopted PLMs for few-shot D2T generation (Chang et al., 2021b;Su et al., 2021a). Kale and Rastogi (2020b) and Ribeiro et al. (2020) showed that PLMs using linearized representations of data can outperform graph neural networks on graph-to-text datasets, recently surpassed again by graph-based models (Ke et al., 2021;Chen et al., 2020a). Although the models make use of generaldomain pretraining tasks, all of them are eventually finetuned on domain-specific data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_11",
            "content": "Pipeline-based D2T Generation Until the recent surge of end-to-end approaches , using several modules connected in a pipeline was a major approach for D2T generation (Gatt and Krahmer, 2018;Reiter, 2007;Reiter and Dale, 1997). Our approach is inspired by the pipeline approaches, in particular the pipelines utilizing neural modules (Ferreira et al., 2019). In contrast with these approaches, our pipeline works with unstructured data in natural language and it operates in zero-shot setting, i.e. without using any training data from target D2T datasets. Laha et al. (2019) introduce a three-step pipeline for zero-shot D2T generation similar to ours. Unlike the approach we describe here, they use a semiautomatic template generation system, 2 their sentence fusion is rule-based, and they do not address content planning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_12",
            "content": "Content Planning in D2T Generation Content planning, i.e. the task of ordering input facts and aggregating them into individual sentences, is one of the steps of the traditional D2T pipeline (Gatt and Krahmer, 2018). As shown by Moryossef et al. (2019a,b) and confirmed by other works (Puduppully et al., 2019;Trisedya et al., 2020;, including a content plan improves the quality of outputs in neural D2T pipelines. Unlike the aforementioned planners, which use predicates or keys from D2T datasets for representing the data items, our planner is trained on ordering sentences in natural language.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_13",
            "content": "Sentence Ordering Sentence ordering is the task of organizing a set of natural language sentences to increase the coherence of a text (Barzilay et al., 2001;Lapata, 2003). Several neural methods for this task were proposed, using either interactions between pairs of sentences Li and Jurafsky, 2017), global interactions (Gong et al., 2016;Wang and Wan, 2019), or combination of both (Cui et al., 2020). We base our ordering module ( \u00a75.2) on the recent work of Calizzano et al. (2021), who use a pointer network (Wang and Wan, 2019;Vinyals et al., 2015) on top of a PLM.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_14",
            "content": "Aggregating Input into Sentences Typically, multiple pieces of input information need to be merged into a single sentence. Previous works (Wiseman et al., 2018;Shao et al., 2019;Shen et al., 2020;Xu et al., 2021) capture the segments which correspond to individual parts of the input as latent variables. Unlike these works, we adopt a simpler scenario using an already ordered sequence of facts (see \u00a73.1), into which we selectively insert delimiters to mark sentence boundaries.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_15",
            "content": "We introduce paragraph compression (PC) as a new task and the final step in our D2T generation pipeline. This task combines several standard natural-language tasks including sentence fusion, rephrasing, and coreference resolution. Unlike text summarization or simplification Jiang et al., 2020), we aim to convey the complete semantics of the text without omitting any facts. In contrast to sentence fusion (Geva et al., 2019;Barzilay and McKeown, 2005) or sentence compression (Filippova and Altun, 2013), we operate in the context of multiple sentences in a paragraph. The task is the central focus of our WIKIFLUENT corpus ( \u00a74).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_16",
            "content": "Method",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "136-ARR_v2_17",
            "content": "In this section, we provide the formal description of our proposed approach. We focus on the task of producing a natural language description Y for a set of n RDF triples X \" tx 1 , . . . , x n u. Each triple x i \" ts i , p i , o i u consists of subject s i , predicate p i , and object o i .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_18",
            "content": "Our pipeline proceeds as follows. Given a set of triples X on the input, we:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_19",
            "content": "(1) transform the triples into facts, which are sentences in natural language, (2) sort the facts using an ordering module, (3) insert sentence delimiters between the sorted facts using an aggregation module, (4) input the ordered sequence of facts with delimiters into a paragraph compression module, which generates the final description Y . The individual steps are described in the following sections: transforming individual triples to text ( \u00a73.1), ordering ( \u00a73.2), aggregation ( \u00a73.3), and paragraph compression ( \u00a73.4).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_20",
            "content": "Transforming Triples to Facts",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "136-ARR_v2_21",
            "content": "The first step in our pipeline involves transforming each of the input triples x i P X into a fact f i P F using a transformation T : X \u00d1 F . We define a fact f i as a single sentence in natural language describing x i . The transformation serves two purposes: (a) preparing the data for the subsequent text-to-text operations, (b) introducing in-domain knowledge about the semantics of individual predicates. This step can be realized e.g. using a simple template for each predicate (cf. \u00a75.1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_22",
            "content": "Ordering the Facts",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "136-ARR_v2_23",
            "content": "We assume that the default order of triples X is random and the same applies for the respective facts F . Note, however, that that F is a indeed set of meaningful sentences. We can use this to our advantage and apply a sentence ordering model to maximize the coherency of the paragraph resulting from their concatenation. An example outcome of such operation may be grouping together facts mentioning birth date and birth place of a person, followed by their occupation (see Figure 1). The ordering module allows downstream modules to only focus on operations over neighboring sentences.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_24",
            "content": "Formally, we apply the ordering model OpF q to get an ordered sequence of facts: F o \" tf o 1 , . . . , f on u, where o 1:n is a permutation of indices. We describe our ordering model in \u00a75.2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_25",
            "content": "Aggregating the Facts",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "136-ARR_v2_26",
            "content": "Some facts will be typically mentioned together in a single sentence. Considering the previous example, occupation is likely to be mentioned separately, while birth date and birth place are likely to be mentioned together. Using an ordered sequence of facts as input, we can apply an aggregation model to decide which facts should be merged into a single sentence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_27",
            "content": "Formally, the aggregation model takes a sequence of ordered facts F o as input and produces a sequence of sentence delimiters ApF o q \" t\u03b4 o 1 , \u03b4 o 2 , . . . , \u03b4 o n\u00b41 u; \u03b4 i P t0, 1u. The output \u03b4 i \" 1 means that the neighboring facts should be mentioned separately, i.e. the neighboring sentences should not be fused. Conversely, \u03b4 i \" 0 means that the facts should be aggregated and their corresponding sentences should be fused. We describe our aggregation model in \u00a75.3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_28",
            "content": "Paragraph Compression",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "136-ARR_v2_29",
            "content": "The paragraph compression (PC) model is a generative model which outputs the final text description. It has two main objectives: (a) fusing related sentences, i.e., sentences i and j in between which \u03b4 i \" 0, and (b) rephrasing the text to improve its fluency, e.g. fixing disfluencies in the templates, replacing noun phrases with refering expressions, etc. The goal of the task is to preserve the semantics of the text which is an already ordered sequence of sentences, so the edits will typically be minor. Formally, the model takes as input the ordered sequence of facts with delimiters",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_30",
            "content": "F a \" tf o 1 , \u03b4 o 1 , f o 2 , . . . , \u03b4 o n\u00b41 ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_31",
            "content": "WIKIFLUENT Corpus",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "136-ARR_v2_32",
            "content": "Here we descibe the process of building a largescale synthetic corpus WIKIFLUENT. The corpus provides training data for the neural models which we use in our implementation of the ordering, aggregation, and paragraph compression modules (cf. \u00a75).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_33",
            "content": "Our goal is to cover a broad range of domains while capturing the sentence style in D2T generation with respect to both the input facts and the target descriptions. In other words, we aim to build a corpus in which (1) the input is a set of simple, template-like sentences, (2) the output is a fluent text in natural language preserving the semantics of the input. As we describe below in detail, we achieve that by using human-written paragraphs in English Wikipedia and applying split-and-rephrase and coreference resolution models to obtain synthetic source texts. The process is illustrated in Figure 2; corpus statistics are included in Appendix A.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_34",
            "content": "Data Source",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "136-ARR_v2_35",
            "content": "For building the WIKIFLUENT corpus, we extracted 934k first paragraphs of articles from a Wikipedia dump 3 using WikiExtractor (Attardi, 2015). Wikipedia is commonly used for large-scale pretraining of D2T generation models (Jin et al., 2020;Chen et al., 2020a). Although it is not biasfree, it provides more balanced sample of natural language use than typical D2T generation datasets. We used the first paragraphs of Wikipedia entries, which contain mostly concise, fact-based descriptions. We selected paragraphs with length between 3",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_36",
            "content": "The Westmeath Examiner is a weekly newspaper in Westmeath, Ireland.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_37",
            "content": "It is located in Westmeath, Ireland.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_38",
            "content": "The Westmeath Examiner is a weekly newspaper.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_39",
            "content": "The Westmeath Examiner is a weekly newspaper.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_40",
            "content": "It was founded in 1882.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_41",
            "content": "The Westmeath Examiner is located in Westmeath, Ireland.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_42",
            "content": "The Westmeath Examiner was founded in 1882. The building process of the WIKIFLUENT corpus. We apply a split-and-rephrase model on each sentence in the paragraph and resolve coreferences in the split sentences. The result is a set of simple sentences which together convey the same meaning as the original paragraph. The synthesized sentences are used as input into our models, the original human-written texts are used as ground truth. 30-430 characters; filtering out lists, disambiguations, and repeated and malformed paragraphs. To balance the length of inputs, we selected 250k examples each from 4 equally sized length ranges (30-130 characters, etc.).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_43",
            "content": "Split-and-Rephrase",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "136-ARR_v2_44",
            "content": "To generate a set of simple sentences, we divide each paragraph into sentences using NLTK (Bird, 2006) and apply a split-and-rephrase model on each sentence. Split-and-rephrase is a task of splitting a complex sentence into a meaning preserving sequence of shorter sentences . The process is illustrated in the upper part of Figure 2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_45",
            "content": "We train our split-and-rephrase model on the large-scale WikiSplit corpus by Botha et al. (2018), containing human-made sentence splits from Wikipedia edit history. Following the same setup as for a paragraph compression model ( \u00a73.4), we train BART-base (Lewis et al., 2020) on the WikiSplit dataset in a sequence-to-sequence setting. Next, we apply the trained split-and-rephrase model on each sentence in our Wikipedia-based corpus, uniformly randomly choosing between 0-2 recursive calls to ensure that the splits are not deterministic. If the sentence cannot be meaningfully split, the model tends to duplicate the sentence on the output; in that case, we use only the original sentence and do not proceed with the splitting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_46",
            "content": "Coreference Replacement",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "136-ARR_v2_47",
            "content": "As the next step, we concatenate the split sentences and apply a coreference resolution model (Gardner et al., 2018;Lee et al., 2018) in order to replace referring expressions with their antencendents (e.g., pronouns with noun phrases). The motivation for this step is to match the style of the facts (see \u00a73.1), which do not use pronouns since each fact describes a single triple only. Note that this procedure replaces the referring expressions only in the synthesized sentences (which are used as input) and keeps them in the original paragraphs (which are used as ground truth). As a consequence, the paragraph compression module is implicitly trained to generate referring expressions in the final description.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_48",
            "content": "Filtering",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "136-ARR_v2_49",
            "content": "To ensure that the generated sentences convey the same semantics as the original paragraph, we use a pretrained RoBERTa model 4 (Liu et al., 2019) trained on the MultiNLI dataset (Williams et al., 2018) for checking the semantic accuracy of the generated text. Following Du\u0161ek and Kasner (2020), we test if the original paragraph entails each of the synthesized sentences (checking for omissions), and if the set of concatenated synthesized sentences entails the original paragraph (checking for hallucinations). In a filtered version of the WIKIFLUENT corpus, we include only the examples without omissions or hallucinations (as computed by the model), reducing it to 714k examples (approximately 75% of the original size).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_50",
            "content": "Implementation",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "136-ARR_v2_51",
            "content": "In this section, we describe how we implement our pipeline modules ( \u00a73) using simple template transformations ( \u00a75.1) and neural models trained on the WIKIFLUENT dataset ( \u00a75.2-5.4). 5",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_52",
            "content": "Templates",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "136-ARR_v2_53",
            "content": "We transform triples into facts ( \u00a73.1) using a singletriple template t i for each predicate. For example, if p i \" instrument, then T pp i q \" \"s i plays o i \" (cf. Table 1). We follow previous work in which simple hand-crafted templates have been used as an efficient way of introducing domain knowledge (Kale and Rastogi, 2020a;Kasner and Du\u0161ek, 2020a) template generation engines (Laha et al., 2019;Heidari et al., 2021;Mehta et al., 2021), the approach may produce less fluent outputs, but it minimizes manual workload and makes it easier to control the quality of the input for the subsequent steps.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_54",
            "content": "Ordering Model",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "136-ARR_v2_55",
            "content": "For our ordering model ( \u00a73.2), we use the Simple Pointer model from Calizzano et al. (2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_56",
            "content": "The model is based on a pretrained BART-base extended with a pointer network from Wang and Wan (2019). We provide a short description of the model here; for details please refer to Calizzano et al. (2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_57",
            "content": "In the encoding phase, facts F are concatenated and tokenized. Each fact is surrounded by special tokens denoting the beginning (<s>) and the end (</s>) of the fact. The sequence is processed by the BART encoder, generating a sequence of encoder states E for each end token </s> representing the preceding fact.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_58",
            "content": "The decoding proceeds autoregressively. To bootstrap the decoding process, the pair of tokens <s></s> is fed into the decoder, producing the decoder state d 1 . The pointer network (attending to d 1 and E), selects the first ordered fact f o 1 , which is fed into the decoder in the next step (d 2 \"<s>f o 1 </s>). The process is repeated until the all the facts are decoded in a particular order.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_59",
            "content": "The pointer network computes the probability of a fact to be on the j-th position, using the encoder output E and the decoder output state d j . The network is based on the scaled dot product attention, where d j is the query and encoder outputs E i are the keys:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_60",
            "content": "Q \" d j W Q K \" EW K P j \" softmax \u02c6QK T ? b \u02d9.",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_61",
            "content": "A dam is a barrier obstructing flowing water.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_62",
            "content": "A dam is a barrier.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_63",
            "content": "A dam obstructs flowing water. Here W Q and W K P R b\u02c6b , b is the dimension of BART hidden states, and P j P R n`1 is the probability distribution for the j-th position (i.e., P ji is the probability that fact f i is on the j-th position).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_64",
            "content": "We train the model using the synthesized simple sentences in the WIKIFLUENT corpus, randomly shuffling the order of the sentences and training the model to restore their original order.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_65",
            "content": "Aggregation Model",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "136-ARR_v2_66",
            "content": "We base our aggregation model ( \u00a73.3) on RoBERTa-large (Liu et al., 2019) with a token classification head. 6 Similarly to the ordering model ( \u00a75.2), we input the sequence of (now ordered) facts F o into the model, separating each pair of facts f o i with a special token </s> (used by the model as a separator). Subsequently, the token classification layer classifies each separator </s> i position into two classes t0, 1u corresponding to the delimiter \u03b4 i . We ignore the outputs for the nonseparator tokens while computing cross-entropy loss.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_67",
            "content": "We create the training examples using the synthesized sentences in the WIKIFLUENT corpus, in which we set \u03b4 i \" 0 for the sentences i, i`1 which were originally aggregated (i.e., are the result of splitting a single sentence) and \u03b4 i \" 1 otherwise.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_68",
            "content": "Paragraph Compression Model",
            "ntype": "title",
            "meta": {
                "section": "5.4"
            }
        },
        {
            "ix": "136-ARR_v2_69",
            "content": "We adopt BART-base for our paragraph compression model. We finetune the model on the WIK-IFLUENT corpus, concatenating the synthesized sentences on the input. We add delimiters between the sentences i and i `1 where \u03b4 i \" 1 using a special token <sep>, which we add to the model vocabulary. As shown in Keskar et al. (2019), including control codes for training the model can steer the model towards producing certain outputs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_70",
            "content": "Here we expect that the model will learn to fuse the sentences between which there are no delimiters on the input. We evaluate how the model learns to respect the order and aggregation markers in \u00a77.3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_71",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "136-ARR_v2_72",
            "content": "We train our pipeline modules on the WIKIFLU-ENT corpus as described in \u00a75. Next, we use these modules without finetuning for generating descriptions for RDF triples on two English D2T datasets, WebNLG and E2E.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_73",
            "content": "Datasets The datasets differ in domain, size, textual style, and number of predicates (see Appendix A for details):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_74",
            "content": "\u2022 WebNLG Ferreira et al., 2020) contains RDF triples from DBPedia (Auer et al., 2007) and their crowdsourced descriptions. We use version 1.4 of the dataset for comparison to prior work. We hand-crafted templates for all 354 predicates, including unseen predicates in the test set. 7 \u2022 E2E (Novikova et al., 2017; contains restaurant recommendations in the form of attribute-value pairs. We use the cleaned version of the dataset (Du\u0161ek et al., 2019). Following previous work, we transform the attribute-value pairs into RDF triples (using the restaurant name as a subject) and then apply the same setup as for WebNLG. We created a template for each of the 8 attributes manually.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_75",
            "content": "In order to evaluate individual components of our pipeline, we train three versions of the paragraph compression model (see \u00a75.4).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_76",
            "content": "The models share the same architecture and targets, but differ in their inputs:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_77",
            "content": "\u2022 PC -the model takes as an input ordered facts with delimiters (as described in \u00a73.4), \u2022 PC+AGG -the model takes as an input ordered facts without delimiters (i.e., the aggregation is left implicitly to the model), \u2022 PC+ORD+AGG -the model takes as an input facts in random order and without delimiters (i.e., both ordering and aggregation are left implicitly to the model). Correspondingly, we test three versions of the pipeline in our ablation study (see Figure 3): \u2022 3-STAGE -a full version of the pipeline consisting of the ordering model (ORD), the aggregation model (AGG) and the PC model (following the full pipeline from \u00a73), \u2022 2-STAGE -a pipeline consisting of the ORD model and the PC+AGG model, \u2022 1-STAGE -a single stage consisting of the PC+ORD+AGG model. We evaluate all versions of the pipeline with PC models trained on the full and filtered versions of the WIKIFLUENT dataset (see \u00a74).",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_78",
            "content": "Evaluation and Discussion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "136-ARR_v2_79",
            "content": "Our main aim is the evaluation of our pipeline on the downstream task of D2T generation. We evaluate outputs from the {1,2,3}-STAGE variants of our pipeline using automatic metrics ( \u00a77.1), and we perform a detailed manual error analysis of the model outputs ( \u00a77.2). We also evaluate the performance of the content planning modules and the ability of the PC module to follow the content plan ( \u00a77.3). In \u00a77.4, we include an intrinsic evaluation of our modules on the WIKIFLUENT test set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_80",
            "content": "Automatic Metrics",
            "ntype": "title",
            "meta": {
                "section": "7.1"
            }
        },
        {
            "ix": "136-ARR_v2_81",
            "content": "Following prior work, we use BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) to evaluate the outputs against the human references. 8 We also evaluate the number of omission and hallucination errors (i.e., facts missing or added, respectively) using a metric from Du\u0161ek and Kasner (2020) based on a RoBERTa model (Liu et al., 2019) pretrained on natural language inference (NLI). 9 We include a diverse set of baselines for comparison. For WebNLG (see Table 3), we compare our systems with the results of:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_82",
            "content": "\u2022 UPF-FORGe and MELBOURNE -systems (grammar-based and supervised, respectively) from the first run of WebNLG Challenge (Gardent et al., 2017), \u2022 Ke et al. (2021) -a state-of-the-art system with a structure-aware encoder and task-specific pretraining, \u2022 Laha et al. (2019) -the only other (to our knowledge) zero-shot D2T generation system applied to WebNLG. For E2E (see Table 4), we compare our systems with the results of: \u2022 TGEN (Du\u0161ek and Jur\u010d\u00ed\u010dek, 2015) -the baseline system for the E2E Challenge , \u2022 Harkous et al. (2020) -a state-of-the-art supervised system on cleaned E2E data. For both datasets, COPY denotes the baseline of copying the facts without further processing.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_83",
            "content": "The automatic evaluation shows that our systems consistently outperform the COPY baseline (e.g., \"12 BLEU points for E2E), which is already strong thanks to our manually curated set of templates. 10 Automatic scores also suggest that our systems are comparable with some older supervised systems. Nevertheless, our systems still underperform the state-of-the-art supervised systems. For this reason, we further focus on manual error analysis in \u00a77.2 to pinpoint the current shortcomings of our approach.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_84",
            "content": "The 2-STAGE system is generally on par with the 3-STAGE system or better, which indicates that explicit aggregation using the AGG model may not be necessary. However, an advantage of having a separate aggregation module is the possibility to control the aggregation step explicitly. The models using the filtered version of the corpus generally produce better results, although they also bring in a larger number of omissions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_85",
            "content": "Manual Error Analysis",
            "ntype": "title",
            "meta": {
                "section": "7.2"
            }
        },
        {
            "ix": "136-ARR_v2_86",
            "content": "Since automatic performance metrics do not provide insights into specific weaknesses of the system (van Miltenburg et al., 2021), we manually examined 100 outputs of the models. We counted the number of errors: factual (hallucinations, omissions, incorrect fact merging, redundancies) and grammatical. The results are summarized in Table 5.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_87",
            "content": "The 1-STAGE model (which has to order the facts implicitly) tends to repeat the facts in the text (especially in E2E) and produces frequent hallucinations. These problems are largely eliminated with the 2-STAGE and 3-STAGE models, which produce Wildwood is a restaurant. Wildwood serves French food. Wildwood is in the riverside. Wildwood is near Raja Indian Cuisine.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_88",
            "content": "Wildwood is a restaurant serving French food. It is in the riverside near Raja Indian Cuisine. Human A amazing French restaurant is called the Wildwood. The restaurant is near the Raja Indian Cuisine in riverside. They love kids. almost no hallucinations or omissions. However, the outputs on WebNLG for all systems suffer from semantic errors resulting from merging of unrelated facts. This mostly happens with unrelated predicates connected to the same subject/object (e.g. \"X was born in Y\", \"X worked as Z\" expressed as \"X worked as Z in Y\"; see Appendix E for examples). This behavior is the main obstacle to ensure factual consistency of the output. As a possible remedy, we propose explicitly controlling the semantics of sentence fusion (Ben-David et al., 2020), e.g. using a variant of constrained decoding (Balakrishnan et al., 2019;.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_89",
            "content": "On the E2E data, which has a simpler triple structure (all predicates share the same subject), the outputs are generally consistent and the 2-STAGE and 3-STAGE models exhibit almost no semantic errors. Grammar errors and disfluencies stem mainly from over-eager paragraph compression or from artifacts in our templates and are relatively minor (e.g., missing \"is\" in \"serves French food and family-friendly\").",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_90",
            "content": "Content Planning",
            "ntype": "title",
            "meta": {
                "section": "7.3"
            }
        },
        {
            "ix": "136-ARR_v2_91",
            "content": "Following and , we report the accuracy and BLEU-2 score of our ordering model on WebNLG against the humangenerated plans from Ferreira et al. (2018). The results are listed in We also evaluate the accuracy of our aggregation model, using triples ordered according to the plans from Ferreira et al. (2018) as input. The accuracy is 0.33 per example and 0.62 per sentence boundary (random baseline is 0.23 and 0.50, respectively). The results show that although our approach is better than the random baseline, there is still room for improvement.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_92",
            "content": "Finally, we manually evaluate how the PC model follows the content plan (i.e., keeping the predefined order and aggregating the sentences according to the delimiters) using 100 randomly chosen examples with more than 1 triple on WebNLG and E2E. We find that the model follows the content plan in 95% and 100% of cases, respectively. The incorrect cases include a fact not properly mentioned or an extra boundary between sentences without a separator. We can thus conclude that the pretraining task successfully teaches the PC model to follow a given content plan.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_93",
            "content": "Intrinsic Evaluation",
            "ntype": "title",
            "meta": {
                "section": "7.4"
            }
        },
        {
            "ix": "136-ARR_v2_94",
            "content": "Aside from the main D2T generation results, we also provide an intrinsic evaluation of our pipeline modules on the WIKIFLUENT test sets. We evaluated the ordering, aggregation, and paragraph compression modules trained on the full WIKIFLUENT corpus. The results for both full and filtered test sets are summarized in Table 7. The PC model achieves high scores, which follows from the fact that we provide it with ground truth content plans (i.e., the ordering and aggregation plan corresponding to the original paragraph). Accuracy of the ordering and aggregation modules is comparable to their performance on D2T datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_95",
            "content": "Future Work",
            "ntype": "title",
            "meta": {
                "section": "8"
            }
        },
        {
            "ix": "136-ARR_v2_96",
            "content": "Our experiments outline several possible future research directions. Automatic generation of facts without using hand-crafted templates (cf. \u00a75.1) could allow applying zero-shot generation systems to datasets with a large number of predicates, such as ToTTo (Parikh et al., 2020). The task of paragraph compression could be used as a task-specific pretraining (Gururangan et al., 2020) for more efficient finetuning of D2T models, e.g., with a small amount of clean data. Consistency checks may be introduced in the pipeline to control the output from the modules, and individual modules may be improved by using more efficient model architectures.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_97",
            "content": "More research is also needed regarding the main shortcoming of our approach, i.e., the semantic errors stemming from merging of facts in improper ways. As we suggested in \u00a77.2, explicitly controlling the semantics of sentence fusion could help to mitigate this issue, while still keeping the advantages of a zero-shot approach.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_98",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "9"
            }
        },
        {
            "ix": "136-ARR_v2_99",
            "content": "We presented an approach for zero-shot D2T generation. The approach uses a pipeline of PLMs trained on general-domain lexical operations over natural language. The pipeline builds upon traditional approaches and consists of three interpretable intermediate steps. By avoiding noisy human-written references from the D2T datasets, our models produce more semantically consitent output. We believe that training models for zeroshot D2T generation using large cross-domain corpora will help to build D2T generation systems with good performance across various domains.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_100",
            "content": "Limitations and Broader Impact",
            "ntype": "title",
            "meta": {
                "section": "10"
            }
        },
        {
            "ix": "136-ARR_v2_101",
            "content": "We study zero-shot D2T generation with the focus on generating descriptions for RDF triples. Although the task of D2T generation has numerous applications, using neural models for D2T generation (especially in the zero-shot context) is still limited to experimental settings (Dale, 2020). Similarly to other recent approaches for D2T generation, our approach relies on PLMs, which are known to reflect the biases in their pretraining corpus (Bender et al., 2021;Rogers, 2021). Our system may therefore rely on spurious correlations for verbalizing e.g. gender or occupation of the entities. Since we cannot guarantee the factual correctness of the outputs of our system, the outputs should be used with caution.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_102",
            "content": "On the flip side, our approach helps to reduce the number of omissions and hallucinations stemming from noise in human-written references. Our work thus contributes to the general aim of D2T generation in conveying the data semantics accurately and without relying on implicit world knowledge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_103",
            "content": "We implemented the models for split-and-rephrase, aggregation, and paragraph compression in Py-Torch Lightning (Paszke et al., 2019), using the PyTorch (Falcon et al., 2019) version of the BART and RoBERTa models from the Huggingface library (Wolf et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_104",
            "content": "We use the Adam (Kingma and Ba, 2015) optimizer (\u03b2 1 \" 0.9, \u03b2 2 \" 0.997, \u03b5 \" 1 \u00b49) with learning rate 2 \u00b45, linear scheduling and 0.1 warmup proportion; batches of size 8 and accumulating gradients with factor 4. We train the models for 1 epoch on a single GeForce RTX 3090 GPU with 24 GB RAM. Training times were approximately 24 hours for the ordering model and 3 hours for the aggregation and paragraph compression models. We use greedy decoding in all our experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_105",
            "content": "For training the ordering model, we used the implementation from Calizzano et al. (2021) 11 including their training parameters. We will integrate the ordering model into our framework.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_106",
            "content": "We provide evaluation of semantic accuracy on the E2E dataset as evaluated with the slot-error script based on matching regular expressions in Table 8 However, please note that our manual investigation of a sample of the data shows that the majority of the errors identified in our model outputs are false. For example, the following regular expression used in the slot-error script: prices?(?: range)?(?:w+)0,3 high matches \"(...) price range and high customer rating (...)\", incorrectly classifying the presence of the extra slot priceRange [high]. This problem is magnified by the consistent outputs of our models, which tend to repeat certain patterns. However, we also manually identified several cases in which an error was found correctly, e.g. the model hallucinating \"3 out of 4 customer rating\" instead of \"3 out of 5 customer rating\".",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_107",
            "content": "Tables 10, 11, 12, and 13 show examples of behavior of our models on the WebNLG dataset. Tables 14 and 15 show examples of behavior of our models on the E2E dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_108",
            "content": "The green color marks the model outputs which are completely correct, the red color marks the errors. For better readability of the input format, we add numeric order identifiers for the individual facts (bold, in squared brackets). These are subsequently used as references in the Order and Aggregation rows. Note that zero-th input in E2E is used as a subject in the RDF triples.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_109",
            "content": "Akron, Ohio is from United States. English language is spoken in Akron, Ohio.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_110",
            "content": "English language is spoken in the United States. Akron, Ohio is from the state of Ohio in the U.S. state of Illinois. 3-stage filtered Akron, Ohio is from United States. English language is spoken in the United States of America. 2-stage filtered Akron, Ohio is from United States. English language is spoken in Akron, Ohio. 1-stage filtered English language is spoken in the United States. Akron, Ohio is from the United Kingdom.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_111",
            "content": "The country of Akron, Ohio, is United States, where English is the language spoken.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_112",
            "content": "Table 11: Incorrect behavior on WebNLG: besides the minor disfluencies caused by the templates (\"Akron, Ohio is from...\"), the models (except for 3-STAGE filtered) tend to hallucinate and merge the facts incorrectly.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v2_113",
            "content": "Oshin Agarwal, Heming Ge, Siamak Shakeri, Rami Al-Rfou, Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Oshin Agarwal",
                    "Heming Ge",
                    "Siamak Shakeri",
                    "Rami Al-Rfou"
                ],
                "title": "Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_114",
            "content": "UNKNOWN, None, 2015, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_115",
            "content": "S\u00f6ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, DBpedia: A Nucleus for a Web of Open Data, 2007, The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "S\u00f6ren Auer",
                    "Christian Bizer",
                    "Georgi Kobilarov",
                    "Jens Lehmann",
                    "Richard Cyganiak",
                    "Zachary Ives"
                ],
                "title": "DBpedia: A Nucleus for a Web of Open Data",
                "pub_date": "2007",
                "pub_title": "The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_116",
            "content": "Anusha Balakrishnan, Jinfeng Rao, Kartikeya Upasani, Michael White, Rajen Subba, Constrained Decoding for Neural NLG from Compositional Representations in Task-Oriented Dialogue, 2019, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Anusha Balakrishnan",
                    "Jinfeng Rao",
                    "Kartikeya Upasani",
                    "Michael White",
                    "Rajen Subba"
                ],
                "title": "Constrained Decoding for Neural NLG from Compositional Representations in Task-Oriented Dialogue",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "136-ARR_v2_117",
            "content": "Satanjeev Banerjee, Alon Lavie, METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments, 2005, Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Satanjeev Banerjee",
                    "Alon Lavie"
                ],
                "title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments",
                "pub_date": "2005",
                "pub_title": "Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_118",
            "content": "Regina Barzilay, No\u00e9mie Elhadad, Kathleen Mckeown, Sentence Ordering in Multidocument Summarization, 2001, Proceedings of the First International Conference on Human Language Technology Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Regina Barzilay",
                    "No\u00e9mie Elhadad",
                    "Kathleen Mckeown"
                ],
                "title": "Sentence Ordering in Multidocument Summarization",
                "pub_date": "2001",
                "pub_title": "Proceedings of the First International Conference on Human Language Technology Research",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_119",
            "content": "Regina Barzilay, Kathleen Mckeown, Sentence Fusion for Multidocument News Summarization, 2005, Comput. Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Regina Barzilay",
                    "Kathleen Mckeown"
                ],
                "title": "Sentence Fusion for Multidocument News Summarization",
                "pub_date": "2005",
                "pub_title": "Comput. Linguistics",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_120",
            "content": "Eyal Ben-David, Orgad Keller, Eric Malmi, Semantically Driven Sentence Fusion: Modeling and Evaluation, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, Findings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Eyal Ben-David",
                    "Orgad Keller",
                    "Eric Malmi"
                ],
                "title": "Semantically Driven Sentence Fusion: Modeling and Evaluation",
                "pub_date": "2020",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020, Findings of ACL",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_121",
            "content": "Emily Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?, 2021, FAccT '21: 2021 ACM Conference on Fairness, Accountability, and Transparency, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Emily Bender",
                    "Timnit Gebru",
                    "Angelina Mcmillan-Major",
                    "Shmargaret Shmitchell"
                ],
                "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?",
                "pub_date": "2021",
                "pub_title": "FAccT '21: 2021 ACM Conference on Fairness, Accountability, and Transparency",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_122",
            "content": "Steven Bird, NLTK: The Natural Language Toolkit, 2006, ACL 2006, 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Steven Bird"
                ],
                "title": "NLTK: The Natural Language Toolkit",
                "pub_date": "2006",
                "pub_title": "ACL 2006, 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_123",
            "content": "Jan Botha, Manaal Faruqui, John Alex, Jason Baldridge, Dipanjan Das, Learning to Split and Rephrase From Wikipedia Edit History, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Jan Botha",
                    "Manaal Faruqui",
                    "John Alex",
                    "Jason Baldridge",
                    "Dipanjan Das"
                ],
                "title": "Learning to Split and Rephrase From Wikipedia Edit History",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_124",
            "content": "R\u00e9mi Calizzano, Malte Ostendorff, Georg Rehm, Ordering sentences and paragraphs with pretrained encoder-decoder transformers and pointer ensembles, 2021, DocEng '21: ACM Symposium on Document Engineering 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "R\u00e9mi Calizzano",
                    "Malte Ostendorff",
                    "Georg Rehm"
                ],
                "title": "Ordering sentences and paragraphs with pretrained encoder-decoder transformers and pointer ensembles",
                "pub_date": "2021",
                "pub_title": "DocEng '21: ACM Symposium on Document Engineering 2021",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_125",
            "content": "Ernie Chang, Xiaoyu Shen, Hui-Syuan Yeh, Vera Demberg, On Training Instance Selection for Few-Shot Neural Text Generation, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Ernie Chang",
                    "Xiaoyu Shen",
                    "Hui-Syuan Yeh",
                    "Vera Demberg"
                ],
                "title": "On Training Instance Selection for Few-Shot Neural Text Generation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "136-ARR_v2_126",
            "content": "Ernie Chang, Xiaoyu Shen, Dawei Zhu, Vera Demberg, Hui Su, Neural Data-to-Text Generation with LM-based Text Augmentation, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Ernie Chang",
                    "Xiaoyu Shen",
                    "Dawei Zhu",
                    "Vera Demberg",
                    "Hui Su"
                ],
                "title": "Neural Data-to-Text Generation with LM-based Text Augmentation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_127",
            "content": "Wenhu Chen, Yu Su, Xifeng Yan, William Wang, KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Wenhu Chen",
                    "Yu Su",
                    "Xifeng Yan",
                    "William Wang"
                ],
                "title": "KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_128",
            "content": "Xinchi Chen, Xipeng Qiu, Xuanjing Huang, None, 2016, Neural Sentence Ordering. CoRR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Xinchi Chen",
                    "Xipeng Qiu",
                    "Xuanjing Huang"
                ],
                "title": null,
                "pub_date": "2016",
                "pub_title": "Neural Sentence Ordering. CoRR",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_129",
            "content": "Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, William Wang, Few-Shot NLG with Pre-Trained Language Model, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Zhiyu Chen",
                    "Harini Eavani",
                    "Wenhu Chen",
                    "Yinyin Liu",
                    "William Wang"
                ],
                "title": "Few-Shot NLG with Pre-Trained Language Model",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_130",
            "content": "Baiyun Cui, Yingming Li, Zhongfei Zhang, BERT-enhanced Relational Sentence Ordering Network, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Baiyun Cui",
                    "Yingming Li",
                    "Zhongfei Zhang"
                ],
                "title": "BERT-enhanced Relational Sentence Ordering Network",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_131",
            "content": "Robert Dale, Natural language generation: The commercial state of the art in 2020, 2020, Nat. Lang. Eng, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Robert Dale"
                ],
                "title": "Natural language generation: The commercial state of the art in 2020",
                "pub_date": "2020",
                "pub_title": "Nat. Lang. Eng",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_132",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "136-ARR_v2_133",
            "content": "Ond\u0159ej Du\u0161ek, David Howcroft, Verena Rieser, Semantic Noise Matters for Neural Natural Language Generation, 2019, Proceedings of the 12th International Conference on Natural Language Generation, INLG 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Ond\u0159ej Du\u0161ek",
                    "David Howcroft",
                    "Verena Rieser"
                ],
                "title": "Semantic Noise Matters for Neural Natural Language Generation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 12th International Conference on Natural Language Generation, INLG 2019",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_134",
            "content": "Ond\u0159ej Du\u0161ek, Filip Jur\u010d\u00ed\u010dek, Training a Natural Language Generator From Unaligned Data, 2015, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Ond\u0159ej Du\u0161ek",
                    "Filip Jur\u010d\u00ed\u010dek"
                ],
                "title": "Training a Natural Language Generator From Unaligned Data",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "136-ARR_v2_135",
            "content": "Ond\u0159ej Du\u0161ek, Zden\u011bk Kasner, Evaluating Semantic Accuracy of Data-to-Text Generation with Natural Language Inference, 2020, Proceedings of the 13th International Conference on Natural Language Generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Ond\u0159ej Du\u0161ek",
                    "Zden\u011bk Kasner"
                ],
                "title": "Evaluating Semantic Accuracy of Data-to-Text Generation with Natural Language Inference",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 13th International Conference on Natural Language Generation",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_136",
            "content": "Ond\u0159ej Du\u0161ek, Jekaterina Novikova, Verena Rieser, Evaluating the state-of-the-art of End-to-End Natural Language Generation: The E2E NLG challenge, 2020, Comput. Speech Lang, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Ond\u0159ej Du\u0161ek",
                    "Jekaterina Novikova",
                    "Verena Rieser"
                ],
                "title": "Evaluating the state-of-the-art of End-to-End Natural Language Generation: The E2E NLG challenge",
                "pub_date": "2020",
                "pub_title": "Comput. Speech Lang",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_137",
            "content": "UNKNOWN, None, 2019, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_138",
            "content": "Thiago Ferreira, Claire Gardent, Nikolai Ilinykh, Chris Van Der Lee, Simon Mille, Diego Moussallem, Anastasia Shimorina, The 2020 Bilingual, Bi-Directional WebNLG+ Shared Task Overview and Evaluation Results, 2020, Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Thiago Ferreira",
                    "Claire Gardent",
                    "Nikolai Ilinykh",
                    "Chris Van Der Lee",
                    "Simon Mille",
                    "Diego Moussallem",
                    "Anastasia Shimorina"
                ],
                "title": "The 2020 Bilingual, Bi-Directional WebNLG+ Shared Task Overview and Evaluation Results",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_139",
            "content": "Diego Thiago Castro Ferreira, Emiel Moussallem, Sander Krahmer,  Wubben, Enriching the WebNLG corpus, 2018, Proceedings of the 11th International Conference on Natural Language Generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Diego Thiago Castro Ferreira",
                    "Emiel Moussallem",
                    "Sander Krahmer",
                    " Wubben"
                ],
                "title": "Enriching the WebNLG corpus",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 11th International Conference on Natural Language Generation",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_140",
            "content": "Chris Thiago Castro Ferreira,  Van Der Lee, Emiel Emiel Van Miltenburg,  Krahmer, Neural datato-text generation: A comparison between pipeline and end-to-end architectures, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Chris Thiago Castro Ferreira",
                    " Van Der Lee",
                    "Emiel Emiel Van Miltenburg",
                    " Krahmer"
                ],
                "title": "Neural datato-text generation: A comparison between pipeline and end-to-end architectures",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_141",
            "content": "Katja Filippova, Yasemin Altun, Overcoming the Lack of Parallel Data in Sentence Compression, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Katja Filippova",
                    "Yasemin Altun"
                ],
                "title": "Overcoming the Lack of Parallel Data in Sentence Compression",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_142",
            "content": "Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, The WebNLG Challenge: Generating Text from RDF Data, 2017, Proceedings of the 10th International Conference on Natural Language Generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Claire Gardent",
                    "Anastasia Shimorina",
                    "Shashi Narayan",
                    "Laura Perez-Beltrachini"
                ],
                "title": "The WebNLG Challenge: Generating Text from RDF Data",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 10th International Conference on Natural Language Generation",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_143",
            "content": "UNKNOWN, None, 2018, AllenNLP: A Deep Semantic Natural Language Processing Platform, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "AllenNLP: A Deep Semantic Natural Language Processing Platform",
                "pub": "CoRR"
            }
        },
        {
            "ix": "136-ARR_v2_144",
            "content": "Albert Gatt, Emiel Krahmer, Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation, 2018, J. Artif. Intell. Res, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Albert Gatt",
                    "Emiel Krahmer"
                ],
                "title": "Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation",
                "pub_date": "2018",
                "pub_title": "J. Artif. Intell. Res",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_145",
            "content": "Mor Geva, Eric Malmi, Idan Szpektor, Jonathan Berant, DiscoFuse: A Large-Scale Dataset for Discourse-Based Sentence Fusion, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Mor Geva",
                    "Eric Malmi",
                    "Idan Szpektor",
                    "Jonathan Berant"
                ],
                "title": "DiscoFuse: A Large-Scale Dataset for Discourse-Based Sentence Fusion",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "136-ARR_v2_146",
            "content": "UNKNOWN, None, 2016, End-to-End Neural Sentence Ordering Using Pointer Network, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "End-to-End Neural Sentence Ordering Using Pointer Network",
                "pub": "CoRR"
            }
        },
        {
            "ix": "136-ARR_v2_147",
            "content": "Ana Suchin Gururangan, Swabha Marasovi\u0107, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah A Downey,  Smith, Don't Stop Pretraining: Adapt Language Models to Domains and Tasks, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Ana Suchin Gururangan",
                    "Swabha Marasovi\u0107",
                    "Kyle Swayamdipta",
                    "Iz Lo",
                    "Doug Beltagy",
                    "Noah A Downey",
                    " Smith"
                ],
                "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_148",
            "content": "Hamza Harkous, Isabel Groves, Amir Saffari, Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity, 2020, Proceedings of the 28th International Conference on Computational Linguistics, Online.",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Hamza Harkous",
                    "Isabel Groves",
                    "Amir Saffari"
                ],
                "title": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": "Online"
            }
        },
        {
            "ix": "136-ARR_v2_149",
            "content": "Peyman Heidari, Arash Einolghozati, Shashank Jain, Soumya Batra, Lee Callender, Ankit Arun, Shawn Mei, Sonal Gupta, Pinar Donmez, Vikas Bhardwaj, Anuj Kumar, Michael White, Getting to Production with Few-shot Natural Language Generation Models, 2021, Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Peyman Heidari",
                    "Arash Einolghozati",
                    "Shashank Jain",
                    "Soumya Batra",
                    "Lee Callender",
                    "Ankit Arun",
                    "Shawn Mei",
                    "Sonal Gupta",
                    "Pinar Donmez",
                    "Vikas Bhardwaj",
                    "Anuj Kumar",
                    "Michael White"
                ],
                "title": "Getting to Production with Few-shot Natural Language Generation Models",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_150",
            "content": "Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, Wei Xu, Neural CRF Model for Sentence Alignment in Text Simplification, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Chao Jiang",
                    "Mounica Maddela",
                    "Wuwei Lan",
                    "Yang Zhong",
                    "Wei Xu"
                ],
                "title": "Neural CRF Model for Sentence Alignment in Text Simplification",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_151",
            "content": "Zhijing Jin, Qipeng Guo, Xipeng Qiu, Zheng Zhang, GenWiki: A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation, 2020, Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Online.",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Zhijing Jin",
                    "Qipeng Guo",
                    "Xipeng Qiu",
                    "Zheng Zhang"
                ],
                "title": "GenWiki: A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020",
                "pub": "Online"
            }
        },
        {
            "ix": "136-ARR_v2_152",
            "content": "Mihir Kale, Abhinav Rastogi, Template Guided Text Generation for Task-Oriented Dialogue, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Mihir Kale",
                    "Abhinav Rastogi"
                ],
                "title": "Template Guided Text Generation for Task-Oriented Dialogue",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_153",
            "content": "Mihir Kale, Abhinav Rastogi, Text-to-Text Pre-Training for Data-to-Text Tasks, 2020, Proceedings of the 13th International Conference on Natural Language Generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Mihir Kale",
                    "Abhinav Rastogi"
                ],
                "title": "Text-to-Text Pre-Training for Data-to-Text Tasks",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 13th International Conference on Natural Language Generation",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_154",
            "content": "Zden\u011bk Kasner, Ond\u0159ej Du\u0161ek, Data-to-Text Generation with Iterative Text Editing, 2020, Proceedings of the 13th International Conference on Natural Language Generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Zden\u011bk Kasner",
                    "Ond\u0159ej Du\u0161ek"
                ],
                "title": "Data-to-Text Generation with Iterative Text Editing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 13th International Conference on Natural Language Generation",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_155",
            "content": "Zden\u011bk Kasner, Ond\u0159ej Du\u0161ek, Train Hard, Finetune Easy: Multilingual Denoising for RDF-to-Text Generation, 2020, Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Zden\u011bk Kasner",
                    "Ond\u0159ej Du\u0161ek"
                ],
                "title": "Train Hard, Finetune Easy: Multilingual Denoising for RDF-to-Text Generation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_156",
            "content": "Pei Ke, Haozhe Ji, Yu Ran, Xin Cui, Liwei Wang, Linfeng Song, Xiaoyan Zhu, Minlie Huang, JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs, 2021, Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, volume ACL/IJCNLP 2021 of Findings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Pei Ke",
                    "Haozhe Ji",
                    "Yu Ran",
                    "Xin Cui",
                    "Liwei Wang",
                    "Linfeng Song",
                    "Xiaoyan Zhu",
                    "Minlie Huang"
                ],
                "title": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, volume ACL/IJCNLP 2021 of Findings of ACL",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_157",
            "content": "UNKNOWN, None, 2019, CTRL: A Conditional Transformer Language Model for Controllable Generation, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "CTRL: A Conditional Transformer Language Model for Controllable Generation",
                "pub": "CoRR"
            }
        },
        {
            "ix": "136-ARR_v2_158",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A Method for Stochastic Optimization, 2015, 3rd International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "P Diederik",
                    "Jimmy Kingma",
                    " Ba"
                ],
                "title": "Adam: A Method for Stochastic Optimization",
                "pub_date": "2015",
                "pub_title": "3rd International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_159",
            "content": "Anirban Laha, Parag Jain, Abhijit Mishra, Karthik Sankaranarayanan, Scalable Micro-planned Generation of Discourse from Structured Data, 2019, Comput. Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Anirban Laha",
                    "Parag Jain",
                    "Abhijit Mishra",
                    "Karthik Sankaranarayanan"
                ],
                "title": "Scalable Micro-planned Generation of Discourse from Structured Data",
                "pub_date": "2019",
                "pub_title": "Comput. Linguistics",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_160",
            "content": "Mirella Lapata, Probabilistic Text Structuring: Experiments with Sentence Ordering, 2003, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Mirella Lapata"
                ],
                "title": "Probabilistic Text Structuring: Experiments with Sentence Ordering",
                "pub_date": "2003",
                "pub_title": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_161",
            "content": "Kenton Lee, Luheng He, Luke Zettlemoyer, Higher-Order Coreference Resolution with Coarseto-Fine Inference, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Kenton Lee",
                    "Luheng He",
                    "Luke Zettlemoyer"
                ],
                "title": "Higher-Order Coreference Resolution with Coarseto-Fine Inference",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_162",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, BART: Denoising Sequence-to-Sequence Pretraining for Natural Language Generation, Translation, and Comprehension, 2020, Proceedings of the 58th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Mike Lewis",
                    "Yinhan Liu",
                    "Naman Goyal",
                    "Marjan Ghazvininejad",
                    "Abdelrahman Mohamed",
                    "Omer Levy",
                    "Veselin Stoyanov",
                    "Luke Zettlemoyer"
                ],
                "title": "BART: Denoising Sequence-to-Sequence Pretraining for Natural Language Generation, Translation, and Comprehension",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_163",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics, ACL 2020",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_164",
            "content": "Jiwei Li, Dan Jurafsky, Neural Net Models of Open-domain Discourse Coherence, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "Jiwei Li",
                    "Dan Jurafsky"
                ],
                "title": "Neural Net Models of Open-domain Discourse Coherence",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_165",
            "content": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, None, 1907, RoBERTa: A Robustly Optimized BERT Pretraining Approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": [
                    "Yinhan Liu",
                    "Myle Ott",
                    "Naman Goyal",
                    "Jingfei Du",
                    "Mandar Joshi",
                    "Danqi Chen",
                    "Omer Levy",
                    "Mike Lewis",
                    "Luke Zettlemoyer",
                    "Veselin Stoyanov"
                ],
                "title": null,
                "pub_date": "1907",
                "pub_title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_166",
            "content": "UNKNOWN, None, 2021, Improving Compositional Generalization with Self-Training for Data-to-Text Generation, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Improving Compositional Generalization with Self-Training for Data-to-Text Generation",
                "pub": "CoRR"
            }
        },
        {
            "ix": "136-ARR_v2_167",
            "content": "Amit Moryossef, Yoav Goldberg, Ido Dagan, Improving Quality and Efficiency in Plan-based Neural Data-to-text Generation, 2019, Proceedings of the 12th International Conference on Natural Language Generation, INLG 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": [
                    "Amit Moryossef",
                    "Yoav Goldberg",
                    "Ido Dagan"
                ],
                "title": "Improving Quality and Efficiency in Plan-based Neural Data-to-text Generation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 12th International Conference on Natural Language Generation, INLG 2019",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_168",
            "content": "Amit Moryossef, Yoav Goldberg, Ido Dagan, Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b55",
                "authors": [
                    "Amit Moryossef",
                    "Yoav Goldberg",
                    "Ido Dagan"
                ],
                "title": "Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "136-ARR_v2_169",
            "content": "Shashi Narayan, Claire Gardent, Shay Cohen, Anastasia Shimorina, Split and Rephrase, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b56",
                "authors": [
                    "Shashi Narayan",
                    "Claire Gardent",
                    "Shay Cohen",
                    "Anastasia Shimorina"
                ],
                "title": "Split and Rephrase",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_170",
            "content": "Jekaterina Novikova, Ond\u0159ej Du\u0161ek, Verena Rieser, The E2E Dataset: New Challenges For End-to-End Generation, 2017, Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, .",
            "ntype": "ref",
            "meta": {
                "xid": "b57",
                "authors": [
                    "Jekaterina Novikova",
                    "Ond\u0159ej Du\u0161ek",
                    "Verena Rieser"
                ],
                "title": "The E2E Dataset: New Challenges For End-to-End Generation",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_171",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a Method for Automatic Evaluation of Machine Translation, 2002-07-06, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b58",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
                "pub_date": "2002-07-06",
                "pub_title": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_172",
            "content": "P Ankur, Xuezhi Parikh, Sebastian Wang, Manaal Gehrmann, Bhuwan Faruqui, Diyi Dhingra, Dipanjan Yang,  Das, ToTTo: A Controlled Table-To-Text Generation Dataset, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b59",
                "authors": [
                    "P Ankur",
                    "Xuezhi Parikh",
                    "Sebastian Wang",
                    "Manaal Gehrmann",
                    "Bhuwan Faruqui",
                    "Diyi Dhingra",
                    "Dipanjan Yang",
                    " Das"
                ],
                "title": "ToTTo: A Controlled Table-To-Text Generation Dataset",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_173",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b60",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_174",
            "content": "Zachary Yang, Martin Devito, Alykhan Raison, Sasank Tejani, Benoit Chilamkurthy, Lu Steiner, Junjie Fang, Soumith Bai,  Chintala, PyTorch: An Imperative Style, High-Performance Deep Learning Library, 2019, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b61",
                "authors": [
                    "Zachary Yang",
                    "Martin Devito",
                    "Alykhan Raison",
                    "Sasank Tejani",
                    "Benoit Chilamkurthy",
                    "Lu Steiner",
                    "Junjie Fang",
                    "Soumith Bai",
                    " Chintala"
                ],
                "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
                "pub_date": "2019",
                "pub_title": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_175",
            "content": "Ratish Puduppully, Li Dong, Mirella Lapata, The Thirty-First Innovative Applications of Artificial Intelligence Conference, 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b62",
                "authors": [
                    "Ratish Puduppully",
                    "Li Dong",
                    "Mirella Lapata"
                ],
                "title": "The Thirty-First Innovative Applications of Artificial Intelligence Conference",
                "pub_date": "2019",
                "pub_title": "The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_176",
            "content": "Cl\u00e9ment Rebuffel, Marco Roberti, Laure Soulier, Geoffrey Scoutheeten, Rossella Cancelliere, Patrick Gallinari, Controlling hallucinations at word level in data-to-text generation, 2022, Data Min. Knowl. Discov, .",
            "ntype": "ref",
            "meta": {
                "xid": "b63",
                "authors": [
                    "Cl\u00e9ment Rebuffel",
                    "Marco Roberti",
                    "Laure Soulier",
                    "Geoffrey Scoutheeten",
                    "Rossella Cancelliere",
                    "Patrick Gallinari"
                ],
                "title": "Controlling hallucinations at word level in data-to-text generation",
                "pub_date": "2022",
                "pub_title": "Data Min. Knowl. Discov",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_177",
            "content": "Ehud Reiter, An Architecture for Data-to-Text Systems, 2007, Proceedings of the Eleventh European Workshop on Natural Language Generation, ENLG 2007, Schloss Dagstuhl, .",
            "ntype": "ref",
            "meta": {
                "xid": "b64",
                "authors": [
                    "Ehud Reiter"
                ],
                "title": "An Architecture for Data-to-Text Systems",
                "pub_date": "2007",
                "pub_title": "Proceedings of the Eleventh European Workshop on Natural Language Generation, ENLG 2007, Schloss Dagstuhl",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_178",
            "content": "Ehud Reiter, Robert Dale, Building applied natural language generation systems, 1997, Nat. Lang. Eng, .",
            "ntype": "ref",
            "meta": {
                "xid": "b65",
                "authors": [
                    "Ehud Reiter",
                    "Robert Dale"
                ],
                "title": "Building applied natural language generation systems",
                "pub_date": "1997",
                "pub_title": "Nat. Lang. Eng",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_179",
            "content": "UNKNOWN, None, 2007, Hinrich Sch\u00fctze, and Iryna Gurevych. 2020. Investigating Pretrained Language Models for Graph-to-Text Generation. CoRR, abs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b66",
                "authors": null,
                "title": null,
                "pub_date": "2007",
                "pub_title": "Hinrich Sch\u00fctze, and Iryna Gurevych. 2020. Investigating Pretrained Language Models for Graph-to-Text Generation. CoRR, abs",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_180",
            "content": "Anna Rogers, Changing the World by Changing the Data, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b67",
                "authors": [
                    "Anna Rogers"
                ],
                "title": "Changing the World by Changing the Data",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "136-ARR_v2_181",
            "content": "Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, Xiaoyan Zhu, Long and Diverse Text Generation with Planning-based Hierarchical Variational Model, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b68",
                "authors": [
                    "Zhihong Shao",
                    "Minlie Huang",
                    "Jiangtao Wen",
                    "Wenfei Xu",
                    "Xiaoyan Zhu"
                ],
                "title": "Long and Diverse Text Generation with Planning-based Hierarchical Variational Model",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_182",
            "content": "Xiaoyu Shen, Ernie Chang, Hui Su, Cheng Niu, Dietrich Klakow, Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b69",
                "authors": [
                    "Xiaoyu Shen",
                    "Ernie Chang",
                    "Hui Su",
                    "Cheng Niu",
                    "Dietrich Klakow"
                ],
                "title": "Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_183",
            "content": "Yixuan Su, Zaiqiao Meng, Simon Baker, Nigel Collier, Few-Shot Table-to-Text Generation with Prototype Memory, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b70",
                "authors": [
                    "Yixuan Su",
                    "Zaiqiao Meng",
                    "Simon Baker",
                    "Nigel Collier"
                ],
                "title": "Few-Shot Table-to-Text Generation with Prototype Memory",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_184",
            "content": "Yixuan Su, David Vandyke, Sihui Wang, Yimai Fang, Nigel Collier, Plan-then-Generate: Controlled Data-to-Text Generation via Planning, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b71",
                "authors": [
                    "Yixuan Su",
                    "David Vandyke",
                    "Sihui Wang",
                    "Yimai Fang",
                    "Nigel Collier"
                ],
                "title": "Plan-then-Generate: Controlled Data-to-Text Generation via Planning",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_185",
            "content": "Jianzhong Bayu Distiawan Trisedya, Rui Qi,  Zhang, Sentence Generation for Entity Description with Content-Plan Attention, 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b72",
                "authors": [
                    "Jianzhong Bayu Distiawan Trisedya",
                    "Rui Qi",
                    " Zhang"
                ],
                "title": "Sentence Generation for Entity Description with Content-Plan Attention",
                "pub_date": "2020",
                "pub_title": "The Thirty-Second Innovative Applications of Artificial Intelligence Conference",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_186",
            "content": "Miruna-Adriana Emiel Van Miltenburg, Ond\u0159ej Clinciu, Dimitra Du\u0161ek, Stephanie Gkatzia, Leo Inglis, Saad Lepp\u00e4nen, Emma Mahamood, Stephanie Manning, Craig Schoch, Luou Thomson,  Wen, Underreporting of errors in NLG output, and what to do about it, 2021, Proceedings of the 14th International Conference on Natural Language Generation, INLG 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b73",
                "authors": [
                    "Miruna-Adriana Emiel Van Miltenburg",
                    "Ond\u0159ej Clinciu",
                    "Dimitra Du\u0161ek",
                    "Stephanie Gkatzia",
                    "Leo Inglis",
                    "Saad Lepp\u00e4nen",
                    "Emma Mahamood",
                    "Stephanie Manning",
                    "Craig Schoch",
                    "Luou Thomson",
                    " Wen"
                ],
                "title": "Underreporting of errors in NLG output, and what to do about it",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 14th International Conference on Natural Language Generation, INLG 2021",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_187",
            "content": "Oriol Vinyals, Meire Fortunato, Navdeep Jaitly, Pointer networks, 2015, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b74",
                "authors": [
                    "Oriol Vinyals",
                    "Meire Fortunato",
                    "Navdeep Jaitly"
                ],
                "title": "Pointer networks",
                "pub_date": "2015",
                "pub_title": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_188",
            "content": "Tianming Wang, Xiaojun Wan, Hierarchical Attention Networks for Sentence Ordering, 2019, The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b75",
                "authors": [
                    "Tianming Wang",
                    "Xiaojun Wan"
                ],
                "title": "Hierarchical Attention Networks for Sentence Ordering",
                "pub_date": "2019",
                "pub_title": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_189",
            "content": "Yufei Wang, Ian Wood, Stephen Wan, Mark Dras, Mark Johnson, Mention Flags (MF): Constraining Transformer-based Text Generators, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b76",
                "authors": [
                    "Yufei Wang",
                    "Ian Wood",
                    "Stephen Wan",
                    "Mark Dras",
                    "Mark Johnson"
                ],
                "title": "Mention Flags (MF): Constraining Transformer-based Text Generators",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "136-ARR_v2_190",
            "content": "Adina Williams, Nikita Nangia, Samuel , A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b77",
                "authors": [
                    "Adina Williams",
                    "Nikita Nangia",
                    "Samuel "
                ],
                "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "136-ARR_v2_191",
            "content": "Sam Wiseman, Stuart Shieber, Alexander Rush, Learning Neural Templates for Text Generation, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b78",
                "authors": [
                    "Sam Wiseman",
                    "Stuart Shieber",
                    "Alexander Rush"
                ],
                "title": "Learning Neural Templates for Text Generation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_192",
            "content": "UNKNOWN, None, 1910, HuggingFace's Transformers: State-of-the-art Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b79",
                "authors": null,
                "title": null,
                "pub_date": "1910",
                "pub_title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_193",
            "content": "Xinnuo Xu, Ond\u0159ej Du\u0161ek, Verena Rieser, Ioannis Konstas, AggGen: Ordering and Aggregating while Generating, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b80",
                "authors": [
                    "Xinnuo Xu",
                    "Ond\u0159ej Du\u0161ek",
                    "Verena Rieser",
                    "Ioannis Konstas"
                ],
                "title": "AggGen: Ordering and Aggregating while Generating",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "136-ARR_v2_194",
            "content": "Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu, PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization, 2020, Proceedings of the 37th International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b81",
                "authors": [
                    "Jingqing Zhang",
                    "Yao Zhao",
                    "Mohammad Saleh",
                    "Peter Liu"
                ],
                "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 37th International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v2_195",
            "content": "Chao Zhao, Marilyn Walker, Snigdha Chaturvedi, Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b82",
                "authors": [
                    "Chao Zhao",
                    "Marilyn Walker",
                    "Snigdha Chaturvedi"
                ],
                "title": "Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "136-ARR_v2_0@0",
            "content": "Neural Pipeline for Zero-Shot Data-to-Text Generation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_0",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_2@0",
            "content": "In data-to-text (D2T) generation, training on in-domain data leads to overfitting to the data representation and repeating training data noise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_2",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_2@1",
            "content": "We examine how to avoid finetuning pretrained language models (PLMs) on D2T generation datasets while still taking advantage of surface realization capabilities of PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_2",
            "start": 144,
            "end": 312,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_2@2",
            "content": "Inspired by pipeline approaches, we propose to generate text by transforming single-item descriptions with a sequence of modules trained on generaldomain text-based operations: ordering, aggregation, and paragraph compression.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_2",
            "start": 314,
            "end": 539,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_2@3",
            "content": "We train PLMs for performing these operations on a synthetic corpus WIKIFLUENT which we build from English Wikipedia.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_2",
            "start": 541,
            "end": 657,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_2@4",
            "content": "Our experiments on two major triple-to-text datasets-WebNLG and E2E-show that our approach enables D2T generation from RDF triples in zero-shot settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_2",
            "start": 659,
            "end": 811,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_2@5",
            "content": "1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_2",
            "start": 813,
            "end": 813,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_4@0",
            "content": "The aim of data-to-text (D2T) generation is to produce natural language descriptions of structured data (Gatt and Krahmer, 2018;Reiter and Dale, 1997).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_4",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_4@1",
            "content": "Although pipelines of rule-based D2T generation modules are still used in practice (Dale, 2020), end-to-end approaches based on PLMs recently showed superior benchmark performance (Ke et al., 2021;Chen et al., 2020a;Ferreira et al., 2020;Kale and Rastogi, 2020b;Ribeiro et al., 2020), surpassing pipeline systems (Ferreira et al., 2019) in both automatic and human evaluation metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_4",
            "start": 152,
            "end": 535,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_5@0",
            "content": "Finetuning PLMs on human-written references is widely accepted as a standard approach for adapting PLMs to the D2T generation objective and achieving good performance on a given benchmark (Agarwal et al., 2021;Ke et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_5",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_5@1",
            "content": "However, finetuning a model on the domain-specific data leads to overfitting to the particular benchmark, decreasing performance on out-of-domain 1 Our code and data is available at https://github.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_5",
            "start": 228,
            "end": 424,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_5@2",
            "content": "com/kasnerz/zeroshot-d2t-pipeline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_5",
            "start": 426,
            "end": 459,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_5@3",
            "content": "In-domain knowledge is included only in the simple hand-crafted templates for each predicate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_5",
            "start": 461,
            "end": 553,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_6@0",
            "content": "data (Laha et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_6",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_6@1",
            "content": "Gathering a large set of references for a particular domain is also costly and time-consuming as it usually requires collecting human-written references through crowdsourcing .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_6",
            "start": 26,
            "end": 201,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_6@2",
            "content": "These problems can be partially mitigated using few-shot approaches (Chen et al., 2020b;Ke et al., 2021;Su et al., 2021a), which operate with only several dozens or hundreds of annotated examples, but the robustness of these approaches is questionable-selecting a representative set of examples which would improve performance is difficult (Chang et al., 2021a), and the limited sample is often noisy, increasing the chance of hallucinations and omissions (Du\u0161ek et al., 2019;Harkous et al., 2020;Rebuffel et al., 2022).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_6",
            "start": 203,
            "end": 722,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_7@0",
            "content": "In this paper, we present a zero-shot alternative to the traditional finetuning paradigm by formulating the D2T generation from RDF triples as a sequence of general-domain operations over text in natural language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_7",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_7@1",
            "content": "We start by transforming individual triples to text using trivial templates, which we subsequently order, aggregate, and compress on the paragraph level to produce the resulting description of the data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_7",
            "start": 214,
            "end": 415,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_7@2",
            "content": "In constrast to traditional pipeline systems, all our pipeline modules are built upon PLMs and operate over sentences in natural language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_7",
            "start": 417,
            "end": 554,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_7@3",
            "content": "The modules are trained on our new WIKI-FLUENT corpus, which contains 934k examples of first paragraphs from the English Wikipedia, each supplied with a synthesized set of simple templatelike sentences which together convey the meaning of the original paragraph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_7",
            "start": 556,
            "end": 817,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_7@4",
            "content": "Our approach allows generating natural language descriptions from RDF triples with a minimum amount of domain-specific rules or knowledge and without using training data from the D2T datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_7",
            "start": 819,
            "end": 1010,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_7@5",
            "content": "Although our approach is primarily a probe into the territory of zero-shot approaches and cannot yet match the quality of stateof-the-art models, we show that it can yield large improvements upon simple baselines and match older supervised systems on automatic metrics for text fluency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_7",
            "start": 1012,
            "end": 1297,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_7@6",
            "content": "Moreover, the semantic accuracy metrics and our manual error analysis suggest that our approach offers a way to prevent omissions and hallucinations common in few-shot approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_7",
            "start": 1299,
            "end": 1477,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_8@0",
            "content": "Our contributions are the following: (1) We propose an alternative D2T generation approach based on general-domain text-to-text operations (ordering, aggregation, and paragraph compression).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_8",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_9@0",
            "content": "(2) We introduce a synthetic WIKIFLUENT corpus containing 934k sentences based on English Wikipedia, providing training data for the operations in (1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_9",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_9@1",
            "content": "(3) We apply our system on two D2T datasets and evaluate its performance both automatically and manually, including the contribution of individual pipeline modules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_9",
            "start": 152,
            "end": 315,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_9@2",
            "content": "(4) We release our code, data, pretrained models, and system outputs to ease future research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_9",
            "start": 317,
            "end": 409,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_9@3",
            "content": "1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_9",
            "start": 411,
            "end": 411,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_10@0",
            "content": "2 Related Work D2T Generation with PLMs Large neural language models pretrained on self-supervised tasks (Lewis et al., 2020;Liu et al., 2019;Devlin et al., 2019) have recently gained a lot of traction in D2T generation research (Ferreira et al., 2020;Kasner and Du\u0161ek, 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_10",
            "start": 0,
            "end": 276,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_10@1",
            "content": "Following Chen et al. (2020b), other works adopted PLMs for few-shot D2T generation (Chang et al., 2021b;Su et al., 2021a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_10",
            "start": 278,
            "end": 400,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_10@2",
            "content": "Kale and Rastogi (2020b) and Ribeiro et al. (2020) showed that PLMs using linearized representations of data can outperform graph neural networks on graph-to-text datasets, recently surpassed again by graph-based models (Ke et al., 2021;Chen et al., 2020a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_10",
            "start": 402,
            "end": 658,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_10@3",
            "content": "Although the models make use of generaldomain pretraining tasks, all of them are eventually finetuned on domain-specific data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_10",
            "start": 660,
            "end": 785,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_11@0",
            "content": "Pipeline-based D2T Generation Until the recent surge of end-to-end approaches , using several modules connected in a pipeline was a major approach for D2T generation (Gatt and Krahmer, 2018;Reiter, 2007;Reiter and Dale, 1997).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_11",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_11@1",
            "content": "Our approach is inspired by the pipeline approaches, in particular the pipelines utilizing neural modules (Ferreira et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_11",
            "start": 227,
            "end": 356,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_11@2",
            "content": "In contrast with these approaches, our pipeline works with unstructured data in natural language and it operates in zero-shot setting, i.e. without using any training data from target D2T datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_11",
            "start": 358,
            "end": 554,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_11@3",
            "content": "Laha et al. (2019) introduce a three-step pipeline for zero-shot D2T generation similar to ours.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_11",
            "start": 556,
            "end": 651,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_11@4",
            "content": "Unlike the approach we describe here, they use a semiautomatic template generation system, 2 their sentence fusion is rule-based, and they do not address content planning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_11",
            "start": 653,
            "end": 823,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_12@0",
            "content": "Content Planning in D2T Generation Content planning, i.e. the task of ordering input facts and aggregating them into individual sentences, is one of the steps of the traditional D2T pipeline (Gatt and Krahmer, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_12",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_12@1",
            "content": "As shown by Moryossef et al. (2019a,b) and confirmed by other works (Puduppully et al., 2019;Trisedya et al., 2020;, including a content plan improves the quality of outputs in neural D2T pipelines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_12",
            "start": 217,
            "end": 414,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_12@2",
            "content": "Unlike the aforementioned planners, which use predicates or keys from D2T datasets for representing the data items, our planner is trained on ordering sentences in natural language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_12",
            "start": 416,
            "end": 596,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_13@0",
            "content": "Sentence Ordering Sentence ordering is the task of organizing a set of natural language sentences to increase the coherence of a text (Barzilay et al., 2001;Lapata, 2003).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_13",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_13@1",
            "content": "Several neural methods for this task were proposed, using either interactions between pairs of sentences Li and Jurafsky, 2017), global interactions (Gong et al., 2016;Wang and Wan, 2019), or combination of both (Cui et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_13",
            "start": 172,
            "end": 402,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_13@2",
            "content": "We base our ordering module ( \u00a75.2) on the recent work of Calizzano et al. (2021), who use a pointer network (Wang and Wan, 2019;Vinyals et al., 2015) on top of a PLM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_13",
            "start": 404,
            "end": 570,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_14@0",
            "content": "Aggregating Input into Sentences Typically, multiple pieces of input information need to be merged into a single sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_14",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_14@1",
            "content": "Previous works (Wiseman et al., 2018;Shao et al., 2019;Shen et al., 2020;Xu et al., 2021) capture the segments which correspond to individual parts of the input as latent variables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_14",
            "start": 123,
            "end": 303,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_14@2",
            "content": "Unlike these works, we adopt a simpler scenario using an already ordered sequence of facts (see \u00a73.1), into which we selectively insert delimiters to mark sentence boundaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_14",
            "start": 305,
            "end": 479,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_15@0",
            "content": "We introduce paragraph compression (PC) as a new task and the final step in our D2T generation pipeline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_15",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_15@1",
            "content": "This task combines several standard natural-language tasks including sentence fusion, rephrasing, and coreference resolution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_15",
            "start": 105,
            "end": 229,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_15@2",
            "content": "Unlike text summarization or simplification Jiang et al., 2020), we aim to convey the complete semantics of the text without omitting any facts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_15",
            "start": 231,
            "end": 374,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_15@3",
            "content": "In contrast to sentence fusion (Geva et al., 2019;Barzilay and McKeown, 2005) or sentence compression (Filippova and Altun, 2013), we operate in the context of multiple sentences in a paragraph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_15",
            "start": 376,
            "end": 569,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_15@4",
            "content": "The task is the central focus of our WIKIFLUENT corpus ( \u00a74).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_15",
            "start": 571,
            "end": 631,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_16@0",
            "content": "Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_16",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_17@0",
            "content": "In this section, we provide the formal description of our proposed approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_17",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_17@1",
            "content": "We focus on the task of producing a natural language description Y for a set of n RDF triples X \" tx 1 , . . . , x n u. Each triple x i \" ts i , p i , o i u consists of subject s i , predicate p i , and object o i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_17",
            "start": 77,
            "end": 291,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_18@0",
            "content": "Our pipeline proceeds as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_18",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_18@1",
            "content": "Given a set of triples X on the input, we:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_18",
            "start": 34,
            "end": 75,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_19@0",
            "content": "(1) transform the triples into facts, which are sentences in natural language, (2) sort the facts using an ordering module, (3) insert sentence delimiters between the sorted facts using an aggregation module, (4) input the ordered sequence of facts with delimiters into a paragraph compression module, which generates the final description Y .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_19",
            "start": 0,
            "end": 342,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_19@1",
            "content": "The individual steps are described in the following sections: transforming individual triples to text ( \u00a73.1), ordering ( \u00a73.2), aggregation ( \u00a73.3), and paragraph compression ( \u00a73.4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_19",
            "start": 344,
            "end": 527,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_20@0",
            "content": "Transforming Triples to Facts",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_20",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_21@0",
            "content": "The first step in our pipeline involves transforming each of the input triples x i P X into a fact f i P F using a transformation T : X \u00d1 F .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_21",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_21@1",
            "content": "We define a fact f i as a single sentence in natural language describing x i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_21",
            "start": 142,
            "end": 219,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_21@2",
            "content": "The transformation serves two purposes: (a) preparing the data for the subsequent text-to-text operations, (b) introducing in-domain knowledge about the semantics of individual predicates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_21",
            "start": 221,
            "end": 408,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_21@3",
            "content": "This step can be realized e.g. using a simple template for each predicate (cf. \u00a75.1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_21",
            "start": 410,
            "end": 494,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_22@0",
            "content": "Ordering the Facts",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_22",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_23@0",
            "content": "We assume that the default order of triples X is random and the same applies for the respective facts F .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_23",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_23@1",
            "content": "Note, however, that that F is a indeed set of meaningful sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_23",
            "start": 106,
            "end": 172,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_23@2",
            "content": "We can use this to our advantage and apply a sentence ordering model to maximize the coherency of the paragraph resulting from their concatenation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_23",
            "start": 174,
            "end": 320,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_23@3",
            "content": "An example outcome of such operation may be grouping together facts mentioning birth date and birth place of a person, followed by their occupation (see Figure 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_23",
            "start": 322,
            "end": 484,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_23@4",
            "content": "The ordering module allows downstream modules to only focus on operations over neighboring sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_23",
            "start": 486,
            "end": 586,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_24@0",
            "content": "Formally, we apply the ordering model OpF q to get an ordered sequence of facts: F o \" tf o 1 , . . . , f on u, where o 1:n is a permutation of indices.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_24",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_24@1",
            "content": "We describe our ordering model in \u00a75.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_24",
            "start": 153,
            "end": 191,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_25@0",
            "content": "Aggregating the Facts",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_25",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_26@0",
            "content": "Some facts will be typically mentioned together in a single sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_26",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_26@1",
            "content": "Considering the previous example, occupation is likely to be mentioned separately, while birth date and birth place are likely to be mentioned together.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_26",
            "start": 70,
            "end": 221,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_26@2",
            "content": "Using an ordered sequence of facts as input, we can apply an aggregation model to decide which facts should be merged into a single sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_26",
            "start": 223,
            "end": 363,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_27@0",
            "content": "Formally, the aggregation model takes a sequence of ordered facts F o as input and produces a sequence of sentence delimiters ApF o q \" t\u03b4 o 1 , \u03b4 o 2 , . . . , \u03b4 o n\u00b41 u; \u03b4 i P t0, 1u. The output \u03b4 i \" 1 means that the neighboring facts should be mentioned separately, i.e. the neighboring sentences should not be fused.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_27",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_27@1",
            "content": "Conversely, \u03b4 i \" 0 means that the facts should be aggregated and their corresponding sentences should be fused.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_27",
            "start": 322,
            "end": 433,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_27@2",
            "content": "We describe our aggregation model in \u00a75.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_27",
            "start": 435,
            "end": 476,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_28@0",
            "content": "Paragraph Compression",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_28",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_29@0",
            "content": "The paragraph compression (PC) model is a generative model which outputs the final text description.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_29",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_29@1",
            "content": "It has two main objectives: (a) fusing related sentences, i.e., sentences i and j in between which \u03b4 i \" 0, and (b) rephrasing the text to improve its fluency, e.g. fixing disfluencies in the templates, replacing noun phrases with refering expressions, etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_29",
            "start": 101,
            "end": 357,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_29@2",
            "content": "The goal of the task is to preserve the semantics of the text which is an already ordered sequence of sentences, so the edits will typically be minor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_29",
            "start": 359,
            "end": 508,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_29@3",
            "content": "Formally, the model takes as input the ordered sequence of facts with delimiters",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_29",
            "start": 510,
            "end": 589,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_30@0",
            "content": "F a \" tf o 1 , \u03b4 o 1 , f o 2 , . . . , \u03b4 o n\u00b41 ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_30",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_31@0",
            "content": "WIKIFLUENT Corpus",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_31",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_32@0",
            "content": "Here we descibe the process of building a largescale synthetic corpus WIKIFLUENT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_32",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_32@1",
            "content": "The corpus provides training data for the neural models which we use in our implementation of the ordering, aggregation, and paragraph compression modules (cf. \u00a75).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_32",
            "start": 82,
            "end": 245,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_33@0",
            "content": "Our goal is to cover a broad range of domains while capturing the sentence style in D2T generation with respect to both the input facts and the target descriptions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_33",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_33@1",
            "content": "In other words, we aim to build a corpus in which (1) the input is a set of simple, template-like sentences, (2) the output is a fluent text in natural language preserving the semantics of the input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_33",
            "start": 165,
            "end": 363,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_33@2",
            "content": "As we describe below in detail, we achieve that by using human-written paragraphs in English Wikipedia and applying split-and-rephrase and coreference resolution models to obtain synthetic source texts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_33",
            "start": 365,
            "end": 566,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_33@3",
            "content": "The process is illustrated in Figure 2; corpus statistics are included in Appendix A.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_33",
            "start": 568,
            "end": 652,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_34@0",
            "content": "Data Source",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_34",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_35@0",
            "content": "For building the WIKIFLUENT corpus, we extracted 934k first paragraphs of articles from a Wikipedia dump 3 using WikiExtractor (Attardi, 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_35",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_35@1",
            "content": "Wikipedia is commonly used for large-scale pretraining of D2T generation models (Jin et al., 2020;Chen et al., 2020a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_35",
            "start": 144,
            "end": 261,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_35@2",
            "content": "Although it is not biasfree, it provides more balanced sample of natural language use than typical D2T generation datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_35",
            "start": 263,
            "end": 385,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_35@3",
            "content": "We used the first paragraphs of Wikipedia entries, which contain mostly concise, fact-based descriptions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_35",
            "start": 387,
            "end": 491,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_35@4",
            "content": "We selected paragraphs with length between 3",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_35",
            "start": 493,
            "end": 536,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_36@0",
            "content": "The Westmeath Examiner is a weekly newspaper in Westmeath, Ireland.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_36",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_37@0",
            "content": "It is located in Westmeath, Ireland.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_37",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_38@0",
            "content": "The Westmeath Examiner is a weekly newspaper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_38",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_39@0",
            "content": "The Westmeath Examiner is a weekly newspaper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_39",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_40@0",
            "content": "It was founded in 1882.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_40",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_41@0",
            "content": "The Westmeath Examiner is located in Westmeath, Ireland.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_41",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_42@0",
            "content": "The Westmeath Examiner was founded in 1882.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_42",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_42@1",
            "content": "The building process of the WIKIFLUENT corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_42",
            "start": 44,
            "end": 89,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_42@2",
            "content": "We apply a split-and-rephrase model on each sentence in the paragraph and resolve coreferences in the split sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_42",
            "start": 91,
            "end": 208,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_42@3",
            "content": "The result is a set of simple sentences which together convey the same meaning as the original paragraph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_42",
            "start": 210,
            "end": 314,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_42@4",
            "content": "The synthesized sentences are used as input into our models, the original human-written texts are used as ground truth.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_42",
            "start": 316,
            "end": 434,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_42@5",
            "content": "30-430 characters; filtering out lists, disambiguations, and repeated and malformed paragraphs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_42",
            "start": 436,
            "end": 530,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_42@6",
            "content": "To balance the length of inputs, we selected 250k examples each from 4 equally sized length ranges (30-130 characters, etc.).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_42",
            "start": 532,
            "end": 656,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_43@0",
            "content": "Split-and-Rephrase",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_43",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_44@0",
            "content": "To generate a set of simple sentences, we divide each paragraph into sentences using NLTK (Bird, 2006) and apply a split-and-rephrase model on each sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_44",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_44@1",
            "content": "Split-and-rephrase is a task of splitting a complex sentence into a meaning preserving sequence of shorter sentences .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_44",
            "start": 158,
            "end": 275,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_44@2",
            "content": "The process is illustrated in the upper part of Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_44",
            "start": 277,
            "end": 333,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_45@0",
            "content": "We train our split-and-rephrase model on the large-scale WikiSplit corpus by Botha et al. (2018), containing human-made sentence splits from Wikipedia edit history.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_45",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_45@1",
            "content": "Following the same setup as for a paragraph compression model ( \u00a73.4), we train BART-base (Lewis et al., 2020) on the WikiSplit dataset in a sequence-to-sequence setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_45",
            "start": 165,
            "end": 334,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_45@2",
            "content": "Next, we apply the trained split-and-rephrase model on each sentence in our Wikipedia-based corpus, uniformly randomly choosing between 0-2 recursive calls to ensure that the splits are not deterministic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_45",
            "start": 336,
            "end": 539,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_45@3",
            "content": "If the sentence cannot be meaningfully split, the model tends to duplicate the sentence on the output; in that case, we use only the original sentence and do not proceed with the splitting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_45",
            "start": 541,
            "end": 729,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_46@0",
            "content": "Coreference Replacement",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_46",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_47@0",
            "content": "As the next step, we concatenate the split sentences and apply a coreference resolution model (Gardner et al., 2018;Lee et al., 2018) in order to replace referring expressions with their antencendents (e.g., pronouns with noun phrases).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_47",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_47@1",
            "content": "The motivation for this step is to match the style of the facts (see \u00a73.1), which do not use pronouns since each fact describes a single triple only.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_47",
            "start": 237,
            "end": 385,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_47@2",
            "content": "Note that this procedure replaces the referring expressions only in the synthesized sentences (which are used as input) and keeps them in the original paragraphs (which are used as ground truth).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_47",
            "start": 387,
            "end": 581,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_47@3",
            "content": "As a consequence, the paragraph compression module is implicitly trained to generate referring expressions in the final description.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_47",
            "start": 583,
            "end": 714,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_48@0",
            "content": "Filtering",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_48",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_49@0",
            "content": "To ensure that the generated sentences convey the same semantics as the original paragraph, we use a pretrained RoBERTa model 4 (Liu et al., 2019) trained on the MultiNLI dataset (Williams et al., 2018) for checking the semantic accuracy of the generated text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_49",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_49@1",
            "content": "Following Du\u0161ek and Kasner (2020), we test if the original paragraph entails each of the synthesized sentences (checking for omissions), and if the set of concatenated synthesized sentences entails the original paragraph (checking for hallucinations).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_49",
            "start": 261,
            "end": 511,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_49@2",
            "content": "In a filtered version of the WIKIFLUENT corpus, we include only the examples without omissions or hallucinations (as computed by the model), reducing it to 714k examples (approximately 75% of the original size).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_49",
            "start": 513,
            "end": 723,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_50@0",
            "content": "Implementation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_50",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_51@0",
            "content": "In this section, we describe how we implement our pipeline modules ( \u00a73) using simple template transformations ( \u00a75.1) and neural models trained on the WIKIFLUENT dataset ( \u00a75.2-5.4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_51",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_51@1",
            "content": "5",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_51",
            "start": 184,
            "end": 184,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_52@0",
            "content": "Templates",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_52",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_53@0",
            "content": "We transform triples into facts ( \u00a73.1) using a singletriple template t i for each predicate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_53",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_53@1",
            "content": "For example, if p i \" instrument, then T pp i q \" \"s i plays o i \" (cf. Table 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_53",
            "start": 94,
            "end": 174,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_53@2",
            "content": "We follow previous work in which simple hand-crafted templates have been used as an efficient way of introducing domain knowledge (Kale and Rastogi, 2020a;Kasner and Du\u0161ek, 2020a) template generation engines (Laha et al., 2019;Heidari et al., 2021;Mehta et al., 2021), the approach may produce less fluent outputs, but it minimizes manual workload and makes it easier to control the quality of the input for the subsequent steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_53",
            "start": 176,
            "end": 604,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_54@0",
            "content": "Ordering Model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_54",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_55@0",
            "content": "For our ordering model ( \u00a73.2), we use the Simple Pointer model from Calizzano et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_55",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_56@0",
            "content": "The model is based on a pretrained BART-base extended with a pointer network from Wang and Wan (2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_56",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_56@1",
            "content": "We provide a short description of the model here; for details please refer to Calizzano et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_56",
            "start": 103,
            "end": 204,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_57@0",
            "content": "In the encoding phase, facts F are concatenated and tokenized.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_57",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_57@1",
            "content": "Each fact is surrounded by special tokens denoting the beginning (<s>) and the end (</s>) of the fact.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_57",
            "start": 63,
            "end": 164,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_57@2",
            "content": "The sequence is processed by the BART encoder, generating a sequence of encoder states E for each end token </s> representing the preceding fact.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_57",
            "start": 166,
            "end": 310,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_58@0",
            "content": "The decoding proceeds autoregressively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_58",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_58@1",
            "content": "To bootstrap the decoding process, the pair of tokens <s></s> is fed into the decoder, producing the decoder state d 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_58",
            "start": 40,
            "end": 159,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_58@2",
            "content": "The pointer network (attending to d 1 and E), selects the first ordered fact f o 1 , which is fed into the decoder in the next step (d 2 \"<s>f o 1 </s>).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_58",
            "start": 161,
            "end": 313,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_58@3",
            "content": "The process is repeated until the all the facts are decoded in a particular order.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_58",
            "start": 315,
            "end": 396,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_59@0",
            "content": "The pointer network computes the probability of a fact to be on the j-th position, using the encoder output E and the decoder output state d j .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_59",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_59@1",
            "content": "The network is based on the scaled dot product attention, where d j is the query and encoder outputs E i are the keys:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_59",
            "start": 145,
            "end": 262,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_60@0",
            "content": "Q \" d j W Q K \" EW K P j \" softmax \u02c6QK T ? b \u02d9.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_60",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_61@0",
            "content": "A dam is a barrier obstructing flowing water.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_61",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_62@0",
            "content": "A dam is a barrier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_62",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_63@0",
            "content": "A dam obstructs flowing water.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_63",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_63@1",
            "content": "Here W Q and W K P R b\u02c6b , b is the dimension of BART hidden states, and P j P R n`1 is the probability distribution for the j-th position (i.e., P ji is the probability that fact f i is on the j-th position).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_63",
            "start": 31,
            "end": 239,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_64@0",
            "content": "We train the model using the synthesized simple sentences in the WIKIFLUENT corpus, randomly shuffling the order of the sentences and training the model to restore their original order.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_64",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_65@0",
            "content": "Aggregation Model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_65",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_66@0",
            "content": "We base our aggregation model ( \u00a73.3) on RoBERTa-large (Liu et al., 2019) with a token classification head.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_66",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_66@1",
            "content": "6 Similarly to the ordering model ( \u00a75.2), we input the sequence of (now ordered) facts F o into the model, separating each pair of facts f o i with a special token </s> (used by the model as a separator).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_66",
            "start": 108,
            "end": 312,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_66@2",
            "content": "Subsequently, the token classification layer classifies each separator </s> i position into two classes t0, 1u corresponding to the delimiter \u03b4 i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_66",
            "start": 314,
            "end": 460,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_66@3",
            "content": "We ignore the outputs for the nonseparator tokens while computing cross-entropy loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_66",
            "start": 462,
            "end": 546,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_67@0",
            "content": "We create the training examples using the synthesized sentences in the WIKIFLUENT corpus, in which we set \u03b4 i \" 0 for the sentences i, i`1 which were originally aggregated (i.e., are the result of splitting a single sentence) and \u03b4 i \" 1 otherwise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_67",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_68@0",
            "content": "Paragraph Compression Model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_68",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_69@0",
            "content": "We adopt BART-base for our paragraph compression model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_69",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_69@1",
            "content": "We finetune the model on the WIK-IFLUENT corpus, concatenating the synthesized sentences on the input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_69",
            "start": 56,
            "end": 157,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_69@2",
            "content": "We add delimiters between the sentences i and i `1 where \u03b4 i \" 1 using a special token <sep>, which we add to the model vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_69",
            "start": 159,
            "end": 289,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_69@3",
            "content": "As shown in Keskar et al. (2019), including control codes for training the model can steer the model towards producing certain outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_69",
            "start": 291,
            "end": 425,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_70@0",
            "content": "Here we expect that the model will learn to fuse the sentences between which there are no delimiters on the input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_70",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_70@1",
            "content": "We evaluate how the model learns to respect the order and aggregation markers in \u00a77.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_70",
            "start": 115,
            "end": 200,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_71@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_71",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_72@0",
            "content": "We train our pipeline modules on the WIKIFLU-ENT corpus as described in \u00a75.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_72",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_72@1",
            "content": "Next, we use these modules without finetuning for generating descriptions for RDF triples on two English D2T datasets, WebNLG and E2E.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_72",
            "start": 76,
            "end": 209,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_73@0",
            "content": "Datasets The datasets differ in domain, size, textual style, and number of predicates (see Appendix A for details):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_73",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_74@0",
            "content": "\u2022 WebNLG Ferreira et al., 2020) contains RDF triples from DBPedia (Auer et al., 2007) and their crowdsourced descriptions. We use version 1.4 of the dataset for comparison to prior work. We hand-crafted templates for all 354 predicates, including unseen predicates in the test set. 7 \u2022 E2E (Novikova et al., 2017; contains restaurant recommendations in the form of attribute-value pairs. We use the cleaned version of the dataset (Du\u0161ek et al., 2019). Following previous work, we transform the attribute-value pairs into RDF triples (using the restaurant name as a subject) and then apply the same setup as for WebNLG. We created a template for each of the 8 attributes manually.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_74",
            "start": 0,
            "end": 678,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_75@0",
            "content": "In order to evaluate individual components of our pipeline, we train three versions of the paragraph compression model (see \u00a75.4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_75",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_76@0",
            "content": "The models share the same architecture and targets, but differ in their inputs:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_76",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_77@0",
            "content": "\u2022 PC -the model takes as an input ordered facts with delimiters (as described in \u00a73.4), \u2022 PC+AGG -the model takes as an input ordered facts without delimiters (i.e., the aggregation is left implicitly to the model), \u2022 PC+ORD+AGG -the model takes as an input facts in random order and without delimiters (i.e., both ordering and aggregation are left implicitly to the model). Correspondingly, we test three versions of the pipeline in our ablation study (see Figure 3): \u2022 3-STAGE -a full version of the pipeline consisting of the ordering model (ORD), the aggregation model (AGG) and the PC model (following the full pipeline from \u00a73), \u2022 2-STAGE -a pipeline consisting of the ORD model and the PC+AGG model, \u2022 1-STAGE -a single stage consisting of the PC+ORD+AGG model. We evaluate all versions of the pipeline with PC models trained on the full and filtered versions of the WIKIFLUENT dataset (see \u00a74).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_77",
            "start": 0,
            "end": 901,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_78@0",
            "content": "Evaluation and Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_78",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_79@0",
            "content": "Our main aim is the evaluation of our pipeline on the downstream task of D2T generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_79",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_79@1",
            "content": "We evaluate outputs from the {1,2,3}-STAGE variants of our pipeline using automatic metrics ( \u00a77.1), and we perform a detailed manual error analysis of the model outputs ( \u00a77.2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_79",
            "start": 89,
            "end": 266,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_79@2",
            "content": "We also evaluate the performance of the content planning modules and the ability of the PC module to follow the content plan ( \u00a77.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_79",
            "start": 268,
            "end": 400,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_79@3",
            "content": "In \u00a77.4, we include an intrinsic evaluation of our modules on the WIKIFLUENT test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_79",
            "start": 402,
            "end": 487,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_80@0",
            "content": "Automatic Metrics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_80",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_81@0",
            "content": "Following prior work, we use BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) to evaluate the outputs against the human references.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_81",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_81@1",
            "content": "8 We also evaluate the number of omission and hallucination errors (i.e., facts missing or added, respectively) using a metric from Du\u0161ek and Kasner (2020) based on a RoBERTa model (Liu et al., 2019) pretrained on natural language inference (NLI).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_81",
            "start": 150,
            "end": 396,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_81@2",
            "content": "9 We include a diverse set of baselines for comparison.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_81",
            "start": 398,
            "end": 452,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_81@3",
            "content": "For WebNLG (see Table 3), we compare our systems with the results of:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_81",
            "start": 454,
            "end": 522,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_82@0",
            "content": "\u2022 UPF-FORGe and MELBOURNE -systems (grammar-based and supervised, respectively) from the first run of WebNLG Challenge (Gardent et al., 2017), \u2022 Ke et al. (2021) -a state-of-the-art system with a structure-aware encoder and task-specific pretraining, \u2022 Laha et al. (2019) -the only other (to our knowledge) zero-shot D2T generation system applied to WebNLG. For E2E (see Table 4), we compare our systems with the results of: \u2022 TGEN (Du\u0161ek and Jur\u010d\u00ed\u010dek, 2015) -the baseline system for the E2E Challenge , \u2022 Harkous et al. (2020) -a state-of-the-art supervised system on cleaned E2E data. For both datasets, COPY denotes the baseline of copying the facts without further processing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_82",
            "start": 0,
            "end": 679,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_83@0",
            "content": "The automatic evaluation shows that our systems consistently outperform the COPY baseline (e.g., \"12 BLEU points for E2E), which is already strong thanks to our manually curated set of templates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_83",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_83@1",
            "content": "10 Automatic scores also suggest that our systems are comparable with some older supervised systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_83",
            "start": 196,
            "end": 295,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_83@2",
            "content": "Nevertheless, our systems still underperform the state-of-the-art supervised systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_83",
            "start": 297,
            "end": 381,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_83@3",
            "content": "For this reason, we further focus on manual error analysis in \u00a77.2 to pinpoint the current shortcomings of our approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_83",
            "start": 383,
            "end": 502,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_84@0",
            "content": "The 2-STAGE system is generally on par with the 3-STAGE system or better, which indicates that explicit aggregation using the AGG model may not be necessary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_84",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_84@1",
            "content": "However, an advantage of having a separate aggregation module is the possibility to control the aggregation step explicitly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_84",
            "start": 158,
            "end": 281,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_84@2",
            "content": "The models using the filtered version of the corpus generally produce better results, although they also bring in a larger number of omissions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_84",
            "start": 283,
            "end": 425,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_85@0",
            "content": "Manual Error Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_85",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_86@0",
            "content": "Since automatic performance metrics do not provide insights into specific weaknesses of the system (van Miltenburg et al., 2021), we manually examined 100 outputs of the models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_86",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_86@1",
            "content": "We counted the number of errors: factual (hallucinations, omissions, incorrect fact merging, redundancies) and grammatical.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_86",
            "start": 178,
            "end": 300,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_86@2",
            "content": "The results are summarized in Table 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_86",
            "start": 302,
            "end": 339,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_87@0",
            "content": "The 1-STAGE model (which has to order the facts implicitly) tends to repeat the facts in the text (especially in E2E) and produces frequent hallucinations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_87",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_87@1",
            "content": "These problems are largely eliminated with the 2-STAGE and 3-STAGE models, which produce Wildwood is a restaurant.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_87",
            "start": 156,
            "end": 269,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_87@2",
            "content": "Wildwood serves French food.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_87",
            "start": 271,
            "end": 298,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_87@3",
            "content": "Wildwood is in the riverside.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_87",
            "start": 300,
            "end": 328,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_87@4",
            "content": "Wildwood is near Raja Indian Cuisine.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_87",
            "start": 330,
            "end": 366,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_88@0",
            "content": "Wildwood is a restaurant serving French food.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_88",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_88@1",
            "content": "It is in the riverside near Raja Indian Cuisine.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_88",
            "start": 46,
            "end": 93,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_88@2",
            "content": "Human A amazing French restaurant is called the Wildwood.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_88",
            "start": 95,
            "end": 151,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_88@3",
            "content": "The restaurant is near the Raja Indian Cuisine in riverside.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_88",
            "start": 153,
            "end": 212,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_88@4",
            "content": "They love kids.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_88",
            "start": 214,
            "end": 228,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_88@5",
            "content": "almost no hallucinations or omissions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_88",
            "start": 230,
            "end": 267,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_88@6",
            "content": "However, the outputs on WebNLG for all systems suffer from semantic errors resulting from merging of unrelated facts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_88",
            "start": 269,
            "end": 385,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_88@7",
            "content": "This mostly happens with unrelated predicates connected to the same subject/object (e.g. \"X was born in Y\", \"X worked as Z\" expressed as \"X worked as Z in Y\"; see Appendix E for examples).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_88",
            "start": 387,
            "end": 574,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_88@8",
            "content": "This behavior is the main obstacle to ensure factual consistency of the output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_88",
            "start": 576,
            "end": 654,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_88@9",
            "content": "As a possible remedy, we propose explicitly controlling the semantics of sentence fusion (Ben-David et al., 2020), e.g. using a variant of constrained decoding (Balakrishnan et al., 2019;.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_88",
            "start": 656,
            "end": 843,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_89@0",
            "content": "On the E2E data, which has a simpler triple structure (all predicates share the same subject), the outputs are generally consistent and the 2-STAGE and 3-STAGE models exhibit almost no semantic errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_89",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_89@1",
            "content": "Grammar errors and disfluencies stem mainly from over-eager paragraph compression or from artifacts in our templates and are relatively minor (e.g., missing \"is\" in \"serves French food and family-friendly\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_89",
            "start": 202,
            "end": 408,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_90@0",
            "content": "Content Planning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_90",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_91@0",
            "content": "Following and , we report the accuracy and BLEU-2 score of our ordering model on WebNLG against the humangenerated plans from Ferreira et al. (2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_91",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_91@1",
            "content": "The results are listed in We also evaluate the accuracy of our aggregation model, using triples ordered according to the plans from Ferreira et al. (2018) as input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_91",
            "start": 150,
            "end": 313,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_91@2",
            "content": "The accuracy is 0.33 per example and 0.62 per sentence boundary (random baseline is 0.23 and 0.50, respectively).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_91",
            "start": 315,
            "end": 427,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_91@3",
            "content": "The results show that although our approach is better than the random baseline, there is still room for improvement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_91",
            "start": 429,
            "end": 544,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_92@0",
            "content": "Finally, we manually evaluate how the PC model follows the content plan (i.e., keeping the predefined order and aggregating the sentences according to the delimiters) using 100 randomly chosen examples with more than 1 triple on WebNLG and E2E.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_92",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_92@1",
            "content": "We find that the model follows the content plan in 95% and 100% of cases, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_92",
            "start": 245,
            "end": 331,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_92@2",
            "content": "The incorrect cases include a fact not properly mentioned or an extra boundary between sentences without a separator.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_92",
            "start": 333,
            "end": 449,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_92@3",
            "content": "We can thus conclude that the pretraining task successfully teaches the PC model to follow a given content plan.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_92",
            "start": 451,
            "end": 562,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_93@0",
            "content": "Intrinsic Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_93",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_94@0",
            "content": "Aside from the main D2T generation results, we also provide an intrinsic evaluation of our pipeline modules on the WIKIFLUENT test sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_94",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_94@1",
            "content": "We evaluated the ordering, aggregation, and paragraph compression modules trained on the full WIKIFLUENT corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_94",
            "start": 137,
            "end": 248,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_94@2",
            "content": "The results for both full and filtered test sets are summarized in Table 7.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_94",
            "start": 250,
            "end": 324,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_94@3",
            "content": "The PC model achieves high scores, which follows from the fact that we provide it with ground truth content plans (i.e., the ordering and aggregation plan corresponding to the original paragraph).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_94",
            "start": 326,
            "end": 521,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_94@4",
            "content": "Accuracy of the ordering and aggregation modules is comparable to their performance on D2T datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_94",
            "start": 523,
            "end": 622,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_95@0",
            "content": "Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_95",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_96@0",
            "content": "Our experiments outline several possible future research directions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_96",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_96@1",
            "content": "Automatic generation of facts without using hand-crafted templates (cf. \u00a75.1) could allow applying zero-shot generation systems to datasets with a large number of predicates, such as ToTTo (Parikh et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_96",
            "start": 69,
            "end": 279,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_96@2",
            "content": "The task of paragraph compression could be used as a task-specific pretraining (Gururangan et al., 2020) for more efficient finetuning of D2T models, e.g., with a small amount of clean data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_96",
            "start": 281,
            "end": 470,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_96@3",
            "content": "Consistency checks may be introduced in the pipeline to control the output from the modules, and individual modules may be improved by using more efficient model architectures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_96",
            "start": 472,
            "end": 647,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_97@0",
            "content": "More research is also needed regarding the main shortcoming of our approach, i.e., the semantic errors stemming from merging of facts in improper ways.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_97",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_97@1",
            "content": "As we suggested in \u00a77.2, explicitly controlling the semantics of sentence fusion could help to mitigate this issue, while still keeping the advantages of a zero-shot approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_97",
            "start": 152,
            "end": 326,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_98@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_98",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_99@0",
            "content": "We presented an approach for zero-shot D2T generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_99",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_99@1",
            "content": "The approach uses a pipeline of PLMs trained on general-domain lexical operations over natural language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_99",
            "start": 55,
            "end": 158,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_99@2",
            "content": "The pipeline builds upon traditional approaches and consists of three interpretable intermediate steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_99",
            "start": 160,
            "end": 262,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_99@3",
            "content": "By avoiding noisy human-written references from the D2T datasets, our models produce more semantically consitent output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_99",
            "start": 264,
            "end": 383,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_99@4",
            "content": "We believe that training models for zeroshot D2T generation using large cross-domain corpora will help to build D2T generation systems with good performance across various domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_99",
            "start": 385,
            "end": 564,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_100@0",
            "content": "Limitations and Broader Impact",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_100",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_101@0",
            "content": "We study zero-shot D2T generation with the focus on generating descriptions for RDF triples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_101",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_101@1",
            "content": "Although the task of D2T generation has numerous applications, using neural models for D2T generation (especially in the zero-shot context) is still limited to experimental settings (Dale, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_101",
            "start": 93,
            "end": 287,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_101@2",
            "content": "Similarly to other recent approaches for D2T generation, our approach relies on PLMs, which are known to reflect the biases in their pretraining corpus (Bender et al., 2021;Rogers, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_101",
            "start": 289,
            "end": 475,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_101@3",
            "content": "Our system may therefore rely on spurious correlations for verbalizing e.g. gender or occupation of the entities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_101",
            "start": 477,
            "end": 589,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_101@4",
            "content": "Since we cannot guarantee the factual correctness of the outputs of our system, the outputs should be used with caution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_101",
            "start": 591,
            "end": 710,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_102@0",
            "content": "On the flip side, our approach helps to reduce the number of omissions and hallucinations stemming from noise in human-written references.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_102",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_102@1",
            "content": "Our work thus contributes to the general aim of D2T generation in conveying the data semantics accurately and without relying on implicit world knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_102",
            "start": 139,
            "end": 292,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_103@0",
            "content": "We implemented the models for split-and-rephrase, aggregation, and paragraph compression in Py-Torch Lightning (Paszke et al., 2019), using the PyTorch (Falcon et al., 2019) version of the BART and RoBERTa models from the Huggingface library (Wolf et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_103",
            "start": 0,
            "end": 261,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_104@0",
            "content": "We use the Adam (Kingma and Ba, 2015) optimizer (\u03b2 1 \" 0.9, \u03b2 2 \" 0.997, \u03b5 \" 1 \u00b49) with learning rate 2 \u00b45, linear scheduling and 0.1 warmup proportion; batches of size 8 and accumulating gradients with factor 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_104",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_104@1",
            "content": "We train the models for 1 epoch on a single GeForce RTX 3090 GPU with 24 GB RAM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_104",
            "start": 213,
            "end": 292,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_104@2",
            "content": "Training times were approximately 24 hours for the ordering model and 3 hours for the aggregation and paragraph compression models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_104",
            "start": 294,
            "end": 424,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_104@3",
            "content": "We use greedy decoding in all our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_104",
            "start": 426,
            "end": 471,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_105@0",
            "content": "For training the ordering model, we used the implementation from Calizzano et al. (2021) 11 including their training parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_105",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_105@1",
            "content": "We will integrate the ordering model into our framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_105",
            "start": 129,
            "end": 184,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_106@0",
            "content": "We provide evaluation of semantic accuracy on the E2E dataset as evaluated with the slot-error script based on matching regular expressions in Table 8 However, please note that our manual investigation of a sample of the data shows that the majority of the errors identified in our model outputs are false.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_106",
            "start": 0,
            "end": 305,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_106@1",
            "content": "For example, the following regular expression used in the slot-error script: prices?(?: range)?(?:w+)0,3 high matches \"(...) price range and high customer rating (...)\", incorrectly classifying the presence of the extra slot priceRange [high].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_106",
            "start": 307,
            "end": 549,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_106@2",
            "content": "This problem is magnified by the consistent outputs of our models, which tend to repeat certain patterns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_106",
            "start": 551,
            "end": 655,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_106@3",
            "content": "However, we also manually identified several cases in which an error was found correctly, e.g. the model hallucinating \"3 out of 4 customer rating\" instead of \"3 out of 5 customer rating\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_106",
            "start": 657,
            "end": 844,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_107@0",
            "content": "Tables 10, 11, 12, and 13 show examples of behavior of our models on the WebNLG dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_107",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_107@1",
            "content": "Tables 14 and 15 show examples of behavior of our models on the E2E dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_107",
            "start": 89,
            "end": 164,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_108@0",
            "content": "The green color marks the model outputs which are completely correct, the red color marks the errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_108",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_108@1",
            "content": "For better readability of the input format, we add numeric order identifiers for the individual facts (bold, in squared brackets).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_108",
            "start": 102,
            "end": 231,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_108@2",
            "content": "These are subsequently used as references in the Order and Aggregation rows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_108",
            "start": 233,
            "end": 308,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_108@3",
            "content": "Note that zero-th input in E2E is used as a subject in the RDF triples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_108",
            "start": 310,
            "end": 380,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_109@0",
            "content": "Akron, Ohio is from United States. English language is spoken in Akron, Ohio.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_109",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_110@0",
            "content": "English language is spoken in the United States.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_110",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_110@1",
            "content": "Akron, Ohio is from the state of Ohio in the U.S. state of Illinois.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_110",
            "start": 49,
            "end": 116,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_110@2",
            "content": "3-stage filtered Akron, Ohio is from United States.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_110",
            "start": 118,
            "end": 168,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_110@3",
            "content": "English language is spoken in the United States of America.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_110",
            "start": 170,
            "end": 228,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_110@4",
            "content": "2-stage filtered Akron, Ohio is from United States.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_110",
            "start": 230,
            "end": 280,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_110@5",
            "content": "English language is spoken in Akron, Ohio.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_110",
            "start": 282,
            "end": 323,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_110@6",
            "content": "1-stage filtered English language is spoken in the United States.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_110",
            "start": 325,
            "end": 389,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_110@7",
            "content": "Akron, Ohio is from the United Kingdom.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_110",
            "start": 391,
            "end": 429,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_111@0",
            "content": "The country of Akron, Ohio, is United States, where English is the language spoken.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_111",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_112@0",
            "content": "Table 11: Incorrect behavior on WebNLG: besides the minor disfluencies caused by the templates (\"Akron, Ohio is from...\"), the models (except for 3-STAGE filtered) tend to hallucinate and merge the facts incorrectly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_112",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_113@0",
            "content": "Oshin Agarwal, Heming Ge, Siamak Shakeri, Rami Al-Rfou, Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_113",
            "start": 0,
            "end": 324,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_114@0",
            "content": "UNKNOWN, None, 2015, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_114",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_115@0",
            "content": "S\u00f6ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, DBpedia: A Nucleus for a Web of Open Data, 2007, The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_115",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_116@0",
            "content": "Anusha Balakrishnan, Jinfeng Rao, Kartikeya Upasani, Michael White, Rajen Subba, Constrained Decoding for Neural NLG from Compositional Representations in Task-Oriented Dialogue, 2019, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_116",
            "start": 0,
            "end": 291,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_117@0",
            "content": "Satanjeev Banerjee, Alon Lavie, METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments, 2005, Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_117",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_118@0",
            "content": "Regina Barzilay, No\u00e9mie Elhadad, Kathleen Mckeown, Sentence Ordering in Multidocument Summarization, 2001, Proceedings of the First International Conference on Human Language Technology Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_118",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_119@0",
            "content": "Regina Barzilay, Kathleen Mckeown, Sentence Fusion for Multidocument News Summarization, 2005, Comput. Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_119",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_120@0",
            "content": "Eyal Ben-David, Orgad Keller, Eric Malmi, Semantically Driven Sentence Fusion: Modeling and Evaluation, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, Findings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_120",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_121@0",
            "content": "Emily Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?, 2021, FAccT '21: 2021 ACM Conference on Fairness, Accountability, and Transparency, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_121",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_122@0",
            "content": "Steven Bird, NLTK: The Natural Language Toolkit, 2006, ACL 2006, 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_122",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_123@0",
            "content": "Jan Botha, Manaal Faruqui, John Alex, Jason Baldridge, Dipanjan Das, Learning to Split and Rephrase From Wikipedia Edit History, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_123",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_124@0",
            "content": "R\u00e9mi Calizzano, Malte Ostendorff, Georg Rehm, Ordering sentences and paragraphs with pretrained encoder-decoder transformers and pointer ensembles, 2021, DocEng '21: ACM Symposium on Document Engineering 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_124",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_125@0",
            "content": "Ernie Chang, Xiaoyu Shen, Hui-Syuan Yeh, Vera Demberg, On Training Instance Selection for Few-Shot Neural Text Generation, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_125",
            "start": 0,
            "end": 322,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_126@0",
            "content": "Ernie Chang, Xiaoyu Shen, Dawei Zhu, Vera Demberg, Hui Su, Neural Data-to-Text Generation with LM-based Text Augmentation, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_126",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_127@0",
            "content": "Wenhu Chen, Yu Su, Xifeng Yan, William Wang, KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_127",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_128@0",
            "content": "Xinchi Chen, Xipeng Qiu, Xuanjing Huang, None, 2016, Neural Sentence Ordering. CoRR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_128",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_129@0",
            "content": "Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, William Wang, Few-Shot NLG with Pre-Trained Language Model, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_129",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_130@0",
            "content": "Baiyun Cui, Yingming Li, Zhongfei Zhang, BERT-enhanced Relational Sentence Ordering Network, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_130",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_131@0",
            "content": "Robert Dale, Natural language generation: The commercial state of the art in 2020, 2020, Nat. Lang. Eng, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_131",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_132@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_132",
            "start": 0,
            "end": 331,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_133@0",
            "content": "Ond\u0159ej Du\u0161ek, David Howcroft, Verena Rieser, Semantic Noise Matters for Neural Natural Language Generation, 2019, Proceedings of the 12th International Conference on Natural Language Generation, INLG 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_133",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_134@0",
            "content": "Ond\u0159ej Du\u0161ek, Filip Jur\u010d\u00ed\u010dek, Training a Natural Language Generator From Unaligned Data, 2015, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_134",
            "start": 0,
            "end": 324,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_135@0",
            "content": "Ond\u0159ej Du\u0161ek, Zden\u011bk Kasner, Evaluating Semantic Accuracy of Data-to-Text Generation with Natural Language Inference, 2020, Proceedings of the 13th International Conference on Natural Language Generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_135",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_136@0",
            "content": "Ond\u0159ej Du\u0161ek, Jekaterina Novikova, Verena Rieser, Evaluating the state-of-the-art of End-to-End Natural Language Generation: The E2E NLG challenge, 2020, Comput. Speech Lang, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_136",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_137@0",
            "content": "UNKNOWN, None, 2019, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_137",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_138@0",
            "content": "Thiago Ferreira, Claire Gardent, Nikolai Ilinykh, Chris Van Der Lee, Simon Mille, Diego Moussallem, Anastasia Shimorina, The 2020 Bilingual, Bi-Directional WebNLG+ Shared Task Overview and Evaluation Results, 2020, Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_138",
            "start": 0,
            "end": 325,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_139@0",
            "content": "Diego Thiago Castro Ferreira, Emiel Moussallem, Sander Krahmer,  Wubben, Enriching the WebNLG corpus, 2018, Proceedings of the 11th International Conference on Natural Language Generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_139",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_140@0",
            "content": "Chris Thiago Castro Ferreira,  Van Der Lee, Emiel Emiel Van Miltenburg,  Krahmer, Neural datato-text generation: A comparison between pipeline and end-to-end architectures, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_140",
            "start": 0,
            "end": 341,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_141@0",
            "content": "Katja Filippova, Yasemin Altun, Overcoming the Lack of Parallel Data in Sentence Compression, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_141",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_142@0",
            "content": "Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, The WebNLG Challenge: Generating Text from RDF Data, 2017, Proceedings of the 10th International Conference on Natural Language Generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_142",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_143@0",
            "content": "UNKNOWN, None, 2018, AllenNLP: A Deep Semantic Natural Language Processing Platform, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_143",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_144@0",
            "content": "Albert Gatt, Emiel Krahmer, Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation, 2018, J. Artif. Intell. Res, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_144",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_145@0",
            "content": "Mor Geva, Eric Malmi, Idan Szpektor, Jonathan Berant, DiscoFuse: A Large-Scale Dataset for Discourse-Based Sentence Fusion, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_145",
            "start": 0,
            "end": 311,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_146@0",
            "content": "UNKNOWN, None, 2016, End-to-End Neural Sentence Ordering Using Pointer Network, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_146",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_147@0",
            "content": "Ana Suchin Gururangan, Swabha Marasovi\u0107, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah A Downey,  Smith, Don't Stop Pretraining: Adapt Language Models to Domains and Tasks, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_147",
            "start": 0,
            "end": 266,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_148@0",
            "content": "Hamza Harkous, Isabel Groves, Amir Saffari, Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity, 2020, Proceedings of the 28th International Conference on Computational Linguistics, Online.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_148",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_149@0",
            "content": "Peyman Heidari, Arash Einolghozati, Shashank Jain, Soumya Batra, Lee Callender, Ankit Arun, Shawn Mei, Sonal Gupta, Pinar Donmez, Vikas Bhardwaj, Anuj Kumar, Michael White, Getting to Production with Few-shot Natural Language Generation Models, 2021, Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_149",
            "start": 0,
            "end": 347,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_150@0",
            "content": "Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, Wei Xu, Neural CRF Model for Sentence Alignment in Text Simplification, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_150",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_151@0",
            "content": "Zhijing Jin, Qipeng Guo, Xipeng Qiu, Zheng Zhang, GenWiki: A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation, 2020, Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Online.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_151",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_152@0",
            "content": "Mihir Kale, Abhinav Rastogi, Template Guided Text Generation for Task-Oriented Dialogue, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_152",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_153@0",
            "content": "Mihir Kale, Abhinav Rastogi, Text-to-Text Pre-Training for Data-to-Text Tasks, 2020, Proceedings of the 13th International Conference on Natural Language Generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_153",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_154@0",
            "content": "Zden\u011bk Kasner, Ond\u0159ej Du\u0161ek, Data-to-Text Generation with Iterative Text Editing, 2020, Proceedings of the 13th International Conference on Natural Language Generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_154",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_155@0",
            "content": "Zden\u011bk Kasner, Ond\u0159ej Du\u0161ek, Train Hard, Finetune Easy: Multilingual Denoising for RDF-to-Text Generation, 2020, Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_155",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_156@0",
            "content": "Pei Ke, Haozhe Ji, Yu Ran, Xin Cui, Liwei Wang, Linfeng Song, Xiaoyan Zhu, Minlie Huang, JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs, 2021, Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, volume ACL/IJCNLP 2021 of Findings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_156",
            "start": 0,
            "end": 307,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_157@0",
            "content": "UNKNOWN, None, 2019, CTRL: A Conditional Transformer Language Model for Controllable Generation, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_157",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_158@0",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A Method for Stochastic Optimization, 2015, 3rd International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_158",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_159@0",
            "content": "Anirban Laha, Parag Jain, Abhijit Mishra, Karthik Sankaranarayanan, Scalable Micro-planned Generation of Discourse from Structured Data, 2019, Comput. Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_159",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_160@0",
            "content": "Mirella Lapata, Probabilistic Text Structuring: Experiments with Sentence Ordering, 2003, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_160",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_161@0",
            "content": "Kenton Lee, Luheng He, Luke Zettlemoyer, Higher-Order Coreference Resolution with Coarseto-Fine Inference, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_161",
            "start": 0,
            "end": 268,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_162@0",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, BART: Denoising Sequence-to-Sequence Pretraining for Natural Language Generation, Translation, and Comprehension, 2020, Proceedings of the 58th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_162",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_163@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_163",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_164@0",
            "content": "Jiwei Li, Dan Jurafsky, Neural Net Models of Open-domain Discourse Coherence, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_164",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_165@0",
            "content": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, None, 1907, RoBERTa: A Robustly Optimized BERT Pretraining Approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_165",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_166@0",
            "content": "UNKNOWN, None, 2021, Improving Compositional Generalization with Self-Training for Data-to-Text Generation, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_166",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_167@0",
            "content": "Amit Moryossef, Yoav Goldberg, Ido Dagan, Improving Quality and Efficiency in Plan-based Neural Data-to-text Generation, 2019, Proceedings of the 12th International Conference on Natural Language Generation, INLG 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_167",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_168@0",
            "content": "Amit Moryossef, Yoav Goldberg, Ido Dagan, Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_168",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_169@0",
            "content": "Shashi Narayan, Claire Gardent, Shay Cohen, Anastasia Shimorina, Split and Rephrase, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_169",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_170@0",
            "content": "Jekaterina Novikova, Ond\u0159ej Du\u0161ek, Verena Rieser, The E2E Dataset: New Challenges For End-to-End Generation, 2017, Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_170",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_171@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a Method for Automatic Evaluation of Machine Translation, 2002-07-06, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_171",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_172@0",
            "content": "P Ankur, Xuezhi Parikh, Sebastian Wang, Manaal Gehrmann, Bhuwan Faruqui, Diyi Dhingra, Dipanjan Yang,  Das, ToTTo: A Controlled Table-To-Text Generation Dataset, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_172",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_173@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_173",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_174@0",
            "content": "Zachary Yang, Martin Devito, Alykhan Raison, Sasank Tejani, Benoit Chilamkurthy, Lu Steiner, Junjie Fang, Soumith Bai,  Chintala, PyTorch: An Imperative Style, High-Performance Deep Learning Library, 2019, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_174",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_175@0",
            "content": "Ratish Puduppully, Li Dong, Mirella Lapata, The Thirty-First Innovative Applications of Artificial Intelligence Conference, 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_175",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_176@0",
            "content": "Cl\u00e9ment Rebuffel, Marco Roberti, Laure Soulier, Geoffrey Scoutheeten, Rossella Cancelliere, Patrick Gallinari, Controlling hallucinations at word level in data-to-text generation, 2022, Data Min. Knowl. Discov, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_176",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_177@0",
            "content": "Ehud Reiter, An Architecture for Data-to-Text Systems, 2007, Proceedings of the Eleventh European Workshop on Natural Language Generation, ENLG 2007, Schloss Dagstuhl, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_177",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_178@0",
            "content": "Ehud Reiter, Robert Dale, Building applied natural language generation systems, 1997, Nat. Lang. Eng, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_178",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_179@0",
            "content": "UNKNOWN, None, 2007, Hinrich Sch\u00fctze, and Iryna Gurevych. 2020. Investigating Pretrained Language Models for Graph-to-Text Generation. CoRR, abs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_179",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_180@0",
            "content": "Anna Rogers, Changing the World by Changing the Data, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_180",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_181@0",
            "content": "Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, Xiaoyan Zhu, Long and Diverse Text Generation with Planning-based Hierarchical Variational Model, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_181",
            "start": 0,
            "end": 319,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_182@0",
            "content": "Xiaoyu Shen, Ernie Chang, Hui Su, Cheng Niu, Dietrich Klakow, Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_182",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_183@0",
            "content": "Yixuan Su, Zaiqiao Meng, Simon Baker, Nigel Collier, Few-Shot Table-to-Text Generation with Prototype Memory, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_183",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_184@0",
            "content": "Yixuan Su, David Vandyke, Sihui Wang, Yimai Fang, Nigel Collier, Plan-then-Generate: Controlled Data-to-Text Generation via Planning, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_184",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_185@0",
            "content": "Jianzhong Bayu Distiawan Trisedya, Rui Qi,  Zhang, Sentence Generation for Entity Description with Content-Plan Attention, 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_185",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_186@0",
            "content": "Miruna-Adriana Emiel Van Miltenburg, Ond\u0159ej Clinciu, Dimitra Du\u0161ek, Stephanie Gkatzia, Leo Inglis, Saad Lepp\u00e4nen, Emma Mahamood, Stephanie Manning, Craig Schoch, Luou Thomson,  Wen, Underreporting of errors in NLG output, and what to do about it, 2021, Proceedings of the 14th International Conference on Natural Language Generation, INLG 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_186",
            "start": 0,
            "end": 345,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_187@0",
            "content": "Oriol Vinyals, Meire Fortunato, Navdeep Jaitly, Pointer networks, 2015, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_187",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_188@0",
            "content": "Tianming Wang, Xiaojun Wan, Hierarchical Attention Networks for Sentence Ordering, 2019, The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_188",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_189@0",
            "content": "Yufei Wang, Ian Wood, Stephen Wan, Mark Dras, Mark Johnson, Mention Flags (MF): Constraining Transformer-based Text Generators, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_189",
            "start": 0,
            "end": 326,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_190@0",
            "content": "Adina Williams, Nikita Nangia, Samuel , A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_190",
            "start": 0,
            "end": 281,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_191@0",
            "content": "Sam Wiseman, Stuart Shieber, Alexander Rush, Learning Neural Templates for Text Generation, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_191",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_192@0",
            "content": "UNKNOWN, None, 1910, HuggingFace's Transformers: State-of-the-art Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_192",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_193@0",
            "content": "Xinnuo Xu, Ond\u0159ej Du\u0161ek, Verena Rieser, Ioannis Konstas, AggGen: Ordering and Aggregating while Generating, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_193",
            "start": 0,
            "end": 306,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_194@0",
            "content": "Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu, PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization, 2020, Proceedings of the 37th International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_194",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "136-ARR_v2_195@0",
            "content": "Chao Zhao, Marilyn Walker, Snigdha Chaturvedi, Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v2_195",
            "start": 0,
            "end": 239,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "136-ARR_v2_0",
            "tgt_ix": "136-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_0",
            "tgt_ix": "136-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_1",
            "tgt_ix": "136-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_1",
            "tgt_ix": "136-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_0",
            "tgt_ix": "136-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_2",
            "tgt_ix": "136-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_4",
            "tgt_ix": "136-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_5",
            "tgt_ix": "136-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_6",
            "tgt_ix": "136-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_7",
            "tgt_ix": "136-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_8",
            "tgt_ix": "136-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_9",
            "tgt_ix": "136-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_10",
            "tgt_ix": "136-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_11",
            "tgt_ix": "136-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_12",
            "tgt_ix": "136-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_13",
            "tgt_ix": "136-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_3",
            "tgt_ix": "136-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_3",
            "tgt_ix": "136-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_3",
            "tgt_ix": "136-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_3",
            "tgt_ix": "136-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_3",
            "tgt_ix": "136-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_3",
            "tgt_ix": "136-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_3",
            "tgt_ix": "136-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_3",
            "tgt_ix": "136-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_3",
            "tgt_ix": "136-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_3",
            "tgt_ix": "136-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_3",
            "tgt_ix": "136-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_3",
            "tgt_ix": "136-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_3",
            "tgt_ix": "136-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_14",
            "tgt_ix": "136-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_0",
            "tgt_ix": "136-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_15",
            "tgt_ix": "136-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_17",
            "tgt_ix": "136-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_18",
            "tgt_ix": "136-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_16",
            "tgt_ix": "136-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_16",
            "tgt_ix": "136-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_16",
            "tgt_ix": "136-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_16",
            "tgt_ix": "136-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_16",
            "tgt_ix": "136-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_19",
            "tgt_ix": "136-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_20",
            "tgt_ix": "136-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_20",
            "tgt_ix": "136-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_16",
            "tgt_ix": "136-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_21",
            "tgt_ix": "136-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_23",
            "tgt_ix": "136-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_22",
            "tgt_ix": "136-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_22",
            "tgt_ix": "136-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_22",
            "tgt_ix": "136-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_16",
            "tgt_ix": "136-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_24",
            "tgt_ix": "136-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_26",
            "tgt_ix": "136-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_25",
            "tgt_ix": "136-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_25",
            "tgt_ix": "136-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_25",
            "tgt_ix": "136-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_16",
            "tgt_ix": "136-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_27",
            "tgt_ix": "136-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_29",
            "tgt_ix": "136-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_28",
            "tgt_ix": "136-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_28",
            "tgt_ix": "136-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_28",
            "tgt_ix": "136-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_0",
            "tgt_ix": "136-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_30",
            "tgt_ix": "136-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_32",
            "tgt_ix": "136-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_31",
            "tgt_ix": "136-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_31",
            "tgt_ix": "136-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_31",
            "tgt_ix": "136-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_31",
            "tgt_ix": "136-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_33",
            "tgt_ix": "136-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_34",
            "tgt_ix": "136-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_34",
            "tgt_ix": "136-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_36",
            "tgt_ix": "136-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_37",
            "tgt_ix": "136-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_34",
            "tgt_ix": "136-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_34",
            "tgt_ix": "136-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_34",
            "tgt_ix": "136-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_35",
            "tgt_ix": "136-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_34",
            "tgt_ix": "136-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_38",
            "tgt_ix": "136-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_34",
            "tgt_ix": "136-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_39",
            "tgt_ix": "136-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_41",
            "tgt_ix": "136-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_34",
            "tgt_ix": "136-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_34",
            "tgt_ix": "136-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_40",
            "tgt_ix": "136-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_31",
            "tgt_ix": "136-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_42",
            "tgt_ix": "136-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_44",
            "tgt_ix": "136-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_43",
            "tgt_ix": "136-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_43",
            "tgt_ix": "136-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_43",
            "tgt_ix": "136-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_31",
            "tgt_ix": "136-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_45",
            "tgt_ix": "136-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_46",
            "tgt_ix": "136-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_46",
            "tgt_ix": "136-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_31",
            "tgt_ix": "136-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_47",
            "tgt_ix": "136-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_48",
            "tgt_ix": "136-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_48",
            "tgt_ix": "136-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_0",
            "tgt_ix": "136-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_49",
            "tgt_ix": "136-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_50",
            "tgt_ix": "136-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_50",
            "tgt_ix": "136-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_50",
            "tgt_ix": "136-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_51",
            "tgt_ix": "136-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_52",
            "tgt_ix": "136-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_52",
            "tgt_ix": "136-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_50",
            "tgt_ix": "136-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_53",
            "tgt_ix": "136-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_55",
            "tgt_ix": "136-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_56",
            "tgt_ix": "136-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_57",
            "tgt_ix": "136-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_58",
            "tgt_ix": "136-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_59",
            "tgt_ix": "136-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_60",
            "tgt_ix": "136-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_61",
            "tgt_ix": "136-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_54",
            "tgt_ix": "136-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_54",
            "tgt_ix": "136-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_54",
            "tgt_ix": "136-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_54",
            "tgt_ix": "136-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_54",
            "tgt_ix": "136-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_54",
            "tgt_ix": "136-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_54",
            "tgt_ix": "136-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_54",
            "tgt_ix": "136-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_54",
            "tgt_ix": "136-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_63",
            "tgt_ix": "136-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_54",
            "tgt_ix": "136-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_54",
            "tgt_ix": "136-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_62",
            "tgt_ix": "136-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_50",
            "tgt_ix": "136-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_64",
            "tgt_ix": "136-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_66",
            "tgt_ix": "136-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_65",
            "tgt_ix": "136-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_65",
            "tgt_ix": "136-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_65",
            "tgt_ix": "136-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_50",
            "tgt_ix": "136-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_67",
            "tgt_ix": "136-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_69",
            "tgt_ix": "136-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_68",
            "tgt_ix": "136-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_68",
            "tgt_ix": "136-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_68",
            "tgt_ix": "136-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_0",
            "tgt_ix": "136-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_70",
            "tgt_ix": "136-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_72",
            "tgt_ix": "136-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_73",
            "tgt_ix": "136-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_71",
            "tgt_ix": "136-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_71",
            "tgt_ix": "136-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_71",
            "tgt_ix": "136-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_71",
            "tgt_ix": "136-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_75",
            "tgt_ix": "136-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_76",
            "tgt_ix": "136-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_71",
            "tgt_ix": "136-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_71",
            "tgt_ix": "136-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_71",
            "tgt_ix": "136-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_0",
            "tgt_ix": "136-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_78",
            "tgt_ix": "136-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_78",
            "tgt_ix": "136-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_78",
            "tgt_ix": "136-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_79",
            "tgt_ix": "136-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_81",
            "tgt_ix": "136-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_83",
            "tgt_ix": "136-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_80",
            "tgt_ix": "136-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_80",
            "tgt_ix": "136-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_80",
            "tgt_ix": "136-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_80",
            "tgt_ix": "136-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_80",
            "tgt_ix": "136-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_78",
            "tgt_ix": "136-ARR_v2_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_84",
            "tgt_ix": "136-ARR_v2_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_86",
            "tgt_ix": "136-ARR_v2_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_85",
            "tgt_ix": "136-ARR_v2_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_85",
            "tgt_ix": "136-ARR_v2_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_85",
            "tgt_ix": "136-ARR_v2_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_88",
            "tgt_ix": "136-ARR_v2_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_85",
            "tgt_ix": "136-ARR_v2_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_85",
            "tgt_ix": "136-ARR_v2_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_87",
            "tgt_ix": "136-ARR_v2_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_78",
            "tgt_ix": "136-ARR_v2_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_89",
            "tgt_ix": "136-ARR_v2_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_91",
            "tgt_ix": "136-ARR_v2_92",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_90",
            "tgt_ix": "136-ARR_v2_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_90",
            "tgt_ix": "136-ARR_v2_92",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_90",
            "tgt_ix": "136-ARR_v2_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_78",
            "tgt_ix": "136-ARR_v2_93",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_92",
            "tgt_ix": "136-ARR_v2_93",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_93",
            "tgt_ix": "136-ARR_v2_94",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_93",
            "tgt_ix": "136-ARR_v2_94",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_0",
            "tgt_ix": "136-ARR_v2_95",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_94",
            "tgt_ix": "136-ARR_v2_95",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_96",
            "tgt_ix": "136-ARR_v2_97",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_95",
            "tgt_ix": "136-ARR_v2_96",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_95",
            "tgt_ix": "136-ARR_v2_97",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_95",
            "tgt_ix": "136-ARR_v2_96",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_0",
            "tgt_ix": "136-ARR_v2_98",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_97",
            "tgt_ix": "136-ARR_v2_98",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_98",
            "tgt_ix": "136-ARR_v2_99",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_98",
            "tgt_ix": "136-ARR_v2_99",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_3",
            "tgt_ix": "136-ARR_v2_100",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_99",
            "tgt_ix": "136-ARR_v2_100",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_101",
            "tgt_ix": "136-ARR_v2_102",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_100",
            "tgt_ix": "136-ARR_v2_101",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_100",
            "tgt_ix": "136-ARR_v2_102",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_100",
            "tgt_ix": "136-ARR_v2_101",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_103",
            "tgt_ix": "136-ARR_v2_104",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_104",
            "tgt_ix": "136-ARR_v2_105",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_100",
            "tgt_ix": "136-ARR_v2_103",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_100",
            "tgt_ix": "136-ARR_v2_104",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_100",
            "tgt_ix": "136-ARR_v2_105",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_102",
            "tgt_ix": "136-ARR_v2_103",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_100",
            "tgt_ix": "136-ARR_v2_106",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_105",
            "tgt_ix": "136-ARR_v2_106",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_107",
            "tgt_ix": "136-ARR_v2_108",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_100",
            "tgt_ix": "136-ARR_v2_107",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_100",
            "tgt_ix": "136-ARR_v2_108",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_106",
            "tgt_ix": "136-ARR_v2_107",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_100",
            "tgt_ix": "136-ARR_v2_109",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_108",
            "tgt_ix": "136-ARR_v2_109",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_100",
            "tgt_ix": "136-ARR_v2_110",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_109",
            "tgt_ix": "136-ARR_v2_110",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_111",
            "tgt_ix": "136-ARR_v2_112",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_100",
            "tgt_ix": "136-ARR_v2_111",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_100",
            "tgt_ix": "136-ARR_v2_112",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_110",
            "tgt_ix": "136-ARR_v2_111",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v2_0",
            "tgt_ix": "136-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_1",
            "tgt_ix": "136-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_2",
            "tgt_ix": "136-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_2",
            "tgt_ix": "136-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_2",
            "tgt_ix": "136-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_2",
            "tgt_ix": "136-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_2",
            "tgt_ix": "136-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_2",
            "tgt_ix": "136-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_3",
            "tgt_ix": "136-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_4",
            "tgt_ix": "136-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_4",
            "tgt_ix": "136-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_5",
            "tgt_ix": "136-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_5",
            "tgt_ix": "136-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_5",
            "tgt_ix": "136-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_5",
            "tgt_ix": "136-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_6",
            "tgt_ix": "136-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_6",
            "tgt_ix": "136-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_6",
            "tgt_ix": "136-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_7",
            "tgt_ix": "136-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_7",
            "tgt_ix": "136-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_7",
            "tgt_ix": "136-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_7",
            "tgt_ix": "136-ARR_v2_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_7",
            "tgt_ix": "136-ARR_v2_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_7",
            "tgt_ix": "136-ARR_v2_7@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_7",
            "tgt_ix": "136-ARR_v2_7@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_8",
            "tgt_ix": "136-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_9",
            "tgt_ix": "136-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_9",
            "tgt_ix": "136-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_9",
            "tgt_ix": "136-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_9",
            "tgt_ix": "136-ARR_v2_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_10",
            "tgt_ix": "136-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_10",
            "tgt_ix": "136-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_10",
            "tgt_ix": "136-ARR_v2_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_10",
            "tgt_ix": "136-ARR_v2_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_11",
            "tgt_ix": "136-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_11",
            "tgt_ix": "136-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_11",
            "tgt_ix": "136-ARR_v2_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_11",
            "tgt_ix": "136-ARR_v2_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_11",
            "tgt_ix": "136-ARR_v2_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_12",
            "tgt_ix": "136-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_12",
            "tgt_ix": "136-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_12",
            "tgt_ix": "136-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_13",
            "tgt_ix": "136-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_13",
            "tgt_ix": "136-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_13",
            "tgt_ix": "136-ARR_v2_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_14",
            "tgt_ix": "136-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_14",
            "tgt_ix": "136-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_14",
            "tgt_ix": "136-ARR_v2_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_15",
            "tgt_ix": "136-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_15",
            "tgt_ix": "136-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_15",
            "tgt_ix": "136-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_15",
            "tgt_ix": "136-ARR_v2_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_15",
            "tgt_ix": "136-ARR_v2_15@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_16",
            "tgt_ix": "136-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_17",
            "tgt_ix": "136-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_17",
            "tgt_ix": "136-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_18",
            "tgt_ix": "136-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_18",
            "tgt_ix": "136-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_19",
            "tgt_ix": "136-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_19",
            "tgt_ix": "136-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_20",
            "tgt_ix": "136-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_21",
            "tgt_ix": "136-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_21",
            "tgt_ix": "136-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_21",
            "tgt_ix": "136-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_21",
            "tgt_ix": "136-ARR_v2_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_22",
            "tgt_ix": "136-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_23",
            "tgt_ix": "136-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_23",
            "tgt_ix": "136-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_23",
            "tgt_ix": "136-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_23",
            "tgt_ix": "136-ARR_v2_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_23",
            "tgt_ix": "136-ARR_v2_23@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_24",
            "tgt_ix": "136-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_24",
            "tgt_ix": "136-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_25",
            "tgt_ix": "136-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_26",
            "tgt_ix": "136-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_26",
            "tgt_ix": "136-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_26",
            "tgt_ix": "136-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_27",
            "tgt_ix": "136-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_27",
            "tgt_ix": "136-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_27",
            "tgt_ix": "136-ARR_v2_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_28",
            "tgt_ix": "136-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_29",
            "tgt_ix": "136-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_29",
            "tgt_ix": "136-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_29",
            "tgt_ix": "136-ARR_v2_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_29",
            "tgt_ix": "136-ARR_v2_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_30",
            "tgt_ix": "136-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_31",
            "tgt_ix": "136-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_32",
            "tgt_ix": "136-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_32",
            "tgt_ix": "136-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_33",
            "tgt_ix": "136-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_33",
            "tgt_ix": "136-ARR_v2_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_33",
            "tgt_ix": "136-ARR_v2_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_33",
            "tgt_ix": "136-ARR_v2_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_34",
            "tgt_ix": "136-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_35",
            "tgt_ix": "136-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_35",
            "tgt_ix": "136-ARR_v2_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_35",
            "tgt_ix": "136-ARR_v2_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_35",
            "tgt_ix": "136-ARR_v2_35@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_35",
            "tgt_ix": "136-ARR_v2_35@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_36",
            "tgt_ix": "136-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_37",
            "tgt_ix": "136-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_38",
            "tgt_ix": "136-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_39",
            "tgt_ix": "136-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_40",
            "tgt_ix": "136-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_41",
            "tgt_ix": "136-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_42",
            "tgt_ix": "136-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_42",
            "tgt_ix": "136-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_42",
            "tgt_ix": "136-ARR_v2_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_42",
            "tgt_ix": "136-ARR_v2_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_42",
            "tgt_ix": "136-ARR_v2_42@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_42",
            "tgt_ix": "136-ARR_v2_42@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_42",
            "tgt_ix": "136-ARR_v2_42@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_43",
            "tgt_ix": "136-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_44",
            "tgt_ix": "136-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_44",
            "tgt_ix": "136-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_44",
            "tgt_ix": "136-ARR_v2_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_45",
            "tgt_ix": "136-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_45",
            "tgt_ix": "136-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_45",
            "tgt_ix": "136-ARR_v2_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_45",
            "tgt_ix": "136-ARR_v2_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_46",
            "tgt_ix": "136-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_47",
            "tgt_ix": "136-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_47",
            "tgt_ix": "136-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_47",
            "tgt_ix": "136-ARR_v2_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_47",
            "tgt_ix": "136-ARR_v2_47@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_48",
            "tgt_ix": "136-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_49",
            "tgt_ix": "136-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_49",
            "tgt_ix": "136-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_49",
            "tgt_ix": "136-ARR_v2_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_50",
            "tgt_ix": "136-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_51",
            "tgt_ix": "136-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_51",
            "tgt_ix": "136-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_52",
            "tgt_ix": "136-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_53",
            "tgt_ix": "136-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_53",
            "tgt_ix": "136-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_53",
            "tgt_ix": "136-ARR_v2_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_54",
            "tgt_ix": "136-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_55",
            "tgt_ix": "136-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_56",
            "tgt_ix": "136-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_56",
            "tgt_ix": "136-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_57",
            "tgt_ix": "136-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_57",
            "tgt_ix": "136-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_57",
            "tgt_ix": "136-ARR_v2_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_58",
            "tgt_ix": "136-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_58",
            "tgt_ix": "136-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_58",
            "tgt_ix": "136-ARR_v2_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_58",
            "tgt_ix": "136-ARR_v2_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_59",
            "tgt_ix": "136-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_59",
            "tgt_ix": "136-ARR_v2_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_60",
            "tgt_ix": "136-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_61",
            "tgt_ix": "136-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_62",
            "tgt_ix": "136-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_63",
            "tgt_ix": "136-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_63",
            "tgt_ix": "136-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_64",
            "tgt_ix": "136-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_65",
            "tgt_ix": "136-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_66",
            "tgt_ix": "136-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_66",
            "tgt_ix": "136-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_66",
            "tgt_ix": "136-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_66",
            "tgt_ix": "136-ARR_v2_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_67",
            "tgt_ix": "136-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_68",
            "tgt_ix": "136-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_69",
            "tgt_ix": "136-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_69",
            "tgt_ix": "136-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_69",
            "tgt_ix": "136-ARR_v2_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_69",
            "tgt_ix": "136-ARR_v2_69@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_70",
            "tgt_ix": "136-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_70",
            "tgt_ix": "136-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_71",
            "tgt_ix": "136-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_72",
            "tgt_ix": "136-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_72",
            "tgt_ix": "136-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_73",
            "tgt_ix": "136-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_74",
            "tgt_ix": "136-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_75",
            "tgt_ix": "136-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_76",
            "tgt_ix": "136-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_77",
            "tgt_ix": "136-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_78",
            "tgt_ix": "136-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_79",
            "tgt_ix": "136-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_79",
            "tgt_ix": "136-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_79",
            "tgt_ix": "136-ARR_v2_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_79",
            "tgt_ix": "136-ARR_v2_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_80",
            "tgt_ix": "136-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_81",
            "tgt_ix": "136-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_81",
            "tgt_ix": "136-ARR_v2_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_81",
            "tgt_ix": "136-ARR_v2_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_81",
            "tgt_ix": "136-ARR_v2_81@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_82",
            "tgt_ix": "136-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_83",
            "tgt_ix": "136-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_83",
            "tgt_ix": "136-ARR_v2_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_83",
            "tgt_ix": "136-ARR_v2_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_83",
            "tgt_ix": "136-ARR_v2_83@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_84",
            "tgt_ix": "136-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_84",
            "tgt_ix": "136-ARR_v2_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_84",
            "tgt_ix": "136-ARR_v2_84@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_85",
            "tgt_ix": "136-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_86",
            "tgt_ix": "136-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_86",
            "tgt_ix": "136-ARR_v2_86@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_86",
            "tgt_ix": "136-ARR_v2_86@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_87",
            "tgt_ix": "136-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_87",
            "tgt_ix": "136-ARR_v2_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_87",
            "tgt_ix": "136-ARR_v2_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_87",
            "tgt_ix": "136-ARR_v2_87@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_87",
            "tgt_ix": "136-ARR_v2_87@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_88",
            "tgt_ix": "136-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_88",
            "tgt_ix": "136-ARR_v2_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_88",
            "tgt_ix": "136-ARR_v2_88@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_88",
            "tgt_ix": "136-ARR_v2_88@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_88",
            "tgt_ix": "136-ARR_v2_88@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_88",
            "tgt_ix": "136-ARR_v2_88@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_88",
            "tgt_ix": "136-ARR_v2_88@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_88",
            "tgt_ix": "136-ARR_v2_88@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_88",
            "tgt_ix": "136-ARR_v2_88@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_88",
            "tgt_ix": "136-ARR_v2_88@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_89",
            "tgt_ix": "136-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_89",
            "tgt_ix": "136-ARR_v2_89@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_90",
            "tgt_ix": "136-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_91",
            "tgt_ix": "136-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_91",
            "tgt_ix": "136-ARR_v2_91@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_91",
            "tgt_ix": "136-ARR_v2_91@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_91",
            "tgt_ix": "136-ARR_v2_91@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_92",
            "tgt_ix": "136-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_92",
            "tgt_ix": "136-ARR_v2_92@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_92",
            "tgt_ix": "136-ARR_v2_92@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_92",
            "tgt_ix": "136-ARR_v2_92@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_93",
            "tgt_ix": "136-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_94",
            "tgt_ix": "136-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_94",
            "tgt_ix": "136-ARR_v2_94@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_94",
            "tgt_ix": "136-ARR_v2_94@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_94",
            "tgt_ix": "136-ARR_v2_94@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_94",
            "tgt_ix": "136-ARR_v2_94@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_95",
            "tgt_ix": "136-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_96",
            "tgt_ix": "136-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_96",
            "tgt_ix": "136-ARR_v2_96@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_96",
            "tgt_ix": "136-ARR_v2_96@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_96",
            "tgt_ix": "136-ARR_v2_96@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_97",
            "tgt_ix": "136-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_97",
            "tgt_ix": "136-ARR_v2_97@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_98",
            "tgt_ix": "136-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_99",
            "tgt_ix": "136-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_99",
            "tgt_ix": "136-ARR_v2_99@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_99",
            "tgt_ix": "136-ARR_v2_99@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_99",
            "tgt_ix": "136-ARR_v2_99@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_99",
            "tgt_ix": "136-ARR_v2_99@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_100",
            "tgt_ix": "136-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_101",
            "tgt_ix": "136-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_101",
            "tgt_ix": "136-ARR_v2_101@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_101",
            "tgt_ix": "136-ARR_v2_101@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_101",
            "tgt_ix": "136-ARR_v2_101@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_101",
            "tgt_ix": "136-ARR_v2_101@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_102",
            "tgt_ix": "136-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_102",
            "tgt_ix": "136-ARR_v2_102@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_103",
            "tgt_ix": "136-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_104",
            "tgt_ix": "136-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_104",
            "tgt_ix": "136-ARR_v2_104@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_104",
            "tgt_ix": "136-ARR_v2_104@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_104",
            "tgt_ix": "136-ARR_v2_104@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_105",
            "tgt_ix": "136-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_105",
            "tgt_ix": "136-ARR_v2_105@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_106",
            "tgt_ix": "136-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_106",
            "tgt_ix": "136-ARR_v2_106@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_106",
            "tgt_ix": "136-ARR_v2_106@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_106",
            "tgt_ix": "136-ARR_v2_106@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_107",
            "tgt_ix": "136-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_107",
            "tgt_ix": "136-ARR_v2_107@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_108",
            "tgt_ix": "136-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_108",
            "tgt_ix": "136-ARR_v2_108@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_108",
            "tgt_ix": "136-ARR_v2_108@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_108",
            "tgt_ix": "136-ARR_v2_108@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_109",
            "tgt_ix": "136-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_110",
            "tgt_ix": "136-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_110",
            "tgt_ix": "136-ARR_v2_110@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_110",
            "tgt_ix": "136-ARR_v2_110@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_110",
            "tgt_ix": "136-ARR_v2_110@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_110",
            "tgt_ix": "136-ARR_v2_110@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_110",
            "tgt_ix": "136-ARR_v2_110@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_110",
            "tgt_ix": "136-ARR_v2_110@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_110",
            "tgt_ix": "136-ARR_v2_110@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_111",
            "tgt_ix": "136-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_112",
            "tgt_ix": "136-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_113",
            "tgt_ix": "136-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_114",
            "tgt_ix": "136-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_115",
            "tgt_ix": "136-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_116",
            "tgt_ix": "136-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_117",
            "tgt_ix": "136-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_118",
            "tgt_ix": "136-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_119",
            "tgt_ix": "136-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_120",
            "tgt_ix": "136-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_121",
            "tgt_ix": "136-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_122",
            "tgt_ix": "136-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_123",
            "tgt_ix": "136-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_124",
            "tgt_ix": "136-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_125",
            "tgt_ix": "136-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_126",
            "tgt_ix": "136-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_127",
            "tgt_ix": "136-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_128",
            "tgt_ix": "136-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_129",
            "tgt_ix": "136-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_130",
            "tgt_ix": "136-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_131",
            "tgt_ix": "136-ARR_v2_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_132",
            "tgt_ix": "136-ARR_v2_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_133",
            "tgt_ix": "136-ARR_v2_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_134",
            "tgt_ix": "136-ARR_v2_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_135",
            "tgt_ix": "136-ARR_v2_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_136",
            "tgt_ix": "136-ARR_v2_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_137",
            "tgt_ix": "136-ARR_v2_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_138",
            "tgt_ix": "136-ARR_v2_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_139",
            "tgt_ix": "136-ARR_v2_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_140",
            "tgt_ix": "136-ARR_v2_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_141",
            "tgt_ix": "136-ARR_v2_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_142",
            "tgt_ix": "136-ARR_v2_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_143",
            "tgt_ix": "136-ARR_v2_143@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_144",
            "tgt_ix": "136-ARR_v2_144@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_145",
            "tgt_ix": "136-ARR_v2_145@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_146",
            "tgt_ix": "136-ARR_v2_146@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_147",
            "tgt_ix": "136-ARR_v2_147@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_148",
            "tgt_ix": "136-ARR_v2_148@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_149",
            "tgt_ix": "136-ARR_v2_149@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_150",
            "tgt_ix": "136-ARR_v2_150@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_151",
            "tgt_ix": "136-ARR_v2_151@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_152",
            "tgt_ix": "136-ARR_v2_152@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_153",
            "tgt_ix": "136-ARR_v2_153@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_154",
            "tgt_ix": "136-ARR_v2_154@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_155",
            "tgt_ix": "136-ARR_v2_155@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_156",
            "tgt_ix": "136-ARR_v2_156@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_157",
            "tgt_ix": "136-ARR_v2_157@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_158",
            "tgt_ix": "136-ARR_v2_158@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_159",
            "tgt_ix": "136-ARR_v2_159@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_160",
            "tgt_ix": "136-ARR_v2_160@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_161",
            "tgt_ix": "136-ARR_v2_161@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_162",
            "tgt_ix": "136-ARR_v2_162@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_163",
            "tgt_ix": "136-ARR_v2_163@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_164",
            "tgt_ix": "136-ARR_v2_164@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_165",
            "tgt_ix": "136-ARR_v2_165@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_166",
            "tgt_ix": "136-ARR_v2_166@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_167",
            "tgt_ix": "136-ARR_v2_167@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_168",
            "tgt_ix": "136-ARR_v2_168@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_169",
            "tgt_ix": "136-ARR_v2_169@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_170",
            "tgt_ix": "136-ARR_v2_170@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_171",
            "tgt_ix": "136-ARR_v2_171@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_172",
            "tgt_ix": "136-ARR_v2_172@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_173",
            "tgt_ix": "136-ARR_v2_173@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_174",
            "tgt_ix": "136-ARR_v2_174@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_175",
            "tgt_ix": "136-ARR_v2_175@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_176",
            "tgt_ix": "136-ARR_v2_176@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_177",
            "tgt_ix": "136-ARR_v2_177@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_178",
            "tgt_ix": "136-ARR_v2_178@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_179",
            "tgt_ix": "136-ARR_v2_179@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_180",
            "tgt_ix": "136-ARR_v2_180@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_181",
            "tgt_ix": "136-ARR_v2_181@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_182",
            "tgt_ix": "136-ARR_v2_182@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_183",
            "tgt_ix": "136-ARR_v2_183@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_184",
            "tgt_ix": "136-ARR_v2_184@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_185",
            "tgt_ix": "136-ARR_v2_185@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_186",
            "tgt_ix": "136-ARR_v2_186@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_187",
            "tgt_ix": "136-ARR_v2_187@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_188",
            "tgt_ix": "136-ARR_v2_188@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_189",
            "tgt_ix": "136-ARR_v2_189@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_190",
            "tgt_ix": "136-ARR_v2_190@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_191",
            "tgt_ix": "136-ARR_v2_191@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_192",
            "tgt_ix": "136-ARR_v2_192@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_193",
            "tgt_ix": "136-ARR_v2_193@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_194",
            "tgt_ix": "136-ARR_v2_194@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v2_195",
            "tgt_ix": "136-ARR_v2_195@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1227,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "136-ARR",
        "version": 2
    }
}