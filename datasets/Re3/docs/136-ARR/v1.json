{
    "nodes": [
        {
            "ix": "136-ARR_v1_0",
            "content": "Neural Pipeline for Zero-Shot Data-to-Text Generation",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_2",
            "content": "In data-to-text (D2T) generation, training on in-domain data leads to overfitting to the data representation and repeating training data noise. We examine how to avoid finetuning the pretrained language models (PLMs) on D2T generation datasets while still taking advantage of surface realization capabilities of PLMs. Inspired by pipeline approaches, we propose to generate text by rephrasing single-item templates using a sequence of modules trained on general-domain text-based operations-ordering, aggregation, and paragraph compression. We train PLMs for performing these operations on a synthetic corpus WIKIFLUENT which we build from English Wikipedia. Our experiments on two major triple-to-text datasets-WebNLG and E2Eshow that our approach enables D2T generation from RDF triples in zero-shot settings. 1",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "136-ARR_v1_4",
            "content": "The aim of data-to-text (D2T) generation is to produce natural language descriptions of structured data (Gatt and Krahmer, 2018;Reiter and Dale, 1997). Although pipelines of rule-based D2T generation modules are still used in practice (Dale, 2020), end-to-end approaches based on PLMs recently showed superior benchmark performance (Ke et al., 2021;Chen et al., 2020a;Ferreira et al., 2020;Kale and Rastogi, 2020b;Ribeiro et al., 2020), surpassing pipeline systems (Ferreira et al., 2019) in both automatic and human evaluation metrics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_5",
            "content": "Finetuning PLMs on human-written references is widely accepted as a standard approach for adapting PLMs to the D2T generation objective and achieving good performance on a given benchmark (Agarwal et al., 2021;Ke et al., 2021). Nevertheless, this approach brings issues: Most obviously, finetuning the model for the domain-specific 1 The anonymized version of our code and data is available at https://anonymous.4open.science/r/ zeroshot-d2t-pipeline/. In-domain knowledge is included only in the simple hand-crafted templates for each predicate.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_6",
            "content": "data distribution leads to overfitting on the particular benchmark, decreasing performance on outof-distribution data (Laha et al., 2020). Moreover, collecting a large set of references for a particular domain is costly and time-consuming, as the data are usually collected using crowdsourcing . Few-shot approaches are an alternative, requring only several tens or hundreds of annotated examples (Chen et al., 2020c;Ke et al., 2021;Su et al., 2021a). However, robustness of these approaches is questionable-selecting a representative set of examples which would improve performance is difficult (Chang et al., 2021a), and the limited sample is often noisy, increasing the chance of hallucinations and omissions (Du\u0161ek et al., 2019;Harkous et al., 2020;Rebuffel et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_7",
            "content": "In this paper, we provide an alternative to this traditional paradigm by formulating the D2T generation from RDF triples as a sequence of generaldomain operations over text in natural language. We start by transforming individual triples to text using trivial templates, which we subsequently or-der, aggregate, and compress on the paragraph level to produce the resulting description of the data. All the pipeline modules operate over natural language text and are built upon PLMs trained on our WIKIFLUENT corpus. WIKIFLUENT contains 934k examples of first paragraphs from the English Wikipedia, each supplied with a synthesized set of simple template-like sentences conveying the same meaning. Our approach allows generating natural language descriptions from triples with a minimum amount of domain-specific rules or knowledge and without using training data from the D2T datasets. We show that our approach can yield large improvements upon simple baselines and match older supervised systems in terms of fluency, while bringing potential for further improvements and advantages with respect to controllability.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_8",
            "content": "Our contributions are the following: (1) We propose an alternative D2T generation approach based on general-domain text-to-text operations (ordering, aggregation, and paragraph compression). ( 2) We introduce a synthetic WIKIFLUENT corpus containing 934k sentences based on English Wikipedia, providing training data for the operations in (1). (3) We apply our system on two D2T datasets and evaluate its performance both automatically and manually, including the contribution of individual pipeline modules. (4) We release our code, data, pretrained models, and system outputs to ease future research.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_9",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "136-ARR_v1_10",
            "content": "D2T Generation with PLMs Large neural language models pretrained on self-supervised tasks (Lewis et al., 2020;Liu et al., 2019;Devlin et al., 2019) have recently gained a lot of traction in D2T generation research (Ferreira et al., 2020). Following Chen et al. (2020c), other works adopt PLMs for few-shot D2T generation (Chang et al., 2021b;Su et al., 2021a). Kale and Rastogi (2020b) and Ribeiro et al. (2020) showed that PLMs using linearized representations of data can outperform graph neural networks on graph-to-text datasets, recently surpassed again by graph-based models (Ke et al., 2021;Chen et al., 2020a). Although the models make use of general-domain pretraining tasks, all of them are eventually finetuned on domain-specific data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_11",
            "content": "Templates in Data-Driven D2T Generation Using simple handcrafted templates for individual keys or predicates is an efficient way of introducing domain knowledge while preventing text-to-text models from overfitting to a specific data format (Heidari et al., 2021;Kale and Rastogi, 2020a;. Transforming individual triples to text is also used in Laha et al. (2020) whose work is the most similar to ours. They also build a three-step pipeline for zero-shot D2T generation, but they use handcrafted rules for producing the output text and do not address content planning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_12",
            "content": "Content Planning in D2T Generation Content planning, i.e. ordering input facts and aggregating them into individual sentences, is a traditional part of the D2T generation pipeline (Ferreira et al., 2019;Gatt and Krahmer, 2018;Reiter and Dale, 1997). As previously demonstrated, using a content plan in neural D2T generation has important impact on the overall text quality (Moryossef et al., 2019a,b;Puduppully et al., 2019;Trisedya et al., 2020). Recently, have shown that using a content plan leads to improved quality of PLM outputs. All the aforementioned models plan directly using predicates or keys in the D2T datasets representing the corresponding data item. Unlike these works, our planner is trained on ordering sentences in natural language.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_13",
            "content": "Sentence Ordering Sentence ordering is the task of organizing a set of natural language sentences to increase the coherence of a text (Barzilay et al., 2001;Lapata, 2003). Several neural methods for this task were proposed, using either interactions between pairs of sentences Li and Jurafsky, 2017), global interactions (Gong et al., 2016;Wang and Wan, 2019), or combination of both (Cui et al., 2020). We base our ordering module ( \u00a75.1) on the recent work of Calizzano et al. (2021), who use a pointer network (Wang and Wan, 2019;Vinyals et al., 2015) on top of a PLM.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_14",
            "content": "Fact Aggregation The compact nature of the target text description results in aggregating multiple facts in a single sentence. Previous works (Wiseman et al., 2018;Shao et al., 2019;Shen et al., 2020;Xu et al., 2021) capture the segments which correspond to individual parts of the input as latent variables. Unlike these works, we adopt a simpler scenario using an already ordered sequence of facts, in which we selectively insert delimiters marking sentence boundaries. Paragraph Compression We introduce paragraph compression as a new task in our D2T generation pipeline. As the last step in the pipeline, it is closely related to linguistic realisation, howeversince we already work with natural language in this step-the focus of our task is on sentence fusion, rephrasing, and coreference resolution. Unlike text summarization or simplification Jiang et al., 2020), we aim to convey the complete semantics of the text without omitting any facts. In contrast to sentence fusion (Geva et al., 2019;Barzilay and McKeown, 2005) or sentence compression (Filippova and Altun, 2013), we operate in the context of multiple sentences in a paragraph. The task is the central focus of our WIKIFLUENT corpus ( \u00a74), which we synthesize using a model for the reverse task, split-andrephrase, i.e. splitting a complex sentence into simpler ones while preserving semantics (Botha et al., 2018;.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_15",
            "content": "Method",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "136-ARR_v1_16",
            "content": "We first give an overview of our neural D2T generation pipeline ( \u00a73.1). Next, we describe the individual steps, starting by applying simple templates to transform data to text ( \u00a73.2), followed by individual modules for ordering ( \u00a73.3), aggregation ( \u00a73.4), and paragraph compression ( \u00a73.5).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_17",
            "content": "Method Overview",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "136-ARR_v1_18",
            "content": "We focus on the task of producing a natural language description Y for a set of n RDF triples X \" tx 1 , . . . , x n u. Each triple x i \" ts i , p i , o i u consists of subject s i , predicate p i , and object o i .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_19",
            "content": "We assume that we can transform each triple x i to a fact f i (where f i is a sentence in natural language describing x i ) by filling the single-triple template t p i P T for the predicate p i :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_20",
            "content": "t p i ps i , o i q \u00d1 f i .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_21",
            "content": "We proceed as follows -given an input X, we:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_22",
            "content": "(1) apply the templates to transform the set of triples X to the set of facts: F \" T pXq \" tf 1 , . . . , f n u ( \u00a73.2), (2) sort the facts F using an ordering module which outputs an ordered sequence of facts F o \" OpF q \" tf o 1 , . . . , f on u ( \u00a73.3), (3) obtain sentence delimiters by inputting the ordered facts F o into an aggregation module ApF o q \" t\u03b4 o 1 , \u03b4 o 2 , . . . , \u03b4 o n\u00b41 u; \u03b4 i P t0, 1u, where \u03b4 o i \" 1 indicates the presence of a delimiter, i.e., that the sentences with facts f o i and f o i`1 should not be fused ( \u00a73.4), (4) input the ordered sequence with delimiters",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_23",
            "content": "F a \" tf o 1 , \u03b4 o 1 , f o 2 , . . . , \u03b4 o n\u00b41",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_24",
            "content": ", f on u into the paragraph compression module which generates the final description P pF a q \" Y ( \u00a73.5).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_25",
            "content": "Templates",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "136-ARR_v1_26",
            "content": "The first step in our pipeline involves transforming each of the input triples X into a set of facts F in natural language by using a template t p i for each predicate p i . We need at least one template for each predicate. Typically, the template will include placeholders which are filled with s i and o i .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_27",
            "content": "The transformation serves two purposes: (a) preparing the data for the subsequent text-to-text operations, (b) introducing in-domain knowledge about the semantics of individual predicates. Note that the filled templates are allowed to contain minor disfluencies since the text will be rephrased in the final step of the pipeline. See \u00a75.5 for our approach to gathering the templates and Figure 2 for examples of the templates we use in our datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_28",
            "content": "We acknowledge that this step may be a bottleneck on datasets with an unconstrained (or very large) set of predicates, which is why we also discuss possibilities for automating this step in \u00a77.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_29",
            "content": "Ordering",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "136-ARR_v1_30",
            "content": "We assume that the default order of triples X (and the respective facts F ) is random. To maximize the coherency of the resulting description, we apply an ordering model O to get an ordered sequence of facts: F o \" tf o 1 , . . . , f on u. The coherence of the final text will also depend on the paragraph compression step, but grouping related facts together (e.g. facts mentioning birth date and birth place) helps the paragraph compression model to focus only on fusing and rephrasing the neighboring sentences. We describe our ordering model in \u00a75.1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_31",
            "content": "Aggregation",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "136-ARR_v1_32",
            "content": "The aggregation model takes a sequence of ordered facts F o as input and produces a sequence of sentence delimiters ApF o q \" t\u03b4 o 1 , \u03b4 o 2 , . . . , \u03b4 o n\u00b41 u; \u03b4 i P t0, 1u. The output \u03b4 i \" 1 means that the neighboring facts are should be mentioned separately, serving as a hint for the paragraph compression model not to fuse the neighboring sentences. Conversely, \u03b4 i \" 0 means that the facts should be aggregated and their corresponding sentences should be fused (see \u00a75.2 and \u00a75.3).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_33",
            "content": "Paragraph Compression",
            "ntype": "title",
            "meta": {
                "section": "3.5"
            }
        },
        {
            "ix": "136-ARR_v1_34",
            "content": "The paragraph compression model (see \u00a75.3 for simplified variants) takes as input the ordered sequence of facts with delimiters F a \" tf o 1 , \u03b4 o 1 , f o 2 , . . . , \u03b4 o n\u00b41 , f on u and produces a resulting text Y . The objectives of the model are two-fold: (a) fusing related sentences, i.e., sentences i and j in between which \u03b4 i \" 0, and (b) rephrasing the text to improve its fluency, e.g. fixing minor disfluencies in the templates, replacing noun phrases with refering expressions, etc. The focus is on minor rephrasing since the goal is to preserve the semantics of the original text.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_35",
            "content": "WIKIFLUENT Corpus",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "136-ARR_v1_36",
            "content": "A key to our approach is building a large-scale synthetic corpus providing training data for the text operations in our pipeline. Our corpus needs to cover a broad range of domains while capturing the sentence style in D2T generation, both regarding the input templates and the target descriptions. In other words, we aim to build a corpus in which:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_37",
            "content": "\u2022 the input is a set of simple, template-like sentences, \u2022 the output is a fluent text in natural language preserving the semantics of the input. As we describe below in detail, we achieve that by applying a split-and-rephrase model and a coreference resolution model on a set of human-written paragraphs in English Wikipedia. We consider the processed text as a source and the original text as the target. The process is illustrated in Figure 3; corpus statistics are included in Appendix A.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_38",
            "content": "Data Source",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "136-ARR_v1_39",
            "content": "For building the WIKIFLUENT corpus, we extracted 934k first paragraphs of articles from a Wikipedia dump 2 using WikiExtractor (Attardi, 2 enwiki-20210401-pages-articles-multistream",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_40",
            "content": "The Westmeath Examiner is a weekly newspaper in Westmeath, Ireland.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_41",
            "content": "It is located in Westmeath, Ireland.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_42",
            "content": "The Westmeath Examiner is a weekly newspaper.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_43",
            "content": "The Westmeath Examiner is a weekly newspaper.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_44",
            "content": "It was founded in 1882.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_45",
            "content": "The Westmeath Examiner is located in Westmeath, Ireland.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_46",
            "content": "The Westmeath Examiner was founded in 1882. 2015). The paragraphs contain mostly concise, factbased descriptions from a wide range of domains. We selected paragraphs with length between 30-430 characters, filtering out lists, disambiguations, repeated and malformed paragraphs. To further ensure that the length of inputs is balanced, we selected 250k examples each from 4 equidistant length ranges (30-130 characters, etc.).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_47",
            "content": "Split-and-Rephrase",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "136-ARR_v1_48",
            "content": "For generating the target set of sentences, we divide each paragraph into sentences using NLTK (Bird, 2006) and apply a split-and-rephrase model on each sentence. Split-and-rephrase is a task of splitting a complex sentence into a meaning preserving sequence of shorter sentences . We train our model on the large-scale WikiSplit corpus by Botha et al. (2018), containing human-made sentence splits from Wikipedia edit history. Following the setup in the rest of our experiments, we train the encoder-decoder PLM BART-base (Lewis et al., 2020) on the WikiSplit dataset in a sequence-to-sequence setting. We apply the trained split-and-rephrase model on each sentence, uniformly randomly choosing between 0-2 recursive calls to ensure that the splits are not deterministic. If the sentence cannot be meaningfully split, the model tends to duplicate the sentence on the output; in that case, we use only the original sentence and do not proceed with the splitting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_49",
            "content": "Coreference Replacement",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "136-ARR_v1_50",
            "content": "Next, we concatenate the split sentences and apply a coreference resolution model (Gardner et al., 2018) in order to replace referring expressions with their antencendents (e.g., pronouns with noun phrases). This allows to better follow the style of the templates in which the entities are always fully verbalized. Since we keep the referring expressions in the original human-written text, we can train the paragraph compression module to generate them in the final text description.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_51",
            "content": "Filtering",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "136-ARR_v1_52",
            "content": "To assert that the generated sentences convey the same semantics as the original paragraph, we use a pretrained RoBERTa model 3 (Liu et al., 2019) trained on the MultiNLI dataset (Williams et al., 2018) for checking the semantic accuracy of the generated text. Following , we test if the original paragraph entails each of the synthesized sentences (checking for omissions), and if the set of concatenated synthesized sentences entails the original paragraph (checking for hallucinations). In a filtered version of the WIK-IFLUENT corpus, we include only the examples without omissions or hallucinations (as computed by the model), reducing it to approximately 3/4 of the original size.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_53",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "136-ARR_v1_54",
            "content": "We show how we build our pipeline ( \u00a75.1-5.4) and discuss the D2T generation datasets which we use for our experiments ( \u00a75.5). The details of our training setup are included in Appendix B.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_55",
            "content": "Ordering Model",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "136-ARR_v1_56",
            "content": "For our ordering model (see \u00a73.3), we use the Simple Pointer model from Calizzano et al. (2021). The model is based on a pretrained BART-base extended with a pointer network from Wang and Wan (2019). We provide a short description of the model here; for details see Calizzano et al. (2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_57",
            "content": "In the encoding phase, facts F are concatenated and tokenized. Each fact is surrounded by special tokens denoting the beginning (<s>) and the end (</s>) of the fact. The sequence is processed by the BART encoder, generating a sequence of encoder states E for each end token </s> representing the preceding fact.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_58",
            "content": "The decoding proceeds autoregressively. To bootstrap the decoding process, the pair of tokens <s></s> is fed into the decoder, producing the decoder state d 1 . The pointer network (attending to d 1 and E), selects the first ordered fact f o 1 , which is fed into the decoder in the next step. The process is repeated until the all the facts are decoded in a particular order.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_59",
            "content": "The pointer network computes the probability of a fact to be on the j-th position, using the encoder output E and the decoder output d j . The network is based on the scaled dot product attention, where d j is the query and encoder outputs E i are the keys:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_60",
            "content": "Q \" d j W Q K \" EW K P j \" softmax \u02c6QK T ? b \u02d9.",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_61",
            "content": "Here W Q and W K P R b\u02c6b , b is the dimension of BART hidden states, and P j P R n`1 is the probability distribution for the j-th position (i.e., P ji is the probability that fact f i is on the j-th position). We train the model using the split sentences in the WIKIFLUENT corpus, randomly shuffling the order of the sentences and training the model to restore their original order.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_62",
            "content": "Aggregation Model",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "136-ARR_v1_63",
            "content": "We base our aggregation model (cf. \u00a73.4) on RoBERTa-large (Liu et al., 2019) with a token classification head. 4 Similarly to the ordering model ( \u00a75.1), we input the sequence of facts F o into the model, separating each pair of facts f o i with a special token </s> (used by the model as a separator). Subsequently, the token classification layer classifies each separator </s> i position into two classes t0, 1u corresponding to the delimiter \u03b4 i . We ignore the outputs for the non-separator tokens while computing the cross-entropy loss.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_64",
            "content": "We create the training examples using the split sentences in the WIKIFLUENT corpus, in which we set \u03b4 i \" 0 for the sentences i, i `1 which were originally aggregated (i.e., are the result of splitting a single sentence) and \u03b4 i \" 1 otherwise.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_65",
            "content": "Paragraph Compression Model",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "136-ARR_v1_66",
            "content": "We adopt BART-base for our paragraph compression model. We train the model in a sequenceto-sequence setting on the WIKIFLUENT corpus, concatenating the split sentences on the input. We add delimiters between sentences i and i `1 where \u03b4 i \" 1 using a special token <sep>, which we add to the model vocabulary. the model can steer the model towards producing certain outputs. We evaluate our model's behavior with respect to ordering and aggregation in \u00a76.3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_67",
            "content": "Ablation Study",
            "ntype": "title",
            "meta": {
                "section": "5.4"
            }
        },
        {
            "ix": "136-ARR_v1_68",
            "content": "In order to evaluate individual components of our pipeline, we train three versions of the PC model (see \u00a75.3). The models share the same architecture and targets, but differ in their inputs:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_69",
            "content": "\u2022 PC -the model takes as an input ordered facts with delimiters (as described in \u00a73.5), \u2022 PC+AGG -the model takes as an input ordered facts without delimiters (i.e., the aggregation is left implicitly to the model), \u2022 PC+ORD+AGG -the model takes as an input facts in random order and without delimiters (i.e., both ordering and aggregation are left implicitly to the model). Subsequently, we test three versions of the pipeline (see Figure 4): \u2022 3-STAGE -a full version of the pipeline consisting of the ordering model, the aggregation model and the PC model (following the full pipeline from \u00a73), \u2022 2-STAGE -a pipeline consisting of the ordering model and the PC+AGG model, \u2022 1-STAGE -a single stage consisting of the PC+ORD+AGG model. We evaluate all versions of the pipeline with PC models trained on the full and filtered versions of the WIKIFLUENT dataset (see \u00a74).",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_70",
            "content": "D2T Datasets",
            "ntype": "title",
            "meta": {
                "section": "5.5"
            }
        },
        {
            "ix": "136-ARR_v1_71",
            "content": "We test our approach on two English D2T datasets, WebNLG and E2E. They differ in domain, size, textual style, and number of predicates (see Appendix A for details).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_72",
            "content": "WebNLG The WebNLG dataset contains RDF triples from DBPedia (Auer et al., 2007) and their crowdsourced descriptions. The dataset was extended for the WebNLG+ Challenge (Ferreira et al., 2020), but we use the version 1.4 for comparability to prior work. Templates for WebNLG could be extracted from the training data by delexicalizing single-triple examples. However, the examples are noisy and such data would not be available in a zero-shot setup. Therefore, we handcrafted templates for all 354 predicates, including unseen predicates in the test set. 5 E2E The E2E dataset (Novikova et al., 2017; contains restaurant recommendations in the form of attribute-value pairs. We use the cleaned version of the dataset (Du\u0161ek et al., 2019). Following previous work, we transformed the attribute-value pairs into RDF triples (using the restaurant name as a subject) and then applied the same setup as for WebNLG. We created a template for each of the 8 attributes manually.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_73",
            "content": "Evaluation",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "136-ARR_v1_74",
            "content": "We evaluate outputs from the {1,2,3}-STAGE variants of our pipeline automatically ( \u00a76.1) and manually ( \u00a76.2). Further, we evaluate the performance of the content planning modules and the ability of the PC module to follow the content plan ( \u00a76.3).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_75",
            "content": "Automatic Metrics",
            "ntype": "title",
            "meta": {
                "section": "6.1"
            }
        },
        {
            "ix": "136-ARR_v1_76",
            "content": "Following prior work, we use BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) to evaluate the outputs against the human references. 6 We also evaluate the number of omission and hallucination errors (i.e., facts missing or added, respectively) using a metric from based on a RoBERTa model (Liu et al., 2019) Wildwood is a restaurant. Wildwood serves French food. Wildwood is in the riverside. Wildwood is near Raja Indian Cuisine.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_77",
            "content": "Wildwood is a restaurant serving French food. It is in the riverside near Raja Indian Cuisine. Human A amazing French restaurant is called the Wildwood. The restaurant is near the Raja Indian Cuisine in riverside.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_78",
            "content": "They love kids. We include a diverse set of baselines for comparison. For WebNLG (see Table 2), we include the results of UPF-FORGe and MELBOURNE systems from the first run of WebNLG Challenge which are comparable in terms of automatic metrics and semantic errors, and the results of Ke et al. (2021), which is a state-of-theart system using structure-aware encoder and taskspecific pretraining. Laha et al. ( 2020) is (to our knowledge) the only other zero-shot D2T generation system applied on WebNLG. TGEN (Du\u0161ek and Jur\u010d\u00ed\u010dek, 2015) is the baseline system for the E2E Challenge and Harkous et al. (2020) is a state-of-the art supervised system applied on the cleaned E2E (see Table 3). For both datasets, COPY is the baseline of copying the templates verbatim.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_79",
            "content": "The automatic evaluation suggests that while our system lags behind state-of-the-art supervised systems, it shows considerable improvements compared to the COPY baseline (e.g., \"12 BLEU points pendix C for the details. for E2E) and matches performance of some older supervised systems. The COPY baseline is substantially better than the zero-shot system of Laha et al. (2020), suggesting that quality of the templates plays an important role. The 2-STAGE system is generally on par with the 3-STAGE system (or better), which indicates that implicit aggregation using the PC-AGG model may be sufficient. However, an advantage of having a separate aggregation module is the possibility to control the aggregation step explicitly. The filtered version of the dataset generally brings better results, although it brings also an increase in the number of omissions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_80",
            "content": "Manual Evaluation",
            "ntype": "title",
            "meta": {
                "section": "6.2"
            }
        },
        {
            "ix": "136-ARR_v1_81",
            "content": "We manually evaluated 100 outputs of the models regarding factual errors (hallucinations, omissions, incorrect fact merging, redundancies) as well as grammatical errors. The results are listed in Table 4. The 1-STAGE model (which has to order the facts implicitly) tends to repeat the facts in the text (especially in E2E) and produces frequent hallucinations. These problems are only slightly reduced in the filtered version, but they are largely eliminated with 2-STAGE and 3-STAGE models. We note these models create almost no hallucinations or omissions. However, the outputs on WebNLG for all systems suffer from semantic errors resulting from merging of unrelated facts. This mostly happens with unrelated predicates connected to the same subject/object (e.g. \"X was born in Y\", \"X worked as Z\" expressed as \"X worked as Z in Y\"; see Appendix D for examples). On the E2E data, which has a simpler triple structure (all predicates share the same subject), the outputs are generally consistent and the 2-STAGE and 3-STAGE models exhibit almost no semantic errors. As we discuss in \u00a77, more research is needed for ensuring the final consistency of the text. The grammar errors and disfluencies stem mainly from over-eager paragraph compression or from artifacts in our templates; they are relatively minor (e.g., missing \"is\" in \"serves French food and family-friendly\").",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_82",
            "content": "WebNLG E2E H I O R G H I O R G full 3-",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_83",
            "content": "Content Planning",
            "ntype": "title",
            "meta": {
                "section": "6.3"
            }
        },
        {
            "ix": "136-ARR_v1_84",
            "content": "Following and , we report the accuracy (Acc) and BLEU-2 score (B-2) of our ordering model on WebNLG against the human-generated plans from Ferreira et al. (2018).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_85",
            "content": "The results are listed in Table 5. RANDOM is the baseline of generating a random order. The results show that although our approach lacks behind stateof-the-art supervised approaches, it can outperform both the random baseline and the Transformerbased approach from Ferreira et al. (2019) while not using any training examples from WebNLG. We also evaluate the accuracy of our aggregation model, using triples ordered according to the plans from Ferreira et al. (2018) as input. The accuracy is 0.33 per example and 0.62 per sentence boundary (random baseline is 0.23 and 0.50, respectively). The results show that although our approach is better than the random baseline, further",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_86",
            "content": "Transformer (Ferreira et al., 2019) : 52.20 0.35 Step-by-step (Moryossef et al., 2019b) : 70.80 0.47 PLANENC 80.10 0.62 Plan-then-generate 84.97 0.72 RANDOM 47.00 0.29",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_87",
            "content": "BART+ptr (Calizzano et al., 2021) 59.10 0.48",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_88",
            "content": "Table 5: Evaluation of our zero-shot ordering model based on Calizzano et al. (2021). The results marked with : are copied from the respective papers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_89",
            "content": "investigation regarding plausible fact aggregation schemes is needed. Finally, we manually evaluate how the PC model follows the content plan using 100 randomly chosen examples with more than 1 triple on WebNLG and E2E. We find that the model follows the content plan in 95% and 100% of cases, respectively. The incorrect cases include a fact not properly mentioned and an extra boundary between the sentences without a separator. We can thus conclude that the pretraining task successfully teaches the PC model to follow a given content plan.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_90",
            "content": "Discussion and Future Work",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "136-ARR_v1_91",
            "content": "In the current form, our pipeline can be directly applied to generating text from RDF triples (or similarly structured data) which require no extra processing. Further extensions are needed for more complex D2T scenarios, e.g. datasets requiring content selection or common-sense and logical reasoning (Wiseman et al., 2017;Chen et al., 2020b).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_92",
            "content": "Our approach regarding handcrafting a single template for each predicate is quite basic. Generating simple statements from the triples automatically, e.g., using the approach of Laha et al. (2020), could reduce the manual workload and allow applying our approach on datasets with a less constrained set of data attributes such as ToTTo (Parikh et al., 2020) or DART (Nan et al., 2021). Moreover, explicitly including a denoising task for the paragraph compression model could help to tackle the disfluencies in the templates.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_93",
            "content": "More research is also needed on semantic errors stemming from merging of facts in improper ways. We suggest that explicitly controlling the semantics of sentence fusion (Ben-David et al., 2020) could help to mitigate this issue, while still keeping the advantages of a zero-shot approach.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_94",
            "content": "Statistics for the datasets described in the paper are listed in Table 7.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_95",
            "content": "We implemented the models for split-and-rephrase, aggregation, and paragraph compression in Py-Torch Lightning (Paszke et al., 2019), using the PyTorch (Falcon, 2019) version of the BART and RoBERTa models from the Huggingface library (Wolf et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_96",
            "content": "We use the Adam (Kingma and Ba, 2015) optimizer (\u03b2 1 \" 0.9, \u03b2 2 \" 0.997, \u03b5 \" 1 \u00b49) with learning rate 2 \u00b45, linear scheduling and 0.1 warmup proportion; batches of size 8 and accumulating gradients with factor 4. We train the models for 1 epoch on a single GeForce RTX 3090 GPU with 24 GB RAM. We use greedy decoding in all our experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_97",
            "content": "For training the ordering model, we used the implementation from Calizzano et al. (2021) 8 including their training parameters. We plan to fully integrate the ordering model into our framework in the future.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_98",
            "content": "We provide evaluation of semantic accuracy on the E2E dataset as evaluated with the slot-error script based on matching regular expressions in Table 6. 9 Note that our manual investigation of a sample of the data shows that the majority of the errors identified in our model outputs are false. For example, the following regular expression used in the slot-error script: prices?(?: range)?(?:w+)0,3 high matches \"(...) price range and high customer rating (. outputs of our models, which tend to repeat certain patterns. However, we also manually identified several cases in which an error was found correctly, e.g. the model hallucinating \"3 out of 4 customer rating\" instead of \"3 out of 5 customer rating\".",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_99",
            "content": "Tables 8, 9, 10, and 11 show examples of behavior of our models on the WebNLG dataset. Tables 12 and 13 show examples of behavior of our models on the E2E dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_100",
            "content": "The green color marks the model outputs which are completely correct, the red color marks the errors. For better readability of the input format, we add numeric order identifiers for the individual facts (bold, in squared brackets). These are subsequently used as references in the Order and Aggregation rows. Note that zero-th input in E2E is used as a subject in the RDF triples.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_101",
            "content": "Akron, Ohio is from United States. English language is spoken in Akron, Ohio.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_102",
            "content": "English language is spoken in the United States. Akron, Ohio is from the state of Ohio in the U.S. state of Illinois. 3-stage filtered Akron, Ohio is from United States. English language is spoken in the United States of America. 2-stage filtered Akron, Ohio is from United States. English language is spoken in Akron, Ohio. 1-stage filtered English language is spoken in the United States. Akron, Ohio is from the United Kingdom.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_103",
            "content": "The country of Akron, Ohio, is United States, where English is the language spoken.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_104",
            "content": "Table 9: Incorrect behavior on WebNLG: besides the minor disfluencies caused by the templates (\"Akron, Ohio is from...\"), the models (except for 3-STAGE filtered) tend to hallucinate and merge the facts incorrectly.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "136-ARR_v1_105",
            "content": "Oshin Agarwal, Heming Ge, Siamak Shakeri, Rami Al-Rfou, Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Oshin Agarwal",
                    "Heming Ge",
                    "Siamak Shakeri",
                    "Rami Al-Rfou"
                ],
                "title": "Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_106",
            "content": "UNKNOWN, None, 2015, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_107",
            "content": "S\u00f6ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, Dbpedia: A nucleus for a web of open data, 2007, The semantic web, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "S\u00f6ren Auer",
                    "Christian Bizer",
                    "Georgi Kobilarov",
                    "Jens Lehmann",
                    "Richard Cyganiak",
                    "Zachary Ives"
                ],
                "title": "Dbpedia: A nucleus for a web of open data",
                "pub_date": "2007",
                "pub_title": "The semantic web",
                "pub": "Springer"
            }
        },
        {
            "ix": "136-ARR_v1_108",
            "content": "Satanjeev Banerjee, Alon Lavie, METEOR: An automatic metric for MT evaluation with improved correlation with human judgments, 2005, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Satanjeev Banerjee",
                    "Alon Lavie"
                ],
                "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
                "pub_date": "2005",
                "pub_title": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_109",
            "content": "Regina Barzilay, Noemie Elhadad, Kathleen Mckeown, Sentence ordering in multidocument summarization, 2001, Proceedings of the first international conference on Human language technology research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Regina Barzilay",
                    "Noemie Elhadad",
                    "Kathleen Mckeown"
                ],
                "title": "Sentence ordering in multidocument summarization",
                "pub_date": "2001",
                "pub_title": "Proceedings of the first international conference on Human language technology research",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_110",
            "content": "Regina Barzilay,  Kathleen R Mckeown, Sentence fusion for multidocument news summarization, 2005, Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Regina Barzilay",
                    " Kathleen R Mckeown"
                ],
                "title": "Sentence fusion for multidocument news summarization",
                "pub_date": "2005",
                "pub_title": "Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_111",
            "content": "Eyal Ben-David, Orgad Keller, Eric Malmi, Idan Szpektor, and Roi Reichart. 2020. Semantically driven sentence fusion: Modeling and evaluation, , Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Eyal Ben-David",
                    "Orgad Keller",
                    "Eric Malmi"
                ],
                "title": "Idan Szpektor, and Roi Reichart. 2020. Semantically driven sentence fusion: Modeling and evaluation",
                "pub_date": null,
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_112",
            "content": "Steven Bird, Nltk: the natural language toolkit, 2006, Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Steven Bird"
                ],
                "title": "Nltk: the natural language toolkit",
                "pub_date": "2006",
                "pub_title": "Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_113",
            "content": "Jan Botha, Manaal Faruqui, John Alex, Jason Baldridge, Dipanjan Das, Learning to split and rephrase from Wikipedia edit history, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Jan Botha",
                    "Manaal Faruqui",
                    "John Alex",
                    "Jason Baldridge",
                    "Dipanjan Das"
                ],
                "title": "Learning to split and rephrase from Wikipedia edit history",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "136-ARR_v1_114",
            "content": "R\u00e9mi Calizzano, Malte Ostendorff, Georg Rehm, Ordering sentences and paragraphs with pretrained encoder-decoder transformers and pointer ensembles, 2021, Proceedings of the 21st ACM Symposium on Document Engineering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "R\u00e9mi Calizzano",
                    "Malte Ostendorff",
                    "Georg Rehm"
                ],
                "title": "Ordering sentences and paragraphs with pretrained encoder-decoder transformers and pointer ensembles",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 21st ACM Symposium on Document Engineering",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_115",
            "content": "UNKNOWN, None, 2021, On training instance selection for few-shot neural text generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "On training instance selection for few-shot neural text generation",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_116",
            "content": "Ernie Chang, Xiaoyu Shen, Dawei Zhu, Vera Demberg, Hui Su, Neural data-to-text generation with lm-based text augmentation, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Ernie Chang",
                    "Xiaoyu Shen",
                    "Dawei Zhu",
                    "Vera Demberg",
                    "Hui Su"
                ],
                "title": "Neural data-to-text generation with lm-based text augmentation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_117",
            "content": "Wenhu Chen, Yu Su, Xifeng Yan, William Wang, KGPT: Knowledge-grounded pretraining for data-to-text generation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Wenhu Chen",
                    "Yu Su",
                    "Xifeng Yan",
                    "William Wang"
                ],
                "title": "KGPT: Knowledge-grounded pretraining for data-to-text generation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_118",
            "content": "UNKNOWN, None, 2016, Neural sentence ordering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Neural sentence ordering",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_119",
            "content": "Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam Sundaresan, William Wang, Logic2text: High-fidelity natural language generation from logical forms, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Zhiyu Chen",
                    "Wenhu Chen",
                    "Hanwen Zha",
                    "Xiyou Zhou",
                    "Yunkai Zhang",
                    "Sairam Sundaresan",
                    "William Wang"
                ],
                "title": "Logic2text: High-fidelity natural language generation from logical forms",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_120",
            "content": "Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, William Wang, Few-shot NLG with pre-trained language model, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Zhiyu Chen",
                    "Harini Eavani",
                    "Wenhu Chen",
                    "Yinyin Liu",
                    "William Wang"
                ],
                "title": "Few-shot NLG with pre-trained language model",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_121",
            "content": "Baiyun Cui, Yingming Li, Zhongfei Zhang, Bert-enhanced relational sentence ordering network, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Baiyun Cui",
                    "Yingming Li",
                    "Zhongfei Zhang"
                ],
                "title": "Bert-enhanced relational sentence ordering network",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_122",
            "content": "Robert Dale, Natural language generation: The commercial state of the art in 2020, 2020, Natural Language Engineering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Robert Dale"
                ],
                "title": "Natural language generation: The commercial state of the art in 2020",
                "pub_date": "2020",
                "pub_title": "Natural Language Engineering",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_123",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_124",
            "content": "Ond\u0159ej Du\u0161ek, M David, Verena Howcroft,  Rieser, Semantic noise matters for neural natural language generation, 2019, Proceedings of the 12th International Conference on Natural Language Generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Ond\u0159ej Du\u0161ek",
                    "M David",
                    "Verena Howcroft",
                    " Rieser"
                ],
                "title": "Semantic noise matters for neural natural language generation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 12th International Conference on Natural Language Generation",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_125",
            "content": "Ond\u0159ej Du\u0161ek, Filip Jur\u010d\u00ed\u010dek, Training a natural language generator from unaligned data, 2015, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Ond\u0159ej Du\u0161ek",
                    "Filip Jur\u010d\u00ed\u010dek"
                ],
                "title": "Training a natural language generator from unaligned data",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "136-ARR_v1_126",
            "content": "Ond\u0159ej Du\u0161ek, Zden\u011bk Kasner, Evaluating semantic accuracy of data-to-text generation with natural language inference, 2020, Proceedings of the 13th International Conference on Natural Language Generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Ond\u0159ej Du\u0161ek",
                    "Zden\u011bk Kasner"
                ],
                "title": "Evaluating semantic accuracy of data-to-text generation with natural language inference",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 13th International Conference on Natural Language Generation",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_127",
            "content": "Ond\u0159ej Du\u0161ek, Jekaterina Novikova, Verena Rieser, Evaluating the state-of-the-art of end-to-end natural language generation: The e2e nlg challenge, 2020, Computer Speech & Language, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Ond\u0159ej Du\u0161ek",
                    "Jekaterina Novikova",
                    "Verena Rieser"
                ],
                "title": "Evaluating the state-of-the-art of end-to-end natural language generation: The e2e nlg challenge",
                "pub_date": "2020",
                "pub_title": "Computer Speech & Language",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_128",
            "content": "UNKNOWN, None, 2019, Pytorch lightning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Pytorch lightning",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_129",
            "content": "Thiago Ferreira, Claire Gardent, Nikolai Ilinykh, Chris Van Der Lee, Simon Mille, Diego Moussallem, Anastasia Shimorina, The 2020 bilingual, bidirectional webnlg+ shared task overview and evaluation results (webnlg+ 2020), 2020, Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Thiago Ferreira",
                    "Claire Gardent",
                    "Nikolai Ilinykh",
                    "Chris Van Der Lee",
                    "Simon Mille",
                    "Diego Moussallem",
                    "Anastasia Shimorina"
                ],
                "title": "The 2020 bilingual, bidirectional webnlg+ shared task overview and evaluation results (webnlg+ 2020)",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_130",
            "content": "Diego Thiago Castro Ferreira, Emiel Moussallem, Sander Krahmer,  Wubben, Enriching the webnlg corpus, 2018, Proceedings of the 11th International Conference on Natural Language Generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Diego Thiago Castro Ferreira",
                    "Emiel Moussallem",
                    "Sander Krahmer",
                    " Wubben"
                ],
                "title": "Enriching the webnlg corpus",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 11th International Conference on Natural Language Generation",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_131",
            "content": "Chris Thiago Castro Ferreira,  Van Der Lee, Emiel Emiel Van Miltenburg,  Krahmer, Neural datato-text generation: A comparison between pipeline and end-to-end architectures, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Chris Thiago Castro Ferreira",
                    " Van Der Lee",
                    "Emiel Emiel Van Miltenburg",
                    " Krahmer"
                ],
                "title": "Neural datato-text generation: A comparison between pipeline and end-to-end architectures",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_132",
            "content": "Katja Filippova, Yasemin Altun, Overcoming the lack of parallel data in sentence compression, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Katja Filippova",
                    "Yasemin Altun"
                ],
                "title": "Overcoming the lack of parallel data in sentence compression",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_133",
            "content": "Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, The WebNLG challenge: Generating text from RDF data, 2017, Proceedings of the 10th International Conference on Natural Language Generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Claire Gardent",
                    "Anastasia Shimorina",
                    "Shashi Narayan",
                    "Laura Perez-Beltrachini"
                ],
                "title": "The WebNLG challenge: Generating text from RDF data",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 10th International Conference on Natural Language Generation",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_134",
            "content": "Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, F Nelson, Matthew Liu, Michael Peters, Luke Schmitz,  Zettlemoyer, Allennlp: A deep semantic natural language processing platform, 2018, Proceedings of Workshop for NLP Open Source Software (NLP-OSS), .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Matt Gardner",
                    "Joel Grus",
                    "Mark Neumann",
                    "Oyvind Tafjord",
                    "Pradeep Dasigi",
                    "F Nelson",
                    "Matthew Liu",
                    "Michael Peters",
                    "Luke Schmitz",
                    " Zettlemoyer"
                ],
                "title": "Allennlp: A deep semantic natural language processing platform",
                "pub_date": "2018",
                "pub_title": "Proceedings of Workshop for NLP Open Source Software (NLP-OSS)",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_135",
            "content": "Albert Gatt, Emiel Krahmer, Survey of the state of the art in natural language generation: Core tasks, applications and evaluation, 2018, Journal of Artificial Intelligence Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Albert Gatt",
                    "Emiel Krahmer"
                ],
                "title": "Survey of the state of the art in natural language generation: Core tasks, applications and evaluation",
                "pub_date": "2018",
                "pub_title": "Journal of Artificial Intelligence Research",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_136",
            "content": "Mor Geva, Eric Malmi, Idan Szpektor, Jonathan Berant, Discofuse: A large-scale dataset for discourse-based sentence fusion, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Mor Geva",
                    "Eric Malmi",
                    "Idan Szpektor",
                    "Jonathan Berant"
                ],
                "title": "Discofuse: A large-scale dataset for discourse-based sentence fusion",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_137",
            "content": "UNKNOWN, None, 2016, End-to-end neural sentence ordering using pointer network, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "End-to-end neural sentence ordering using pointer network",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_138",
            "content": "Hamza Harkous, Isabel Groves, Amir Saffari, Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Hamza Harkous",
                    "Isabel Groves",
                    "Amir Saffari"
                ],
                "title": "Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_139",
            "content": "Peyman Heidari, Arash Einolghozati, Shashank Jain, Soumya Batra, Lee Callender, Ankit Arun, Shawn Mei, Sonal Gupta, Pinar Donmez, Vikas Bhardwaj, Getting to production with few-shot natural language generation models, 2021, Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Peyman Heidari",
                    "Arash Einolghozati",
                    "Shashank Jain",
                    "Soumya Batra",
                    "Lee Callender",
                    "Ankit Arun",
                    "Shawn Mei",
                    "Sonal Gupta",
                    "Pinar Donmez",
                    "Vikas Bhardwaj"
                ],
                "title": "Getting to production with few-shot natural language generation models",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_140",
            "content": "Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, Wei Xu, Neural crf model for sentence alignment in text simplification, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Chao Jiang",
                    "Mounica Maddela",
                    "Wuwei Lan",
                    "Yang Zhong",
                    "Wei Xu"
                ],
                "title": "Neural crf model for sentence alignment in text simplification",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_141",
            "content": "Mihir Kale, Abhinav Rastogi, Template guided text generation for task-oriented dialogue, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Mihir Kale",
                    "Abhinav Rastogi"
                ],
                "title": "Template guided text generation for task-oriented dialogue",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_142",
            "content": "Mihir Kale, Abhinav Rastogi, Text-to-text pre-training for data-to-text tasks, 2020, Proceedings of the 13th International Conference on Natural Language Generation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Mihir Kale",
                    "Abhinav Rastogi"
                ],
                "title": "Text-to-text pre-training for data-to-text tasks",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 13th International Conference on Natural Language Generation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "136-ARR_v1_143",
            "content": "Zden\u011bk Kasner, Ond\u0159ej Du\u0161ek, Data-to-text generation with iterative text editing, 2020, Proceedings of the 13th International Conference on Natural Language Generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Zden\u011bk Kasner",
                    "Ond\u0159ej Du\u0161ek"
                ],
                "title": "Data-to-text generation with iterative text editing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 13th International Conference on Natural Language Generation",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_144",
            "content": "UNKNOWN, None, 2021, Jointgt: Graph-text joint representation learning for text generation from knowledge graphs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Jointgt: Graph-text joint representation learning for text generation from knowledge graphs",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_145",
            "content": "UNKNOWN, None, 2019, Ctrl: A conditional transformer language model for controllable generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Ctrl: A conditional transformer language model for controllable generation",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_146",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A method for stochastic optimization, 2015-05-07, 3rd International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "P Diederik",
                    "Jimmy Kingma",
                    " Ba"
                ],
                "title": "Adam: A method for stochastic optimization",
                "pub_date": "2015-05-07",
                "pub_title": "3rd International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_147",
            "content": "Anirban Laha, Parag Jain, Abhijit Mishra, Karthik Sankaranarayanan, Scalable micro-planned generation of discourse from structured data, 2020, Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Anirban Laha",
                    "Parag Jain",
                    "Abhijit Mishra",
                    "Karthik Sankaranarayanan"
                ],
                "title": "Scalable micro-planned generation of discourse from structured data",
                "pub_date": "2020",
                "pub_title": "Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_148",
            "content": "Mirella Lapata, Probabilistic text structuring: Experiments with sentence ordering, 2003, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Mirella Lapata"
                ],
                "title": "Probabilistic text structuring: Experiments with sentence ordering",
                "pub_date": "2003",
                "pub_title": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_149",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Mike Lewis",
                    "Yinhan Liu",
                    "Naman Goyal",
                    "Marjan Ghazvininejad",
                    "Abdelrahman Mohamed",
                    "Omer Levy",
                    "Veselin Stoyanov",
                    "Luke Zettlemoyer"
                ],
                "title": "Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_150",
            "content": "Jiwei Li, Dan Jurafsky, Neural net models of open-domain discourse coherence, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Jiwei Li",
                    "Dan Jurafsky"
                ],
                "title": "Neural net models of open-domain discourse coherence",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_151",
            "content": "UNKNOWN, None, 2019, Commongen: A constrained text generation dataset towards generative commonsense reasoning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Commongen: A constrained text generation dataset towards generative commonsense reasoning",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_152",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_153",
            "content": "Amit Moryossef, Yoav Goldberg, Ido Dagan, Improving quality and efficiency in plan-based neural data-to-text generation, 2019, Proceedings of the 12th International Conference on Natural Language Generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Amit Moryossef",
                    "Yoav Goldberg",
                    "Ido Dagan"
                ],
                "title": "Improving quality and efficiency in plan-based neural data-to-text generation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 12th International Conference on Natural Language Generation",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_154",
            "content": "UNKNOWN, None, 2019, Step-by-step: Separating planning from realization in neural data-to-text generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Step-by-step: Separating planning from realization in neural data-to-text generation",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_155",
            "content": "Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Dart: Open-domain structured data record to text generation, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Linyong Nan",
                    "Dragomir Radev",
                    "Rui Zhang",
                    "Amrit Rau",
                    "Abhinand Sivaprasad",
                    "Chiachun Hsieh",
                    "Xiangru Tang",
                    "Aadit Vyas",
                    "Neha Verma",
                    "Pranav Krishna"
                ],
                "title": "Dart: Open-domain structured data record to text generation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_156",
            "content": "Shashi Narayan, Claire Gardent, Shay Cohen, Anastasia Shimorina, Split and rephrase, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "Shashi Narayan",
                    "Claire Gardent",
                    "Shay Cohen",
                    "Anastasia Shimorina"
                ],
                "title": "Split and rephrase",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_157",
            "content": "Jekaterina Novikova, Ondrej Du\u0161ek, Verena Rieser, The E2E Dataset: New Challenges for End-to-End Generation, 2017, Proceedings of the 18th Annual Meeting of the Special Interest Group on Discourse and Dialogue, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": [
                    "Jekaterina Novikova",
                    "Ondrej Du\u0161ek",
                    "Verena Rieser"
                ],
                "title": "The E2E Dataset: New Challenges for End-to-End Generation",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 18th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_158",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "pub_date": "2002",
                "pub_title": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "136-ARR_v1_159",
            "content": "Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, Dipanjan Das, Totto: A controlled table-to-text generation dataset, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": [
                    "Ankur Parikh",
                    "Xuezhi Wang",
                    "Sebastian Gehrmann",
                    "Manaal Faruqui",
                    "Bhuwan Dhingra",
                    "Diyi Yang",
                    "Dipanjan Das"
                ],
                "title": "Totto: A controlled table-to-text generation dataset",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_160",
            "content": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Pytorch: An imperative style, high-performance deep learning library, 2019, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b55",
                "authors": [
                    "Adam Paszke",
                    "Sam Gross",
                    "Francisco Massa",
                    "Adam Lerer",
                    "James Bradbury",
                    "Gregory Chanan",
                    "Trevor Killeen",
                    "Zeming Lin",
                    "Natalia Gimelshein",
                    "Luca Antiga"
                ],
                "title": "Pytorch: An imperative style, high-performance deep learning library",
                "pub_date": "2019",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_161",
            "content": "Ratish Puduppully, Li Dong, Mirella Lapata, Data-to-text generation with content selection and planning, 2019, Proceedings of the AAAI conference on artificial intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b56",
                "authors": [
                    "Ratish Puduppully",
                    "Li Dong",
                    "Mirella Lapata"
                ],
                "title": "Data-to-text generation with content selection and planning",
                "pub_date": "2019",
                "pub_title": "Proceedings of the AAAI conference on artificial intelligence",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_162",
            "content": "UNKNOWN, None, , Rossella Cancelliere, and Patrick Gallinari. 2021. Controlling hallucinations at word level in data-to-text generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b57",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Rossella Cancelliere, and Patrick Gallinari. 2021. Controlling hallucinations at word level in data-to-text generation",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_163",
            "content": "Ehud Reiter, Robert Dale, Building applied natural language generation systems, 1997, Natural Language Engineering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b58",
                "authors": [
                    "Ehud Reiter",
                    "Robert Dale"
                ],
                "title": "Building applied natural language generation systems",
                "pub_date": "1997",
                "pub_title": "Natural Language Engineering",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_164",
            "content": "UNKNOWN, None, , Hinrich Sch\u00fctze, and Iryna Gurevych. 2020. Investigating pretrained language models for graph-to-text generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b59",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Hinrich Sch\u00fctze, and Iryna Gurevych. 2020. Investigating pretrained language models for graph-to-text generation",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_165",
            "content": "Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, Xiaoyan Zhu, Long and diverse text generation with planning-based hierarchical variational model, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b60",
                "authors": [
                    "Zhihong Shao",
                    "Minlie Huang",
                    "Jiangtao Wen",
                    "Wenfei Xu",
                    "Xiaoyan Zhu"
                ],
                "title": "Long and diverse text generation with planning-based hierarchical variational model",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_166",
            "content": "Yixuan Su, Zaiqiao Meng, Simon Baker, Nigel Collier, Few-shot table-to-text generation with prototype memory, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b61",
                "authors": [
                    "Yixuan Su",
                    "Zaiqiao Meng",
                    "Simon Baker",
                    "Nigel Collier"
                ],
                "title": "Few-shot table-to-text generation with prototype memory",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_167",
            "content": "UNKNOWN, None, 2021, Plan-then-generate: Controlled data-to-text generation via planning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b62",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Plan-then-generate: Controlled data-to-text generation via planning",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_168",
            "content": "Bayu Trisedya, Jianzhong Qi, Rui Zhang, Sentence generation for entity description with content-plan attention, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b63",
                "authors": [
                    "Bayu Trisedya",
                    "Jianzhong Qi",
                    "Rui Zhang"
                ],
                "title": "Sentence generation for entity description with content-plan attention",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_169",
            "content": "Oriol Vinyals, Meire Fortunato, Navdeep Jaitly, Pointer networks, 2015, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b64",
                "authors": [
                    "Oriol Vinyals",
                    "Meire Fortunato",
                    "Navdeep Jaitly"
                ],
                "title": "Pointer networks",
                "pub_date": "2015",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_170",
            "content": "Tianming Wang, Xiaojun Wan, Hierarchical attention networks for sentence ordering, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b65",
                "authors": [
                    "Tianming Wang",
                    "Xiaojun Wan"
                ],
                "title": "Hierarchical attention networks for sentence ordering",
                "pub_date": "2019",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_171",
            "content": "Adina Williams, Nikita Nangia, Samuel Bowman, A broad-coverage challenge corpus for sentence understanding through inference, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b66",
                "authors": [
                    "Adina Williams",
                    "Nikita Nangia",
                    "Samuel Bowman"
                ],
                "title": "A broad-coverage challenge corpus for sentence understanding through inference",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "136-ARR_v1_172",
            "content": "Sam Wiseman, M Stuart, Alexander M Shieber,  Rush, Challenges in data-to-document generation, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b67",
                "authors": [
                    "Sam Wiseman",
                    "M Stuart",
                    "Alexander M Shieber",
                    " Rush"
                ],
                "title": "Challenges in data-to-document generation",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_173",
            "content": "Sam Wiseman, M Stuart, Alexander M Shieber,  Rush, Learning neural templates for text generation, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b68",
                "authors": [
                    "Sam Wiseman",
                    "M Stuart",
                    "Alexander M Shieber",
                    " Rush"
                ],
                "title": "Learning neural templates for text generation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_174",
            "content": "UNKNOWN, None, 2019, Huggingface's transformers: State-ofthe-art natural language processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b69",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Huggingface's transformers: State-ofthe-art natural language processing",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_175",
            "content": "UNKNOWN, None, , Verena Rieser, and Ioannis Konstas. 2021. Agggen: Ordering and aggregating while generating, .",
            "ntype": "ref",
            "meta": {
                "xid": "b70",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Verena Rieser, and Ioannis Konstas. 2021. Agggen: Ordering and aggregating while generating",
                "pub": null
            }
        },
        {
            "ix": "136-ARR_v1_176",
            "content": "Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu, Pegasus: Pre-training with extracted gap-sentences for abstractive summarization, 2020, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b71",
                "authors": [
                    "Jingqing Zhang",
                    "Yao Zhao",
                    "Mohammad Saleh",
                    "Peter Liu"
                ],
                "title": "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
                "pub_date": "2020",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "136-ARR_v1_177",
            "content": "Chao Zhao, Marilyn Walker, Snigdha Chaturvedi, Bridging the structural gap between encoding and decoding for data-to-text generation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b72",
                "authors": [
                    "Chao Zhao",
                    "Marilyn Walker",
                    "Snigdha Chaturvedi"
                ],
                "title": "Bridging the structural gap between encoding and decoding for data-to-text generation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "136-ARR_v1_0@0",
            "content": "Neural Pipeline for Zero-Shot Data-to-Text Generation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_0",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_2@0",
            "content": "In data-to-text (D2T) generation, training on in-domain data leads to overfitting to the data representation and repeating training data noise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_2",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_2@1",
            "content": "We examine how to avoid finetuning the pretrained language models (PLMs) on D2T generation datasets while still taking advantage of surface realization capabilities of PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_2",
            "start": 144,
            "end": 316,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_2@2",
            "content": "Inspired by pipeline approaches, we propose to generate text by rephrasing single-item templates using a sequence of modules trained on general-domain text-based operations-ordering, aggregation, and paragraph compression.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_2",
            "start": 318,
            "end": 539,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_2@3",
            "content": "We train PLMs for performing these operations on a synthetic corpus WIKIFLUENT which we build from English Wikipedia.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_2",
            "start": 541,
            "end": 657,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_2@4",
            "content": "Our experiments on two major triple-to-text datasets-WebNLG and E2Eshow that our approach enables D2T generation from RDF triples in zero-shot settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_2",
            "start": 659,
            "end": 810,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_2@5",
            "content": "1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_2",
            "start": 812,
            "end": 812,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_4@0",
            "content": "The aim of data-to-text (D2T) generation is to produce natural language descriptions of structured data (Gatt and Krahmer, 2018;Reiter and Dale, 1997).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_4",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_4@1",
            "content": "Although pipelines of rule-based D2T generation modules are still used in practice (Dale, 2020), end-to-end approaches based on PLMs recently showed superior benchmark performance (Ke et al., 2021;Chen et al., 2020a;Ferreira et al., 2020;Kale and Rastogi, 2020b;Ribeiro et al., 2020), surpassing pipeline systems (Ferreira et al., 2019) in both automatic and human evaluation metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_4",
            "start": 152,
            "end": 535,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_5@0",
            "content": "Finetuning PLMs on human-written references is widely accepted as a standard approach for adapting PLMs to the D2T generation objective and achieving good performance on a given benchmark (Agarwal et al., 2021;Ke et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_5",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_5@1",
            "content": "Nevertheless, this approach brings issues: Most obviously, finetuning the model for the domain-specific 1 The anonymized version of our code and data is available at https://anonymous.4open.science/r/ zeroshot-d2t-pipeline/. In-domain knowledge is included only in the simple hand-crafted templates for each predicate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_5",
            "start": 228,
            "end": 545,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_6@0",
            "content": "data distribution leads to overfitting on the particular benchmark, decreasing performance on outof-distribution data (Laha et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_6",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_6@1",
            "content": "Moreover, collecting a large set of references for a particular domain is costly and time-consuming, as the data are usually collected using crowdsourcing .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_6",
            "start": 139,
            "end": 294,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_6@2",
            "content": "Few-shot approaches are an alternative, requring only several tens or hundreds of annotated examples (Chen et al., 2020c;Ke et al., 2021;Su et al., 2021a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_6",
            "start": 296,
            "end": 450,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_6@3",
            "content": "However, robustness of these approaches is questionable-selecting a representative set of examples which would improve performance is difficult (Chang et al., 2021a), and the limited sample is often noisy, increasing the chance of hallucinations and omissions (Du\u0161ek et al., 2019;Harkous et al., 2020;Rebuffel et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_6",
            "start": 452,
            "end": 775,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_7@0",
            "content": "In this paper, we provide an alternative to this traditional paradigm by formulating the D2T generation from RDF triples as a sequence of generaldomain operations over text in natural language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_7",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_7@1",
            "content": "We start by transforming individual triples to text using trivial templates, which we subsequently or-der, aggregate, and compress on the paragraph level to produce the resulting description of the data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_7",
            "start": 194,
            "end": 396,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_7@2",
            "content": "All the pipeline modules operate over natural language text and are built upon PLMs trained on our WIKIFLUENT corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_7",
            "start": 398,
            "end": 514,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_7@3",
            "content": "WIKIFLUENT contains 934k examples of first paragraphs from the English Wikipedia, each supplied with a synthesized set of simple template-like sentences conveying the same meaning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_7",
            "start": 516,
            "end": 695,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_7@4",
            "content": "Our approach allows generating natural language descriptions from triples with a minimum amount of domain-specific rules or knowledge and without using training data from the D2T datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_7",
            "start": 697,
            "end": 884,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_7@5",
            "content": "We show that our approach can yield large improvements upon simple baselines and match older supervised systems in terms of fluency, while bringing potential for further improvements and advantages with respect to controllability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_7",
            "start": 886,
            "end": 1115,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_8@0",
            "content": "Our contributions are the following: (1) We propose an alternative D2T generation approach based on general-domain text-to-text operations (ordering, aggregation, and paragraph compression).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_8",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_8@1",
            "content": "( 2) We introduce a synthetic WIKIFLUENT corpus containing 934k sentences based on English Wikipedia, providing training data for the operations in (1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_8",
            "start": 191,
            "end": 342,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_8@2",
            "content": "(3) We apply our system on two D2T datasets and evaluate its performance both automatically and manually, including the contribution of individual pipeline modules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_8",
            "start": 344,
            "end": 507,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_8@3",
            "content": "(4) We release our code, data, pretrained models, and system outputs to ease future research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_8",
            "start": 509,
            "end": 601,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_9@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_9",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_10@0",
            "content": "D2T Generation with PLMs Large neural language models pretrained on self-supervised tasks (Lewis et al., 2020;Liu et al., 2019;Devlin et al., 2019) have recently gained a lot of traction in D2T generation research (Ferreira et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_10",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_10@1",
            "content": "Following Chen et al. (2020c), other works adopt PLMs for few-shot D2T generation (Chang et al., 2021b;Su et al., 2021a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_10",
            "start": 239,
            "end": 359,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_10@2",
            "content": "Kale and Rastogi (2020b) and Ribeiro et al. (2020) showed that PLMs using linearized representations of data can outperform graph neural networks on graph-to-text datasets, recently surpassed again by graph-based models (Ke et al., 2021;Chen et al., 2020a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_10",
            "start": 361,
            "end": 617,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_10@3",
            "content": "Although the models make use of general-domain pretraining tasks, all of them are eventually finetuned on domain-specific data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_10",
            "start": 619,
            "end": 745,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_11@0",
            "content": "Templates in Data-Driven D2T Generation Using simple handcrafted templates for individual keys or predicates is an efficient way of introducing domain knowledge while preventing text-to-text models from overfitting to a specific data format (Heidari et al., 2021;Kale and Rastogi, 2020a;. Transforming individual triples to text is also used in Laha et al. (2020) whose work is the most similar to ours.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_11",
            "start": 0,
            "end": 402,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_11@1",
            "content": "They also build a three-step pipeline for zero-shot D2T generation, but they use handcrafted rules for producing the output text and do not address content planning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_11",
            "start": 404,
            "end": 568,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_12@0",
            "content": "Content Planning in D2T Generation Content planning, i.e. ordering input facts and aggregating them into individual sentences, is a traditional part of the D2T generation pipeline (Ferreira et al., 2019;Gatt and Krahmer, 2018;Reiter and Dale, 1997).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_12",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_12@1",
            "content": "As previously demonstrated, using a content plan in neural D2T generation has important impact on the overall text quality (Moryossef et al., 2019a,b;Puduppully et al., 2019;Trisedya et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_12",
            "start": 250,
            "end": 446,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_12@2",
            "content": "Recently, have shown that using a content plan leads to improved quality of PLM outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_12",
            "start": 448,
            "end": 535,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_12@3",
            "content": "All the aforementioned models plan directly using predicates or keys in the D2T datasets representing the corresponding data item.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_12",
            "start": 537,
            "end": 666,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_12@4",
            "content": "Unlike these works, our planner is trained on ordering sentences in natural language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_12",
            "start": 668,
            "end": 752,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_13@0",
            "content": "Sentence Ordering Sentence ordering is the task of organizing a set of natural language sentences to increase the coherence of a text (Barzilay et al., 2001;Lapata, 2003).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_13",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_13@1",
            "content": "Several neural methods for this task were proposed, using either interactions between pairs of sentences Li and Jurafsky, 2017), global interactions (Gong et al., 2016;Wang and Wan, 2019), or combination of both (Cui et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_13",
            "start": 172,
            "end": 402,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_13@2",
            "content": "We base our ordering module ( \u00a75.1) on the recent work of Calizzano et al. (2021), who use a pointer network (Wang and Wan, 2019;Vinyals et al., 2015) on top of a PLM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_13",
            "start": 404,
            "end": 570,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_14@0",
            "content": "Fact Aggregation The compact nature of the target text description results in aggregating multiple facts in a single sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_14",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_14@1",
            "content": "Previous works (Wiseman et al., 2018;Shao et al., 2019;Shen et al., 2020;Xu et al., 2021) capture the segments which correspond to individual parts of the input as latent variables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_14",
            "start": 127,
            "end": 307,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_14@2",
            "content": "Unlike these works, we adopt a simpler scenario using an already ordered sequence of facts, in which we selectively insert delimiters marking sentence boundaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_14",
            "start": 309,
            "end": 470,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_14@3",
            "content": "Paragraph Compression We introduce paragraph compression as a new task in our D2T generation pipeline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_14",
            "start": 472,
            "end": 573,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_14@4",
            "content": "As the last step in the pipeline, it is closely related to linguistic realisation, howeversince we already work with natural language in this step-the focus of our task is on sentence fusion, rephrasing, and coreference resolution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_14",
            "start": 575,
            "end": 805,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_14@5",
            "content": "Unlike text summarization or simplification Jiang et al., 2020), we aim to convey the complete semantics of the text without omitting any facts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_14",
            "start": 807,
            "end": 950,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_14@6",
            "content": "In contrast to sentence fusion (Geva et al., 2019;Barzilay and McKeown, 2005) or sentence compression (Filippova and Altun, 2013), we operate in the context of multiple sentences in a paragraph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_14",
            "start": 952,
            "end": 1145,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_14@7",
            "content": "The task is the central focus of our WIKIFLUENT corpus ( \u00a74), which we synthesize using a model for the reverse task, split-andrephrase, i.e. splitting a complex sentence into simpler ones while preserving semantics (Botha et al., 2018;.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_14",
            "start": 1147,
            "end": 1383,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_15@0",
            "content": "Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_15",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_16@0",
            "content": "We first give an overview of our neural D2T generation pipeline ( \u00a73.1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_16",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_16@1",
            "content": "Next, we describe the individual steps, starting by applying simple templates to transform data to text ( \u00a73.2), followed by individual modules for ordering ( \u00a73.3), aggregation ( \u00a73.4), and paragraph compression ( \u00a73.5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_16",
            "start": 73,
            "end": 293,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_17@0",
            "content": "Method Overview",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_17",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_18@0",
            "content": "We focus on the task of producing a natural language description Y for a set of n RDF triples X \" tx 1 , . . . , x n u. Each triple x i \" ts i , p i , o i u consists of subject s i , predicate p i , and object o i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_18",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_19@0",
            "content": "We assume that we can transform each triple x i to a fact f i (where f i is a sentence in natural language describing x i ) by filling the single-triple template t p i P T for the predicate p i :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_19",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_20@0",
            "content": "t p i ps i , o i q \u00d1 f i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_20",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_21@0",
            "content": "We proceed as follows -given an input X, we:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_21",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_22@0",
            "content": "(1) apply the templates to transform the set of triples X to the set of facts: F \" T pXq \" tf 1 , . . . , f n u ( \u00a73.2), (2) sort the facts F using an ordering module which outputs an ordered sequence of facts F o \" OpF q \" tf o 1 , . . . , f on u ( \u00a73.3), (3) obtain sentence delimiters by inputting the ordered facts F o into an aggregation module ApF o q \" t\u03b4 o 1 , \u03b4 o 2 , . . . , \u03b4 o n\u00b41 u; \u03b4 i P t0, 1u, where \u03b4 o i \" 1 indicates the presence of a delimiter, i.e., that the sentences with facts f o i and f o i`1 should not be fused ( \u00a73.4), (4) input the ordered sequence with delimiters",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_22",
            "start": 0,
            "end": 593,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_23@0",
            "content": "F a \" tf o 1 , \u03b4 o 1 , f o 2 , . . . , \u03b4 o n\u00b41",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_23",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_24@0",
            "content": ", f on u into the paragraph compression module which generates the final description P pF a q \" Y ( \u00a73.5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_24",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_25@0",
            "content": "Templates",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_25",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_26@0",
            "content": "The first step in our pipeline involves transforming each of the input triples X into a set of facts F in natural language by using a template t p i for each predicate p i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_26",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_26@1",
            "content": "We need at least one template for each predicate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_26",
            "start": 174,
            "end": 222,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_26@2",
            "content": "Typically, the template will include placeholders which are filled with s i and o i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_26",
            "start": 224,
            "end": 308,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_27@0",
            "content": "The transformation serves two purposes: (a) preparing the data for the subsequent text-to-text operations, (b) introducing in-domain knowledge about the semantics of individual predicates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_27",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_27@1",
            "content": "Note that the filled templates are allowed to contain minor disfluencies since the text will be rephrased in the final step of the pipeline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_27",
            "start": 189,
            "end": 328,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_27@2",
            "content": "See \u00a75.5 for our approach to gathering the templates and Figure 2 for examples of the templates we use in our datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_27",
            "start": 330,
            "end": 448,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_28@0",
            "content": "We acknowledge that this step may be a bottleneck on datasets with an unconstrained (or very large) set of predicates, which is why we also discuss possibilities for automating this step in \u00a77.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_28",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_29@0",
            "content": "Ordering",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_29",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_30@0",
            "content": "We assume that the default order of triples X (and the respective facts F ) is random.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_30",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_30@1",
            "content": "To maximize the coherency of the resulting description, we apply an ordering model O to get an ordered sequence of facts: F o \" tf o 1 , . . . , f on u.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_30",
            "start": 87,
            "end": 238,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_30@2",
            "content": "The coherence of the final text will also depend on the paragraph compression step, but grouping related facts together (e.g. facts mentioning birth date and birth place) helps the paragraph compression model to focus only on fusing and rephrasing the neighboring sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_30",
            "start": 240,
            "end": 513,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_30@3",
            "content": "We describe our ordering model in \u00a75.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_30",
            "start": 515,
            "end": 553,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_31@0",
            "content": "Aggregation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_31",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_32@0",
            "content": "The aggregation model takes a sequence of ordered facts F o as input and produces a sequence of sentence delimiters ApF o q \" t\u03b4 o 1 , \u03b4 o 2 , . . . , \u03b4 o n\u00b41 u; \u03b4 i P t0, 1u. The output \u03b4 i \" 1 means that the neighboring facts are should be mentioned separately, serving as a hint for the paragraph compression model not to fuse the neighboring sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_32",
            "start": 0,
            "end": 355,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_32@1",
            "content": "Conversely, \u03b4 i \" 0 means that the facts should be aggregated and their corresponding sentences should be fused (see \u00a75.2 and \u00a75.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_32",
            "start": 357,
            "end": 488,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_33@0",
            "content": "Paragraph Compression",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_33",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_34@0",
            "content": "The paragraph compression model (see \u00a75.3 for simplified variants) takes as input the ordered sequence of facts with delimiters F a \" tf o 1 , \u03b4 o 1 , f o 2 , . . . , \u03b4 o n\u00b41 , f on u and produces a resulting text Y .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_34",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_34@1",
            "content": "The objectives of the model are two-fold: (a) fusing related sentences, i.e., sentences i and j in between which \u03b4 i \" 0, and (b) rephrasing the text to improve its fluency, e.g. fixing minor disfluencies in the templates, replacing noun phrases with refering expressions, etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_34",
            "start": 218,
            "end": 494,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_34@2",
            "content": "The focus is on minor rephrasing since the goal is to preserve the semantics of the original text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_34",
            "start": 496,
            "end": 593,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_35@0",
            "content": "WIKIFLUENT Corpus",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_35",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_36@0",
            "content": "A key to our approach is building a large-scale synthetic corpus providing training data for the text operations in our pipeline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_36",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_36@1",
            "content": "Our corpus needs to cover a broad range of domains while capturing the sentence style in D2T generation, both regarding the input templates and the target descriptions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_36",
            "start": 130,
            "end": 297,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_36@2",
            "content": "In other words, we aim to build a corpus in which:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_36",
            "start": 299,
            "end": 348,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_37@0",
            "content": "\u2022 the input is a set of simple, template-like sentences, \u2022 the output is a fluent text in natural language preserving the semantics of the input. As we describe below in detail, we achieve that by applying a split-and-rephrase model and a coreference resolution model on a set of human-written paragraphs in English Wikipedia. We consider the processed text as a source and the original text as the target. The process is illustrated in Figure 3; corpus statistics are included in Appendix A.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_37",
            "start": 0,
            "end": 491,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_38@0",
            "content": "Data Source",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_38",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_39@0",
            "content": "For building the WIKIFLUENT corpus, we extracted 934k first paragraphs of articles from a Wikipedia dump 2 using WikiExtractor (Attardi, 2 enwiki-20210401-pages-articles-multistream",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_39",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_40@0",
            "content": "The Westmeath Examiner is a weekly newspaper in Westmeath, Ireland.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_40",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_41@0",
            "content": "It is located in Westmeath, Ireland.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_41",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_42@0",
            "content": "The Westmeath Examiner is a weekly newspaper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_42",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_43@0",
            "content": "The Westmeath Examiner is a weekly newspaper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_43",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_44@0",
            "content": "It was founded in 1882.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_44",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_45@0",
            "content": "The Westmeath Examiner is located in Westmeath, Ireland.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_45",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_46@0",
            "content": "The Westmeath Examiner was founded in 1882. 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_46",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_46@1",
            "content": "The paragraphs contain mostly concise, factbased descriptions from a wide range of domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_46",
            "start": 51,
            "end": 141,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_46@2",
            "content": "We selected paragraphs with length between 30-430 characters, filtering out lists, disambiguations, repeated and malformed paragraphs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_46",
            "start": 143,
            "end": 276,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_46@3",
            "content": "To further ensure that the length of inputs is balanced, we selected 250k examples each from 4 equidistant length ranges (30-130 characters, etc.).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_46",
            "start": 278,
            "end": 424,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_47@0",
            "content": "Split-and-Rephrase",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_47",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_48@0",
            "content": "For generating the target set of sentences, we divide each paragraph into sentences using NLTK (Bird, 2006) and apply a split-and-rephrase model on each sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_48",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_48@1",
            "content": "Split-and-rephrase is a task of splitting a complex sentence into a meaning preserving sequence of shorter sentences .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_48",
            "start": 163,
            "end": 280,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_48@2",
            "content": "We train our model on the large-scale WikiSplit corpus by Botha et al. (2018), containing human-made sentence splits from Wikipedia edit history.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_48",
            "start": 282,
            "end": 426,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_48@3",
            "content": "Following the setup in the rest of our experiments, we train the encoder-decoder PLM BART-base (Lewis et al., 2020) on the WikiSplit dataset in a sequence-to-sequence setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_48",
            "start": 428,
            "end": 602,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_48@4",
            "content": "We apply the trained split-and-rephrase model on each sentence, uniformly randomly choosing between 0-2 recursive calls to ensure that the splits are not deterministic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_48",
            "start": 604,
            "end": 771,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_48@5",
            "content": "If the sentence cannot be meaningfully split, the model tends to duplicate the sentence on the output; in that case, we use only the original sentence and do not proceed with the splitting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_48",
            "start": 773,
            "end": 961,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_49@0",
            "content": "Coreference Replacement",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_49",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_50@0",
            "content": "Next, we concatenate the split sentences and apply a coreference resolution model (Gardner et al., 2018) in order to replace referring expressions with their antencendents (e.g., pronouns with noun phrases).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_50",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_50@1",
            "content": "This allows to better follow the style of the templates in which the entities are always fully verbalized.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_50",
            "start": 208,
            "end": 313,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_50@2",
            "content": "Since we keep the referring expressions in the original human-written text, we can train the paragraph compression module to generate them in the final text description.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_50",
            "start": 315,
            "end": 483,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_51@0",
            "content": "Filtering",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_51",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_52@0",
            "content": "To assert that the generated sentences convey the same semantics as the original paragraph, we use a pretrained RoBERTa model 3 (Liu et al., 2019) trained on the MultiNLI dataset (Williams et al., 2018) for checking the semantic accuracy of the generated text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_52",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_52@1",
            "content": "Following , we test if the original paragraph entails each of the synthesized sentences (checking for omissions), and if the set of concatenated synthesized sentences entails the original paragraph (checking for hallucinations).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_52",
            "start": 261,
            "end": 488,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_52@2",
            "content": "In a filtered version of the WIK-IFLUENT corpus, we include only the examples without omissions or hallucinations (as computed by the model), reducing it to approximately 3/4 of the original size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_52",
            "start": 490,
            "end": 685,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_53@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_53",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_54@0",
            "content": "We show how we build our pipeline ( \u00a75.1-5.4) and discuss the D2T generation datasets which we use for our experiments ( \u00a75.5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_54",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_54@1",
            "content": "The details of our training setup are included in Appendix B.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_54",
            "start": 128,
            "end": 188,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_55@0",
            "content": "Ordering Model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_55",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_56@0",
            "content": "For our ordering model (see \u00a73.3), we use the Simple Pointer model from Calizzano et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_56",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_56@1",
            "content": "The model is based on a pretrained BART-base extended with a pointer network from Wang and Wan (2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_56",
            "start": 97,
            "end": 198,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_56@2",
            "content": "We provide a short description of the model here; for details see Calizzano et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_56",
            "start": 200,
            "end": 289,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_57@0",
            "content": "In the encoding phase, facts F are concatenated and tokenized.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_57",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_57@1",
            "content": "Each fact is surrounded by special tokens denoting the beginning (<s>) and the end (</s>) of the fact.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_57",
            "start": 63,
            "end": 164,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_57@2",
            "content": "The sequence is processed by the BART encoder, generating a sequence of encoder states E for each end token </s> representing the preceding fact.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_57",
            "start": 166,
            "end": 310,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_58@0",
            "content": "The decoding proceeds autoregressively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_58",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_58@1",
            "content": "To bootstrap the decoding process, the pair of tokens <s></s> is fed into the decoder, producing the decoder state d 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_58",
            "start": 40,
            "end": 159,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_58@2",
            "content": "The pointer network (attending to d 1 and E), selects the first ordered fact f o 1 , which is fed into the decoder in the next step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_58",
            "start": 161,
            "end": 292,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_58@3",
            "content": "The process is repeated until the all the facts are decoded in a particular order.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_58",
            "start": 294,
            "end": 375,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_59@0",
            "content": "The pointer network computes the probability of a fact to be on the j-th position, using the encoder output E and the decoder output d j .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_59",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_59@1",
            "content": "The network is based on the scaled dot product attention, where d j is the query and encoder outputs E i are the keys:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_59",
            "start": 139,
            "end": 256,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_60@0",
            "content": "Q \" d j W Q K \" EW K P j \" softmax \u02c6QK T ? b \u02d9.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_60",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_61@0",
            "content": "Here W Q and W K P R b\u02c6b , b is the dimension of BART hidden states, and P j P R n`1 is the probability distribution for the j-th position (i.e., P ji is the probability that fact f i is on the j-th position).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_61",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_61@1",
            "content": "We train the model using the split sentences in the WIKIFLUENT corpus, randomly shuffling the order of the sentences and training the model to restore their original order.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_61",
            "start": 210,
            "end": 381,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_62@0",
            "content": "Aggregation Model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_62",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_63@0",
            "content": "We base our aggregation model (cf. \u00a73.4) on RoBERTa-large (Liu et al., 2019) with a token classification head.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_63",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_63@1",
            "content": "4 Similarly to the ordering model ( \u00a75.1), we input the sequence of facts F o into the model, separating each pair of facts f o i with a special token </s> (used by the model as a separator).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_63",
            "start": 111,
            "end": 301,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_63@2",
            "content": "Subsequently, the token classification layer classifies each separator </s> i position into two classes t0, 1u corresponding to the delimiter \u03b4 i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_63",
            "start": 303,
            "end": 449,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_63@3",
            "content": "We ignore the outputs for the non-separator tokens while computing the cross-entropy loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_63",
            "start": 451,
            "end": 540,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_64@0",
            "content": "We create the training examples using the split sentences in the WIKIFLUENT corpus, in which we set \u03b4 i \" 0 for the sentences i, i `1 which were originally aggregated (i.e., are the result of splitting a single sentence) and \u03b4 i \" 1 otherwise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_64",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_65@0",
            "content": "Paragraph Compression Model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_65",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_66@0",
            "content": "We adopt BART-base for our paragraph compression model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_66",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_66@1",
            "content": "We train the model in a sequenceto-sequence setting on the WIKIFLUENT corpus, concatenating the split sentences on the input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_66",
            "start": 56,
            "end": 180,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_66@2",
            "content": "We add delimiters between sentences i and i `1 where \u03b4 i \" 1 using a special token <sep>, which we add to the model vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_66",
            "start": 182,
            "end": 308,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_66@3",
            "content": "the model can steer the model towards producing certain outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_66",
            "start": 310,
            "end": 373,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_66@4",
            "content": "We evaluate our model's behavior with respect to ordering and aggregation in \u00a76.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_66",
            "start": 375,
            "end": 456,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_67@0",
            "content": "Ablation Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_67",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_68@0",
            "content": "In order to evaluate individual components of our pipeline, we train three versions of the PC model (see \u00a75.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_68",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_68@1",
            "content": "The models share the same architecture and targets, but differ in their inputs:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_68",
            "start": 112,
            "end": 190,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_69@0",
            "content": "\u2022 PC -the model takes as an input ordered facts with delimiters (as described in \u00a73.5), \u2022 PC+AGG -the model takes as an input ordered facts without delimiters (i.e., the aggregation is left implicitly to the model), \u2022 PC+ORD+AGG -the model takes as an input facts in random order and without delimiters (i.e., both ordering and aggregation are left implicitly to the model). Subsequently, we test three versions of the pipeline (see Figure 4): \u2022 3-STAGE -a full version of the pipeline consisting of the ordering model, the aggregation model and the PC model (following the full pipeline from \u00a73), \u2022 2-STAGE -a pipeline consisting of the ordering model and the PC+AGG model, \u2022 1-STAGE -a single stage consisting of the PC+ORD+AGG model. We evaluate all versions of the pipeline with PC models trained on the full and filtered versions of the WIKIFLUENT dataset (see \u00a74).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_69",
            "start": 0,
            "end": 869,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_70@0",
            "content": "D2T Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_70",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_71@0",
            "content": "We test our approach on two English D2T datasets, WebNLG and E2E.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_71",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_71@1",
            "content": "They differ in domain, size, textual style, and number of predicates (see Appendix A for details).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_71",
            "start": 66,
            "end": 163,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_72@0",
            "content": "WebNLG The WebNLG dataset contains RDF triples from DBPedia (Auer et al., 2007) and their crowdsourced descriptions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_72",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_72@1",
            "content": "The dataset was extended for the WebNLG+ Challenge (Ferreira et al., 2020), but we use the version 1.4 for comparability to prior work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_72",
            "start": 117,
            "end": 251,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_72@2",
            "content": "Templates for WebNLG could be extracted from the training data by delexicalizing single-triple examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_72",
            "start": 253,
            "end": 356,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_72@3",
            "content": "However, the examples are noisy and such data would not be available in a zero-shot setup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_72",
            "start": 358,
            "end": 447,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_72@4",
            "content": "Therefore, we handcrafted templates for all 354 predicates, including unseen predicates in the test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_72",
            "start": 449,
            "end": 552,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_72@5",
            "content": "5 E2E The E2E dataset (Novikova et al., 2017; contains restaurant recommendations in the form of attribute-value pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_72",
            "start": 554,
            "end": 672,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_72@6",
            "content": "We use the cleaned version of the dataset (Du\u0161ek et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_72",
            "start": 674,
            "end": 736,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_72@7",
            "content": "Following previous work, we transformed the attribute-value pairs into RDF triples (using the restaurant name as a subject) and then applied the same setup as for WebNLG.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_72",
            "start": 738,
            "end": 907,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_72@8",
            "content": "We created a template for each of the 8 attributes manually.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_72",
            "start": 909,
            "end": 968,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_73@0",
            "content": "Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_73",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_74@0",
            "content": "We evaluate outputs from the {1,2,3}-STAGE variants of our pipeline automatically ( \u00a76.1) and manually ( \u00a76.2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_74",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_74@1",
            "content": "Further, we evaluate the performance of the content planning modules and the ability of the PC module to follow the content plan ( \u00a76.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_74",
            "start": 112,
            "end": 248,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_75@0",
            "content": "Automatic Metrics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_75",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_76@0",
            "content": "Following prior work, we use BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) to evaluate the outputs against the human references.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_76",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_76@1",
            "content": "6 We also evaluate the number of omission and hallucination errors (i.e., facts missing or added, respectively) using a metric from based on a RoBERTa model (Liu et al., 2019) Wildwood is a restaurant.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_76",
            "start": 150,
            "end": 350,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_76@2",
            "content": "Wildwood serves French food.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_76",
            "start": 352,
            "end": 379,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_76@3",
            "content": "Wildwood is in the riverside.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_76",
            "start": 381,
            "end": 409,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_76@4",
            "content": "Wildwood is near Raja Indian Cuisine.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_76",
            "start": 411,
            "end": 447,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_77@0",
            "content": "Wildwood is a restaurant serving French food.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_77",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_77@1",
            "content": "It is in the riverside near Raja Indian Cuisine.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_77",
            "start": 46,
            "end": 93,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_77@2",
            "content": "Human A amazing French restaurant is called the Wildwood.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_77",
            "start": 95,
            "end": 151,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_77@3",
            "content": "The restaurant is near the Raja Indian Cuisine in riverside.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_77",
            "start": 153,
            "end": 212,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_78@0",
            "content": "They love kids.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_78",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_78@1",
            "content": "We include a diverse set of baselines for comparison.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_78",
            "start": 16,
            "end": 68,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_78@2",
            "content": "For WebNLG (see Table 2), we include the results of UPF-FORGe and MELBOURNE systems from the first run of WebNLG Challenge which are comparable in terms of automatic metrics and semantic errors, and the results of Ke et al. (2021), which is a state-of-theart system using structure-aware encoder and taskspecific pretraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_78",
            "start": 70,
            "end": 394,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_78@3",
            "content": "Laha et al. ( 2020) is (to our knowledge) the only other zero-shot D2T generation system applied on WebNLG.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_78",
            "start": 396,
            "end": 502,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_78@4",
            "content": "TGEN (Du\u0161ek and Jur\u010d\u00ed\u010dek, 2015) is the baseline system for the E2E Challenge and Harkous et al. (2020) is a state-of-the art supervised system applied on the cleaned E2E (see Table 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_78",
            "start": 504,
            "end": 687,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_78@5",
            "content": "For both datasets, COPY is the baseline of copying the templates verbatim.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_78",
            "start": 689,
            "end": 762,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_79@0",
            "content": "The automatic evaluation suggests that while our system lags behind state-of-the-art supervised systems, it shows considerable improvements compared to the COPY baseline (e.g., \"12 BLEU points pendix C for the details. for E2E) and matches performance of some older supervised systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_79",
            "start": 0,
            "end": 284,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_79@1",
            "content": "The COPY baseline is substantially better than the zero-shot system of Laha et al. (2020), suggesting that quality of the templates plays an important role.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_79",
            "start": 286,
            "end": 441,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_79@2",
            "content": "The 2-STAGE system is generally on par with the 3-STAGE system (or better), which indicates that implicit aggregation using the PC-AGG model may be sufficient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_79",
            "start": 443,
            "end": 601,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_79@3",
            "content": "However, an advantage of having a separate aggregation module is the possibility to control the aggregation step explicitly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_79",
            "start": 603,
            "end": 726,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_79@4",
            "content": "The filtered version of the dataset generally brings better results, although it brings also an increase in the number of omissions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_79",
            "start": 728,
            "end": 859,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_80@0",
            "content": "Manual Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_80",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_81@0",
            "content": "We manually evaluated 100 outputs of the models regarding factual errors (hallucinations, omissions, incorrect fact merging, redundancies) as well as grammatical errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_81",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_81@1",
            "content": "The results are listed in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_81",
            "start": 170,
            "end": 203,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_81@2",
            "content": "The 1-STAGE model (which has to order the facts implicitly) tends to repeat the facts in the text (especially in E2E) and produces frequent hallucinations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_81",
            "start": 205,
            "end": 359,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_81@3",
            "content": "These problems are only slightly reduced in the filtered version, but they are largely eliminated with 2-STAGE and 3-STAGE models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_81",
            "start": 361,
            "end": 490,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_81@4",
            "content": "We note these models create almost no hallucinations or omissions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_81",
            "start": 492,
            "end": 557,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_81@5",
            "content": "However, the outputs on WebNLG for all systems suffer from semantic errors resulting from merging of unrelated facts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_81",
            "start": 559,
            "end": 675,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_81@6",
            "content": "This mostly happens with unrelated predicates connected to the same subject/object (e.g. \"X was born in Y\", \"X worked as Z\" expressed as \"X worked as Z in Y\"; see Appendix D for examples).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_81",
            "start": 677,
            "end": 864,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_81@7",
            "content": "On the E2E data, which has a simpler triple structure (all predicates share the same subject), the outputs are generally consistent and the 2-STAGE and 3-STAGE models exhibit almost no semantic errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_81",
            "start": 866,
            "end": 1066,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_81@8",
            "content": "As we discuss in \u00a77, more research is needed for ensuring the final consistency of the text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_81",
            "start": 1068,
            "end": 1159,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_81@9",
            "content": "The grammar errors and disfluencies stem mainly from over-eager paragraph compression or from artifacts in our templates; they are relatively minor (e.g., missing \"is\" in \"serves French food and family-friendly\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_81",
            "start": 1161,
            "end": 1373,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_82@0",
            "content": "WebNLG E2E H I O R G H I O R G full 3-",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_82",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_83@0",
            "content": "Content Planning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_83",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_84@0",
            "content": "Following and , we report the accuracy (Acc) and BLEU-2 score (B-2) of our ordering model on WebNLG against the human-generated plans from Ferreira et al. (2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_84",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_85@0",
            "content": "The results are listed in Table 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_85",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_85@1",
            "content": "RANDOM is the baseline of generating a random order.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_85",
            "start": 35,
            "end": 86,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_85@2",
            "content": "The results show that although our approach lacks behind stateof-the-art supervised approaches, it can outperform both the random baseline and the Transformerbased approach from Ferreira et al. (2019) while not using any training examples from WebNLG.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_85",
            "start": 88,
            "end": 338,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_85@3",
            "content": "We also evaluate the accuracy of our aggregation model, using triples ordered according to the plans from Ferreira et al. (2018) as input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_85",
            "start": 340,
            "end": 477,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_85@4",
            "content": "The accuracy is 0.33 per example and 0.62 per sentence boundary (random baseline is 0.23 and 0.50, respectively).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_85",
            "start": 479,
            "end": 591,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_85@5",
            "content": "The results show that although our approach is better than the random baseline, further",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_85",
            "start": 593,
            "end": 679,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_86@0",
            "content": "Transformer (Ferreira et al., 2019) : 52.20 0.35 Step-by-step (Moryossef et al., 2019b) : 70.80 0.47 PLANENC 80.10 0.62 Plan-then-generate 84.97 0.72 RANDOM 47.00 0.29",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_86",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_87@0",
            "content": "BART+ptr (Calizzano et al., 2021) 59.10 0.48",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_87",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_88@0",
            "content": "Table 5: Evaluation of our zero-shot ordering model based on Calizzano et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_88",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_88@1",
            "content": "The results marked with : are copied from the respective papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_88",
            "start": 86,
            "end": 149,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_89@0",
            "content": "investigation regarding plausible fact aggregation schemes is needed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_89",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_89@1",
            "content": "Finally, we manually evaluate how the PC model follows the content plan using 100 randomly chosen examples with more than 1 triple on WebNLG and E2E.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_89",
            "start": 70,
            "end": 218,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_89@2",
            "content": "We find that the model follows the content plan in 95% and 100% of cases, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_89",
            "start": 220,
            "end": 306,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_89@3",
            "content": "The incorrect cases include a fact not properly mentioned and an extra boundary between the sentences without a separator.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_89",
            "start": 308,
            "end": 429,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_89@4",
            "content": "We can thus conclude that the pretraining task successfully teaches the PC model to follow a given content plan.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_89",
            "start": 431,
            "end": 542,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_90@0",
            "content": "Discussion and Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_90",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_91@0",
            "content": "In the current form, our pipeline can be directly applied to generating text from RDF triples (or similarly structured data) which require no extra processing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_91",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_91@1",
            "content": "Further extensions are needed for more complex D2T scenarios, e.g. datasets requiring content selection or common-sense and logical reasoning (Wiseman et al., 2017;Chen et al., 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_91",
            "start": 160,
            "end": 343,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_92@0",
            "content": "Our approach regarding handcrafting a single template for each predicate is quite basic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_92",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_92@1",
            "content": "Generating simple statements from the triples automatically, e.g., using the approach of Laha et al. (2020), could reduce the manual workload and allow applying our approach on datasets with a less constrained set of data attributes such as ToTTo (Parikh et al., 2020) or DART (Nan et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_92",
            "start": 89,
            "end": 384,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_92@2",
            "content": "Moreover, explicitly including a denoising task for the paragraph compression model could help to tackle the disfluencies in the templates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_92",
            "start": 386,
            "end": 524,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_93@0",
            "content": "More research is also needed on semantic errors stemming from merging of facts in improper ways.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_93",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_93@1",
            "content": "We suggest that explicitly controlling the semantics of sentence fusion (Ben-David et al., 2020) could help to mitigate this issue, while still keeping the advantages of a zero-shot approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_93",
            "start": 97,
            "end": 287,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_94@0",
            "content": "Statistics for the datasets described in the paper are listed in Table 7.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_94",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_95@0",
            "content": "We implemented the models for split-and-rephrase, aggregation, and paragraph compression in Py-Torch Lightning (Paszke et al., 2019), using the PyTorch (Falcon, 2019) version of the BART and RoBERTa models from the Huggingface library (Wolf et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_95",
            "start": 0,
            "end": 254,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_96@0",
            "content": "We use the Adam (Kingma and Ba, 2015) optimizer (\u03b2 1 \" 0.9, \u03b2 2 \" 0.997, \u03b5 \" 1 \u00b49) with learning rate 2 \u00b45, linear scheduling and 0.1 warmup proportion; batches of size 8 and accumulating gradients with factor 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_96",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_96@1",
            "content": "We train the models for 1 epoch on a single GeForce RTX 3090 GPU with 24 GB RAM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_96",
            "start": 213,
            "end": 292,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_96@2",
            "content": "We use greedy decoding in all our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_96",
            "start": 294,
            "end": 339,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_97@0",
            "content": "For training the ordering model, we used the implementation from Calizzano et al. (2021) 8 including their training parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_97",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_97@1",
            "content": "We plan to fully integrate the ordering model into our framework in the future.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_97",
            "start": 128,
            "end": 206,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_98@0",
            "content": "We provide evaluation of semantic accuracy on the E2E dataset as evaluated with the slot-error script based on matching regular expressions in Table 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_98",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_98@1",
            "content": "9 Note that our manual investigation of a sample of the data shows that the majority of the errors identified in our model outputs are false.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_98",
            "start": 152,
            "end": 292,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_98@2",
            "content": "For example, the following regular expression used in the slot-error script: prices?(?: range)?(?:w+)0,3 high matches \"(...) price range and high customer rating (. outputs of our models, which tend to repeat certain patterns. However, we also manually identified several cases in which an error was found correctly, e.g. the model hallucinating \"3 out of 4 customer rating\" instead of \"3 out of 5 customer rating\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_98",
            "start": 294,
            "end": 708,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_99@0",
            "content": "Tables 8, 9, 10, and 11 show examples of behavior of our models on the WebNLG dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_99",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_99@1",
            "content": "Tables 12 and 13 show examples of behavior of our models on the E2E dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_99",
            "start": 87,
            "end": 162,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_100@0",
            "content": "The green color marks the model outputs which are completely correct, the red color marks the errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_100",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_100@1",
            "content": "For better readability of the input format, we add numeric order identifiers for the individual facts (bold, in squared brackets).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_100",
            "start": 102,
            "end": 231,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_100@2",
            "content": "These are subsequently used as references in the Order and Aggregation rows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_100",
            "start": 233,
            "end": 308,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_100@3",
            "content": "Note that zero-th input in E2E is used as a subject in the RDF triples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_100",
            "start": 310,
            "end": 380,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_101@0",
            "content": "Akron, Ohio is from United States. English language is spoken in Akron, Ohio.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_101",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_102@0",
            "content": "English language is spoken in the United States.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_102",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_102@1",
            "content": "Akron, Ohio is from the state of Ohio in the U.S. state of Illinois.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_102",
            "start": 49,
            "end": 116,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_102@2",
            "content": "3-stage filtered Akron, Ohio is from United States.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_102",
            "start": 118,
            "end": 168,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_102@3",
            "content": "English language is spoken in the United States of America.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_102",
            "start": 170,
            "end": 228,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_102@4",
            "content": "2-stage filtered Akron, Ohio is from United States.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_102",
            "start": 230,
            "end": 280,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_102@5",
            "content": "English language is spoken in Akron, Ohio.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_102",
            "start": 282,
            "end": 323,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_102@6",
            "content": "1-stage filtered English language is spoken in the United States.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_102",
            "start": 325,
            "end": 389,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_102@7",
            "content": "Akron, Ohio is from the United Kingdom.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_102",
            "start": 391,
            "end": 429,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_103@0",
            "content": "The country of Akron, Ohio, is United States, where English is the language spoken.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_103",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_104@0",
            "content": "Table 9: Incorrect behavior on WebNLG: besides the minor disfluencies caused by the templates (\"Akron, Ohio is from...\"), the models (except for 3-STAGE filtered) tend to hallucinate and merge the facts incorrectly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_104",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_105@0",
            "content": "Oshin Agarwal, Heming Ge, Siamak Shakeri, Rami Al-Rfou, Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_105",
            "start": 0,
            "end": 308,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_106@0",
            "content": "UNKNOWN, None, 2015, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_106",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_107@0",
            "content": "S\u00f6ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, Dbpedia: A nucleus for a web of open data, 2007, The semantic web, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_107",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_108@0",
            "content": "Satanjeev Banerjee, Alon Lavie, METEOR: An automatic metric for MT evaluation with improved correlation with human judgments, 2005, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_108",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_109@0",
            "content": "Regina Barzilay, Noemie Elhadad, Kathleen Mckeown, Sentence ordering in multidocument summarization, 2001, Proceedings of the first international conference on Human language technology research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_109",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_110@0",
            "content": "Regina Barzilay,  Kathleen R Mckeown, Sentence fusion for multidocument news summarization, 2005, Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_110",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_111@0",
            "content": "Eyal Ben-David, Orgad Keller, Eric Malmi, Idan Szpektor, and Roi Reichart. 2020. Semantically driven sentence fusion: Modeling and evaluation, , Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_111",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_112@0",
            "content": "Steven Bird, Nltk: the natural language toolkit, 2006, Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_112",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_113@0",
            "content": "Jan Botha, Manaal Faruqui, John Alex, Jason Baldridge, Dipanjan Das, Learning to split and rephrase from Wikipedia edit history, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_113",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_114@0",
            "content": "R\u00e9mi Calizzano, Malte Ostendorff, Georg Rehm, Ordering sentences and paragraphs with pretrained encoder-decoder transformers and pointer ensembles, 2021, Proceedings of the 21st ACM Symposium on Document Engineering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_114",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_115@0",
            "content": "UNKNOWN, None, 2021, On training instance selection for few-shot neural text generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_115",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_116@0",
            "content": "Ernie Chang, Xiaoyu Shen, Dawei Zhu, Vera Demberg, Hui Su, Neural data-to-text generation with lm-based text augmentation, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_116",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_117@0",
            "content": "Wenhu Chen, Yu Su, Xifeng Yan, William Wang, KGPT: Knowledge-grounded pretraining for data-to-text generation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_117",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_118@0",
            "content": "UNKNOWN, None, 2016, Neural sentence ordering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_118",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_119@0",
            "content": "Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam Sundaresan, William Wang, Logic2text: High-fidelity natural language generation from logical forms, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_119",
            "start": 0,
            "end": 273,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_120@0",
            "content": "Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, William Wang, Few-shot NLG with pre-trained language model, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_120",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_121@0",
            "content": "Baiyun Cui, Yingming Li, Zhongfei Zhang, Bert-enhanced relational sentence ordering network, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_121",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_122@0",
            "content": "Robert Dale, Natural language generation: The commercial state of the art in 2020, 2020, Natural Language Engineering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_122",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_123@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_123",
            "start": 0,
            "end": 294,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_124@0",
            "content": "Ond\u0159ej Du\u0161ek, M David, Verena Howcroft,  Rieser, Semantic noise matters for neural natural language generation, 2019, Proceedings of the 12th International Conference on Natural Language Generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_124",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_125@0",
            "content": "Ond\u0159ej Du\u0161ek, Filip Jur\u010d\u00ed\u010dek, Training a natural language generator from unaligned data, 2015, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_125",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_126@0",
            "content": "Ond\u0159ej Du\u0161ek, Zden\u011bk Kasner, Evaluating semantic accuracy of data-to-text generation with natural language inference, 2020, Proceedings of the 13th International Conference on Natural Language Generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_126",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_127@0",
            "content": "Ond\u0159ej Du\u0161ek, Jekaterina Novikova, Verena Rieser, Evaluating the state-of-the-art of end-to-end natural language generation: The e2e nlg challenge, 2020, Computer Speech & Language, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_127",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_128@0",
            "content": "UNKNOWN, None, 2019, Pytorch lightning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_128",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_129@0",
            "content": "Thiago Ferreira, Claire Gardent, Nikolai Ilinykh, Chris Van Der Lee, Simon Mille, Diego Moussallem, Anastasia Shimorina, The 2020 bilingual, bidirectional webnlg+ shared task overview and evaluation results (webnlg+ 2020), 2020, Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_129",
            "start": 0,
            "end": 339,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_130@0",
            "content": "Diego Thiago Castro Ferreira, Emiel Moussallem, Sander Krahmer,  Wubben, Enriching the webnlg corpus, 2018, Proceedings of the 11th International Conference on Natural Language Generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_130",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_131@0",
            "content": "Chris Thiago Castro Ferreira,  Van Der Lee, Emiel Emiel Van Miltenburg,  Krahmer, Neural datato-text generation: A comparison between pipeline and end-to-end architectures, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_131",
            "start": 0,
            "end": 356,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_132@0",
            "content": "Katja Filippova, Yasemin Altun, Overcoming the lack of parallel data in sentence compression, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_132",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_133@0",
            "content": "Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, The WebNLG challenge: Generating text from RDF data, 2017, Proceedings of the 10th International Conference on Natural Language Generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_133",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_134@0",
            "content": "Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, F Nelson, Matthew Liu, Michael Peters, Luke Schmitz,  Zettlemoyer, Allennlp: A deep semantic natural language processing platform, 2018, Proceedings of Workshop for NLP Open Source Software (NLP-OSS), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_134",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_135@0",
            "content": "Albert Gatt, Emiel Krahmer, Survey of the state of the art in natural language generation: Core tasks, applications and evaluation, 2018, Journal of Artificial Intelligence Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_135",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_136@0",
            "content": "Mor Geva, Eric Malmi, Idan Szpektor, Jonathan Berant, Discofuse: A large-scale dataset for discourse-based sentence fusion, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_136",
            "start": 0,
            "end": 274,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_137@0",
            "content": "UNKNOWN, None, 2016, End-to-end neural sentence ordering using pointer network, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_137",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_138@0",
            "content": "Hamza Harkous, Isabel Groves, Amir Saffari, Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_138",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_139@0",
            "content": "Peyman Heidari, Arash Einolghozati, Shashank Jain, Soumya Batra, Lee Callender, Ankit Arun, Shawn Mei, Sonal Gupta, Pinar Donmez, Vikas Bhardwaj, Getting to production with few-shot natural language generation models, 2021, Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_139",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_140@0",
            "content": "Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, Wei Xu, Neural crf model for sentence alignment in text simplification, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_140",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_141@0",
            "content": "Mihir Kale, Abhinav Rastogi, Template guided text generation for task-oriented dialogue, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_141",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_142@0",
            "content": "Mihir Kale, Abhinav Rastogi, Text-to-text pre-training for data-to-text tasks, 2020, Proceedings of the 13th International Conference on Natural Language Generation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_142",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_143@0",
            "content": "Zden\u011bk Kasner, Ond\u0159ej Du\u0161ek, Data-to-text generation with iterative text editing, 2020, Proceedings of the 13th International Conference on Natural Language Generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_143",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_144@0",
            "content": "UNKNOWN, None, 2021, Jointgt: Graph-text joint representation learning for text generation from knowledge graphs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_144",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_145@0",
            "content": "UNKNOWN, None, 2019, Ctrl: A conditional transformer language model for controllable generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_145",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_146@0",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A method for stochastic optimization, 2015-05-07, 3rd International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_146",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_147@0",
            "content": "Anirban Laha, Parag Jain, Abhijit Mishra, Karthik Sankaranarayanan, Scalable micro-planned generation of discourse from structured data, 2020, Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_147",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_148@0",
            "content": "Mirella Lapata, Probabilistic text structuring: Experiments with sentence ordering, 2003, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_148",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_149@0",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_149",
            "start": 0,
            "end": 331,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_150@0",
            "content": "Jiwei Li, Dan Jurafsky, Neural net models of open-domain discourse coherence, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_150",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_151@0",
            "content": "UNKNOWN, None, 2019, Commongen: A constrained text generation dataset towards generative commonsense reasoning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_151",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_152@0",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_152",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_153@0",
            "content": "Amit Moryossef, Yoav Goldberg, Ido Dagan, Improving quality and efficiency in plan-based neural data-to-text generation, 2019, Proceedings of the 12th International Conference on Natural Language Generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_153",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_154@0",
            "content": "UNKNOWN, None, 2019, Step-by-step: Separating planning from realization in neural data-to-text generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_154",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_155@0",
            "content": "Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Dart: Open-domain structured data record to text generation, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_155",
            "start": 0,
            "end": 353,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_156@0",
            "content": "Shashi Narayan, Claire Gardent, Shay Cohen, Anastasia Shimorina, Split and rephrase, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_156",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_157@0",
            "content": "Jekaterina Novikova, Ondrej Du\u0161ek, Verena Rieser, The E2E Dataset: New Challenges for End-to-End Generation, 2017, Proceedings of the 18th Annual Meeting of the Special Interest Group on Discourse and Dialogue, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_157",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_158@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_158",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_159@0",
            "content": "Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, Dipanjan Das, Totto: A controlled table-to-text generation dataset, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_159",
            "start": 0,
            "end": 260,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_160@0",
            "content": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Pytorch: An imperative style, high-performance deep learning library, 2019, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_160",
            "start": 0,
            "end": 273,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_161@0",
            "content": "Ratish Puduppully, Li Dong, Mirella Lapata, Data-to-text generation with content selection and planning, 2019, Proceedings of the AAAI conference on artificial intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_161",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_162@0",
            "content": "UNKNOWN, None, , Rossella Cancelliere, and Patrick Gallinari. 2021. Controlling hallucinations at word level in data-to-text generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_162",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_163@0",
            "content": "Ehud Reiter, Robert Dale, Building applied natural language generation systems, 1997, Natural Language Engineering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_163",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_164@0",
            "content": "UNKNOWN, None, , Hinrich Sch\u00fctze, and Iryna Gurevych. 2020. Investigating pretrained language models for graph-to-text generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_164",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_165@0",
            "content": "Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, Xiaoyan Zhu, Long and diverse text generation with planning-based hierarchical variational model, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_165",
            "start": 0,
            "end": 334,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_166@0",
            "content": "Yixuan Su, Zaiqiao Meng, Simon Baker, Nigel Collier, Few-shot table-to-text generation with prototype memory, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_166",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_167@0",
            "content": "UNKNOWN, None, 2021, Plan-then-generate: Controlled data-to-text generation via planning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_167",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_168@0",
            "content": "Bayu Trisedya, Jianzhong Qi, Rui Zhang, Sentence generation for entity description with content-plan attention, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_168",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_169@0",
            "content": "Oriol Vinyals, Meire Fortunato, Navdeep Jaitly, Pointer networks, 2015, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_169",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_170@0",
            "content": "Tianming Wang, Xiaojun Wan, Hierarchical attention networks for sentence ordering, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_170",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_171@0",
            "content": "Adina Williams, Nikita Nangia, Samuel Bowman, A broad-coverage challenge corpus for sentence understanding through inference, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_171",
            "start": 0,
            "end": 287,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_172@0",
            "content": "Sam Wiseman, M Stuart, Alexander M Shieber,  Rush, Challenges in data-to-document generation, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_172",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_173@0",
            "content": "Sam Wiseman, M Stuart, Alexander M Shieber,  Rush, Learning neural templates for text generation, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_173",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_174@0",
            "content": "UNKNOWN, None, 2019, Huggingface's transformers: State-ofthe-art natural language processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_174",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_175@0",
            "content": "UNKNOWN, None, , Verena Rieser, and Ioannis Konstas. 2021. Agggen: Ordering and aggregating while generating, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_175",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_176@0",
            "content": "Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu, Pegasus: Pre-training with extracted gap-sentences for abstractive summarization, 2020, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_176",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "136-ARR_v1_177@0",
            "content": "Chao Zhao, Marilyn Walker, Snigdha Chaturvedi, Bridging the structural gap between encoding and decoding for data-to-text generation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "136-ARR_v1_177",
            "start": 0,
            "end": 229,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "136-ARR_v1_0",
            "tgt_ix": "136-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_0",
            "tgt_ix": "136-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_1",
            "tgt_ix": "136-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_1",
            "tgt_ix": "136-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_0",
            "tgt_ix": "136-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_2",
            "tgt_ix": "136-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_4",
            "tgt_ix": "136-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_5",
            "tgt_ix": "136-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_6",
            "tgt_ix": "136-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_7",
            "tgt_ix": "136-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_3",
            "tgt_ix": "136-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_3",
            "tgt_ix": "136-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_3",
            "tgt_ix": "136-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_3",
            "tgt_ix": "136-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_3",
            "tgt_ix": "136-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_3",
            "tgt_ix": "136-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_0",
            "tgt_ix": "136-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_8",
            "tgt_ix": "136-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_10",
            "tgt_ix": "136-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_11",
            "tgt_ix": "136-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_12",
            "tgt_ix": "136-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_13",
            "tgt_ix": "136-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_9",
            "tgt_ix": "136-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_9",
            "tgt_ix": "136-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_9",
            "tgt_ix": "136-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_9",
            "tgt_ix": "136-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_9",
            "tgt_ix": "136-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_9",
            "tgt_ix": "136-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_0",
            "tgt_ix": "136-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_14",
            "tgt_ix": "136-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_15",
            "tgt_ix": "136-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_15",
            "tgt_ix": "136-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_15",
            "tgt_ix": "136-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_16",
            "tgt_ix": "136-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_18",
            "tgt_ix": "136-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_19",
            "tgt_ix": "136-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_20",
            "tgt_ix": "136-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_21",
            "tgt_ix": "136-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_22",
            "tgt_ix": "136-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_23",
            "tgt_ix": "136-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_17",
            "tgt_ix": "136-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_17",
            "tgt_ix": "136-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_17",
            "tgt_ix": "136-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_17",
            "tgt_ix": "136-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_17",
            "tgt_ix": "136-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_17",
            "tgt_ix": "136-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_17",
            "tgt_ix": "136-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_17",
            "tgt_ix": "136-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_15",
            "tgt_ix": "136-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_24",
            "tgt_ix": "136-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_26",
            "tgt_ix": "136-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_27",
            "tgt_ix": "136-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_25",
            "tgt_ix": "136-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_25",
            "tgt_ix": "136-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_25",
            "tgt_ix": "136-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_25",
            "tgt_ix": "136-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_15",
            "tgt_ix": "136-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_28",
            "tgt_ix": "136-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_29",
            "tgt_ix": "136-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_29",
            "tgt_ix": "136-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_15",
            "tgt_ix": "136-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_30",
            "tgt_ix": "136-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_31",
            "tgt_ix": "136-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_31",
            "tgt_ix": "136-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_15",
            "tgt_ix": "136-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_32",
            "tgt_ix": "136-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_33",
            "tgt_ix": "136-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_33",
            "tgt_ix": "136-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_0",
            "tgt_ix": "136-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_34",
            "tgt_ix": "136-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_36",
            "tgt_ix": "136-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_35",
            "tgt_ix": "136-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_35",
            "tgt_ix": "136-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_35",
            "tgt_ix": "136-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_35",
            "tgt_ix": "136-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_39",
            "tgt_ix": "136-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_40",
            "tgt_ix": "136-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_41",
            "tgt_ix": "136-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_38",
            "tgt_ix": "136-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_38",
            "tgt_ix": "136-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_38",
            "tgt_ix": "136-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_38",
            "tgt_ix": "136-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_38",
            "tgt_ix": "136-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_38",
            "tgt_ix": "136-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_42",
            "tgt_ix": "136-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_38",
            "tgt_ix": "136-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_43",
            "tgt_ix": "136-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_45",
            "tgt_ix": "136-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_38",
            "tgt_ix": "136-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_38",
            "tgt_ix": "136-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_44",
            "tgt_ix": "136-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_35",
            "tgt_ix": "136-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_46",
            "tgt_ix": "136-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_47",
            "tgt_ix": "136-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_47",
            "tgt_ix": "136-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_35",
            "tgt_ix": "136-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_48",
            "tgt_ix": "136-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_49",
            "tgt_ix": "136-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_49",
            "tgt_ix": "136-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_35",
            "tgt_ix": "136-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_50",
            "tgt_ix": "136-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_51",
            "tgt_ix": "136-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_51",
            "tgt_ix": "136-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_0",
            "tgt_ix": "136-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_52",
            "tgt_ix": "136-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_53",
            "tgt_ix": "136-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_53",
            "tgt_ix": "136-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_53",
            "tgt_ix": "136-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_54",
            "tgt_ix": "136-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_56",
            "tgt_ix": "136-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_57",
            "tgt_ix": "136-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_58",
            "tgt_ix": "136-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_59",
            "tgt_ix": "136-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_60",
            "tgt_ix": "136-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_55",
            "tgt_ix": "136-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_55",
            "tgt_ix": "136-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_55",
            "tgt_ix": "136-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_55",
            "tgt_ix": "136-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_55",
            "tgt_ix": "136-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_55",
            "tgt_ix": "136-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_55",
            "tgt_ix": "136-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_53",
            "tgt_ix": "136-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_61",
            "tgt_ix": "136-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_63",
            "tgt_ix": "136-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_62",
            "tgt_ix": "136-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_62",
            "tgt_ix": "136-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_62",
            "tgt_ix": "136-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_53",
            "tgt_ix": "136-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_64",
            "tgt_ix": "136-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_65",
            "tgt_ix": "136-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_65",
            "tgt_ix": "136-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_53",
            "tgt_ix": "136-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_66",
            "tgt_ix": "136-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_68",
            "tgt_ix": "136-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_67",
            "tgt_ix": "136-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_67",
            "tgt_ix": "136-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_67",
            "tgt_ix": "136-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_53",
            "tgt_ix": "136-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_71",
            "tgt_ix": "136-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_70",
            "tgt_ix": "136-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_70",
            "tgt_ix": "136-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_70",
            "tgt_ix": "136-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_0",
            "tgt_ix": "136-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_72",
            "tgt_ix": "136-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_73",
            "tgt_ix": "136-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_73",
            "tgt_ix": "136-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_73",
            "tgt_ix": "136-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_74",
            "tgt_ix": "136-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_75",
            "tgt_ix": "136-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_75",
            "tgt_ix": "136-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_77",
            "tgt_ix": "136-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_78",
            "tgt_ix": "136-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_75",
            "tgt_ix": "136-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_75",
            "tgt_ix": "136-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_75",
            "tgt_ix": "136-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_76",
            "tgt_ix": "136-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_73",
            "tgt_ix": "136-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_79",
            "tgt_ix": "136-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_81",
            "tgt_ix": "136-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_80",
            "tgt_ix": "136-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_80",
            "tgt_ix": "136-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_80",
            "tgt_ix": "136-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_73",
            "tgt_ix": "136-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_82",
            "tgt_ix": "136-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_84",
            "tgt_ix": "136-ARR_v1_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_83",
            "tgt_ix": "136-ARR_v1_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_83",
            "tgt_ix": "136-ARR_v1_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_83",
            "tgt_ix": "136-ARR_v1_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_86",
            "tgt_ix": "136-ARR_v1_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_87",
            "tgt_ix": "136-ARR_v1_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_88",
            "tgt_ix": "136-ARR_v1_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_83",
            "tgt_ix": "136-ARR_v1_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_83",
            "tgt_ix": "136-ARR_v1_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_83",
            "tgt_ix": "136-ARR_v1_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_83",
            "tgt_ix": "136-ARR_v1_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_85",
            "tgt_ix": "136-ARR_v1_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_0",
            "tgt_ix": "136-ARR_v1_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_89",
            "tgt_ix": "136-ARR_v1_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_91",
            "tgt_ix": "136-ARR_v1_92",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_92",
            "tgt_ix": "136-ARR_v1_93",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_90",
            "tgt_ix": "136-ARR_v1_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_90",
            "tgt_ix": "136-ARR_v1_92",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_90",
            "tgt_ix": "136-ARR_v1_93",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_90",
            "tgt_ix": "136-ARR_v1_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_90",
            "tgt_ix": "136-ARR_v1_94",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_93",
            "tgt_ix": "136-ARR_v1_94",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_95",
            "tgt_ix": "136-ARR_v1_96",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_90",
            "tgt_ix": "136-ARR_v1_95",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_90",
            "tgt_ix": "136-ARR_v1_96",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_94",
            "tgt_ix": "136-ARR_v1_95",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_90",
            "tgt_ix": "136-ARR_v1_97",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_96",
            "tgt_ix": "136-ARR_v1_97",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_90",
            "tgt_ix": "136-ARR_v1_98",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_97",
            "tgt_ix": "136-ARR_v1_98",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_99",
            "tgt_ix": "136-ARR_v1_100",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_90",
            "tgt_ix": "136-ARR_v1_99",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_90",
            "tgt_ix": "136-ARR_v1_100",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_98",
            "tgt_ix": "136-ARR_v1_99",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_90",
            "tgt_ix": "136-ARR_v1_101",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_100",
            "tgt_ix": "136-ARR_v1_101",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_90",
            "tgt_ix": "136-ARR_v1_102",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_101",
            "tgt_ix": "136-ARR_v1_102",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_103",
            "tgt_ix": "136-ARR_v1_104",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_90",
            "tgt_ix": "136-ARR_v1_103",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_90",
            "tgt_ix": "136-ARR_v1_104",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_102",
            "tgt_ix": "136-ARR_v1_103",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "136-ARR_v1_0",
            "tgt_ix": "136-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_1",
            "tgt_ix": "136-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_2",
            "tgt_ix": "136-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_2",
            "tgt_ix": "136-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_2",
            "tgt_ix": "136-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_2",
            "tgt_ix": "136-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_2",
            "tgt_ix": "136-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_2",
            "tgt_ix": "136-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_3",
            "tgt_ix": "136-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_4",
            "tgt_ix": "136-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_4",
            "tgt_ix": "136-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_5",
            "tgt_ix": "136-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_5",
            "tgt_ix": "136-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_6",
            "tgt_ix": "136-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_6",
            "tgt_ix": "136-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_6",
            "tgt_ix": "136-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_6",
            "tgt_ix": "136-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_7",
            "tgt_ix": "136-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_7",
            "tgt_ix": "136-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_7",
            "tgt_ix": "136-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_7",
            "tgt_ix": "136-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_7",
            "tgt_ix": "136-ARR_v1_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_7",
            "tgt_ix": "136-ARR_v1_7@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_8",
            "tgt_ix": "136-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_8",
            "tgt_ix": "136-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_8",
            "tgt_ix": "136-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_8",
            "tgt_ix": "136-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_9",
            "tgt_ix": "136-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_10",
            "tgt_ix": "136-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_10",
            "tgt_ix": "136-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_10",
            "tgt_ix": "136-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_10",
            "tgt_ix": "136-ARR_v1_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_11",
            "tgt_ix": "136-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_11",
            "tgt_ix": "136-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_12",
            "tgt_ix": "136-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_12",
            "tgt_ix": "136-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_12",
            "tgt_ix": "136-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_12",
            "tgt_ix": "136-ARR_v1_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_12",
            "tgt_ix": "136-ARR_v1_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_13",
            "tgt_ix": "136-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_13",
            "tgt_ix": "136-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_13",
            "tgt_ix": "136-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_14",
            "tgt_ix": "136-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_14",
            "tgt_ix": "136-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_14",
            "tgt_ix": "136-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_14",
            "tgt_ix": "136-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_14",
            "tgt_ix": "136-ARR_v1_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_14",
            "tgt_ix": "136-ARR_v1_14@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_14",
            "tgt_ix": "136-ARR_v1_14@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_14",
            "tgt_ix": "136-ARR_v1_14@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_15",
            "tgt_ix": "136-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_16",
            "tgt_ix": "136-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_16",
            "tgt_ix": "136-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_17",
            "tgt_ix": "136-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_18",
            "tgt_ix": "136-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_19",
            "tgt_ix": "136-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_20",
            "tgt_ix": "136-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_21",
            "tgt_ix": "136-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_22",
            "tgt_ix": "136-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_23",
            "tgt_ix": "136-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_24",
            "tgt_ix": "136-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_25",
            "tgt_ix": "136-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_26",
            "tgt_ix": "136-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_26",
            "tgt_ix": "136-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_26",
            "tgt_ix": "136-ARR_v1_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_27",
            "tgt_ix": "136-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_27",
            "tgt_ix": "136-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_27",
            "tgt_ix": "136-ARR_v1_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_28",
            "tgt_ix": "136-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_29",
            "tgt_ix": "136-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_30",
            "tgt_ix": "136-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_30",
            "tgt_ix": "136-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_30",
            "tgt_ix": "136-ARR_v1_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_30",
            "tgt_ix": "136-ARR_v1_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_31",
            "tgt_ix": "136-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_32",
            "tgt_ix": "136-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_32",
            "tgt_ix": "136-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_33",
            "tgt_ix": "136-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_34",
            "tgt_ix": "136-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_34",
            "tgt_ix": "136-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_34",
            "tgt_ix": "136-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_35",
            "tgt_ix": "136-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_36",
            "tgt_ix": "136-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_36",
            "tgt_ix": "136-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_36",
            "tgt_ix": "136-ARR_v1_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_37",
            "tgt_ix": "136-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_38",
            "tgt_ix": "136-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_39",
            "tgt_ix": "136-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_40",
            "tgt_ix": "136-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_41",
            "tgt_ix": "136-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_42",
            "tgt_ix": "136-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_43",
            "tgt_ix": "136-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_44",
            "tgt_ix": "136-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_45",
            "tgt_ix": "136-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_46",
            "tgt_ix": "136-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_46",
            "tgt_ix": "136-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_46",
            "tgt_ix": "136-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_46",
            "tgt_ix": "136-ARR_v1_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_47",
            "tgt_ix": "136-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_48",
            "tgt_ix": "136-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_48",
            "tgt_ix": "136-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_48",
            "tgt_ix": "136-ARR_v1_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_48",
            "tgt_ix": "136-ARR_v1_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_48",
            "tgt_ix": "136-ARR_v1_48@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_48",
            "tgt_ix": "136-ARR_v1_48@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_49",
            "tgt_ix": "136-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_50",
            "tgt_ix": "136-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_50",
            "tgt_ix": "136-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_50",
            "tgt_ix": "136-ARR_v1_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_51",
            "tgt_ix": "136-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_52",
            "tgt_ix": "136-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_52",
            "tgt_ix": "136-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_52",
            "tgt_ix": "136-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_53",
            "tgt_ix": "136-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_54",
            "tgt_ix": "136-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_54",
            "tgt_ix": "136-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_55",
            "tgt_ix": "136-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_56",
            "tgt_ix": "136-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_56",
            "tgt_ix": "136-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_56",
            "tgt_ix": "136-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_57",
            "tgt_ix": "136-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_57",
            "tgt_ix": "136-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_57",
            "tgt_ix": "136-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_58",
            "tgt_ix": "136-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_58",
            "tgt_ix": "136-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_58",
            "tgt_ix": "136-ARR_v1_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_58",
            "tgt_ix": "136-ARR_v1_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_59",
            "tgt_ix": "136-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_59",
            "tgt_ix": "136-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_60",
            "tgt_ix": "136-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_61",
            "tgt_ix": "136-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_61",
            "tgt_ix": "136-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_62",
            "tgt_ix": "136-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_63",
            "tgt_ix": "136-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_63",
            "tgt_ix": "136-ARR_v1_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_63",
            "tgt_ix": "136-ARR_v1_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_63",
            "tgt_ix": "136-ARR_v1_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_64",
            "tgt_ix": "136-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_65",
            "tgt_ix": "136-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_66",
            "tgt_ix": "136-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_66",
            "tgt_ix": "136-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_66",
            "tgt_ix": "136-ARR_v1_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_66",
            "tgt_ix": "136-ARR_v1_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_66",
            "tgt_ix": "136-ARR_v1_66@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_67",
            "tgt_ix": "136-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_68",
            "tgt_ix": "136-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_68",
            "tgt_ix": "136-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_69",
            "tgt_ix": "136-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_70",
            "tgt_ix": "136-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_71",
            "tgt_ix": "136-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_71",
            "tgt_ix": "136-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_72",
            "tgt_ix": "136-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_72",
            "tgt_ix": "136-ARR_v1_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_72",
            "tgt_ix": "136-ARR_v1_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_72",
            "tgt_ix": "136-ARR_v1_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_72",
            "tgt_ix": "136-ARR_v1_72@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_72",
            "tgt_ix": "136-ARR_v1_72@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_72",
            "tgt_ix": "136-ARR_v1_72@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_72",
            "tgt_ix": "136-ARR_v1_72@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_72",
            "tgt_ix": "136-ARR_v1_72@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_73",
            "tgt_ix": "136-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_74",
            "tgt_ix": "136-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_74",
            "tgt_ix": "136-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_75",
            "tgt_ix": "136-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_76",
            "tgt_ix": "136-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_76",
            "tgt_ix": "136-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_76",
            "tgt_ix": "136-ARR_v1_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_76",
            "tgt_ix": "136-ARR_v1_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_76",
            "tgt_ix": "136-ARR_v1_76@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_77",
            "tgt_ix": "136-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_77",
            "tgt_ix": "136-ARR_v1_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_77",
            "tgt_ix": "136-ARR_v1_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_77",
            "tgt_ix": "136-ARR_v1_77@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_78",
            "tgt_ix": "136-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_78",
            "tgt_ix": "136-ARR_v1_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_78",
            "tgt_ix": "136-ARR_v1_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_78",
            "tgt_ix": "136-ARR_v1_78@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_78",
            "tgt_ix": "136-ARR_v1_78@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_78",
            "tgt_ix": "136-ARR_v1_78@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_79",
            "tgt_ix": "136-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_79",
            "tgt_ix": "136-ARR_v1_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_79",
            "tgt_ix": "136-ARR_v1_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_79",
            "tgt_ix": "136-ARR_v1_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_79",
            "tgt_ix": "136-ARR_v1_79@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_80",
            "tgt_ix": "136-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_81",
            "tgt_ix": "136-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_81",
            "tgt_ix": "136-ARR_v1_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_81",
            "tgt_ix": "136-ARR_v1_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_81",
            "tgt_ix": "136-ARR_v1_81@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_81",
            "tgt_ix": "136-ARR_v1_81@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_81",
            "tgt_ix": "136-ARR_v1_81@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_81",
            "tgt_ix": "136-ARR_v1_81@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_81",
            "tgt_ix": "136-ARR_v1_81@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_81",
            "tgt_ix": "136-ARR_v1_81@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_81",
            "tgt_ix": "136-ARR_v1_81@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_82",
            "tgt_ix": "136-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_83",
            "tgt_ix": "136-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_84",
            "tgt_ix": "136-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_85",
            "tgt_ix": "136-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_85",
            "tgt_ix": "136-ARR_v1_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_85",
            "tgt_ix": "136-ARR_v1_85@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_85",
            "tgt_ix": "136-ARR_v1_85@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_85",
            "tgt_ix": "136-ARR_v1_85@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_85",
            "tgt_ix": "136-ARR_v1_85@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_86",
            "tgt_ix": "136-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_87",
            "tgt_ix": "136-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_88",
            "tgt_ix": "136-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_88",
            "tgt_ix": "136-ARR_v1_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_89",
            "tgt_ix": "136-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_89",
            "tgt_ix": "136-ARR_v1_89@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_89",
            "tgt_ix": "136-ARR_v1_89@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_89",
            "tgt_ix": "136-ARR_v1_89@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_89",
            "tgt_ix": "136-ARR_v1_89@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_90",
            "tgt_ix": "136-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_91",
            "tgt_ix": "136-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_91",
            "tgt_ix": "136-ARR_v1_91@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_92",
            "tgt_ix": "136-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_92",
            "tgt_ix": "136-ARR_v1_92@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_92",
            "tgt_ix": "136-ARR_v1_92@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_93",
            "tgt_ix": "136-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_93",
            "tgt_ix": "136-ARR_v1_93@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_94",
            "tgt_ix": "136-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_95",
            "tgt_ix": "136-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_96",
            "tgt_ix": "136-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_96",
            "tgt_ix": "136-ARR_v1_96@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_96",
            "tgt_ix": "136-ARR_v1_96@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_97",
            "tgt_ix": "136-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_97",
            "tgt_ix": "136-ARR_v1_97@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_98",
            "tgt_ix": "136-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_98",
            "tgt_ix": "136-ARR_v1_98@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_98",
            "tgt_ix": "136-ARR_v1_98@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_99",
            "tgt_ix": "136-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_99",
            "tgt_ix": "136-ARR_v1_99@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_100",
            "tgt_ix": "136-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_100",
            "tgt_ix": "136-ARR_v1_100@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_100",
            "tgt_ix": "136-ARR_v1_100@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_100",
            "tgt_ix": "136-ARR_v1_100@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_101",
            "tgt_ix": "136-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_102",
            "tgt_ix": "136-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_102",
            "tgt_ix": "136-ARR_v1_102@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_102",
            "tgt_ix": "136-ARR_v1_102@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_102",
            "tgt_ix": "136-ARR_v1_102@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_102",
            "tgt_ix": "136-ARR_v1_102@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_102",
            "tgt_ix": "136-ARR_v1_102@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_102",
            "tgt_ix": "136-ARR_v1_102@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_102",
            "tgt_ix": "136-ARR_v1_102@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_103",
            "tgt_ix": "136-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_104",
            "tgt_ix": "136-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_105",
            "tgt_ix": "136-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_106",
            "tgt_ix": "136-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_107",
            "tgt_ix": "136-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_108",
            "tgt_ix": "136-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_109",
            "tgt_ix": "136-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_110",
            "tgt_ix": "136-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_111",
            "tgt_ix": "136-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_112",
            "tgt_ix": "136-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_113",
            "tgt_ix": "136-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_114",
            "tgt_ix": "136-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_115",
            "tgt_ix": "136-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_116",
            "tgt_ix": "136-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_117",
            "tgt_ix": "136-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_118",
            "tgt_ix": "136-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_119",
            "tgt_ix": "136-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_120",
            "tgt_ix": "136-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_121",
            "tgt_ix": "136-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_122",
            "tgt_ix": "136-ARR_v1_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_123",
            "tgt_ix": "136-ARR_v1_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_124",
            "tgt_ix": "136-ARR_v1_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_125",
            "tgt_ix": "136-ARR_v1_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_126",
            "tgt_ix": "136-ARR_v1_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_127",
            "tgt_ix": "136-ARR_v1_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_128",
            "tgt_ix": "136-ARR_v1_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_129",
            "tgt_ix": "136-ARR_v1_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_130",
            "tgt_ix": "136-ARR_v1_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_131",
            "tgt_ix": "136-ARR_v1_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_132",
            "tgt_ix": "136-ARR_v1_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_133",
            "tgt_ix": "136-ARR_v1_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_134",
            "tgt_ix": "136-ARR_v1_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_135",
            "tgt_ix": "136-ARR_v1_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_136",
            "tgt_ix": "136-ARR_v1_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_137",
            "tgt_ix": "136-ARR_v1_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_138",
            "tgt_ix": "136-ARR_v1_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_139",
            "tgt_ix": "136-ARR_v1_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_140",
            "tgt_ix": "136-ARR_v1_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_141",
            "tgt_ix": "136-ARR_v1_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_142",
            "tgt_ix": "136-ARR_v1_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_143",
            "tgt_ix": "136-ARR_v1_143@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_144",
            "tgt_ix": "136-ARR_v1_144@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_145",
            "tgt_ix": "136-ARR_v1_145@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_146",
            "tgt_ix": "136-ARR_v1_146@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_147",
            "tgt_ix": "136-ARR_v1_147@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_148",
            "tgt_ix": "136-ARR_v1_148@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_149",
            "tgt_ix": "136-ARR_v1_149@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_150",
            "tgt_ix": "136-ARR_v1_150@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_151",
            "tgt_ix": "136-ARR_v1_151@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_152",
            "tgt_ix": "136-ARR_v1_152@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_153",
            "tgt_ix": "136-ARR_v1_153@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_154",
            "tgt_ix": "136-ARR_v1_154@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_155",
            "tgt_ix": "136-ARR_v1_155@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_156",
            "tgt_ix": "136-ARR_v1_156@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_157",
            "tgt_ix": "136-ARR_v1_157@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_158",
            "tgt_ix": "136-ARR_v1_158@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_159",
            "tgt_ix": "136-ARR_v1_159@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_160",
            "tgt_ix": "136-ARR_v1_160@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_161",
            "tgt_ix": "136-ARR_v1_161@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_162",
            "tgt_ix": "136-ARR_v1_162@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_163",
            "tgt_ix": "136-ARR_v1_163@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_164",
            "tgt_ix": "136-ARR_v1_164@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_165",
            "tgt_ix": "136-ARR_v1_165@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_166",
            "tgt_ix": "136-ARR_v1_166@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_167",
            "tgt_ix": "136-ARR_v1_167@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_168",
            "tgt_ix": "136-ARR_v1_168@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_169",
            "tgt_ix": "136-ARR_v1_169@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_170",
            "tgt_ix": "136-ARR_v1_170@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_171",
            "tgt_ix": "136-ARR_v1_171@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_172",
            "tgt_ix": "136-ARR_v1_172@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_173",
            "tgt_ix": "136-ARR_v1_173@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_174",
            "tgt_ix": "136-ARR_v1_174@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_175",
            "tgt_ix": "136-ARR_v1_175@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_176",
            "tgt_ix": "136-ARR_v1_176@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "136-ARR_v1_177",
            "tgt_ix": "136-ARR_v1_177@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1541,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "136-ARR",
        "version": 1
    }
}