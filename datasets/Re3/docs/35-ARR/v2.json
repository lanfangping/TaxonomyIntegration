{
    "nodes": [
        {
            "ix": "35-ARR_v2_0",
            "content": "Why don't people use character-level machine translation?",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_2",
            "content": "We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT). Despite evidence in the literature that character-level systems are comparable with subword systems, they are virtually never used in competitive setups in WMT competitions. We empirically show that even with recent modeling innovations in characterlevel natural language processing, characterlevel MT systems still struggle to match their subword-based counterparts. Character-level MT systems show neither better domain robustness, nor better morphological generalization, despite being often so motivated. However, we are able to show robustness towards source side noise and that translation quality does not degrade with increasing beam size at decoding time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "35-ARR_v2_4",
            "content": "The progress in natural language processing (NLP) brought by deep learning is often narrated as removing assumptions about the input data and letting the models learn everything end-to-end. One of the assumptions about input data that seems to resist this trend is (at least partially) linguistically motivated segmentation of input data in machine translation (MT) and NLP in general.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_5",
            "content": "For NMT, several papers have claimed parity of character-based methods with subword models, highlighting advantageous features of such systems. Very recent examples include Gao et al. (2020); Banar et al. (2020); Li et al. (2021). Despite this, character-level methods are rarely used as strong baselines in research papers and shared task submissions, suggesting that character-level models might have drawbacks that are not sufficiently addressed in the literature.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_6",
            "content": "In this paper, we examine what the state of the art in character-level MT really is. We survey existing methods and conduct a meta-analysis of the input segmentation methods used in WMT shared task submissions. We then systematically compare the most recent character-processing architectures, some of them taken from general NLP research and used for the first time in MT. Further, we propose an alternative two-step decoder architecture that unlike standard decoders does not suffer from a slow-down due to the length of character sequences. Following the recent findings on MT decoding, we evaluate different decoding strategies in the character-level context.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_7",
            "content": "Many previous studies on character-level MT drew their conclusions from experiments on rather small datasets and focused only on quantitatively assessed translation quality without further analysis. To compensate for this, we revisit and systematically evaluate the state-of-the-art approaches to character-level neural MT and identify their major strengths and weaknesses on large datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_8",
            "content": "Character-Level Neural MT",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "35-ARR_v2_9",
            "content": "Character-level processing was hardly possible within the statistical MT paradigm that assumed the existence of phrases consisting of semantically rich tokens that roughly correspond to words. Neural sequence-to-sequence models (Sutskever et al., 2014;Bahdanau et al., 2015;Vaswani et al., 2017) do not explicitly work with this assumption. In theory, they can learn to transform any sequence into any sequence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_10",
            "content": "The original sequence-to-sequence models used word-based vocabularies of a limited size and which led to a relatively frequent occurrence of out-of-vocabulary tokens. A typical solution to that problem is subword segmentation (Sennrich et al., 2016;Kudo and Richardson, 2018), which keeps frequent tokens intact and splits less frequent ones into smaller units.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_11",
            "content": "Modeling language on the character level is attractive because it can help overcome several problems of subword models. One-hot representations of words or subwords do not reflect systematic character-level relations between words, potentially harming morphologically rich languages. With subwords, minor typos on the source side lead to radically different input representations resulting in low robustness towards source-side noise (Provilkov et al., 2020;Libovick\u00fd and Fraser, 2020). Models using recurrent neural networks (RNNs) showed early success with character-level segmentation on the decoder side (Chung et al., 2016). Using character-level processing on the encoder side proved harder which was attributed to the features of the attention mechanism which can presumably benefit from semantically rich units (such as subwords) in the encoder. Following this line of thinking, Lee et al. (2017) introduced 1D convolutions with max-pooling that pre-process the character sequence into a sequence of latent wordlike states. Coupled with a character-level decoder, they claimed to match the state-of-the-art subwordbased models. Even though this architecture works well on the character level, it does not generalize further to the byte level . Hybrid approaches combining tokenization into words with the computation of character-based word representations were successfully used with RNNs (Luong and Manning, 2016;Gr\u00f6nroos et al., 2017;Ataman et al., 2019). Later, Cherry et al. (2018) showed that RNNs perform on par with subword models without changing the model architecture if the models are sufficiently large. Kreutzer and Sokolov (2018) support this by showing that RNN models which learn segmentation jointly with the rest of the model are close to character-level.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_12",
            "content": "Character-level modeling with Transformers appears to be more difficult. Gupta et al. (2019) used Transparent Attention to train deep character-level models and needed up to 32 layers to close the gap between the BPE and character models, which makes the model too large for practical use. Libovick\u00fd and Fraser (2020) narrowed the gap between subword and character modeling using curriculum learning by finetuning subword models to character-level. Gao et al. (2020) proposed adding a convolutional sub-layer in the Transformer layers. At the cost of a 30% increase in parameter count, they managed to narrow the gap between subword-and character-based models by half. Banar et al. (2020) reused the convolutional preprocessing layer with constant-size segments of Lee et al. (2017)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_13",
            "content": "Transformer model for translation into English. Without changing the decoder, they reached comparable, but usually slightly worse, translation quality compared to BPE-based models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_14",
            "content": "Shaham and Levy (2021a) revisited characterand byte-level MT on rather small IWSLT datasets. Their results show that character-level and bytelevel models are usually worse than BPE models, but byte-based models without embedding layers often outperform BPE-based models in the out-of-English direction. Using similarly small datasets, Li et al. (2021) claim that character-level modeling outperforms BPE when translating into fusional, agglutinative, and introflexive languages. Nikolov et al. (2018) experimented with character-level models for romanized Chinese. These models performed comparable to models using logographic signs, but significantly worse than models using subwords. Zhang and Komachi (2018) argued that signs in logographic languages carry too much information and were able to improve the translation quality by segmenting Chinese and Japanese into sub-character units while keeping subword segmentation on the English side.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_15",
            "content": "Little is known about other properties of character-level MT beyond the overall translation quality. Sennrich (2017) prepared a set of contrastive English-German sentence pairs and tested them using shallow RNN-based models. They observed that character-based models transliterated better, but captured morphosyntactic agreement worse. Libovick\u00fd and Fraser (2020) evaluated Transformer-based character-level models using MorphEval and came to mixed conclusions. Gupta et al. (2019) and Libovick\u00fd and Fraser (2020) make claims about the noise robustness of the character-level models using synthetic noise. Li et al. (2021) evaluated domain robustness by training models on small domain-specific datasets and evaluating them on unrelated domains, claiming the superiority of character-level models in this setup. On the other hand, Gupta et al. (2019) evaluated the domain robustness in a more natural setup and did not observe higher robustness when evaluating general domain models on domain-specific tests compared to BPE.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_16",
            "content": "Another consideration is longer training and inference times. Character-level systems are significantly slower due to the increased sequence length. Libovick\u00fd and Fraser (2020) reported a 5.6-fold slowdown at training time and a 4.7-fold slowdown at inference time compared to subword models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_17",
            "content": "Recent research on character-level modeling goes beyond MT. Pre-trained multilingual representations are a particularly active area. Clark et al. (2021) propose CANINE. The model shrinks character sequences into fewer hidden states (similar to Lee et al., 2017). They use local self-attention and strided convolutions (instead of highway layers and max-pooling as in Lee's work). Their model is either trained using the masked-language-modeling objective (Devlin et al., 2019) with subword supervision, or in an encoder-decoder setup similar to Raffel et al. (2020). Both methods reach a representation quality comparable to similar subword models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_18",
            "content": "ByT5 (Xue et al., 2021a) and Charformer (Tay et al., 2021) are based on the mT5 model (Xue et al., 2021b) which uses sequence-to-sequence denoising pre-training. Whereas byT5 only uses byte sequences instead of subwords and differs in hyperparameters, Charformer uses convolution and combines character blocks to obtain latent subword representations. These models mostly reach similar results to sub-word models, occasionally outperforming a few of them, in the case of Charformer without a significant slowdown.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_19",
            "content": "WMT submissions",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "35-ARR_v2_20",
            "content": "The Conference on Machine Translation (WMT) organizes annual shared tasks in various use cases of MT. The shared task submissions focus on translation quality rather than the novelty of presented ideas, as most other research papers do. Therefore, we assume that, if character-level models were a fully-fledged alternative to subword models, at least some systems submitted to the shared tasks would use character-level models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_21",
            "content": "We annotated recent system description papers with the input and output segmentation method they used. We focused on information about experiments with character-level models. Since we are primarily interested in the Transformer architecture that became the standard after 2017, we only included system description papers from 2018-2020 Barrault et al., 2019Barrault et al., , 2020.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_22",
            "content": "Transformers were used in 81%, 87%, and 97% of the systems in the respective years. We included the main task on WMT, news translation, and two minor tasks where character-level methods might help: translation robustness (Li et al., 2019;Specia et al., 2020) and translation between similar languages (ibid.). Almost all systems use a subword-based vocabulary (BPE: 81%, 71%, 66% in the respective years; SentencePiece: None in 2018, 9% and 25% in the following ones). Purely word-based (none in 2018, 2% and 3% in the later years) or morphological segmentation (4%, 2%, 3% in the respective years) are rarely used. The average vocabulary size decreases over time (see Figure 2) with a median size remaining at 32k in the last two years. The reason for the decreasing average is probably a higher proportion of systems for low-resource languages, where a smaller vocabulary leads to better translation quality (Sennrich and Zhang, 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_23",
            "content": "Among the 145 annotated system description papers, there were only two that used characterlevel segmentation. Mahata et al. (2018) used a character-level model for Finnish-to-English translation. This system, however, makes many suboptimal design choices and ended up as the last one in the manual evaluation. Scherrer et al. (2019) experimented with character-level systems for similar language translation and observed that characters outperform other segmentations for Spanish-Portuguese translation, but not for Czech-Polish. Knowles et al. (2020) experimented with different subword vocabulary sizes for English-Inuktikut translation and reached the best results using a subword vocabulary of size 1k, which makes it close to the character level. Most of the papers do not even mention character-level segmentation as a viable alternative they would like to pursue in future work (7% in 2018, 2% in 2019, none in 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_24",
            "content": "Character-level methods were more frequently used in WMT17 with RNN-based systems, especially for translation of Finnish \u00d6stling et al., 2017) and less successfully for Chinese (Holtz et al., 2017) and the automatic post-editing task (Vari\u0161 and Bojar, 2017).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_25",
            "content": "On the other hand, Figure 1 shows that the research interest in character-level methods remains approximately the same, or may have slightly increased. For practical solutions in WMT systems, we clearly show that system designers in the WMT community have avoided character-level models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_26",
            "content": "We speculate that the main reasons for not considering character-level modeling are its lower efficiency and the fact that the literature shows no clear improvement of translation quality. Most of the submissions use back-translation (85%, 82%, and 94% in the respective years), often iterated several times (11%, 20%, 16%), which requires both training and inference on large datasets. With the approximately 5-fold slowdown, WMT-scale experiments on character models are not easily tractable.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_27",
            "content": "Evaluated Models",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "35-ARR_v2_28",
            "content": "We evaluate several Transformer-based architectures for character-level MT. A major issue with character-level sequence processing is the sequence length and low information density compared to subword sequences. Architectures for characterlevel sequence processing typically address this issue by locally processing and shrinking the sequences into latent word-like units. In our experiments, we explore several ways to do this.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_29",
            "content": "First, we directly use character embeddings as input to the Transformer. Second, following Banar et al. (2020), we use the convolutional character processing layers proposed by Lee et al. (2017). Third, we replace the convolutions with local selfattention as proposed in the CANINE model (Clark et al., 2021). Finally, we use the recently proposed Charformer architecture (Tay et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_30",
            "content": "Lee-style encoding. Lee et al. (2017) process the sequence of character embeddings with convolutions of different kernel sizes and number of output channels. In the original paper, this was followed by 4 highway layers (Srivastava et al., 2015). In our preliminary experiments, we observed that a too deep stack of highway layers leads to diminishing gradients, and we replaced the second two High-way layers with feedforward sublayers as used in the Transformer architecture (Vaswani et al., 2017). CANINE. Clark et al. (2021) experiment with character-level pre-trained sentence representations. The character-processing architecture is in principle similar to Lee et al. (2017) but uses more modern building blocks. Character embeddings are processed by a Transformer layer with local selfattention which only allows the states to attend to states in their neighborhood. This is followed by downsampling using strided convolution.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_31",
            "content": "Originally, CANINE used a local self-attention span as long as 128 characters. In the case of MT, this would usually span the entire sentence, so we use significantly shorter spans.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_32",
            "content": "Charformer. Unlike previous approaches, Charformer (Tay et al., 2021) does not apply a nonlinearity on the embeddings and gets latent subword representations by repeated averaging of character embeddings. First, it processes the sequence using a 1D convolution, so the states are aware of their mutual local positions in local neighborhoods. Second, non-overlapping character n-grams of length up to N are represented by averages of the respective character embeddings. This means that for each character, there is a vector that represents the character as a member of n-grams of length 1 to N . In the third step, the character blocks are scored with a scoring function (a linear transformation), which can be interpreted as attention over the N different n-gram lengths. The attention scores are used to compute a weighted average over the n-gram representations. Finally, the sequence is downsampled using mean-pooling with window size and stride size N (i.e., the maximum n-gram size).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_33",
            "content": "Whereas Lee-style encoding allows using lowdimensional character embeddings and keeps most parameters in the convolutional layers, CANINE and Charformer need the character representation to have the same dimension as the following Transformer layer stack.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_34",
            "content": "Two-step decoding. The architectures mentioned above allow the Transformer layers to operate more efficiently with a shorter and more information-dense sequence of states. However, while decoding, we need to generate the target character sequence in the original length, by outputting a block of characters in each decoding step. Our preliminary experiments showed that generating blocks of characters non-autoregressively leads to incoherent output. Therefore, we propose a twostep decoding architecture where the stack of Transformer layers operating over the downsampled sequence is followed by a lightweight LSTM autoregressive decoder (see Figure 3). The input to the LSTM decoder is a concatenation of the embedding of the previously generated character and a projection of the Transformer decoder output state. At inference time, the LSTM decoder generates a block of characters and inputs them to the character-level processing layer. The Transformer decoder computes an output state that the LSTM decoder uses to generate another character block. More details are in Appendix A.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_35",
            "content": "Modifying Charformer for the two-step decoding would require a long padding at the beginning of the sequence causing the decoder to diverge. Because of that, we use Lee-style encoding on the decoder side when using Charformer in the encoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_36",
            "content": "First, we conduct all our experiments on the small IWSLT datasets. Then we evaluate the most promising architectures on larger datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_37",
            "content": "Experiments on Small Data",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "35-ARR_v2_38",
            "content": "We implement the models using Huggingface Transformers (Wolf et al., 2020). We take the CA-NINE layer from Huggingface Transformers and use an independent implementation of Charformer 1 . Our source code is available on Github. 2 Hyperparameters and other experimental details can be found in Appendix B.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_39",
            "content": "Experimental Setup",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "35-ARR_v2_40",
            "content": "We evaluate the models on translation between English paired with German, French, and Arabic (with English as both input and output) using the IWSLT 2017 datasets (Cettolo et al., 2017) with a training data size of around 200k sentences for each language pair (see Appendix B for details).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_41",
            "content": "For the subword models, we tokenize the input using the Moses tokenizer (Koehn et al., 2007) and then further split the words into subword units using BPE (Sennrich et al., 2016) with 16k merge operations. For the character models, we limit the vocabulary to 300 UTF-8 characters.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_42",
            "content": "We use the Transformer Base architecture (Vaswani et al., 2017) in all experiments. We make no changes to it in the subword and baseline character experiments. In the later experiments, we replace the embedding lookup with the character processing architectures. For the Lee-style encoder, we chose similar hyperparameters as related work (Banar et al., 2020). For experiments with Charformer and CANINE models, we set the hyperparameters such that they cover the same character span before downsampling as the Lee-style encoder, which causes the models to have fewer parameters than a Lee-style encoder. Note however that for both the Charformer and the CANINE models, the number of parameters is almost independent of the character window width. For all three character processing architectures, we experiment with downsampling factors of 3 and 5 (a 16k BPE vocabulary corresponds to a downsampling factor of about 4 in English).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_43",
            "content": "Translation Quality",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "35-ARR_v2_44",
            "content": "We evaluate the translation quality using the BLEU score (Papineni et al., 2002), the chrF score (Popovi\u0107, 2015) (as implemented in SacreBLEU; Post, 2018), 3 and the COMET score (Rei et al., 2020). We run each experiment 4 times and report the mean value and standard deviation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_45",
            "content": "The results are presented in Table 1. Except for translation into Arabic, where character methods outperform BPEs (which is consistent with the findings of Levy, 2021a andLi et al., 2021), subword methods are always better than characters.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_46",
            "content": "The Lee-style encoder outperforms the two more recent methods and the method of using the character embeddings directly. Charformer performs similarly to using character embeddings directly, .520",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_47",
            "content": "\u00b1-.099",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_48",
            "content": ".044",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_49",
            "content": "\u00b1-.181",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_50",
            "content": "Table 1: Translation quality of the models on the IWSLT data. The fourth column shows the size of the characterprocessing layers expressed as the vocabulary size of Transformer Base having the same number of parameters in the embeddings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_51",
            "content": "CANINE is significantly worse. The results are mostly consistent across the language pairs. Increasing the downsampling rate from 3 to 5 degrades the translation quality for all architectures. Employing the two-step decoder matches the decoding speed of subword models. However, the overall translation quality is much worse.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_52",
            "content": "The three metrics that we use give consistent results in most cases. Often, relatively small differences in BLEU and chrF scores correspond to much bigger differences in the COMET score.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_53",
            "content": "Inference",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "35-ARR_v2_54",
            "content": "Inference algorithms for neural MT have been discussed extensively (Meister et al., 2020;Massarelli et al., 2020;Shi et al., 2020;Shaham and Levy, 2021b) for the subword models. Subword translation quality quickly degrades beyond a certain beam width unless heuristically defined length normalization is applied.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_55",
            "content": "As an alternative, Eikema and Aziz (2020) recently proposed Minimum Bayes Risk (MBR; Goel and Byrne 2000) estimation as an alternative. Assuming that similar sentences should be similarly probable, they propose repeatedly sampling from the model and selecting a sentence that is most similar to other samples. With subword models, MBR performs comparably to beam search.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_56",
            "content": "Intuitive arguments about the inference algorithms are often based on the properties of the subword output distribution. On average, character models will produce distributions with lower perplexity and thus likely suffer more from the exposure bias which might harm sampling from the model. Therefore, there is a risk that these empirical findings do not apply to character-level models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_57",
            "content": "We explore what decoding strategies are best suited for the character-level models. We compare the translation quality of beam search decoding with different degrees of length normalization. 4 Further, we compare length-normalized beam search decoding with MBR (with 100 samples), greedy decoding, and random sampling. We use the chrF as a comparison metric which allows pre-computing the character n-grams and thus faster sentence pair comparison than the originally proposed METEOR (Denkowski and Lavie, 2011).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_58",
            "content": "Figure 4 shows the translation quality of the selected models for different beam sizes. The dotted lines denoting the translation quality without length normalization show that the quality of the subword models quickly deteriorates without length normalization, whereas vanilla and Lee-style characterlevel models do not seem to suffer from this problem.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_59",
            "content": "Table 2 presents the translation quality for different decoding methods. In all cases, beam search is the best strategy. Sampling from character-level models leads to very poor translation quality that in turn also influences the MBR decoding leading to much worse results than beam search.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_60",
            "content": "Our experiments show that beam search with length normalization is the best inference algorithm for character-level models. They also seem to be more resilient towards the beam search curse compared to subword models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_61",
            "content": "Experiments on WMT Data",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "35-ARR_v2_62",
            "content": "Based on the results of the experiments with the IWSLT data, we further experiment only with the Lee-style encoder using a downsampling factor of 3 on the source side. Additionally, we experiment with hybrid systems with a subword encoder and character decoder. We train translation systems of competitive quality on two high-resource language pairs, English-Czech and English-German, and perform an extensive evaluation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_63",
            "content": "Experimental Setup",
            "ntype": "title",
            "meta": {
                "section": "6.1"
            }
        },
        {
            "ix": "35-ARR_v2_64",
            "content": "For English-to-Czech translation, we use the CzEng 2.0 corpus (Kocmi et al., 2020b) that aggregates and curates all sources for this language pair. We use all 66M authentic parallel sentence pairs and 50M back-translated Czech sentences.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_65",
            "content": "For the English-to-German translation, we use a subset of the training data used by Chen et al. (2021). The data consists of 66M authentic sentence pairs filtered from the available data for WMT and 52M back-translated German sentences from News Crawl 2020.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_66",
            "content": "We tag the back-translation data (Caswell et al., 2019). We use the Transformer Big architecture for all experiments with hyperparameters following Popel and Bojar (2018). For the Lee-style encoder, we double the hidden layer sizes compared to the IWSLT experiments (following the hidden size increase between the Transformer Base and Big architectures). In contrast to the previous set of experiments, we use Fairseq (Ott et al., 2019). Our code is available on Github 5 . System outputs are attached to the paper in the ACL anthology.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_67",
            "content": "We evaluate the systems not only on WMT20 test sets but also on data that often motivated the research of character-level methods. We evaluate the out-of-domain performance of the models on the NHS test set from the WMT17 Biomedical Task (Jimeno Yepes et al., 2017) and on the WMT16 IT Domain test set (Bojar et al., 2016). We use the same evaluation metrics as for the IWSLT experiments. We estimate the confidence intervals using bootstrap resampling (Koehn, 2004).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_68",
            "content": "We also assess the gender bias of the systems (Stanovsky et al., 2019;Kocmi et al., 2020a), using a dataset of sentence pairs with stereotypical and non-stereotypical English sentences. We measure the accuracy of gendered nouns and pronouns using word alignment and morphological analysis.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_69",
            "content": "Morphological generalization is often mentioned among the motivations for character-level modeling. Therefore, we evaluate our models using Mor-phEval (Burlot and Yvon, 2017;Burlot et al., 2018). Similar to the gender evaluation, MorphEval also uses contrastive sentence pairs that differ in exactly one morphological feature. Accuracy on the sentences is measured. Besides, we assess how well the models handle lemmas and forms that were unseen at training time. We tokenize and lemmatize all data with UDPipe (Straka and Strakov\u00e1, 2017). On the WMT20 test set, we compute the recall of test lemmas that were not in the training set and the recall of word forms that were not in the training data, but forms of the same lemma were. Note that not generating a particular lemma or form is not necessarily an error. Therefore, we report the recall in contrast with the recall of lemmas and forms that were represented in the training data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_70",
            "content": "Character-level models are also supposed to be more robust towards source-side noise. We evaluate the noise robustness of the systems using synthetic noise. We use TextFlint (Wang et al., 2021) to generate synthetic noise in the source text with simulated typos and spelling errors. We generate 20 noisy versions of the WMT20 test set and report the average chrF score.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_71",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "6.2"
            }
        },
        {
            "ix": "35-ARR_v2_72",
            "content": "The main results are presented in Table 3. The main trends in the translation quality are the same as in the case of IWSLT data: subword models outperform character models. Using Lee-style encoding narrows the quality gap and performs similarly to models with subword tokens on the source side. Although domain robustness often motivates character-level experiments, our experiments show that the trends are domain-independent, except for English-German IT Domain translation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_73",
            "content": "The similar performance of the subword encoder and the Lee-style encoder suggests that the hidden states of the Lee-style encoder can efficiently emulate the subword segmentation. We speculate that the main weaknesses remain on the decoder side.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_74",
            "content": "In the English-to-Czech direction, the characterlevel models perform worse in gender bias evaluation, although they better capture grammatical gender agreement according to the MorphEval benchmark. On the other hand, character-level models make more frequent errors in the tense of coordinated verbs. There are no major differences in recall of novel forms and lemmas.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_75",
            "content": "For the English-to-German translation, characterlevel methods reach better results on the gender benchmark. We speculate that getting gender correct in German might be easier because unlike Czech it does not require subject-verb agreement. The average performance on the MorphEval benchmark is also slightly better for character models. Detailed results on MorphEval are in Tables 7 and 8 in the Appendix. The higher recall of novel forms also suggests slightly better morphological generalization.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_76",
            "content": "The only consistent advantage of the characterlevel models is their robustness towards source side noise. Here, the character-level models outperform both the fully subword model and the subword encoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_77",
            "content": "Conclusions",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "35-ARR_v2_78",
            "content": "In our extensive literature survey, we found evidence that character-level methods should reach comparative translation quality as subword methods, typically at the expense of much higher computation costs. We speculate that the computational cost is the reason why virtually none of the recent WMT systems used character-level methods or mentioned them as a reasonable alternative.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_79",
            "content": "Recently, most innovations in character-level modeling were introduced in the context of pretrained representations. In our comparison of character processing architectures (two of them used for the first time in the context of MT), we showed that 1D convolutions followed by highway layers still deliver the best results for MT.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_80",
            "content": "Character-level systems are still mostly worse than subword systems. Moreover, the recent character-level architectures do not show advantages over vanilla character models, other than improved speed.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_81",
            "content": "To overcome efficiency issues, we proposed a two-step decoding architecture that matches the speed of subword models, however at the expense of a further drop in translation quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_82",
            "content": "Furthermore, we found that conclusions of recent literature on decoding in MT do not generalize for character models. Character models do not suffer from the beam search curse and decoding methods based on sampling perform poorly, here.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_83",
            "content": "Evaluation on competitively large datasets showed that there is still a small quality gap between character and subword models. Character models do not show better domain robustness, and only slightly better morphological generalization in German, although this is often mentioned as important motivation for character-level modeling. The only clear advantage of character models is high robustness towards source-side noise.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_84",
            "content": "In contrast to earlier work on character-level MT, which claimed that decoding is straightforward and which focused on the encoder part of the model, our conclusions are that Lee-style encoding is comparable to subword encoders. Even now, most modeling innovations focus on encoding. Character-level decoding which is both accurate and efficient remains an open research question.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_85",
            "content": "At inference time, the LSTM decoder gets one Transformer state and generates s output characters. The characters are fed to the character processing architecture, which is in turn used to generate the next Transformer decoder state.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_86",
            "content": "We used the tst2010 part of the dataset for validation and tst2015 for testing and did not use any other test sets. The data sizes are presented in Table 4.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_87",
            "content": "All models are trained with initial learning rate: 5 \u2022 10 \u22124 with 4k warmup steps. The batch size is 20k tokens for both BPE and character experiments with update after 3 batches. Label smoothing is set to 0.1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_88",
            "content": "Lee-style. The character embedding dimension is 64. The original paper used kernel sizes from 1 to 8. For ease of implementation, we only use even-sized kernels up to size 9. The encoder uses 1D convolutions of kernel size 1, 3, 5, 7, 9 with 128, 256, 512, 512, 256 filters. Their output is concatenated and projected to the model dimension, followed by 2 highway layers and 2 Transformer feed-forward layers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_89",
            "content": "CANINE. The local self-attention span in the encoder is 4\u00d7 the downsampling factor, in the encoder, equal to the downsampling factor. Two-step decoder. The decoder uses character embeddings with dimension of 64, which is also the size of the projection of the Transformer decoder state. The hidden state size of the LSTM is 128.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_90",
            "content": "The validation BLEU and chrF scores and training and inference times are in Table 5. The training times were measured on machines with GeForce GTX 1080 Ti GPUs and with Intel Xeon E5-2630v4 CPUs (2.20GHz), a single GPU was used.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_91",
            "content": "Note that the experiments on IWSLT were not optimized for speed and are thus not comparable with the times reported on the larger datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_92",
            "content": "We use the Transformer Big architecture as defined FairSeq's standard transformer_wmt_en_de_big_t2t. The Lee-style encoder uses filters sizes 1, 3, 5, 7, 9 of dimensions 256, 512, 1024, 1024, 512. The other parameters remains the same as in the IWSLT experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_93",
            "content": "We set the beta parameters of the Adam optimizer to 0.9 and 0.998 and gradient clipping to 5. The learning rate is 5 \u2022 10 \u22124 with 16k warmup steps. Early stopping is with respect to negative log likelihood with patience 10. We save 5 best checkpoints and do checkpoint averaging before evaluation. The maximum batch size is 1800 tokens for the BPE experiments and 500 for character-level experiments. We train the models on 4 GPUs, so the effective batch size is 4 times bigger.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_94",
            "content": "During training, we evaluated the models by measuring the cross-entropy on the validation set. After model training, we use grid search to estimate the best value of length normalization on the validation set. The translation quality on the validation data is tabulated in Table 6.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_95",
            "content": "The detailed results on the MorphEval benchmark are in Tables 7 (Czech) and 8 (German). The details of the noise evaluation are in Table 9.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "35-ARR_v2_96",
            "content": "Orhan Duygu Ataman,  Firat, A Mattia,  Gangi, On the importance of word boundaries in character-level neural machine translation, 2019, Proceedings of the 3rd Workshop on Neural Generation and Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Orhan Duygu Ataman",
                    " Firat",
                    "A Mattia",
                    " Gangi"
                ],
                "title": "On the importance of word boundaries in character-level neural machine translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 3rd Workshop on Neural Generation and Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_97",
            "content": "Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Neural machine translation by jointly learning to align and translate, 2015-05-07, 3rd International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Dzmitry Bahdanau",
                    "Kyunghyun Cho",
                    "Yoshua Bengio"
                ],
                "title": "Neural machine translation by jointly learning to align and translate",
                "pub_date": "2015-05-07",
                "pub_title": "3rd International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_98",
            "content": "Nikolay Banar, Walter Daelemans, Mike Kestemont, Character-level transformer-based neural machine translation, 2020-12-18, NLPIR 2020: 4th International Conference on Natural Language Processing and Information Retrieval, ACM.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Nikolay Banar",
                    "Walter Daelemans",
                    "Mike Kestemont"
                ],
                "title": "Character-level transformer-based neural machine translation",
                "pub_date": "2020-12-18",
                "pub_title": "NLPIR 2020: 4th International Conference on Natural Language Processing and Information Retrieval",
                "pub": "ACM"
            }
        },
        {
            "ix": "35-ARR_v2_99",
            "content": "Ankur Bapna, Mia Chen, Orhan Firat, Yuan Cao, Yonghui Wu, Training deeper neural machine translation models with transparent attention, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Ankur Bapna",
                    "Mia Chen",
                    "Orhan Firat",
                    "Yuan Cao",
                    "Yonghui Wu"
                ],
                "title": "Training deeper neural machine translation models with transparent attention",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_100",
            "content": "Lo\u00efc Barrault, Magdalena Biesialska, Ond\u0159ej Bojar, Marta Costa-Juss\u00e0, Christian Federmann, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Matthias Huck, Eric Joanis, Tom Kocmi, Philipp Koehn, Chi-Kiu Lo, Nikola Ljube\u0161i\u0107, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, None, , Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (WMT20). In Proceedings of the Fifth Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Lo\u00efc Barrault",
                    "Magdalena Biesialska",
                    "Ond\u0159ej Bojar",
                    "Marta Costa-Juss\u00e0",
                    "Christian Federmann",
                    "Yvette Graham",
                    "Roman Grundkiewicz",
                    "Barry Haddow",
                    "Matthias Huck",
                    "Eric Joanis",
                    "Tom Kocmi",
                    "Philipp Koehn",
                    "Chi-Kiu Lo",
                    "Nikola Ljube\u0161i\u0107",
                    "Christof Monz",
                    "Makoto Morishita",
                    "Masaaki Nagata",
                    "Toshiaki Nakazawa"
                ],
                "title": null,
                "pub_date": null,
                "pub_title": "Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (WMT20). In Proceedings of the Fifth Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_101",
            "content": "Lo\u00efc Barrault, Ond\u0159ej Bojar, Marta Costa-Juss\u00e0, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias M\u00fcller, Findings of the 2019 conference on machine translation (WMT19), 2019, Proceedings of the Fourth Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Lo\u00efc Barrault",
                    "Ond\u0159ej Bojar",
                    "Marta Costa-Juss\u00e0",
                    "Christian Federmann",
                    "Mark Fishel",
                    "Yvette Graham",
                    "Barry Haddow",
                    "Matthias Huck",
                    "Philipp Koehn",
                    "Shervin Malmasi",
                    "Christof Monz",
                    "Mathias M\u00fcller"
                ],
                "title": "Findings of the 2019 conference on machine translation (WMT19)",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fourth Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_102",
            "content": "Ond\u0159ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aur\u00e9lie N\u00e9v\u00e9ol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, Marcos Zampieri, Findings of the 2016 conference on machine translation, 2016, Proceedings of the First Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Ond\u0159ej Bojar",
                    "Rajen Chatterjee",
                    "Christian Federmann",
                    "Yvette Graham",
                    "Barry Haddow",
                    "Matthias Huck",
                    "Antonio Yepes",
                    "Philipp Koehn",
                    "Varvara Logacheva",
                    "Christof Monz",
                    "Matteo Negri",
                    "Aur\u00e9lie N\u00e9v\u00e9ol",
                    "Mariana Neves",
                    "Martin Popel",
                    "Matt Post",
                    "Raphael Rubino",
                    "Carolina Scarton",
                    "Lucia Specia",
                    "Marco Turchi",
                    "Karin Verspoor",
                    "Marcos Zampieri"
                ],
                "title": "Findings of the 2016 conference on machine translation",
                "pub_date": "2016",
                "pub_title": "Proceedings of the First Conference on Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_103",
            "content": "Ond\u0159ej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Philipp Koehn, Christof Monz, Findings of the 2018 conference on machine translation (WMT18), 2018, Proceedings of the Third Conference on Machine Translation: Shared Task Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Ond\u0159ej Bojar",
                    "Christian Federmann",
                    "Mark Fishel",
                    "Yvette Graham",
                    "Barry Haddow",
                    "Philipp Koehn",
                    "Christof Monz"
                ],
                "title": "Findings of the 2018 conference on machine translation (WMT18)",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_104",
            "content": "Franck Burlot, Yves Scherrer, Vinit Ravishankar, Ond\u0159ej Bojar, Stig-Arne Gr\u00f6nroos, Maarit Koponen, Tommi Nieminen, Fran\u00e7ois Yvon, ; English-Czech,  English-German, Turkish-English English-Finnish, The WMT'18 morpheval test suites for, 2018, Proceedings of the Third Conference on Machine Translation: Shared Task Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Franck Burlot",
                    "Yves Scherrer",
                    "Vinit Ravishankar",
                    "Ond\u0159ej Bojar",
                    "Stig-Arne Gr\u00f6nroos",
                    "Maarit Koponen",
                    "Tommi Nieminen",
                    "Fran\u00e7ois Yvon",
                    "; English-Czech",
                    " English-German",
                    "Turkish-English English-Finnish"
                ],
                "title": "The WMT'18 morpheval test suites for",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_105",
            "content": "Franck Burlot, Fran\u00e7ois Yvon, Evaluating the morphological competence of machine translation systems, 2017, Proceedings of the Second Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Franck Burlot",
                    "Fran\u00e7ois Yvon"
                ],
                "title": "Evaluating the morphological competence of machine translation systems",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Second Conference on Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_106",
            "content": "Isaac Caswell, Ciprian Chelba, David Grangier, Tagged back-translation, 2019, Proceedings of the Fourth Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Isaac Caswell",
                    "Ciprian Chelba",
                    "David Grangier"
                ],
                "title": "Tagged back-translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fourth Conference on Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_107",
            "content": "Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Niehues Jan, St\u00fcker Sebastian, Sudoh Katsuitho, Yoshino Koichiro, Federmann Christian, Overview of the iwslt 2017 evaluation campaign, 2017, International Workshop on Spoken Language Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Mauro Cettolo",
                    "Marcello Federico",
                    "Luisa Bentivogli",
                    "Niehues Jan",
                    "St\u00fcker Sebastian",
                    "Sudoh Katsuitho",
                    "Yoshino Koichiro",
                    "Federmann Christian"
                ],
                "title": "Overview of the iwslt 2017 evaluation campaign",
                "pub_date": "2017",
                "pub_title": "International Workshop on Spoken Language Translation",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_108",
            "content": "Pinzhen Chen, Jind\u0159ich Helcl, Ulrich Germann, Laurie Burchell, Nikolay Bogoychev, Antonio Valerio Miceli, Jonas Barone, Alexandra Waldendorf, Kenneth Birch,  Heafield, 2021. The University of Edinburgh's English-German and English-Hausa submissions to the WMT21 news translation task, , Proceedings of the Sixth Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Pinzhen Chen",
                    "Jind\u0159ich Helcl",
                    "Ulrich Germann",
                    "Laurie Burchell",
                    "Nikolay Bogoychev",
                    "Antonio Valerio Miceli",
                    "Jonas Barone",
                    "Alexandra Waldendorf",
                    "Kenneth Birch",
                    " Heafield"
                ],
                "title": "2021. The University of Edinburgh's English-German and English-Hausa submissions to the WMT21 news translation task",
                "pub_date": null,
                "pub_title": "Proceedings of the Sixth Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_109",
            "content": "Colin Cherry, George Foster, Ankur Bapna, Orhan Firat, Wolfgang Macherey, Revisiting character-based neural machine translation with capacity and compression, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Colin Cherry",
                    "George Foster",
                    "Ankur Bapna",
                    "Orhan Firat",
                    "Wolfgang Macherey"
                ],
                "title": "Revisiting character-based neural machine translation with capacity and compression",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_110",
            "content": "UNKNOWN, None, 2016, A character-level decoder without explicit segmentation for neural machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "A character-level decoder without explicit segmentation for neural machine translation",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_111",
            "content": "UNKNOWN, None, , Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_112",
            "content": "UNKNOWN, None, 2021, CANINE: pre-training an efficient tokenization-free encoder for language representation, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "CANINE: pre-training an efficient tokenization-free encoder for language representation",
                "pub": "CoRR"
            }
        },
        {
            "ix": "35-ARR_v2_113",
            "content": "Marta Costa-Juss\u00e0, Carlos Escolano, Jos\u00e9 Fonollosa, Byte-based neural machine translation, 2017, Proceedings of the First Workshop on Subword and Character Level Models in NLP, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Marta Costa-Juss\u00e0",
                    "Carlos Escolano",
                    "Jos\u00e9 Fonollosa"
                ],
                "title": "Byte-based neural machine translation",
                "pub_date": "2017",
                "pub_title": "Proceedings of the First Workshop on Subword and Character Level Models in NLP",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_114",
            "content": "Michael Denkowski, Alon Lavie, Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems, 2011, Proceedings of the Sixth Workshop on Statistical Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Michael Denkowski",
                    "Alon Lavie"
                ],
                "title": "Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems",
                "pub_date": "2011",
                "pub_title": "Proceedings of the Sixth Workshop on Statistical Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_115",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "35-ARR_v2_116",
            "content": "Bryan Eikema, Wilker Aziz, Is MAP decoding all you need? the inadequacy of the mode in neural machine translation, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Bryan Eikema",
                    "Wilker Aziz"
                ],
                "title": "Is MAP decoding all you need? the inadequacy of the mode in neural machine translation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_117",
            "content": "Carlos Escolano, Marta Costa-Juss\u00e0, Jos\u00e9 Fonollosa, The TALP-UPC neural machine translation system for German/Finnish-English using the inverse direction model in rescoring, 2017, Proceedings of the Second Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Carlos Escolano",
                    "Marta Costa-Juss\u00e0",
                    "Jos\u00e9 Fonollosa"
                ],
                "title": "The TALP-UPC neural machine translation system for German/Finnish-English using the inverse direction model in rescoring",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Second Conference on Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_118",
            "content": "Yingqiang Gao, I Nikola, Yuhuang Nikolov, Richard Hu,  Hahnloser, Character-level translation with self-attention, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Yingqiang Gao",
                    "I Nikola",
                    "Yuhuang Nikolov",
                    "Richard Hu",
                    " Hahnloser"
                ],
                "title": "Character-level translation with self-attention",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_119",
            "content": "Vaibhava Goel, William Byrne, Minimum bayes-risk automatic speech recognition, 2000, Comput. Speech Lang, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Vaibhava Goel",
                    "William Byrne"
                ],
                "title": "Minimum bayes-risk automatic speech recognition",
                "pub_date": "2000",
                "pub_title": "Comput. Speech Lang",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_120",
            "content": "Stig-Arne Gr\u00f6nroos, Sami Virpioja, Mikko Kurimo, Extending hybrid word-character neural machine translation with multi-task learning of morphological analysis, 2017, Proceedings of the Second Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Stig-Arne Gr\u00f6nroos",
                    "Sami Virpioja",
                    "Mikko Kurimo"
                ],
                "title": "Extending hybrid word-character neural machine translation with multi-task learning of morphological analysis",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Second Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_121",
            "content": "UNKNOWN, None, 1911, Character-based NMT with transformer. CoRR, abs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": null,
                "title": null,
                "pub_date": "1911",
                "pub_title": "Character-based NMT with transformer. CoRR, abs",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_122",
            "content": "Chester Holtz, Chuyang Ke, Daniel Gildea, University of Rochester WMT 2017 NMT system submission, 2017, Proceedings of the Second Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Chester Holtz",
                    "Chuyang Ke",
                    "Daniel Gildea"
                ],
                "title": "University of Rochester WMT 2017 NMT system submission",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Second Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_123",
            "content": "Antonio Jimeno Yepes, Aur\u00e9lie N\u00e9v\u00e9ol, Mariana Neves, Karin Verspoor, Ond\u0159ej Bojar, Arthur Boyer, Cristian Grozea, Barry Haddow, Madeleine Kittner, Yvonne Lichtblau, Pavel Pecina, Roland Roller, Rudolf Rosa, Amy Siu, Philippe Thomas, Saskia Trescher, Findings of the WMT 2017 biomedical translation shared task, 2017, Proceedings of the Second Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Antonio Jimeno Yepes",
                    "Aur\u00e9lie N\u00e9v\u00e9ol",
                    "Mariana Neves",
                    "Karin Verspoor",
                    "Ond\u0159ej Bojar",
                    "Arthur Boyer",
                    "Cristian Grozea",
                    "Barry Haddow",
                    "Madeleine Kittner",
                    "Yvonne Lichtblau",
                    "Pavel Pecina",
                    "Roland Roller",
                    "Rudolf Rosa",
                    "Amy Siu",
                    "Philippe Thomas",
                    "Saskia Trescher"
                ],
                "title": "Findings of the WMT 2017 biomedical translation shared task",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Second Conference on Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_124",
            "content": "Rebecca Knowles, Darlene Stewart, Samuel Larkin, Patrick Littell, None, 2020, NRC systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Rebecca Knowles",
                    "Darlene Stewart",
                    "Samuel Larkin",
                    "Patrick Littell"
                ],
                "title": null,
                "pub_date": "2020",
                "pub_title": "NRC systems",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_125",
            "content": ", Inuktitut-English news translation task, , Proceedings of the Fifth Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [],
                "title": "Inuktitut-English news translation task",
                "pub_date": null,
                "pub_title": "Proceedings of the Fifth Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_126",
            "content": "Tom Kocmi, Tomasz Limisiewicz, Gabriel Stanovsky, Gender coreference and bias evaluation at WMT 2020, 2020, Proceedings of the Fifth Conference on Machine Translation, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Tom Kocmi",
                    "Tomasz Limisiewicz",
                    "Gabriel Stanovsky"
                ],
                "title": "Gender coreference and bias evaluation at WMT 2020",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Fifth Conference on Machine Translation",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_127",
            "content": "UNKNOWN, None, 2007, Announcing czeng 2.0 parallel corpus with over 2 gigawords. CoRR, abs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": "2007",
                "pub_title": "Announcing czeng 2.0 parallel corpus with over 2 gigawords. CoRR, abs",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_128",
            "content": "Philipp Koehn, Statistical significance tests for machine translation evaluation, 2004, Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Philipp Koehn"
                ],
                "title": "Statistical significance tests for machine translation evaluation",
                "pub_date": "2004",
                "pub_title": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_129",
            "content": "Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond\u0159ej Bojar, Alexandra Constantin, Evan Herbst, Moses: Open source toolkit for statistical machine translation, 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Philipp Koehn",
                    "Hieu Hoang",
                    "Alexandra Birch",
                    "Chris Callison-Burch",
                    "Marcello Federico",
                    "Nicola Bertoldi",
                    "Brooke Cowan",
                    "Wade Shen",
                    "Christine Moran",
                    "Richard Zens",
                    "Chris Dyer",
                    "Ond\u0159ej Bojar",
                    "Alexandra Constantin",
                    "Evan Herbst"
                ],
                "title": "Moses: Open source toolkit for statistical machine translation",
                "pub_date": "2007",
                "pub_title": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_130",
            "content": "Julia Kreutzer, Artem Sokolov, Learning to segment inputs for NMT favors character-level processing, 2018, Proceedings of the 15th International Conference on Spoken Language Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Julia Kreutzer",
                    "Artem Sokolov"
                ],
                "title": "Learning to segment inputs for NMT favors character-level processing",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 15th International Conference on Spoken Language Translation",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_131",
            "content": "Taku Kudo, John Richardson, SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Taku Kudo",
                    "John Richardson"
                ],
                "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_132",
            "content": "Jason Lee, Kyunghyun Cho, Thomas Hofmann, Fully character-level neural machine translation without explicit segmentation, 2017, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Jason Lee",
                    "Kyunghyun Cho",
                    "Thomas Hofmann"
                ],
                "title": "Fully character-level neural machine translation without explicit segmentation",
                "pub_date": "2017",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_133",
            "content": "Jiahuan Li, Yutong Shen, Shujian Huang, Xinyu Dai, Jiajun Chen, When is char better than subword: A systematic study of segmentation algorithms for neural machine translation, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Jiahuan Li",
                    "Yutong Shen",
                    "Shujian Huang",
                    "Xinyu Dai",
                    "Jiajun Chen"
                ],
                "title": "When is char better than subword: A systematic study of segmentation algorithms for neural machine translation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "35-ARR_v2_134",
            "content": "Xian Li, Paul Michel, Antonios Anastasopoulos, Yonatan Belinkov, Nadir Durrani, Orhan Firat, Philipp Koehn, Graham Neubig, Juan Pino, Hassan Sajjad, Findings of the first shared task on machine translation robustness, 2019, Proceedings of the Fourth Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Xian Li",
                    "Paul Michel",
                    "Antonios Anastasopoulos",
                    "Yonatan Belinkov",
                    "Nadir Durrani",
                    "Orhan Firat",
                    "Philipp Koehn",
                    "Graham Neubig",
                    "Juan Pino",
                    "Hassan Sajjad"
                ],
                "title": "Findings of the first shared task on machine translation robustness",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fourth Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_135",
            "content": "Jind\u0159ich Libovick\u00fd, Alexander Fraser, Towards reasonably-sized character-level transformer NMT by finetuning subword systems, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Jind\u0159ich Libovick\u00fd",
                    "Alexander Fraser"
                ],
                "title": "Towards reasonably-sized character-level transformer NMT by finetuning subword systems",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_136",
            "content": "Minh-Thang Luong, Christopher Manning, Achieving open vocabulary neural machine translation with hybrid word-character models, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Minh-Thang Luong",
                    "Christopher Manning"
                ],
                "title": "Achieving open vocabulary neural machine translation with hybrid word-character models",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_137",
            "content": "Dipankar Sainik Kumar Mahata, Sivaji Das,  Bandyopadhyay, JUCBNMT at WMT2018 news translation task: Character based neural machine translation of Finnish to English, 2018, Proceedings of the Third Conference on Machine Translation: Shared Task Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Dipankar Sainik Kumar Mahata",
                    "Sivaji Das",
                    " Bandyopadhyay"
                ],
                "title": "JUCBNMT at WMT2018 news translation task: Character based neural machine translation of Finnish to English",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_138",
            "content": "UNKNOWN, None, , , Vassilis Plachouras.",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": "Vassilis Plachouras"
            }
        },
        {
            "ix": "35-ARR_v2_139",
            "content": "Fabrizio Silvestri, Sebastian Riedel, How decoding strategies affect the verifiability of generated text, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Fabrizio Silvestri",
                    "Sebastian Riedel"
                ],
                "title": "How decoding strategies affect the verifiability of generated text",
                "pub_date": "2020",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_140",
            "content": "Clara Meister, Ryan Cotterell, Tim Vieira, If beam search is the answer, what was the question?, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Clara Meister",
                    "Ryan Cotterell",
                    "Tim Vieira"
                ],
                "title": "If beam search is the answer, what was the question?",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_141",
            "content": "I Nikola, Yuhuang Nikolov, Mi Hu, Richard Tan,  Hahnloser, Character-level Chinese-English translation through ASCII encoding, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "I Nikola",
                    "Yuhuang Nikolov",
                    "Mi Hu",
                    "Richard Tan",
                    " Hahnloser"
                ],
                "title": "Character-level Chinese-English translation through ASCII encoding",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Third Conference on Machine Translation: Research Papers",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_142",
            "content": "Robert \u00d6stling, Yves Scherrer, J\u00f6rg Tiedemann, Gongbo Tang, Tommi Nieminen, The Helsinki neural machine translation system, 2017, Proceedings of the Second Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Robert \u00d6stling",
                    "Yves Scherrer",
                    "J\u00f6rg Tiedemann",
                    "Gongbo Tang",
                    "Tommi Nieminen"
                ],
                "title": "The Helsinki neural machine translation system",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Second Conference on Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_143",
            "content": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli, fairseq: A fast, extensible toolkit for sequence modeling, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Myle Ott",
                    "Sergey Edunov",
                    "Alexei Baevski",
                    "Angela Fan",
                    "Sam Gross",
                    "Nathan Ng",
                    "David Grangier",
                    "Michael Auli"
                ],
                "title": "fairseq: A fast, extensible toolkit for sequence modeling",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_144",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "pub_date": "2002",
                "pub_title": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_145",
            "content": "Martin Popel, Ond\u0159ej Bojar, Training Tips for the Transformer Model, 2018, The Prague Bulletin of Mathematical Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Martin Popel",
                    "Ond\u0159ej Bojar"
                ],
                "title": "Training Tips for the Transformer Model",
                "pub_date": "2018",
                "pub_title": "The Prague Bulletin of Mathematical Linguistics",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_146",
            "content": "Maja Popovi\u0107, chrF: character n-gram F-score for automatic MT evaluation, 2015, Proceedings of the Tenth Workshop on Statistical Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Maja Popovi\u0107"
                ],
                "title": "chrF: character n-gram F-score for automatic MT evaluation",
                "pub_date": "2015",
                "pub_title": "Proceedings of the Tenth Workshop on Statistical Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_147",
            "content": "Matt Post, A call for clarity in reporting BLEU scores, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "Matt Post"
                ],
                "title": "A call for clarity in reporting BLEU scores",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Third Conference on Machine Translation: Research Papers",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_148",
            "content": "Ivan Provilkov, Dmitrii Emelianenko, Elena Voita, BPE-dropout: Simple and effective subword regularization, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": [
                    "Ivan Provilkov",
                    "Dmitrii Emelianenko",
                    "Elena Voita"
                ],
                "title": "BPE-dropout: Simple and effective subword regularization",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_149",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-totext transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": [
                    "Colin Raffel",
                    "Noam Shazeer",
                    "Adam Roberts",
                    "Katherine Lee",
                    "Sharan Narang",
                    "Michael Matena",
                    "Yanqi Zhou",
                    "Wei Li",
                    "Peter Liu"
                ],
                "title": "Exploring the limits of transfer learning with a unified text-totext transformer",
                "pub_date": "2020",
                "pub_title": "Journal of Machine Learning Research",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_150",
            "content": "Ricardo Rei, Craig Stewart, Ana Farinha, Alon Lavie, COMET: A neural framework for MT evaluation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": [
                    "Ricardo Rei",
                    "Craig Stewart",
                    "Ana Farinha",
                    "Alon Lavie"
                ],
                "title": "COMET: A neural framework for MT evaluation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_151",
            "content": "Yves Scherrer, Ra\u00fal V\u00e1zquez, Sami Virpioja, The University of Helsinki submissions to the WMT19 similar language translation task, 2019, Proceedings of the Fourth Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b55",
                "authors": [
                    "Yves Scherrer",
                    "Ra\u00fal V\u00e1zquez",
                    "Sami Virpioja"
                ],
                "title": "The University of Helsinki submissions to the WMT19 similar language translation task",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fourth Conference on Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_152",
            "content": "Rico Sennrich, How grammatical is characterlevel neural machine translation? assessing MT quality with contrastive translation pairs, 2017, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b56",
                "authors": [
                    "Rico Sennrich"
                ],
                "title": "How grammatical is characterlevel neural machine translation? assessing MT quality with contrastive translation pairs",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_153",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Neural machine translation of rare words with subword units, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b57",
                "authors": [
                    "Rico Sennrich",
                    "Barry Haddow",
                    "Alexandra Birch"
                ],
                "title": "Neural machine translation of rare words with subword units",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "35-ARR_v2_154",
            "content": "Rico Sennrich, Biao Zhang, Revisiting lowresource neural machine translation: A case study, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b58",
                "authors": [
                    "Rico Sennrich",
                    "Biao Zhang"
                ],
                "title": "Revisiting lowresource neural machine translation: A case study",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_155",
            "content": "Uri Shaham, Omer Levy, Neural machine translation without embeddings, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b59",
                "authors": [
                    "Uri Shaham",
                    "Omer Levy"
                ],
                "title": "Neural machine translation without embeddings",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_156",
            "content": "UNKNOWN, None, 2021, What do you get when you cross beam search with nucleus sampling?, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b60",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "What do you get when you cross beam search with nucleus sampling?",
                "pub": "CoRR"
            }
        },
        {
            "ix": "35-ARR_v2_157",
            "content": "UNKNOWN, None, 2012, Why neural machine translation prefers empty outputs. CoRR, abs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b61",
                "authors": null,
                "title": null,
                "pub_date": "2012",
                "pub_title": "Why neural machine translation prefers empty outputs. CoRR, abs",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_158",
            "content": "Lucia Specia, Zhenhao Li, Juan Pino, Vishrav Chaudhary, Francisco Guzm\u00e1n, Graham Neubig, Nadir Durrani, Yonatan Belinkov, Philipp Koehn, Hassan Sajjad, Paul Michel, Xian Li, 2020. Findings of the WMT 2020 shared task on machine translation robustness, , Proceedings of the Fifth Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b62",
                "authors": [
                    "Lucia Specia",
                    "Zhenhao Li",
                    "Juan Pino",
                    "Vishrav Chaudhary",
                    "Francisco Guzm\u00e1n",
                    "Graham Neubig",
                    "Nadir Durrani",
                    "Yonatan Belinkov",
                    "Philipp Koehn",
                    "Hassan Sajjad",
                    "Paul Michel",
                    "Xian Li"
                ],
                "title": "2020. Findings of the WMT 2020 shared task on machine translation robustness",
                "pub_date": null,
                "pub_title": "Proceedings of the Fifth Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_159",
            "content": "UNKNOWN, None, 2015, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b63",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_160",
            "content": "Gabriel Stanovsky, Noah Smith, Luke Zettlemoyer, Evaluating gender bias in machine translation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b64",
                "authors": [
                    "Gabriel Stanovsky",
                    "Noah Smith",
                    "Luke Zettlemoyer"
                ],
                "title": "Evaluating gender bias in machine translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_161",
            "content": "Milan Straka, Jana Strakov\u00e1, Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe, 2017, Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b65",
                "authors": [
                    "Milan Straka",
                    "Jana Strakov\u00e1"
                ],
                "title": "Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe",
                "pub_date": "2017",
                "pub_title": "Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_162",
            "content": "Ilya Sutskever, Oriol Vinyals, V Quoc,  Le, Sequence to sequence learning with neural networks, 2014-12-08, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b66",
                "authors": [
                    "Ilya Sutskever",
                    "Oriol Vinyals",
                    "V Quoc",
                    " Le"
                ],
                "title": "Sequence to sequence learning with neural networks",
                "pub_date": "2014-12-08",
                "pub_title": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_163",
            "content": "UNKNOWN, None, 2021, Charformer: Fast character transformers via gradient-based subword tokenization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b67",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Charformer: Fast character transformers via gradient-based subword tokenization",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_164",
            "content": "Du\u0161an Vari\u0161, Ond\u0159ej Bojar, CUNI system for WMT17 automatic post-editing task, 2017, Proceedings of the Second Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b68",
                "authors": [
                    "Du\u0161an Vari\u0161",
                    "Ond\u0159ej Bojar"
                ],
                "title": "CUNI system for WMT17 automatic post-editing task",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Second Conference on Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "35-ARR_v2_165",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b69",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "Lukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017-12-04",
                "pub_title": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_166",
            "content": "Xiao Wang, Qin Liu, Tao Gui, Qi Zhang, Yicheng Zou, Xin Zhou, Jiacheng Ye, Yongxin Zhang, Rui Zheng, Zexiong Pang, Qinzhuo Wu, Zhengyan Li, Chong Zhang, Ruotian Ma, Zichu Fei, Ruijian Cai, Jun Zhao, Xingwu Hu, Zhiheng Yan, Yiding Tan, Yuan Hu, Qiyuan Bian, Zhihua Liu, Shan Qin, Bolin Zhu, Xiaoyu Xing, Jinlan Fu, Yue Zhang, Minlong Peng, Xiaoqing Zheng, Yaqian Zhou, Zhongyu Wei, TextFlint: Unified multilingual robustness evaluation toolkit for natural language processing, , Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b70",
                "authors": [
                    "Xiao Wang",
                    "Qin Liu",
                    "Tao Gui",
                    "Qi Zhang",
                    "Yicheng Zou",
                    "Xin Zhou",
                    "Jiacheng Ye",
                    "Yongxin Zhang",
                    "Rui Zheng",
                    "Zexiong Pang",
                    "Qinzhuo Wu",
                    "Zhengyan Li",
                    "Chong Zhang",
                    "Ruotian Ma",
                    "Zichu Fei",
                    "Ruijian Cai",
                    "Jun Zhao",
                    "Xingwu Hu",
                    "Zhiheng Yan",
                    "Yiding Tan",
                    "Yuan Hu",
                    "Qiyuan Bian",
                    "Zhihua Liu",
                    "Shan Qin",
                    "Bolin Zhu",
                    "Xiaoyu Xing",
                    "Jinlan Fu",
                    "Yue Zhang",
                    "Minlong Peng",
                    "Xiaoqing Zheng",
                    "Yaqian Zhou",
                    "Zhongyu Wei"
                ],
                "title": "TextFlint: Unified multilingual robustness evaluation toolkit for natural language processing",
                "pub_date": null,
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_167",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger,  Drame, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b71",
                "authors": [
                    "Thomas Wolf",
                    "Lysandre Debut",
                    "Victor Sanh",
                    "Julien Chaumond",
                    "Clement Delangue",
                    "Anthony Moi",
                    "Pierric Cistac",
                    "Tim Rault",
                    "Remi Louf",
                    "Morgan Funtowicz",
                    "Joe Davison",
                    "Sam Shleifer",
                    "Clara Patrick Von Platen",
                    "Yacine Ma",
                    "Julien Jernite",
                    "Canwen Plu",
                    "Teven Xu",
                    "Sylvain Scao",
                    "Mariama Gugger",
                    " Drame"
                ],
                "title": "Transformers: State-of-the-art natural language processing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_168",
            "content": "UNKNOWN, None, 2021, Byt5: Towards a tokenfree future with pre-trained byte-to-byte models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b72",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Byt5: Towards a tokenfree future with pre-trained byte-to-byte models",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_169",
            "content": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021b. mT5: A massively multilingual pre-trained text-to-text transformer, , Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b73",
                "authors": [
                    "Linting Xue",
                    "Noah Constant",
                    "Adam Roberts",
                    "Mihir Kale",
                    "Rami Al-Rfou",
                    "Aditya Siddhant"
                ],
                "title": "Aditya Barua, and Colin Raffel. 2021b. mT5: A massively multilingual pre-trained text-to-text transformer",
                "pub_date": null,
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "35-ARR_v2_170",
            "content": "Longtu Zhang, Mamoru Komachi, Neural machine translation of logographic language using sub-character level information, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b74",
                "authors": [
                    "Longtu Zhang",
                    "Mamoru Komachi"
                ],
                "title": "Neural machine translation of logographic language using sub-character level information",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Third Conference on Machine Translation: Research Papers",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "35-ARR_v2_0@0",
            "content": "Why don't people use character-level machine translation?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_0",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_2@0",
            "content": "We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_2",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_2@1",
            "content": "Despite evidence in the literature that character-level systems are comparable with subword systems, they are virtually never used in competitive setups in WMT competitions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_2",
            "start": 149,
            "end": 321,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_2@2",
            "content": "We empirically show that even with recent modeling innovations in characterlevel natural language processing, characterlevel MT systems still struggle to match their subword-based counterparts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_2",
            "start": 323,
            "end": 515,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_2@3",
            "content": "Character-level MT systems show neither better domain robustness, nor better morphological generalization, despite being often so motivated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_2",
            "start": 517,
            "end": 656,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_2@4",
            "content": "However, we are able to show robustness towards source side noise and that translation quality does not degrade with increasing beam size at decoding time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_2",
            "start": 658,
            "end": 812,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_4@0",
            "content": "The progress in natural language processing (NLP) brought by deep learning is often narrated as removing assumptions about the input data and letting the models learn everything end-to-end.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_4",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_4@1",
            "content": "One of the assumptions about input data that seems to resist this trend is (at least partially) linguistically motivated segmentation of input data in machine translation (MT) and NLP in general.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_4",
            "start": 190,
            "end": 384,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_5@0",
            "content": "For NMT, several papers have claimed parity of character-based methods with subword models, highlighting advantageous features of such systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_5",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_5@1",
            "content": "Very recent examples include Gao et al. (2020); Banar et al. (2020); Li et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_5",
            "start": 144,
            "end": 229,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_5@2",
            "content": "Despite this, character-level methods are rarely used as strong baselines in research papers and shared task submissions, suggesting that character-level models might have drawbacks that are not sufficiently addressed in the literature.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_5",
            "start": 231,
            "end": 466,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_6@0",
            "content": "In this paper, we examine what the state of the art in character-level MT really is.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_6",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_6@1",
            "content": "We survey existing methods and conduct a meta-analysis of the input segmentation methods used in WMT shared task submissions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_6",
            "start": 85,
            "end": 209,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_6@2",
            "content": "We then systematically compare the most recent character-processing architectures, some of them taken from general NLP research and used for the first time in MT. Further, we propose an alternative two-step decoder architecture that unlike standard decoders does not suffer from a slow-down due to the length of character sequences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_6",
            "start": 211,
            "end": 542,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_6@3",
            "content": "Following the recent findings on MT decoding, we evaluate different decoding strategies in the character-level context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_6",
            "start": 544,
            "end": 662,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_7@0",
            "content": "Many previous studies on character-level MT drew their conclusions from experiments on rather small datasets and focused only on quantitatively assessed translation quality without further analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_7",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_7@1",
            "content": "To compensate for this, we revisit and systematically evaluate the state-of-the-art approaches to character-level neural MT and identify their major strengths and weaknesses on large datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_7",
            "start": 199,
            "end": 390,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_8@0",
            "content": "Character-Level Neural MT",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_8",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_9@0",
            "content": "Character-level processing was hardly possible within the statistical MT paradigm that assumed the existence of phrases consisting of semantically rich tokens that roughly correspond to words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_9",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_9@1",
            "content": "Neural sequence-to-sequence models (Sutskever et al., 2014;Bahdanau et al., 2015;Vaswani et al., 2017) do not explicitly work with this assumption.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_9",
            "start": 193,
            "end": 339,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_9@2",
            "content": "In theory, they can learn to transform any sequence into any sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_9",
            "start": 341,
            "end": 410,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_10@0",
            "content": "The original sequence-to-sequence models used word-based vocabularies of a limited size and which led to a relatively frequent occurrence of out-of-vocabulary tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_10",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_10@1",
            "content": "A typical solution to that problem is subword segmentation (Sennrich et al., 2016;Kudo and Richardson, 2018), which keeps frequent tokens intact and splits less frequent ones into smaller units.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_10",
            "start": 167,
            "end": 360,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_11@0",
            "content": "Modeling language on the character level is attractive because it can help overcome several problems of subword models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_11",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_11@1",
            "content": "One-hot representations of words or subwords do not reflect systematic character-level relations between words, potentially harming morphologically rich languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_11",
            "start": 120,
            "end": 282,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_11@2",
            "content": "With subwords, minor typos on the source side lead to radically different input representations resulting in low robustness towards source-side noise (Provilkov et al., 2020;Libovick\u00fd and Fraser, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_11",
            "start": 284,
            "end": 485,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_11@3",
            "content": "Models using recurrent neural networks (RNNs) showed early success with character-level segmentation on the decoder side (Chung et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_11",
            "start": 487,
            "end": 628,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_11@4",
            "content": "Using character-level processing on the encoder side proved harder which was attributed to the features of the attention mechanism which can presumably benefit from semantically rich units (such as subwords) in the encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_11",
            "start": 630,
            "end": 852,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_11@5",
            "content": "Following this line of thinking, Lee et al. (2017) introduced 1D convolutions with max-pooling that pre-process the character sequence into a sequence of latent wordlike states.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_11",
            "start": 854,
            "end": 1030,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_11@6",
            "content": "Coupled with a character-level decoder, they claimed to match the state-of-the-art subwordbased models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_11",
            "start": 1032,
            "end": 1134,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_11@7",
            "content": "Even though this architecture works well on the character level, it does not generalize further to the byte level .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_11",
            "start": 1136,
            "end": 1250,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_11@8",
            "content": "Hybrid approaches combining tokenization into words with the computation of character-based word representations were successfully used with RNNs (Luong and Manning, 2016;Gr\u00f6nroos et al., 2017;Ataman et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_11",
            "start": 1252,
            "end": 1465,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_11@9",
            "content": "Later, Cherry et al. (2018) showed that RNNs perform on par with subword models without changing the model architecture if the models are sufficiently large.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_11",
            "start": 1467,
            "end": 1623,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_11@10",
            "content": "Kreutzer and Sokolov (2018) support this by showing that RNN models which learn segmentation jointly with the rest of the model are close to character-level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_11",
            "start": 1625,
            "end": 1781,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_12@0",
            "content": "Character-level modeling with Transformers appears to be more difficult.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_12",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_12@1",
            "content": "Gupta et al. (2019) used Transparent Attention to train deep character-level models and needed up to 32 layers to close the gap between the BPE and character models, which makes the model too large for practical use.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_12",
            "start": 73,
            "end": 288,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_12@2",
            "content": "Libovick\u00fd and Fraser (2020) narrowed the gap between subword and character modeling using curriculum learning by finetuning subword models to character-level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_12",
            "start": 290,
            "end": 447,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_12@3",
            "content": "Gao et al. (2020) proposed adding a convolutional sub-layer in the Transformer layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_12",
            "start": 449,
            "end": 534,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_12@4",
            "content": "At the cost of a 30% increase in parameter count, they managed to narrow the gap between subword-and character-based models by half.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_12",
            "start": 536,
            "end": 667,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_12@5",
            "content": "Banar et al. (2020) reused the convolutional preprocessing layer with constant-size segments of Lee et al. (2017)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_12",
            "start": 669,
            "end": 781,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_13@0",
            "content": "Transformer model for translation into English. Without changing the decoder, they reached comparable, but usually slightly worse, translation quality compared to BPE-based models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_13",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_14@0",
            "content": "Shaham and Levy (2021a) revisited characterand byte-level MT on rather small IWSLT datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_14",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_14@1",
            "content": "Their results show that character-level and bytelevel models are usually worse than BPE models, but byte-based models without embedding layers often outperform BPE-based models in the out-of-English direction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_14",
            "start": 93,
            "end": 301,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_14@2",
            "content": "Using similarly small datasets, Li et al. (2021) claim that character-level modeling outperforms BPE when translating into fusional, agglutinative, and introflexive languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_14",
            "start": 303,
            "end": 477,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_14@3",
            "content": "Nikolov et al. (2018) experimented with character-level models for romanized Chinese.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_14",
            "start": 479,
            "end": 563,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_14@4",
            "content": "These models performed comparable to models using logographic signs, but significantly worse than models using subwords.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_14",
            "start": 565,
            "end": 684,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_14@5",
            "content": "Zhang and Komachi (2018) argued that signs in logographic languages carry too much information and were able to improve the translation quality by segmenting Chinese and Japanese into sub-character units while keeping subword segmentation on the English side.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_14",
            "start": 686,
            "end": 944,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_15@0",
            "content": "Little is known about other properties of character-level MT beyond the overall translation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_15",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_15@1",
            "content": "Sennrich (2017) prepared a set of contrastive English-German sentence pairs and tested them using shallow RNN-based models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_15",
            "start": 101,
            "end": 223,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_15@2",
            "content": "They observed that character-based models transliterated better, but captured morphosyntactic agreement worse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_15",
            "start": 225,
            "end": 334,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_15@3",
            "content": "Libovick\u00fd and Fraser (2020) evaluated Transformer-based character-level models using MorphEval and came to mixed conclusions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_15",
            "start": 336,
            "end": 460,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_15@4",
            "content": "Gupta et al. (2019) and Libovick\u00fd and Fraser (2020) make claims about the noise robustness of the character-level models using synthetic noise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_15",
            "start": 462,
            "end": 604,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_15@5",
            "content": "Li et al. (2021) evaluated domain robustness by training models on small domain-specific datasets and evaluating them on unrelated domains, claiming the superiority of character-level models in this setup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_15",
            "start": 606,
            "end": 810,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_15@6",
            "content": "On the other hand, Gupta et al. (2019) evaluated the domain robustness in a more natural setup and did not observe higher robustness when evaluating general domain models on domain-specific tests compared to BPE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_15",
            "start": 812,
            "end": 1023,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_16@0",
            "content": "Another consideration is longer training and inference times.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_16",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_16@1",
            "content": "Character-level systems are significantly slower due to the increased sequence length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_16",
            "start": 62,
            "end": 147,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_16@2",
            "content": "Libovick\u00fd and Fraser (2020) reported a 5.6-fold slowdown at training time and a 4.7-fold slowdown at inference time compared to subword models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_16",
            "start": 149,
            "end": 291,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_17@0",
            "content": "Recent research on character-level modeling goes beyond MT. Pre-trained multilingual representations are a particularly active area.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_17",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_17@1",
            "content": "Clark et al. (2021) propose CANINE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_17",
            "start": 133,
            "end": 167,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_17@2",
            "content": "The model shrinks character sequences into fewer hidden states (similar to Lee et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_17",
            "start": 169,
            "end": 261,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_17@3",
            "content": "They use local self-attention and strided convolutions (instead of highway layers and max-pooling as in Lee's work).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_17",
            "start": 263,
            "end": 378,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_17@4",
            "content": "Their model is either trained using the masked-language-modeling objective (Devlin et al., 2019) with subword supervision, or in an encoder-decoder setup similar to Raffel et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_17",
            "start": 380,
            "end": 565,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_17@5",
            "content": "Both methods reach a representation quality comparable to similar subword models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_17",
            "start": 567,
            "end": 647,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_18@0",
            "content": "ByT5 (Xue et al., 2021a) and Charformer (Tay et al., 2021) are based on the mT5 model (Xue et al., 2021b) which uses sequence-to-sequence denoising pre-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_18",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_18@1",
            "content": "Whereas byT5 only uses byte sequences instead of subwords and differs in hyperparameters, Charformer uses convolution and combines character blocks to obtain latent subword representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_18",
            "start": 162,
            "end": 350,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_18@2",
            "content": "These models mostly reach similar results to sub-word models, occasionally outperforming a few of them, in the case of Charformer without a significant slowdown.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_18",
            "start": 352,
            "end": 512,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_19@0",
            "content": "WMT submissions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_19",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_20@0",
            "content": "The Conference on Machine Translation (WMT) organizes annual shared tasks in various use cases of MT. The shared task submissions focus on translation quality rather than the novelty of presented ideas, as most other research papers do.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_20",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_20@1",
            "content": "Therefore, we assume that, if character-level models were a fully-fledged alternative to subword models, at least some systems submitted to the shared tasks would use character-level models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_20",
            "start": 237,
            "end": 426,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_21@0",
            "content": "We annotated recent system description papers with the input and output segmentation method they used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_21",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_21@1",
            "content": "We focused on information about experiments with character-level models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_21",
            "start": 103,
            "end": 174,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_21@2",
            "content": "Since we are primarily interested in the Transformer architecture that became the standard after 2017, we only included system description papers from 2018-2020 Barrault et al., 2019Barrault et al., , 2020.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_21",
            "start": 176,
            "end": 381,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_22@0",
            "content": "Transformers were used in 81%, 87%, and 97% of the systems in the respective years.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_22",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_22@1",
            "content": "We included the main task on WMT, news translation, and two minor tasks where character-level methods might help: translation robustness (Li et al., 2019;Specia et al., 2020) and translation between similar languages (ibid.).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_22",
            "start": 84,
            "end": 308,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_22@2",
            "content": "Almost all systems use a subword-based vocabulary (BPE: 81%, 71%, 66% in the respective years; SentencePiece: None in 2018, 9% and 25% in the following ones).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_22",
            "start": 310,
            "end": 467,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_22@3",
            "content": "Purely word-based (none in 2018, 2% and 3% in the later years) or morphological segmentation (4%, 2%, 3% in the respective years) are rarely used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_22",
            "start": 469,
            "end": 614,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_22@4",
            "content": "The average vocabulary size decreases over time (see Figure 2) with a median size remaining at 32k in the last two years.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_22",
            "start": 616,
            "end": 736,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_22@5",
            "content": "The reason for the decreasing average is probably a higher proportion of systems for low-resource languages, where a smaller vocabulary leads to better translation quality (Sennrich and Zhang, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_22",
            "start": 738,
            "end": 936,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_23@0",
            "content": "Among the 145 annotated system description papers, there were only two that used characterlevel segmentation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_23",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_23@1",
            "content": "Mahata et al. (2018) used a character-level model for Finnish-to-English translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_23",
            "start": 110,
            "end": 194,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_23@2",
            "content": "This system, however, makes many suboptimal design choices and ended up as the last one in the manual evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_23",
            "start": 196,
            "end": 308,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_23@3",
            "content": "Scherrer et al. (2019) experimented with character-level systems for similar language translation and observed that characters outperform other segmentations for Spanish-Portuguese translation, but not for Czech-Polish.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_23",
            "start": 310,
            "end": 528,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_23@4",
            "content": "Knowles et al. (2020) experimented with different subword vocabulary sizes for English-Inuktikut translation and reached the best results using a subword vocabulary of size 1k, which makes it close to the character level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_23",
            "start": 530,
            "end": 750,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_23@5",
            "content": "Most of the papers do not even mention character-level segmentation as a viable alternative they would like to pursue in future work (7% in 2018, 2% in 2019, none in 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_23",
            "start": 752,
            "end": 923,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_24@0",
            "content": "Character-level methods were more frequently used in WMT17 with RNN-based systems, especially for translation of Finnish \u00d6stling et al., 2017) and less successfully for Chinese (Holtz et al., 2017) and the automatic post-editing task (Vari\u0161 and Bojar, 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_24",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_25@0",
            "content": "On the other hand, Figure 1 shows that the research interest in character-level methods remains approximately the same, or may have slightly increased.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_25",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_25@1",
            "content": "For practical solutions in WMT systems, we clearly show that system designers in the WMT community have avoided character-level models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_25",
            "start": 152,
            "end": 286,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_26@0",
            "content": "We speculate that the main reasons for not considering character-level modeling are its lower efficiency and the fact that the literature shows no clear improvement of translation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_26",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_26@1",
            "content": "Most of the submissions use back-translation (85%, 82%, and 94% in the respective years), often iterated several times (11%, 20%, 16%), which requires both training and inference on large datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_26",
            "start": 189,
            "end": 385,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_26@2",
            "content": "With the approximately 5-fold slowdown, WMT-scale experiments on character models are not easily tractable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_26",
            "start": 387,
            "end": 493,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_27@0",
            "content": "Evaluated Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_27",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_28@0",
            "content": "We evaluate several Transformer-based architectures for character-level MT. A major issue with character-level sequence processing is the sequence length and low information density compared to subword sequences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_28",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_28@1",
            "content": "Architectures for characterlevel sequence processing typically address this issue by locally processing and shrinking the sequences into latent word-like units.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_28",
            "start": 213,
            "end": 372,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_28@2",
            "content": "In our experiments, we explore several ways to do this.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_28",
            "start": 374,
            "end": 428,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_29@0",
            "content": "First, we directly use character embeddings as input to the Transformer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_29",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_29@1",
            "content": "Second, following Banar et al. (2020), we use the convolutional character processing layers proposed by Lee et al. (2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_29",
            "start": 73,
            "end": 194,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_29@2",
            "content": "Third, we replace the convolutions with local selfattention as proposed in the CANINE model (Clark et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_29",
            "start": 196,
            "end": 308,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_29@3",
            "content": "Finally, we use the recently proposed Charformer architecture (Tay et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_29",
            "start": 310,
            "end": 390,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_30@0",
            "content": "Lee-style encoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_30",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_30@1",
            "content": "Lee et al. (2017) process the sequence of character embeddings with convolutions of different kernel sizes and number of output channels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_30",
            "start": 20,
            "end": 156,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_30@2",
            "content": "In the original paper, this was followed by 4 highway layers (Srivastava et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_30",
            "start": 158,
            "end": 244,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_30@3",
            "content": "In our preliminary experiments, we observed that a too deep stack of highway layers leads to diminishing gradients, and we replaced the second two High-way layers with feedforward sublayers as used in the Transformer architecture (Vaswani et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_30",
            "start": 246,
            "end": 498,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_30@4",
            "content": "CANINE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_30",
            "start": 500,
            "end": 506,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_30@5",
            "content": "Clark et al. (2021) experiment with character-level pre-trained sentence representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_30",
            "start": 508,
            "end": 596,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_30@6",
            "content": "The character-processing architecture is in principle similar to Lee et al. (2017) but uses more modern building blocks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_30",
            "start": 598,
            "end": 717,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_30@7",
            "content": "Character embeddings are processed by a Transformer layer with local selfattention which only allows the states to attend to states in their neighborhood.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_30",
            "start": 719,
            "end": 872,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_30@8",
            "content": "This is followed by downsampling using strided convolution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_30",
            "start": 874,
            "end": 932,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_31@0",
            "content": "Originally, CANINE used a local self-attention span as long as 128 characters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_31",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_31@1",
            "content": "In the case of MT, this would usually span the entire sentence, so we use significantly shorter spans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_31",
            "start": 79,
            "end": 180,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_32@0",
            "content": "Charformer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_32",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_32@1",
            "content": "Unlike previous approaches, Charformer (Tay et al., 2021) does not apply a nonlinearity on the embeddings and gets latent subword representations by repeated averaging of character embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_32",
            "start": 12,
            "end": 203,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_32@2",
            "content": "First, it processes the sequence using a 1D convolution, so the states are aware of their mutual local positions in local neighborhoods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_32",
            "start": 205,
            "end": 340,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_32@3",
            "content": "Second, non-overlapping character n-grams of length up to N are represented by averages of the respective character embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_32",
            "start": 342,
            "end": 468,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_32@4",
            "content": "This means that for each character, there is a vector that represents the character as a member of n-grams of length 1 to N .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_32",
            "start": 470,
            "end": 594,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_32@5",
            "content": "In the third step, the character blocks are scored with a scoring function (a linear transformation), which can be interpreted as attention over the N different n-gram lengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_32",
            "start": 596,
            "end": 771,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_32@6",
            "content": "The attention scores are used to compute a weighted average over the n-gram representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_32",
            "start": 773,
            "end": 864,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_32@7",
            "content": "Finally, the sequence is downsampled using mean-pooling with window size and stride size N (i.e., the maximum n-gram size).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_32",
            "start": 866,
            "end": 988,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_33@0",
            "content": "Whereas Lee-style encoding allows using lowdimensional character embeddings and keeps most parameters in the convolutional layers, CANINE and Charformer need the character representation to have the same dimension as the following Transformer layer stack.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_33",
            "start": 0,
            "end": 254,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_34@0",
            "content": "Two-step decoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_34",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_34@1",
            "content": "The architectures mentioned above allow the Transformer layers to operate more efficiently with a shorter and more information-dense sequence of states.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_34",
            "start": 19,
            "end": 170,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_34@2",
            "content": "However, while decoding, we need to generate the target character sequence in the original length, by outputting a block of characters in each decoding step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_34",
            "start": 172,
            "end": 328,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_34@3",
            "content": "Our preliminary experiments showed that generating blocks of characters non-autoregressively leads to incoherent output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_34",
            "start": 330,
            "end": 449,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_34@4",
            "content": "Therefore, we propose a twostep decoding architecture where the stack of Transformer layers operating over the downsampled sequence is followed by a lightweight LSTM autoregressive decoder (see Figure 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_34",
            "start": 451,
            "end": 654,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_34@5",
            "content": "The input to the LSTM decoder is a concatenation of the embedding of the previously generated character and a projection of the Transformer decoder output state.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_34",
            "start": 656,
            "end": 816,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_34@6",
            "content": "At inference time, the LSTM decoder generates a block of characters and inputs them to the character-level processing layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_34",
            "start": 818,
            "end": 941,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_34@7",
            "content": "The Transformer decoder computes an output state that the LSTM decoder uses to generate another character block.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_34",
            "start": 943,
            "end": 1054,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_34@8",
            "content": "More details are in Appendix A.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_34",
            "start": 1056,
            "end": 1086,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_35@0",
            "content": "Modifying Charformer for the two-step decoding would require a long padding at the beginning of the sequence causing the decoder to diverge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_35",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_35@1",
            "content": "Because of that, we use Lee-style encoding on the decoder side when using Charformer in the encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_35",
            "start": 141,
            "end": 240,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_36@0",
            "content": "First, we conduct all our experiments on the small IWSLT datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_36",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_36@1",
            "content": "Then we evaluate the most promising architectures on larger datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_36",
            "start": 67,
            "end": 135,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_37@0",
            "content": "Experiments on Small Data",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_37",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_38@0",
            "content": "We implement the models using Huggingface Transformers (Wolf et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_38",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_38@1",
            "content": "We take the CA-NINE layer from Huggingface Transformers and use an independent implementation of Charformer 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_38",
            "start": 76,
            "end": 186,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_38@2",
            "content": "Our source code is available on Github.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_38",
            "start": 188,
            "end": 226,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_38@3",
            "content": "2 Hyperparameters and other experimental details can be found in Appendix B.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_38",
            "start": 228,
            "end": 303,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_39@0",
            "content": "Experimental Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_39",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_40@0",
            "content": "We evaluate the models on translation between English paired with German, French, and Arabic (with English as both input and output) using the IWSLT 2017 datasets (Cettolo et al., 2017) with a training data size of around 200k sentences for each language pair (see Appendix B for details).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_40",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_41@0",
            "content": "For the subword models, we tokenize the input using the Moses tokenizer (Koehn et al., 2007) and then further split the words into subword units using BPE (Sennrich et al., 2016) with 16k merge operations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_41",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_41@1",
            "content": "For the character models, we limit the vocabulary to 300 UTF-8 characters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_41",
            "start": 206,
            "end": 279,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_42@0",
            "content": "We use the Transformer Base architecture (Vaswani et al., 2017) in all experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_42",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_42@1",
            "content": "We make no changes to it in the subword and baseline character experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_42",
            "start": 84,
            "end": 158,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_42@2",
            "content": "In the later experiments, we replace the embedding lookup with the character processing architectures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_42",
            "start": 160,
            "end": 261,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_42@3",
            "content": "For the Lee-style encoder, we chose similar hyperparameters as related work (Banar et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_42",
            "start": 263,
            "end": 359,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_42@4",
            "content": "For experiments with Charformer and CANINE models, we set the hyperparameters such that they cover the same character span before downsampling as the Lee-style encoder, which causes the models to have fewer parameters than a Lee-style encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_42",
            "start": 361,
            "end": 603,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_42@5",
            "content": "Note however that for both the Charformer and the CANINE models, the number of parameters is almost independent of the character window width.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_42",
            "start": 605,
            "end": 746,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_42@6",
            "content": "For all three character processing architectures, we experiment with downsampling factors of 3 and 5 (a 16k BPE vocabulary corresponds to a downsampling factor of about 4 in English).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_42",
            "start": 748,
            "end": 930,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_43@0",
            "content": "Translation Quality",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_43",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_44@0",
            "content": "We evaluate the translation quality using the BLEU score (Papineni et al., 2002), the chrF score (Popovi\u0107, 2015) (as implemented in SacreBLEU; Post, 2018), 3 and the COMET score (Rei et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_44",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_44@1",
            "content": "We run each experiment 4 times and report the mean value and standard deviation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_44",
            "start": 198,
            "end": 277,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_45@0",
            "content": "The results are presented in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_45",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_45@1",
            "content": "Except for translation into Arabic, where character methods outperform BPEs (which is consistent with the findings of Levy, 2021a andLi et al., 2021), subword methods are always better than characters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_45",
            "start": 38,
            "end": 238,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_46@0",
            "content": "The Lee-style encoder outperforms the two more recent methods and the method of using the character embeddings directly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_46",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_46@1",
            "content": "Charformer performs similarly to using character embeddings directly, .520",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_46",
            "start": 121,
            "end": 194,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_47@0",
            "content": "\u00b1-.099",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_47",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_48@0",
            "content": ".044",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_48",
            "start": 0,
            "end": 3,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_49@0",
            "content": "\u00b1-.181",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_49",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_50@0",
            "content": "Table 1: Translation quality of the models on the IWSLT data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_50",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_50@1",
            "content": "The fourth column shows the size of the characterprocessing layers expressed as the vocabulary size of Transformer Base having the same number of parameters in the embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_50",
            "start": 62,
            "end": 236,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_51@0",
            "content": "CANINE is significantly worse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_51",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_51@1",
            "content": "The results are mostly consistent across the language pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_51",
            "start": 31,
            "end": 90,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_51@2",
            "content": "Increasing the downsampling rate from 3 to 5 degrades the translation quality for all architectures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_51",
            "start": 92,
            "end": 191,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_51@3",
            "content": "Employing the two-step decoder matches the decoding speed of subword models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_51",
            "start": 193,
            "end": 268,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_51@4",
            "content": "However, the overall translation quality is much worse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_51",
            "start": 270,
            "end": 324,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_52@0",
            "content": "The three metrics that we use give consistent results in most cases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_52",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_52@1",
            "content": "Often, relatively small differences in BLEU and chrF scores correspond to much bigger differences in the COMET score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_52",
            "start": 69,
            "end": 185,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_53@0",
            "content": "Inference",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_53",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_54@0",
            "content": "Inference algorithms for neural MT have been discussed extensively (Meister et al., 2020;Massarelli et al., 2020;Shi et al., 2020;Shaham and Levy, 2021b) for the subword models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_54",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_54@1",
            "content": "Subword translation quality quickly degrades beyond a certain beam width unless heuristically defined length normalization is applied.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_54",
            "start": 178,
            "end": 311,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_55@0",
            "content": "As an alternative, Eikema and Aziz (2020) recently proposed Minimum Bayes Risk (MBR; Goel and Byrne 2000) estimation as an alternative.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_55",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_55@1",
            "content": "Assuming that similar sentences should be similarly probable, they propose repeatedly sampling from the model and selecting a sentence that is most similar to other samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_55",
            "start": 136,
            "end": 308,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_55@2",
            "content": "With subword models, MBR performs comparably to beam search.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_55",
            "start": 310,
            "end": 369,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_56@0",
            "content": "Intuitive arguments about the inference algorithms are often based on the properties of the subword output distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_56",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_56@1",
            "content": "On average, character models will produce distributions with lower perplexity and thus likely suffer more from the exposure bias which might harm sampling from the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_56",
            "start": 121,
            "end": 290,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_56@2",
            "content": "Therefore, there is a risk that these empirical findings do not apply to character-level models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_56",
            "start": 292,
            "end": 387,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_57@0",
            "content": "We explore what decoding strategies are best suited for the character-level models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_57",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_57@1",
            "content": "We compare the translation quality of beam search decoding with different degrees of length normalization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_57",
            "start": 84,
            "end": 189,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_57@2",
            "content": "4 Further, we compare length-normalized beam search decoding with MBR (with 100 samples), greedy decoding, and random sampling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_57",
            "start": 191,
            "end": 317,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_57@3",
            "content": "We use the chrF as a comparison metric which allows pre-computing the character n-grams and thus faster sentence pair comparison than the originally proposed METEOR (Denkowski and Lavie, 2011).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_57",
            "start": 319,
            "end": 511,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_58@0",
            "content": "Figure 4 shows the translation quality of the selected models for different beam sizes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_58",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_58@1",
            "content": "The dotted lines denoting the translation quality without length normalization show that the quality of the subword models quickly deteriorates without length normalization, whereas vanilla and Lee-style characterlevel models do not seem to suffer from this problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_58",
            "start": 88,
            "end": 353,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_59@0",
            "content": "Table 2 presents the translation quality for different decoding methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_59",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_59@1",
            "content": "In all cases, beam search is the best strategy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_59",
            "start": 73,
            "end": 119,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_59@2",
            "content": "Sampling from character-level models leads to very poor translation quality that in turn also influences the MBR decoding leading to much worse results than beam search.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_59",
            "start": 121,
            "end": 289,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_60@0",
            "content": "Our experiments show that beam search with length normalization is the best inference algorithm for character-level models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_60",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_60@1",
            "content": "They also seem to be more resilient towards the beam search curse compared to subword models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_60",
            "start": 124,
            "end": 216,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_61@0",
            "content": "Experiments on WMT Data",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_61",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_62@0",
            "content": "Based on the results of the experiments with the IWSLT data, we further experiment only with the Lee-style encoder using a downsampling factor of 3 on the source side.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_62",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_62@1",
            "content": "Additionally, we experiment with hybrid systems with a subword encoder and character decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_62",
            "start": 168,
            "end": 260,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_62@2",
            "content": "We train translation systems of competitive quality on two high-resource language pairs, English-Czech and English-German, and perform an extensive evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_62",
            "start": 262,
            "end": 420,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_63@0",
            "content": "Experimental Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_63",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_64@0",
            "content": "For English-to-Czech translation, we use the CzEng 2.0 corpus (Kocmi et al., 2020b) that aggregates and curates all sources for this language pair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_64",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_64@1",
            "content": "We use all 66M authentic parallel sentence pairs and 50M back-translated Czech sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_64",
            "start": 148,
            "end": 236,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_65@0",
            "content": "For the English-to-German translation, we use a subset of the training data used by Chen et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_65",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_65@1",
            "content": "The data consists of 66M authentic sentence pairs filtered from the available data for WMT and 52M back-translated German sentences from News Crawl 2020.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_65",
            "start": 104,
            "end": 256,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_66@0",
            "content": "We tag the back-translation data (Caswell et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_66",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_66@1",
            "content": "We use the Transformer Big architecture for all experiments with hyperparameters following Popel and Bojar (2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_66",
            "start": 57,
            "end": 170,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_66@2",
            "content": "For the Lee-style encoder, we double the hidden layer sizes compared to the IWSLT experiments (following the hidden size increase between the Transformer Base and Big architectures).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_66",
            "start": 172,
            "end": 353,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_66@3",
            "content": "In contrast to the previous set of experiments, we use Fairseq (Ott et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_66",
            "start": 355,
            "end": 436,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_66@4",
            "content": "Our code is available on Github 5 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_66",
            "start": 438,
            "end": 472,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_66@5",
            "content": "System outputs are attached to the paper in the ACL anthology.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_66",
            "start": 474,
            "end": 535,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_67@0",
            "content": "We evaluate the systems not only on WMT20 test sets but also on data that often motivated the research of character-level methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_67",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_67@1",
            "content": "We evaluate the out-of-domain performance of the models on the NHS test set from the WMT17 Biomedical Task (Jimeno Yepes et al., 2017) and on the WMT16 IT Domain test set (Bojar et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_67",
            "start": 131,
            "end": 322,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_67@2",
            "content": "We use the same evaluation metrics as for the IWSLT experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_67",
            "start": 324,
            "end": 387,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_67@3",
            "content": "We estimate the confidence intervals using bootstrap resampling (Koehn, 2004).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_67",
            "start": 389,
            "end": 466,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_68@0",
            "content": "We also assess the gender bias of the systems (Stanovsky et al., 2019;Kocmi et al., 2020a), using a dataset of sentence pairs with stereotypical and non-stereotypical English sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_68",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_68@1",
            "content": "We measure the accuracy of gendered nouns and pronouns using word alignment and morphological analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_68",
            "start": 186,
            "end": 288,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_69@0",
            "content": "Morphological generalization is often mentioned among the motivations for character-level modeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_69",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_69@1",
            "content": "Therefore, we evaluate our models using Mor-phEval (Burlot and Yvon, 2017;Burlot et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_69",
            "start": 100,
            "end": 194,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_69@2",
            "content": "Similar to the gender evaluation, MorphEval also uses contrastive sentence pairs that differ in exactly one morphological feature.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_69",
            "start": 196,
            "end": 325,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_69@3",
            "content": "Accuracy on the sentences is measured.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_69",
            "start": 327,
            "end": 364,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_69@4",
            "content": "Besides, we assess how well the models handle lemmas and forms that were unseen at training time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_69",
            "start": 366,
            "end": 462,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_69@5",
            "content": "We tokenize and lemmatize all data with UDPipe (Straka and Strakov\u00e1, 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_69",
            "start": 464,
            "end": 538,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_69@6",
            "content": "On the WMT20 test set, we compute the recall of test lemmas that were not in the training set and the recall of word forms that were not in the training data, but forms of the same lemma were.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_69",
            "start": 540,
            "end": 731,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_69@7",
            "content": "Note that not generating a particular lemma or form is not necessarily an error.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_69",
            "start": 733,
            "end": 812,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_69@8",
            "content": "Therefore, we report the recall in contrast with the recall of lemmas and forms that were represented in the training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_69",
            "start": 814,
            "end": 936,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_70@0",
            "content": "Character-level models are also supposed to be more robust towards source-side noise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_70",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_70@1",
            "content": "We evaluate the noise robustness of the systems using synthetic noise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_70",
            "start": 86,
            "end": 155,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_70@2",
            "content": "We use TextFlint (Wang et al., 2021) to generate synthetic noise in the source text with simulated typos and spelling errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_70",
            "start": 157,
            "end": 281,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_70@3",
            "content": "We generate 20 noisy versions of the WMT20 test set and report the average chrF score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_70",
            "start": 283,
            "end": 368,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_71@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_71",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_72@0",
            "content": "The main results are presented in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_72",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_72@1",
            "content": "The main trends in the translation quality are the same as in the case of IWSLT data: subword models outperform character models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_72",
            "start": 43,
            "end": 171,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_72@2",
            "content": "Using Lee-style encoding narrows the quality gap and performs similarly to models with subword tokens on the source side.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_72",
            "start": 173,
            "end": 293,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_72@3",
            "content": "Although domain robustness often motivates character-level experiments, our experiments show that the trends are domain-independent, except for English-German IT Domain translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_72",
            "start": 295,
            "end": 475,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_73@0",
            "content": "The similar performance of the subword encoder and the Lee-style encoder suggests that the hidden states of the Lee-style encoder can efficiently emulate the subword segmentation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_73",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_73@1",
            "content": "We speculate that the main weaknesses remain on the decoder side.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_73",
            "start": 180,
            "end": 244,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_74@0",
            "content": "In the English-to-Czech direction, the characterlevel models perform worse in gender bias evaluation, although they better capture grammatical gender agreement according to the MorphEval benchmark.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_74",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_74@1",
            "content": "On the other hand, character-level models make more frequent errors in the tense of coordinated verbs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_74",
            "start": 198,
            "end": 299,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_74@2",
            "content": "There are no major differences in recall of novel forms and lemmas.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_74",
            "start": 301,
            "end": 367,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_75@0",
            "content": "For the English-to-German translation, characterlevel methods reach better results on the gender benchmark.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_75",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_75@1",
            "content": "We speculate that getting gender correct in German might be easier because unlike Czech it does not require subject-verb agreement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_75",
            "start": 108,
            "end": 238,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_75@2",
            "content": "The average performance on the MorphEval benchmark is also slightly better for character models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_75",
            "start": 240,
            "end": 335,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_75@3",
            "content": "Detailed results on MorphEval are in Tables 7 and 8 in the Appendix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_75",
            "start": 337,
            "end": 404,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_75@4",
            "content": "The higher recall of novel forms also suggests slightly better morphological generalization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_75",
            "start": 406,
            "end": 497,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_76@0",
            "content": "The only consistent advantage of the characterlevel models is their robustness towards source side noise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_76",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_76@1",
            "content": "Here, the character-level models outperform both the fully subword model and the subword encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_76",
            "start": 106,
            "end": 202,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_77@0",
            "content": "Conclusions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_77",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_78@0",
            "content": "In our extensive literature survey, we found evidence that character-level methods should reach comparative translation quality as subword methods, typically at the expense of much higher computation costs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_78",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_78@1",
            "content": "We speculate that the computational cost is the reason why virtually none of the recent WMT systems used character-level methods or mentioned them as a reasonable alternative.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_78",
            "start": 207,
            "end": 381,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_79@0",
            "content": "Recently, most innovations in character-level modeling were introduced in the context of pretrained representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_79",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_79@1",
            "content": "In our comparison of character processing architectures (two of them used for the first time in the context of MT), we showed that 1D convolutions followed by highway layers still deliver the best results for MT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_79",
            "start": 117,
            "end": 328,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_80@0",
            "content": "Character-level systems are still mostly worse than subword systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_80",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_80@1",
            "content": "Moreover, the recent character-level architectures do not show advantages over vanilla character models, other than improved speed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_80",
            "start": 69,
            "end": 199,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_81@0",
            "content": "To overcome efficiency issues, we proposed a two-step decoding architecture that matches the speed of subword models, however at the expense of a further drop in translation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_81",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_82@0",
            "content": "Furthermore, we found that conclusions of recent literature on decoding in MT do not generalize for character models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_82",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_82@1",
            "content": "Character models do not suffer from the beam search curse and decoding methods based on sampling perform poorly, here.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_82",
            "start": 118,
            "end": 235,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_83@0",
            "content": "Evaluation on competitively large datasets showed that there is still a small quality gap between character and subword models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_83",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_83@1",
            "content": "Character models do not show better domain robustness, and only slightly better morphological generalization in German, although this is often mentioned as important motivation for character-level modeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_83",
            "start": 128,
            "end": 333,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_83@2",
            "content": "The only clear advantage of character models is high robustness towards source-side noise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_83",
            "start": 335,
            "end": 424,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_84@0",
            "content": "In contrast to earlier work on character-level MT, which claimed that decoding is straightforward and which focused on the encoder part of the model, our conclusions are that Lee-style encoding is comparable to subword encoders.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_84",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_84@1",
            "content": "Even now, most modeling innovations focus on encoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_84",
            "start": 229,
            "end": 282,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_84@2",
            "content": "Character-level decoding which is both accurate and efficient remains an open research question.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_84",
            "start": 284,
            "end": 379,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_85@0",
            "content": "At inference time, the LSTM decoder gets one Transformer state and generates s output characters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_85",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_85@1",
            "content": "The characters are fed to the character processing architecture, which is in turn used to generate the next Transformer decoder state.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_85",
            "start": 98,
            "end": 231,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_86@0",
            "content": "We used the tst2010 part of the dataset for validation and tst2015 for testing and did not use any other test sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_86",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_86@1",
            "content": "The data sizes are presented in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_86",
            "start": 116,
            "end": 155,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_87@0",
            "content": "All models are trained with initial learning rate: 5 \u2022 10 \u22124 with 4k warmup steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_87",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_87@1",
            "content": "The batch size is 20k tokens for both BPE and character experiments with update after 3 batches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_87",
            "start": 83,
            "end": 178,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_87@2",
            "content": "Label smoothing is set to 0.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_87",
            "start": 180,
            "end": 209,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_88@0",
            "content": "Lee-style.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_88",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_88@1",
            "content": "The character embedding dimension is 64.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_88",
            "start": 11,
            "end": 50,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_88@2",
            "content": "The original paper used kernel sizes from 1 to",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_88",
            "start": 52,
            "end": 97,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_88@3",
            "content": "8. For ease of implementation, we only use even-sized kernels up to size",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_88",
            "start": 99,
            "end": 170,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_88@4",
            "content": "9. The encoder uses 1D convolutions of kernel size 1, 3, 5, 7, 9 with 128, 256, 512, 512, 256 filters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_88",
            "start": 172,
            "end": 273,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_88@5",
            "content": "Their output is concatenated and projected to the model dimension, followed by 2 highway layers and 2 Transformer feed-forward layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_88",
            "start": 275,
            "end": 408,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_89@0",
            "content": "CANINE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_89",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_89@1",
            "content": "The local self-attention span in the encoder is 4\u00d7 the downsampling factor, in the encoder, equal to the downsampling factor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_89",
            "start": 8,
            "end": 132,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_89@2",
            "content": "Two-step decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_89",
            "start": 134,
            "end": 150,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_89@3",
            "content": "The decoder uses character embeddings with dimension of 64, which is also the size of the projection of the Transformer decoder state.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_89",
            "start": 152,
            "end": 285,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_89@4",
            "content": "The hidden state size of the LSTM is 128.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_89",
            "start": 287,
            "end": 327,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_90@0",
            "content": "The validation BLEU and chrF scores and training and inference times are in Table 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_90",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_90@1",
            "content": "The training times were measured on machines with GeForce GTX 1080 Ti GPUs and with Intel Xeon E5-2630v4 CPUs (2.20GHz), a single GPU was used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_90",
            "start": 85,
            "end": 227,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_91@0",
            "content": "Note that the experiments on IWSLT were not optimized for speed and are thus not comparable with the times reported on the larger datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_91",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_92@0",
            "content": "We use the Transformer Big architecture as defined FairSeq's standard transformer_wmt_en_de_big_t2t. The Lee-style encoder uses filters sizes 1, 3, 5, 7, 9 of dimensions 256, 512, 1024, 1024, 512.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_92",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_92@1",
            "content": "The other parameters remains the same as in the IWSLT experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_92",
            "start": 197,
            "end": 262,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_93@0",
            "content": "We set the beta parameters of the Adam optimizer to 0.9 and 0.998 and gradient clipping to 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_93",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_93@1",
            "content": "The learning rate is 5 \u2022 10 \u22124 with 16k warmup steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_93",
            "start": 94,
            "end": 146,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_93@2",
            "content": "Early stopping is with respect to negative log likelihood with patience 10.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_93",
            "start": 148,
            "end": 222,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_93@3",
            "content": "We save 5 best checkpoints and do checkpoint averaging before evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_93",
            "start": 224,
            "end": 296,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_93@4",
            "content": "The maximum batch size is 1800 tokens for the BPE experiments and 500 for character-level experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_93",
            "start": 298,
            "end": 399,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_93@5",
            "content": "We train the models on 4 GPUs, so the effective batch size is 4 times bigger.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_93",
            "start": 401,
            "end": 477,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_94@0",
            "content": "During training, we evaluated the models by measuring the cross-entropy on the validation set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_94",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_94@1",
            "content": "After model training, we use grid search to estimate the best value of length normalization on the validation set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_94",
            "start": 95,
            "end": 208,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_94@2",
            "content": "The translation quality on the validation data is tabulated in Table 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_94",
            "start": 210,
            "end": 280,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_95@0",
            "content": "The detailed results on the MorphEval benchmark are in Tables 7 (Czech) and 8 (German).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_95",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_95@1",
            "content": "The details of the noise evaluation are in Table 9.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_95",
            "start": 88,
            "end": 138,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_96@0",
            "content": "Orhan Duygu Ataman,  Firat, A Mattia,  Gangi, On the importance of word boundaries in character-level neural machine translation, 2019, Proceedings of the 3rd Workshop on Neural Generation and Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_96",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_97@0",
            "content": "Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Neural machine translation by jointly learning to align and translate, 2015-05-07, 3rd International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_97",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_98@0",
            "content": "Nikolay Banar, Walter Daelemans, Mike Kestemont, Character-level transformer-based neural machine translation, 2020-12-18, NLPIR 2020: 4th International Conference on Natural Language Processing and Information Retrieval, ACM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_98",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_99@0",
            "content": "Ankur Bapna, Mia Chen, Orhan Firat, Yuan Cao, Yonghui Wu, Training deeper neural machine translation models with transparent attention, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_99",
            "start": 0,
            "end": 271,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_100@0",
            "content": "Lo\u00efc Barrault, Magdalena Biesialska, Ond\u0159ej Bojar, Marta Costa-Juss\u00e0, Christian Federmann, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Matthias Huck, Eric Joanis, Tom Kocmi, Philipp Koehn, Chi-Kiu Lo, Nikola Ljube\u0161i\u0107, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, None, , Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (WMT20). In Proceedings of the Fifth Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_100",
            "start": 0,
            "end": 477,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_101@0",
            "content": "Lo\u00efc Barrault, Ond\u0159ej Bojar, Marta Costa-Juss\u00e0, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias M\u00fcller, Findings of the 2019 conference on machine translation (WMT19), 2019, Proceedings of the Fourth Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_101",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_102@0",
            "content": "Ond\u0159ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aur\u00e9lie N\u00e9v\u00e9ol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, Marcos Zampieri, Findings of the 2016 conference on machine translation, 2016, Proceedings of the First Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_102",
            "start": 0,
            "end": 489,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_103@0",
            "content": "Ond\u0159ej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Philipp Koehn, Christof Monz, Findings of the 2018 conference on machine translation (WMT18), 2018, Proceedings of the Third Conference on Machine Translation: Shared Task Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_103",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_104@0",
            "content": "Franck Burlot, Yves Scherrer, Vinit Ravishankar, Ond\u0159ej Bojar, Stig-Arne Gr\u00f6nroos, Maarit Koponen, Tommi Nieminen, Fran\u00e7ois Yvon, ; English-Czech,  English-German, Turkish-English English-Finnish, The WMT'18 morpheval test suites for, 2018, Proceedings of the Third Conference on Machine Translation: Shared Task Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_104",
            "start": 0,
            "end": 321,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_105@0",
            "content": "Franck Burlot, Fran\u00e7ois Yvon, Evaluating the morphological competence of machine translation systems, 2017, Proceedings of the Second Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_105",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_106@0",
            "content": "Isaac Caswell, Ciprian Chelba, David Grangier, Tagged back-translation, 2019, Proceedings of the Fourth Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_106",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_107@0",
            "content": "Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Niehues Jan, St\u00fcker Sebastian, Sudoh Katsuitho, Yoshino Koichiro, Federmann Christian, Overview of the iwslt 2017 evaluation campaign, 2017, International Workshop on Spoken Language Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_107",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_108@0",
            "content": "Pinzhen Chen, Jind\u0159ich Helcl, Ulrich Germann, Laurie Burchell, Nikolay Bogoychev, Antonio Valerio Miceli, Jonas Barone, Alexandra Waldendorf, Kenneth Birch,  Heafield, 2021. The University of Edinburgh's English-German and English-Hausa submissions to the WMT21 news translation task, , Proceedings of the Sixth Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_108",
            "start": 0,
            "end": 347,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_109@0",
            "content": "Colin Cherry, George Foster, Ankur Bapna, Orhan Firat, Wolfgang Macherey, Revisiting character-based neural machine translation with capacity and compression, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_109",
            "start": 0,
            "end": 294,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_110@0",
            "content": "UNKNOWN, None, 2016, A character-level decoder without explicit segmentation for neural machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_110",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_111@0",
            "content": "UNKNOWN, None, , Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_111",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_112@0",
            "content": "UNKNOWN, None, 2021, CANINE: pre-training an efficient tokenization-free encoder for language representation, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_112",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_113@0",
            "content": "Marta Costa-Juss\u00e0, Carlos Escolano, Jos\u00e9 Fonollosa, Byte-based neural machine translation, 2017, Proceedings of the First Workshop on Subword and Character Level Models in NLP, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_113",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_114@0",
            "content": "Michael Denkowski, Alon Lavie, Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems, 2011, Proceedings of the Sixth Workshop on Statistical Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_114",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_115@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_115",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_116@0",
            "content": "Bryan Eikema, Wilker Aziz, Is MAP decoding all you need? the inadequacy of the mode in neural machine translation, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_116",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_117@0",
            "content": "Carlos Escolano, Marta Costa-Juss\u00e0, Jos\u00e9 Fonollosa, The TALP-UPC neural machine translation system for German/Finnish-English using the inverse direction model in rescoring, 2017, Proceedings of the Second Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_117",
            "start": 0,
            "end": 282,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_118@0",
            "content": "Yingqiang Gao, I Nikola, Yuhuang Nikolov, Richard Hu,  Hahnloser, Character-level translation with self-attention, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_118",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_119@0",
            "content": "Vaibhava Goel, William Byrne, Minimum bayes-risk automatic speech recognition, 2000, Comput. Speech Lang, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_119",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_120@0",
            "content": "Stig-Arne Gr\u00f6nroos, Sami Virpioja, Mikko Kurimo, Extending hybrid word-character neural machine translation with multi-task learning of morphological analysis, 2017, Proceedings of the Second Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_120",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_121@0",
            "content": "UNKNOWN, None, 1911, Character-based NMT with transformer. CoRR, abs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_121",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_122@0",
            "content": "Chester Holtz, Chuyang Ke, Daniel Gildea, University of Rochester WMT 2017 NMT system submission, 2017, Proceedings of the Second Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_122",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_123@0",
            "content": "Antonio Jimeno Yepes, Aur\u00e9lie N\u00e9v\u00e9ol, Mariana Neves, Karin Verspoor, Ond\u0159ej Bojar, Arthur Boyer, Cristian Grozea, Barry Haddow, Madeleine Kittner, Yvonne Lichtblau, Pavel Pecina, Roland Roller, Rudolf Rosa, Amy Siu, Philippe Thomas, Saskia Trescher, Findings of the WMT 2017 biomedical translation shared task, 2017, Proceedings of the Second Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_123",
            "start": 0,
            "end": 419,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_124@0",
            "content": "Rebecca Knowles, Darlene Stewart, Samuel Larkin, Patrick Littell, None, 2020, NRC systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_124",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_125@0",
            "content": ", Inuktitut-English news translation task, , Proceedings of the Fifth Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_125",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_126@0",
            "content": "Tom Kocmi, Tomasz Limisiewicz, Gabriel Stanovsky, Gender coreference and bias evaluation at WMT 2020, 2020, Proceedings of the Fifth Conference on Machine Translation, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_126",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_127@0",
            "content": "UNKNOWN, None, 2007, Announcing czeng 2.0 parallel corpus with over 2 gigawords. CoRR, abs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_127",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_128@0",
            "content": "Philipp Koehn, Statistical significance tests for machine translation evaluation, 2004, Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_128",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_129@0",
            "content": "Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond\u0159ej Bojar, Alexandra Constantin, Evan Herbst, Moses: Open source toolkit for statistical machine translation, 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_129",
            "start": 0,
            "end": 439,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_130@0",
            "content": "Julia Kreutzer, Artem Sokolov, Learning to segment inputs for NMT favors character-level processing, 2018, Proceedings of the 15th International Conference on Spoken Language Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_130",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_131@0",
            "content": "Taku Kudo, John Richardson, SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_131",
            "start": 0,
            "end": 297,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_132@0",
            "content": "Jason Lee, Kyunghyun Cho, Thomas Hofmann, Fully character-level neural machine translation without explicit segmentation, 2017, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_132",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_133@0",
            "content": "Jiahuan Li, Yutong Shen, Shujian Huang, Xinyu Dai, Jiajun Chen, When is char better than subword: A systematic study of segmentation algorithms for neural machine translation, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_133",
            "start": 0,
            "end": 358,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_134@0",
            "content": "Xian Li, Paul Michel, Antonios Anastasopoulos, Yonatan Belinkov, Nadir Durrani, Orhan Firat, Philipp Koehn, Graham Neubig, Juan Pino, Hassan Sajjad, Findings of the first shared task on machine translation robustness, 2019, Proceedings of the Fourth Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_134",
            "start": 0,
            "end": 285,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_135@0",
            "content": "Jind\u0159ich Libovick\u00fd, Alexander Fraser, Towards reasonably-sized character-level transformer NMT by finetuning subword systems, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_135",
            "start": 0,
            "end": 277,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_136@0",
            "content": "Minh-Thang Luong, Christopher Manning, Achieving open vocabulary neural machine translation with hybrid word-character models, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_136",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_137@0",
            "content": "Dipankar Sainik Kumar Mahata, Sivaji Das,  Bandyopadhyay, JUCBNMT at WMT2018 news translation task: Character based neural machine translation of Finnish to English, 2018, Proceedings of the Third Conference on Machine Translation: Shared Task Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_137",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_138@0",
            "content": "UNKNOWN, None, , , Vassilis Plachouras.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_138",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_139@0",
            "content": "Fabrizio Silvestri, Sebastian Riedel, How decoding strategies affect the verifiability of generated text, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_139",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_140@0",
            "content": "Clara Meister, Ryan Cotterell, Tim Vieira, If beam search is the answer, what was the question?, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_140",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_141@0",
            "content": "I Nikola, Yuhuang Nikolov, Mi Hu, Richard Tan,  Hahnloser, Character-level Chinese-English translation through ASCII encoding, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_141",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_142@0",
            "content": "Robert \u00d6stling, Yves Scherrer, J\u00f6rg Tiedemann, Gongbo Tang, Tommi Nieminen, The Helsinki neural machine translation system, 2017, Proceedings of the Second Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_142",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_143@0",
            "content": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli, fairseq: A fast, extensible toolkit for sequence modeling, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_143",
            "start": 0,
            "end": 343,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_144@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_144",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_145@0",
            "content": "Martin Popel, Ond\u0159ej Bojar, Training Tips for the Transformer Model, 2018, The Prague Bulletin of Mathematical Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_145",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_146@0",
            "content": "Maja Popovi\u0107, chrF: character n-gram F-score for automatic MT evaluation, 2015, Proceedings of the Tenth Workshop on Statistical Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_146",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_147@0",
            "content": "Matt Post, A call for clarity in reporting BLEU scores, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_147",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_148@0",
            "content": "Ivan Provilkov, Dmitrii Emelianenko, Elena Voita, BPE-dropout: Simple and effective subword regularization, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_148",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_149@0",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-totext transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_149",
            "start": 0,
            "end": 245,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_150@0",
            "content": "Ricardo Rei, Craig Stewart, Ana Farinha, Alon Lavie, COMET: A neural framework for MT evaluation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_150",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_151@0",
            "content": "Yves Scherrer, Ra\u00fal V\u00e1zquez, Sami Virpioja, The University of Helsinki submissions to the WMT19 similar language translation task, 2019, Proceedings of the Fourth Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_151",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_152@0",
            "content": "Rico Sennrich, How grammatical is characterlevel neural machine translation? assessing MT quality with contrastive translation pairs, 2017, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_152",
            "start": 0,
            "end": 290,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_153@0",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Neural machine translation of rare words with subword units, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_153",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_154@0",
            "content": "Rico Sennrich, Biao Zhang, Revisiting lowresource neural machine translation: A case study, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_154",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_155@0",
            "content": "Uri Shaham, Omer Levy, Neural machine translation without embeddings, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_155",
            "start": 0,
            "end": 261,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_156@0",
            "content": "UNKNOWN, None, 2021, What do you get when you cross beam search with nucleus sampling?, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_156",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_157@0",
            "content": "UNKNOWN, None, 2012, Why neural machine translation prefers empty outputs. CoRR, abs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_157",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_158@0",
            "content": "Lucia Specia, Zhenhao Li, Juan Pino, Vishrav Chaudhary, Francisco Guzm\u00e1n, Graham Neubig, Nadir Durrani, Yonatan Belinkov, Philipp Koehn, Hassan Sajjad, Paul Michel, Xian Li, 2020. Findings of the WMT 2020 shared task on machine translation robustness, , Proceedings of the Fifth Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_158",
            "start": 0,
            "end": 314,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_159@0",
            "content": "UNKNOWN, None, 2015, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_159",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_160@0",
            "content": "Gabriel Stanovsky, Noah Smith, Luke Zettlemoyer, Evaluating gender bias in machine translation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_160",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_161@0",
            "content": "Milan Straka, Jana Strakov\u00e1, Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe, 2017, Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_161",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_162@0",
            "content": "Ilya Sutskever, Oriol Vinyals, V Quoc,  Le, Sequence to sequence learning with neural networks, 2014-12-08, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_162",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_163@0",
            "content": "UNKNOWN, None, 2021, Charformer: Fast character transformers via gradient-based subword tokenization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_163",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_164@0",
            "content": "Du\u0161an Vari\u0161, Ond\u0159ej Bojar, CUNI system for WMT17 automatic post-editing task, 2017, Proceedings of the Second Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_164",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_165@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_165",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_166@0",
            "content": "Xiao Wang, Qin Liu, Tao Gui, Qi Zhang, Yicheng Zou, Xin Zhou, Jiacheng Ye, Yongxin Zhang, Rui Zheng, Zexiong Pang, Qinzhuo Wu, Zhengyan Li, Chong Zhang, Ruotian Ma, Zichu Fei, Ruijian Cai, Jun Zhao, Xingwu Hu, Zhiheng Yan, Yiding Tan, Yuan Hu, Qiyuan Bian, Zhihua Liu, Shan Qin, Bolin Zhu, Xiaoyu Xing, Jinlan Fu, Yue Zhang, Minlong Peng, Xiaoqing Zheng, Yaqian Zhou, Zhongyu Wei, TextFlint: Unified multilingual robustness evaluation toolkit for natural language processing, , Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_166",
            "start": 0,
            "end": 665,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_167@0",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger,  Drame, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_167",
            "start": 0,
            "end": 463,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_168@0",
            "content": "UNKNOWN, None, 2021, Byt5: Towards a tokenfree future with pre-trained byte-to-byte models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_168",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_169@0",
            "content": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021b. mT5: A massively multilingual pre-trained text-to-text transformer, , Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_169",
            "start": 0,
            "end": 338,
            "label": {}
        },
        {
            "ix": "35-ARR_v2_170@0",
            "content": "Longtu Zhang, Mamoru Komachi, Neural machine translation of logographic language using sub-character level information, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "35-ARR_v2_170",
            "start": 0,
            "end": 203,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "35-ARR_v2_0",
            "tgt_ix": "35-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_0",
            "tgt_ix": "35-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_1",
            "tgt_ix": "35-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_1",
            "tgt_ix": "35-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_0",
            "tgt_ix": "35-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_2",
            "tgt_ix": "35-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_4",
            "tgt_ix": "35-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_5",
            "tgt_ix": "35-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_6",
            "tgt_ix": "35-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_3",
            "tgt_ix": "35-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_3",
            "tgt_ix": "35-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_3",
            "tgt_ix": "35-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_3",
            "tgt_ix": "35-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_3",
            "tgt_ix": "35-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_0",
            "tgt_ix": "35-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_7",
            "tgt_ix": "35-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_9",
            "tgt_ix": "35-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_10",
            "tgt_ix": "35-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_11",
            "tgt_ix": "35-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_8",
            "tgt_ix": "35-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_8",
            "tgt_ix": "35-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_8",
            "tgt_ix": "35-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_8",
            "tgt_ix": "35-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_8",
            "tgt_ix": "35-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_13",
            "tgt_ix": "35-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_14",
            "tgt_ix": "35-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_15",
            "tgt_ix": "35-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_16",
            "tgt_ix": "35-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_17",
            "tgt_ix": "35-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_8",
            "tgt_ix": "35-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_8",
            "tgt_ix": "35-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_8",
            "tgt_ix": "35-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_8",
            "tgt_ix": "35-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_8",
            "tgt_ix": "35-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_8",
            "tgt_ix": "35-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_12",
            "tgt_ix": "35-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_0",
            "tgt_ix": "35-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_18",
            "tgt_ix": "35-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_20",
            "tgt_ix": "35-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_21",
            "tgt_ix": "35-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_22",
            "tgt_ix": "35-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_23",
            "tgt_ix": "35-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_24",
            "tgt_ix": "35-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_25",
            "tgt_ix": "35-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_19",
            "tgt_ix": "35-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_19",
            "tgt_ix": "35-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_19",
            "tgt_ix": "35-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_19",
            "tgt_ix": "35-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_19",
            "tgt_ix": "35-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_19",
            "tgt_ix": "35-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_19",
            "tgt_ix": "35-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_19",
            "tgt_ix": "35-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_0",
            "tgt_ix": "35-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_26",
            "tgt_ix": "35-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_28",
            "tgt_ix": "35-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_29",
            "tgt_ix": "35-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_30",
            "tgt_ix": "35-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_31",
            "tgt_ix": "35-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_32",
            "tgt_ix": "35-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_33",
            "tgt_ix": "35-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_34",
            "tgt_ix": "35-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_35",
            "tgt_ix": "35-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_27",
            "tgt_ix": "35-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_27",
            "tgt_ix": "35-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_27",
            "tgt_ix": "35-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_27",
            "tgt_ix": "35-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_27",
            "tgt_ix": "35-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_27",
            "tgt_ix": "35-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_27",
            "tgt_ix": "35-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_27",
            "tgt_ix": "35-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_27",
            "tgt_ix": "35-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_27",
            "tgt_ix": "35-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_0",
            "tgt_ix": "35-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_36",
            "tgt_ix": "35-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_37",
            "tgt_ix": "35-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_37",
            "tgt_ix": "35-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_37",
            "tgt_ix": "35-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_38",
            "tgt_ix": "35-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_40",
            "tgt_ix": "35-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_41",
            "tgt_ix": "35-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_39",
            "tgt_ix": "35-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_39",
            "tgt_ix": "35-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_39",
            "tgt_ix": "35-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_39",
            "tgt_ix": "35-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_37",
            "tgt_ix": "35-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_42",
            "tgt_ix": "35-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_44",
            "tgt_ix": "35-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_45",
            "tgt_ix": "35-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_46",
            "tgt_ix": "35-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_47",
            "tgt_ix": "35-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_48",
            "tgt_ix": "35-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_49",
            "tgt_ix": "35-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_50",
            "tgt_ix": "35-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_51",
            "tgt_ix": "35-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_43",
            "tgt_ix": "35-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_43",
            "tgt_ix": "35-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_43",
            "tgt_ix": "35-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_43",
            "tgt_ix": "35-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_43",
            "tgt_ix": "35-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_43",
            "tgt_ix": "35-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_43",
            "tgt_ix": "35-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_43",
            "tgt_ix": "35-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_43",
            "tgt_ix": "35-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_43",
            "tgt_ix": "35-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_37",
            "tgt_ix": "35-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_52",
            "tgt_ix": "35-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_54",
            "tgt_ix": "35-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_55",
            "tgt_ix": "35-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_56",
            "tgt_ix": "35-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_57",
            "tgt_ix": "35-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_58",
            "tgt_ix": "35-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_59",
            "tgt_ix": "35-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_53",
            "tgt_ix": "35-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_53",
            "tgt_ix": "35-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_53",
            "tgt_ix": "35-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_53",
            "tgt_ix": "35-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_53",
            "tgt_ix": "35-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_53",
            "tgt_ix": "35-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_53",
            "tgt_ix": "35-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_53",
            "tgt_ix": "35-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_0",
            "tgt_ix": "35-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_60",
            "tgt_ix": "35-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_61",
            "tgt_ix": "35-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_61",
            "tgt_ix": "35-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_61",
            "tgt_ix": "35-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_62",
            "tgt_ix": "35-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_64",
            "tgt_ix": "35-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_65",
            "tgt_ix": "35-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_66",
            "tgt_ix": "35-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_67",
            "tgt_ix": "35-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_68",
            "tgt_ix": "35-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_69",
            "tgt_ix": "35-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_63",
            "tgt_ix": "35-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_63",
            "tgt_ix": "35-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_63",
            "tgt_ix": "35-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_63",
            "tgt_ix": "35-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_63",
            "tgt_ix": "35-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_63",
            "tgt_ix": "35-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_63",
            "tgt_ix": "35-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_63",
            "tgt_ix": "35-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_61",
            "tgt_ix": "35-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_70",
            "tgt_ix": "35-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_72",
            "tgt_ix": "35-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_73",
            "tgt_ix": "35-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_74",
            "tgt_ix": "35-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_75",
            "tgt_ix": "35-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_71",
            "tgt_ix": "35-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_71",
            "tgt_ix": "35-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_71",
            "tgt_ix": "35-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_71",
            "tgt_ix": "35-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_71",
            "tgt_ix": "35-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_71",
            "tgt_ix": "35-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_0",
            "tgt_ix": "35-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_76",
            "tgt_ix": "35-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_78",
            "tgt_ix": "35-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_79",
            "tgt_ix": "35-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_80",
            "tgt_ix": "35-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_81",
            "tgt_ix": "35-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_82",
            "tgt_ix": "35-ARR_v2_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_83",
            "tgt_ix": "35-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_84",
            "tgt_ix": "35-ARR_v2_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_85",
            "tgt_ix": "35-ARR_v2_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_87",
            "tgt_ix": "35-ARR_v2_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_88",
            "tgt_ix": "35-ARR_v2_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_86",
            "tgt_ix": "35-ARR_v2_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_90",
            "tgt_ix": "35-ARR_v2_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_89",
            "tgt_ix": "35-ARR_v2_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_92",
            "tgt_ix": "35-ARR_v2_93",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_92",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_93",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_91",
            "tgt_ix": "35-ARR_v2_92",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_94",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_93",
            "tgt_ix": "35-ARR_v2_94",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_95",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_94",
            "tgt_ix": "35-ARR_v2_95",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "35-ARR_v2_0",
            "tgt_ix": "35-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_1",
            "tgt_ix": "35-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_2",
            "tgt_ix": "35-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_2",
            "tgt_ix": "35-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_2",
            "tgt_ix": "35-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_2",
            "tgt_ix": "35-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_2",
            "tgt_ix": "35-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_3",
            "tgt_ix": "35-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_4",
            "tgt_ix": "35-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_4",
            "tgt_ix": "35-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_5",
            "tgt_ix": "35-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_5",
            "tgt_ix": "35-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_5",
            "tgt_ix": "35-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_6",
            "tgt_ix": "35-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_6",
            "tgt_ix": "35-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_6",
            "tgt_ix": "35-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_6",
            "tgt_ix": "35-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_7",
            "tgt_ix": "35-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_7",
            "tgt_ix": "35-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_8",
            "tgt_ix": "35-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_9",
            "tgt_ix": "35-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_9",
            "tgt_ix": "35-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_9",
            "tgt_ix": "35-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_10",
            "tgt_ix": "35-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_10",
            "tgt_ix": "35-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_11",
            "tgt_ix": "35-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_11",
            "tgt_ix": "35-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_11",
            "tgt_ix": "35-ARR_v2_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_11",
            "tgt_ix": "35-ARR_v2_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_11",
            "tgt_ix": "35-ARR_v2_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_11",
            "tgt_ix": "35-ARR_v2_11@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_11",
            "tgt_ix": "35-ARR_v2_11@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_11",
            "tgt_ix": "35-ARR_v2_11@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_11",
            "tgt_ix": "35-ARR_v2_11@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_11",
            "tgt_ix": "35-ARR_v2_11@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_11",
            "tgt_ix": "35-ARR_v2_11@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_12",
            "tgt_ix": "35-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_12",
            "tgt_ix": "35-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_12",
            "tgt_ix": "35-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_12",
            "tgt_ix": "35-ARR_v2_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_12",
            "tgt_ix": "35-ARR_v2_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_12",
            "tgt_ix": "35-ARR_v2_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_13",
            "tgt_ix": "35-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_14",
            "tgt_ix": "35-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_14",
            "tgt_ix": "35-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_14",
            "tgt_ix": "35-ARR_v2_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_14",
            "tgt_ix": "35-ARR_v2_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_14",
            "tgt_ix": "35-ARR_v2_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_14",
            "tgt_ix": "35-ARR_v2_14@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_15",
            "tgt_ix": "35-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_15",
            "tgt_ix": "35-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_15",
            "tgt_ix": "35-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_15",
            "tgt_ix": "35-ARR_v2_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_15",
            "tgt_ix": "35-ARR_v2_15@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_15",
            "tgt_ix": "35-ARR_v2_15@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_15",
            "tgt_ix": "35-ARR_v2_15@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_16",
            "tgt_ix": "35-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_16",
            "tgt_ix": "35-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_16",
            "tgt_ix": "35-ARR_v2_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_17",
            "tgt_ix": "35-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_17",
            "tgt_ix": "35-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_17",
            "tgt_ix": "35-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_17",
            "tgt_ix": "35-ARR_v2_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_17",
            "tgt_ix": "35-ARR_v2_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_17",
            "tgt_ix": "35-ARR_v2_17@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_18",
            "tgt_ix": "35-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_18",
            "tgt_ix": "35-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_18",
            "tgt_ix": "35-ARR_v2_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_19",
            "tgt_ix": "35-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_20",
            "tgt_ix": "35-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_20",
            "tgt_ix": "35-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_21",
            "tgt_ix": "35-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_21",
            "tgt_ix": "35-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_21",
            "tgt_ix": "35-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_22",
            "tgt_ix": "35-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_22",
            "tgt_ix": "35-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_22",
            "tgt_ix": "35-ARR_v2_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_22",
            "tgt_ix": "35-ARR_v2_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_22",
            "tgt_ix": "35-ARR_v2_22@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_22",
            "tgt_ix": "35-ARR_v2_22@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_23",
            "tgt_ix": "35-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_23",
            "tgt_ix": "35-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_23",
            "tgt_ix": "35-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_23",
            "tgt_ix": "35-ARR_v2_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_23",
            "tgt_ix": "35-ARR_v2_23@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_23",
            "tgt_ix": "35-ARR_v2_23@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_24",
            "tgt_ix": "35-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_25",
            "tgt_ix": "35-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_25",
            "tgt_ix": "35-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_26",
            "tgt_ix": "35-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_26",
            "tgt_ix": "35-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_26",
            "tgt_ix": "35-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_27",
            "tgt_ix": "35-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_28",
            "tgt_ix": "35-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_28",
            "tgt_ix": "35-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_28",
            "tgt_ix": "35-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_29",
            "tgt_ix": "35-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_29",
            "tgt_ix": "35-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_29",
            "tgt_ix": "35-ARR_v2_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_29",
            "tgt_ix": "35-ARR_v2_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_30",
            "tgt_ix": "35-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_30",
            "tgt_ix": "35-ARR_v2_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_30",
            "tgt_ix": "35-ARR_v2_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_30",
            "tgt_ix": "35-ARR_v2_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_30",
            "tgt_ix": "35-ARR_v2_30@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_30",
            "tgt_ix": "35-ARR_v2_30@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_30",
            "tgt_ix": "35-ARR_v2_30@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_30",
            "tgt_ix": "35-ARR_v2_30@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_30",
            "tgt_ix": "35-ARR_v2_30@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_31",
            "tgt_ix": "35-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_31",
            "tgt_ix": "35-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_32",
            "tgt_ix": "35-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_32",
            "tgt_ix": "35-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_32",
            "tgt_ix": "35-ARR_v2_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_32",
            "tgt_ix": "35-ARR_v2_32@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_32",
            "tgt_ix": "35-ARR_v2_32@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_32",
            "tgt_ix": "35-ARR_v2_32@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_32",
            "tgt_ix": "35-ARR_v2_32@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_32",
            "tgt_ix": "35-ARR_v2_32@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_33",
            "tgt_ix": "35-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_34",
            "tgt_ix": "35-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_34",
            "tgt_ix": "35-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_34",
            "tgt_ix": "35-ARR_v2_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_34",
            "tgt_ix": "35-ARR_v2_34@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_34",
            "tgt_ix": "35-ARR_v2_34@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_34",
            "tgt_ix": "35-ARR_v2_34@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_34",
            "tgt_ix": "35-ARR_v2_34@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_34",
            "tgt_ix": "35-ARR_v2_34@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_34",
            "tgt_ix": "35-ARR_v2_34@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_35",
            "tgt_ix": "35-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_35",
            "tgt_ix": "35-ARR_v2_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_36",
            "tgt_ix": "35-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_36",
            "tgt_ix": "35-ARR_v2_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_37",
            "tgt_ix": "35-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_38",
            "tgt_ix": "35-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_38",
            "tgt_ix": "35-ARR_v2_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_38",
            "tgt_ix": "35-ARR_v2_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_38",
            "tgt_ix": "35-ARR_v2_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_39",
            "tgt_ix": "35-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_40",
            "tgt_ix": "35-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_41",
            "tgt_ix": "35-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_41",
            "tgt_ix": "35-ARR_v2_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_42",
            "tgt_ix": "35-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_42",
            "tgt_ix": "35-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_42",
            "tgt_ix": "35-ARR_v2_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_42",
            "tgt_ix": "35-ARR_v2_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_42",
            "tgt_ix": "35-ARR_v2_42@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_42",
            "tgt_ix": "35-ARR_v2_42@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_42",
            "tgt_ix": "35-ARR_v2_42@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_43",
            "tgt_ix": "35-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_44",
            "tgt_ix": "35-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_44",
            "tgt_ix": "35-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_45",
            "tgt_ix": "35-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_45",
            "tgt_ix": "35-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_46",
            "tgt_ix": "35-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_46",
            "tgt_ix": "35-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_47",
            "tgt_ix": "35-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_48",
            "tgt_ix": "35-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_49",
            "tgt_ix": "35-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_50",
            "tgt_ix": "35-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_50",
            "tgt_ix": "35-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_51",
            "tgt_ix": "35-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_51",
            "tgt_ix": "35-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_51",
            "tgt_ix": "35-ARR_v2_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_51",
            "tgt_ix": "35-ARR_v2_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_51",
            "tgt_ix": "35-ARR_v2_51@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_52",
            "tgt_ix": "35-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_52",
            "tgt_ix": "35-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_53",
            "tgt_ix": "35-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_54",
            "tgt_ix": "35-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_54",
            "tgt_ix": "35-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_55",
            "tgt_ix": "35-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_55",
            "tgt_ix": "35-ARR_v2_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_55",
            "tgt_ix": "35-ARR_v2_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_56",
            "tgt_ix": "35-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_56",
            "tgt_ix": "35-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_56",
            "tgt_ix": "35-ARR_v2_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_57",
            "tgt_ix": "35-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_57",
            "tgt_ix": "35-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_57",
            "tgt_ix": "35-ARR_v2_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_57",
            "tgt_ix": "35-ARR_v2_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_58",
            "tgt_ix": "35-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_58",
            "tgt_ix": "35-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_59",
            "tgt_ix": "35-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_59",
            "tgt_ix": "35-ARR_v2_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_59",
            "tgt_ix": "35-ARR_v2_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_60",
            "tgt_ix": "35-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_60",
            "tgt_ix": "35-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_61",
            "tgt_ix": "35-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_62",
            "tgt_ix": "35-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_62",
            "tgt_ix": "35-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_62",
            "tgt_ix": "35-ARR_v2_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_63",
            "tgt_ix": "35-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_64",
            "tgt_ix": "35-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_64",
            "tgt_ix": "35-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_65",
            "tgt_ix": "35-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_65",
            "tgt_ix": "35-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_66",
            "tgt_ix": "35-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_66",
            "tgt_ix": "35-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_66",
            "tgt_ix": "35-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_66",
            "tgt_ix": "35-ARR_v2_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_66",
            "tgt_ix": "35-ARR_v2_66@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_66",
            "tgt_ix": "35-ARR_v2_66@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_67",
            "tgt_ix": "35-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_67",
            "tgt_ix": "35-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_67",
            "tgt_ix": "35-ARR_v2_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_67",
            "tgt_ix": "35-ARR_v2_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_68",
            "tgt_ix": "35-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_68",
            "tgt_ix": "35-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_69",
            "tgt_ix": "35-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_69",
            "tgt_ix": "35-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_69",
            "tgt_ix": "35-ARR_v2_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_69",
            "tgt_ix": "35-ARR_v2_69@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_69",
            "tgt_ix": "35-ARR_v2_69@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_69",
            "tgt_ix": "35-ARR_v2_69@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_69",
            "tgt_ix": "35-ARR_v2_69@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_69",
            "tgt_ix": "35-ARR_v2_69@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_69",
            "tgt_ix": "35-ARR_v2_69@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_70",
            "tgt_ix": "35-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_70",
            "tgt_ix": "35-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_70",
            "tgt_ix": "35-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_70",
            "tgt_ix": "35-ARR_v2_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_71",
            "tgt_ix": "35-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_72",
            "tgt_ix": "35-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_72",
            "tgt_ix": "35-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_72",
            "tgt_ix": "35-ARR_v2_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_72",
            "tgt_ix": "35-ARR_v2_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_73",
            "tgt_ix": "35-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_73",
            "tgt_ix": "35-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_74",
            "tgt_ix": "35-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_74",
            "tgt_ix": "35-ARR_v2_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_74",
            "tgt_ix": "35-ARR_v2_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_75",
            "tgt_ix": "35-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_75",
            "tgt_ix": "35-ARR_v2_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_75",
            "tgt_ix": "35-ARR_v2_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_75",
            "tgt_ix": "35-ARR_v2_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_75",
            "tgt_ix": "35-ARR_v2_75@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_76",
            "tgt_ix": "35-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_76",
            "tgt_ix": "35-ARR_v2_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_77",
            "tgt_ix": "35-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_78",
            "tgt_ix": "35-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_78",
            "tgt_ix": "35-ARR_v2_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_79",
            "tgt_ix": "35-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_79",
            "tgt_ix": "35-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_80",
            "tgt_ix": "35-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_80",
            "tgt_ix": "35-ARR_v2_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_81",
            "tgt_ix": "35-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_82",
            "tgt_ix": "35-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_82",
            "tgt_ix": "35-ARR_v2_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_83",
            "tgt_ix": "35-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_83",
            "tgt_ix": "35-ARR_v2_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_83",
            "tgt_ix": "35-ARR_v2_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_84",
            "tgt_ix": "35-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_84",
            "tgt_ix": "35-ARR_v2_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_84",
            "tgt_ix": "35-ARR_v2_84@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_85",
            "tgt_ix": "35-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_85",
            "tgt_ix": "35-ARR_v2_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_86",
            "tgt_ix": "35-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_86",
            "tgt_ix": "35-ARR_v2_86@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_87",
            "tgt_ix": "35-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_87",
            "tgt_ix": "35-ARR_v2_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_87",
            "tgt_ix": "35-ARR_v2_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_88",
            "tgt_ix": "35-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_88",
            "tgt_ix": "35-ARR_v2_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_88",
            "tgt_ix": "35-ARR_v2_88@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_88",
            "tgt_ix": "35-ARR_v2_88@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_88",
            "tgt_ix": "35-ARR_v2_88@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_88",
            "tgt_ix": "35-ARR_v2_88@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_89",
            "tgt_ix": "35-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_89",
            "tgt_ix": "35-ARR_v2_89@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_89",
            "tgt_ix": "35-ARR_v2_89@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_89",
            "tgt_ix": "35-ARR_v2_89@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_89",
            "tgt_ix": "35-ARR_v2_89@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_90",
            "tgt_ix": "35-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_90",
            "tgt_ix": "35-ARR_v2_90@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_91",
            "tgt_ix": "35-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_92",
            "tgt_ix": "35-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_92",
            "tgt_ix": "35-ARR_v2_92@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_93",
            "tgt_ix": "35-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_93",
            "tgt_ix": "35-ARR_v2_93@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_93",
            "tgt_ix": "35-ARR_v2_93@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_93",
            "tgt_ix": "35-ARR_v2_93@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_93",
            "tgt_ix": "35-ARR_v2_93@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_93",
            "tgt_ix": "35-ARR_v2_93@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_94",
            "tgt_ix": "35-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_94",
            "tgt_ix": "35-ARR_v2_94@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_94",
            "tgt_ix": "35-ARR_v2_94@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_95",
            "tgt_ix": "35-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_95",
            "tgt_ix": "35-ARR_v2_95@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_96",
            "tgt_ix": "35-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_97",
            "tgt_ix": "35-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_98",
            "tgt_ix": "35-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_99",
            "tgt_ix": "35-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_100",
            "tgt_ix": "35-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_101",
            "tgt_ix": "35-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_102",
            "tgt_ix": "35-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_103",
            "tgt_ix": "35-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_104",
            "tgt_ix": "35-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_105",
            "tgt_ix": "35-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_106",
            "tgt_ix": "35-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_107",
            "tgt_ix": "35-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_108",
            "tgt_ix": "35-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_109",
            "tgt_ix": "35-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_110",
            "tgt_ix": "35-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_111",
            "tgt_ix": "35-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_112",
            "tgt_ix": "35-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_113",
            "tgt_ix": "35-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_114",
            "tgt_ix": "35-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_115",
            "tgt_ix": "35-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_116",
            "tgt_ix": "35-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_117",
            "tgt_ix": "35-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_118",
            "tgt_ix": "35-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_119",
            "tgt_ix": "35-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_120",
            "tgt_ix": "35-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_121",
            "tgt_ix": "35-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_122",
            "tgt_ix": "35-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_123",
            "tgt_ix": "35-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_124",
            "tgt_ix": "35-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_125",
            "tgt_ix": "35-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_126",
            "tgt_ix": "35-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_127",
            "tgt_ix": "35-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_128",
            "tgt_ix": "35-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_129",
            "tgt_ix": "35-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_130",
            "tgt_ix": "35-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_131",
            "tgt_ix": "35-ARR_v2_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_132",
            "tgt_ix": "35-ARR_v2_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_133",
            "tgt_ix": "35-ARR_v2_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_134",
            "tgt_ix": "35-ARR_v2_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_135",
            "tgt_ix": "35-ARR_v2_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_136",
            "tgt_ix": "35-ARR_v2_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_137",
            "tgt_ix": "35-ARR_v2_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_138",
            "tgt_ix": "35-ARR_v2_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_139",
            "tgt_ix": "35-ARR_v2_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_140",
            "tgt_ix": "35-ARR_v2_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_141",
            "tgt_ix": "35-ARR_v2_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_142",
            "tgt_ix": "35-ARR_v2_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_143",
            "tgt_ix": "35-ARR_v2_143@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_144",
            "tgt_ix": "35-ARR_v2_144@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_145",
            "tgt_ix": "35-ARR_v2_145@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_146",
            "tgt_ix": "35-ARR_v2_146@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_147",
            "tgt_ix": "35-ARR_v2_147@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_148",
            "tgt_ix": "35-ARR_v2_148@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_149",
            "tgt_ix": "35-ARR_v2_149@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_150",
            "tgt_ix": "35-ARR_v2_150@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_151",
            "tgt_ix": "35-ARR_v2_151@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_152",
            "tgt_ix": "35-ARR_v2_152@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_153",
            "tgt_ix": "35-ARR_v2_153@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_154",
            "tgt_ix": "35-ARR_v2_154@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_155",
            "tgt_ix": "35-ARR_v2_155@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_156",
            "tgt_ix": "35-ARR_v2_156@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_157",
            "tgt_ix": "35-ARR_v2_157@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_158",
            "tgt_ix": "35-ARR_v2_158@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_159",
            "tgt_ix": "35-ARR_v2_159@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_160",
            "tgt_ix": "35-ARR_v2_160@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_161",
            "tgt_ix": "35-ARR_v2_161@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_162",
            "tgt_ix": "35-ARR_v2_162@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_163",
            "tgt_ix": "35-ARR_v2_163@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_164",
            "tgt_ix": "35-ARR_v2_164@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_165",
            "tgt_ix": "35-ARR_v2_165@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_166",
            "tgt_ix": "35-ARR_v2_166@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_167",
            "tgt_ix": "35-ARR_v2_167@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_168",
            "tgt_ix": "35-ARR_v2_168@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_169",
            "tgt_ix": "35-ARR_v2_169@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "35-ARR_v2_170",
            "tgt_ix": "35-ARR_v2_170@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1130,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "35-ARR",
        "version": 2
    }
}