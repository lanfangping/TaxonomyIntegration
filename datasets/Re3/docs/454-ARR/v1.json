{
    "nodes": [
        {
            "ix": "454-ARR_v1_0",
            "content": "AbductionRules: Training Transformers to Explain Unexpected Inputs",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_2",
            "content": "Transformers have recently been shown to be capable of reliably performing logical reasoning over facts and rules expressed in natural language, but abductive reasoning -inference to the best explanation of an unexpected observation -has been underexplored despite significant applications to scientific discovery, common-sense reasoning, and model interpretability.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_3",
            "content": "This paper presents AbductionRules, a group of natural language datasets designed to train and test generalisable abduction over naturallanguage knowledge bases. We use these datasets to finetune pretrained Transformers and discuss their performance, finding that our models learned generalisable abductive techniques but also learned to exploit the structure of our data. Finally, we discuss the viability of this approach to abductive reasoning and ways in which it may be improved in future work.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_4",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "454-ARR_v1_5",
            "content": "Since its introduction, models based on the Transformer (Vaswani et al., 2017) have, due to their learning ability and Turing-completeness (Bhattamishra et al., 2020), sparked research into their use in many applications beyond their original purpose of natural language processing (NLP), including image processing and generation (Parmar et al., 2018;Chen et al., 2020), theorem proving (Polu and Sutskever, 2020;Welleck et al., 2021), and chess (Noever et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_6",
            "content": "One such task is logical inference -reasoning over first-order logic (FOL) knowledge bases (collections of facts and rules). Given a knowledge base, one may attempt to find logical implications (deduction), discover rules that extrapolate patterns in known facts (induction), or infer facts that would explain surprising observations (abduction). More specifically, if a newly observed fact p cannot be deduced from an existing knowledge base, abduction is the process of finding one or more facts that, if added to the knowledge base, would allow p to be deduced from existing rules. Figure 1 demonstrates the difference between these three kinds of inference.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_7",
            "content": "Traditionally, FOL is represented using a formal mathematical syntax, with facts resembling HUMAN(SOCRATES) and rules resembling \u2200X : HUMAN(X) =\u21d2 MORTAL(X). Clark et al. (2020) recently pioneered an alternative approach we call natural-language logic, which might represent these as \"Socrates is human\" and \"Humans are mortal\". This approach, properly followed, retains the precision of the mathematical syntax while also taking advantage of Transformers' NLP aptitude and pretraining. This approach also allows reasoning over texts not written in formal representations. Clark et al. (2020) examined their models' potential for deduction only. Tafjord et al. (2021) extended this work to explore abduction but retained a focus on deduction.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_8",
            "content": "Our goal is to use the natural-language logic approach to train Transformers to perform abductive reasoning with the following properties:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_9",
            "content": "\u2022 Generalisable: Be able to apply techniques outside domains in which they were learned. \u2022 Generative: Produce explanations rather than labelling them as sufficient or insufficient. \u2022 Single-hop: Produce direct explanations. Instead of \"plants are green because chlorophyll is green because green light is not used in photosynthesis\", prefer \"plants are green because chlorophyll is green\". If further explanation is desired, abduction can be applied again. \u2022 Discerning: Prefer simpler explanations. \u2022 Explicit: Use given knowledge bases rather than relying on pretraining.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_10",
            "content": "Our efforts to train abduction in this way are motivated by multiple potential applications. \u2022 Ray (2007) describes the use of automated abduction in scientific discovery. Since much scientific knowledge exists in the form of natural language rather than formal representations, advances in natural-language abduction would greatly assist in automating the scientific method by helping to explain experimental observations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_11",
            "content": "\u2022 Ignatiev et al. (2019) describe the use of abduction to interpret deep learning models similar to Transformers, which are infamously difficult to interpret. \u2022 Abduction may also help solve the longstanding problem of automating common-sense reasoning. Transformers excel at memorising common knowledge but routinely fail to capture any underlying reasoning. Training these models to explain their own outputs may remedy this problem by providing a way to integrate this fractured knowledge into a more connected model of reality.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_12",
            "content": "We present the following contributions:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_13",
            "content": "\u2022 A collection of datasets for training and testing natural-language abduction. \u2022 A method of synthetically generating more realistic natural-language logic datasets. \u2022 Experimental results showing that Transformers can perform abductive reasoning without additional architecture.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_14",
            "content": "2 Related Work",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_15",
            "content": "Natural-language logic",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "454-ARR_v1_16",
            "content": "Our work builds on the RuleTaker line of research on natural-language logic. This line began with Clark et al. (2020), who developed RuleTakers to reason deductively over FOL knowledge bases expressed in natural language, judging given facts to be true or false. These achieved promising results but failed to accurately explain their reasoning or generalise to inferences requiring more steps than were seen at training time. PRover (Saha et al., 2020) achieved greater explainability by generating proofs of its answers. Similarly, the Iterative variant of ProofWriter (Tafjord et al., 2021) chained single-hop deductions rather than reasoning through multi-hop deductions all at once, making its reasoning transparent and easily generalisable to unseen depths. multiPRover (Saha et al., 2021) also made use of this iterative approach.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_17",
            "content": "The generalisability and interpretability of iterative single-hop reasoning are why we seek to train single-hop abduction. Tafjord et al. (2021) also adapted their deductionbased datasets to train abductive reasoning, achieving success but training multi-hop abduction only, and also requiring models to output every possible explanation. By contrast, we seek to train models to discern between simpler and more complex explanations -for example, to prefer explanations requiring fewer unknown facts. Bhagavatula et al. (2019) presented two more abduction-based datasets: \u03b1-NLI, which tests models' ability to choose which of two hypotheses better explains an observation, and \u03b1-NLG, a generative version of the same dataset. These datasets do not give supporting knowledge bases -all background information must come from pretraining. While this is a valuable approach, we seek to investigate how well Transformers can reason over given knowledge bases to incorporate explicit background knowledge. Gontier et al. (2020) investigated Transformers' ability to perform inductive reasoning in natural language, finding them able to extrapolate patterns in given proofs but again unable to generalise to more complex proofs. Saparov and Mitchell (2021) developed an alternative approach to classifying the ProofWriter datasets that does not reason over natural language, instead using a symbolic, Bayesian approach and using abductive reasoning to satisfy constraints.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_18",
            "content": "Other adjacent work",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "454-ARR_v1_19",
            "content": "Their models' superior performance demonstrates that while Transformers are effective at logical reasoning, they may benefit from more specialised architecture.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_20",
            "content": "Methodology",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "454-ARR_v1_21",
            "content": "Prior to our work, there existed no dataset capable of training or testing the kind of abductive reasoning we seek. We therefore present Abduction-Rules, a natural-language logic dataset designed for this task, and use it to train and test several models based on a pretrained Text-to-Text Transfer Transformer, or T5 (Raffel et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_22",
            "content": "Datasets",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "454-ARR_v1_23",
            "content": "AbductionRules has three main predecessors.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_24",
            "content": "Rule Reasoning",
            "ntype": "title",
            "meta": {
                "section": "3.1.1"
            }
        },
        {
            "ix": "454-ARR_v1_25",
            "content": "The Rule Reasoning dataset developed by Clark et al. (2020) was, to our knowledge, the first naturallanguage logic dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_26",
            "content": "To create this dataset, FOL predicates (e.g. BIG(LION)) were procedurally generated, entities (LION) and attributes (BIG(X)) were extracted, and templates (\"The {entity} is {attribute}\") were used to create natural-language logic translations (\"The lion is big\"). Rules were created similarly (e.g. \u2200X : BIG(X) =\u21d2 BLUE(X) became \"If something is big then it is blue\"). Facts and rules were grouped into knowledge bases, each with several questions; the model's task is to label each question true or false.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_27",
            "content": "The Rule Reasoning dataset includes knowledge bases in several domains; those in the animaldomain use animals as entities while those in the person-domain use peoples' names. All subsequent datasets similarly use these two domains. The animal-domain includes multi-entity facts (CHASES(LION, MOUSE), or \"the lion chases the mouse\"). For our purposes, we consider the lion to be the main entity and \"chases the mouse\" to be an attribute of the lion.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_28",
            "content": "ParaRules",
            "ntype": "title",
            "meta": {
                "section": "3.1.2"
            }
        },
        {
            "ix": "454-ARR_v1_29",
            "content": "Recognising that their translations of mathematical syntax into natural language were strict and unrealistic (e.g. \"Charlie is green. Charlie is rough.\"), Clark et al. (2020) also produced ParaRules, which contained knowledge bases and questions similar to those in the Rule Reasoning dataset, but were paraphrased into more colloquial language (e.g. \"Charlie has green teeth and rough skin.\"). This approach much better prepares Transformers to reason logically over naturally-occurring texts but requires large amounts of human labour to produce. For this reason, ParaRules is much smaller than the Rule Reasoning dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_30",
            "content": "PARARULE Plus",
            "ntype": "title",
            "meta": {
                "section": "3.1.3"
            }
        },
        {
            "ix": "454-ARR_v1_31",
            "content": "Seeing the value in RuleTaker's size and ease of production as well as the greater utility of ParaRules, Bao (2021) produced PARARULE Plus, a compromise between the Rule Reasoning dataset and ParaRules that procedurally rephrases all rules during generation by using various templates. PARARULE Plus also avoids being entirely context-free by pooling related attributes (such as \"big\", \"strong\", \"high\" and \"huge\") and only giving entities attributes from one pool. While PARARULE Plus falls short of ParaRules' variety, its greater collection of rephrased rules is highly valuable.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_32",
            "content": "AbductionRules",
            "ntype": "title",
            "meta": {
                "section": "3.1.4"
            }
        },
        {
            "ix": "454-ARR_v1_33",
            "content": "We adapt the open-source code used to generate PARARULE Plus to create AbductionRules 1 , making the following changes:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_34",
            "content": "\u2022 Instead of labelling questions (for our purposes, \"observations\") with \"true\" or \"false\", we use the lone fact (or \"explanation\") that would prove or disprove it. \u2022 We ensure that no two knowledge bases in the same dataset give the same attributes to the same entities to avoid repeats. This reduces the size of the datasets; to compensate, we increase the number of entities. \u2022 While each rule has a single condition in PARARULE Plus (\"If something is cute, then...\"), we give three (\"If something is cute, funny, and adorable, then...\"), with an entity that satisfies exactly two conditions; the model must identify the third.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_35",
            "content": "After making these changes, we produce datasets with increasing levels of complexity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_36",
            "content": "\u2022 The first complexity level contains no further changes from PARARULE Plus and yields the dataset Abduction-Animal-0.1.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_37",
            "content": "Facts: The squirrel is quiet . The leopard is slow. The dog is adorable. The crocodile is heavy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_38",
            "content": "The leopard is boring. The leopard is angry. The crocodile is awful. The leopard attacks the squirrel.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_39",
            "content": "The dog is small. The dog is cute. The squirrel is nice. The crocodile likes the dog. The squirrel is kind .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_40",
            "content": "Rules: If something is cute, is adorable, and is furry, then it is also lovely. All animals that are obese, are awful, and are heavy, are big. If an animal is fierce, sees the squirrel, and likes the dog, it is tired. Things that are smart , are kind , and are quiet , are also round . If an animal chases the dog, is boring, and attacks the squirrel, then it is also strong. All things that are slow, are sleepy, and are angry, are rough.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_41",
            "content": "Observation: The squirrel is round .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_42",
            "content": "Explanation: The squirrel is smart . \u2022 At the second complexity level, we shuffle all knowledge bases to prevent models from exploiting the constant position of all sentences and attributes. This yields the dataset Abduction-Animal-0.2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_43",
            "content": "\u2022 At the third complexity level, we procedurally rephrase rules with random variations instead of using the same templates as PARARULE Plus. For example, the animal-domain FOL rule \u2200X : (BIG(X) \u2227 HEAVY(X) \u2227 FIERCE(X)) =\u21d2 STRONG(X) might be rephrased as \"All animals that are big, are heavy, and are fierce, are also strong\" or \"If something is heavy, is fierce, and is big, it is strong\", among many other similar variations. Notably, this rephrasing process involves reordering all attributes so that attributes contained in correct abductions might be first, second, or third. This yields the datasets Abduction-Animal-Simple and Abduction-Person-Simple. Figure 2 contains an example item from Animal-Simple. 2",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_44",
            "content": "2 For brevity, we omit the \"Abduction-\" prefix when dis-This method of procedural rule rephrasing represents a useful iteration on the naturallanguage logic approach and leaves room for further improvement. Concentrated work in this line of research may produce synthetic natural-language logic datasets that are larger yet exhibit much wider variety, making this approach more powerful and robust.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_45",
            "content": "\u2022 At the fourth and final complexity level, we add extraneous confounding rules to knowledge bases. While lower complexity levels only ever have one rule that could explain a given observation, here we create two variations of every (single-entity) rule; one replaces a satisfied condition with an unsatisfied condition, while the other replaces all three conditions. All replacements come from different pools. This yields the datasets Abduction-Animal and Abduction-Person.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_46",
            "content": "Figure 3 contains simplified examples of data from each complexity level.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_47",
            "content": "We intend each successive complexity level to remove additional idiosyncrasies that might be exploited in lieu of using abduction (i.e. used to \"cheat\"), so that this exploitation can be detected. We also intend the fourth to train models to favour simpler explanations when strictly more complex explanations are available.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_48",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "454-ARR_v1_49",
            "content": "We use AbductionRules to train 8 models based on the pretrained T5 implementation from the Hug-gingFace Transformers library (Wolf et al., 2020). 3 We first use each training set to train 1 model, yielding 6 models trained at 4 complexity levels across 2 domains. To compare domains and complexity levels, we test all models on all test sets, giving us intra-domain results (isolating the effect of the complexity), and inter-domain results (some isolating the effect of the domain). We expect each successive complexity level to train a better-quality model and the two domains to be mostly comparable with some variation attributable to the animaldomain's multi-entity facts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_50",
            "content": "If our approach were adapted to models extensively trained to reason on many domains, we expect that teaching abduction in every domain would cussing the AbductionRules datasets within this paper.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_51",
            "content": "3 All code used for experiments in this paper can be found at https://anonymous.4open.science/r/ AbductionRules-D89D. be prohibitively expensive. Therefore, we seek to investigate Transformers' ability to transfer abductive reasoning techniques to domains where these techniques have not been taught but are nonetheless familiar to the Transformer. To this end, we train two more cross-domain models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_52",
            "content": "\u2022 We train one model on our simplest dataset and our most complex dataset in another domain, i.e. Animal-0.1 and Person. We name this model Person+Animal-0.1. \u2022 We train another model on the simplest person-domain dataset and the most complex animal-domain dataset, i.e. Person-Simple and Animal, to compare the two domains. We name this model Animal+Person-Simple.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_53",
            "content": "While we are interested in these cross-domain models' performance on all datasets, we are particularly interested in their results on the most complex dataset on which they were not trained (Abduction-Animal and Abduction-Person, respectively). We treat performance on these datasets as a proxy for Transformers' ability to apply abductive reasoning outside the domains in which it was trained.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_54",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "454-ARR_v1_55",
            "content": "Table 1 contains our results, showing the percentage of abductions correctly performed by each model on each test set. 4 Note that no model ever gave a correct explanation outside the domain(s) in which it was trained. On the surface, this would suggest that our models were unable to generalise to new domains. However, inspection of inter-domain results shows that this is not entirely accurate; many explanations contain errors but nonetheless identify the ground-truth 4 All our results can be found at https://anonymous. 4open.science/r/AbductionRules-D89D.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_56",
            "content": "explanation. For example, the animal models commonly appended \"The\" to correct explanations, as in \"The Bob is small\"; while this is incorrect, it nonetheless indicates the correct explanation in a way that suggests the model still performed the correct abduction. We distinguish between two kinds of errors in correct-yet-useful explanations: lossless errors and lossy errors.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_57",
            "content": "Lossless errors",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "454-ARR_v1_58",
            "content": "Explanations with lossless errors failed to match the correct explanation character-for-character but allowed it to be reliably identified.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_59",
            "content": "We found several ways in which recognisably correct explanations differed from the ground-truth, such as extra words (\"The Bob is small\", \"The lion is attacks the mouse\"), looping (\"The dog is is is is is small\"), and incorrect grammar (\"The anne is wealthy\"). While these errors point towards flaws in training, it is a strength of natural-language logic and soft reasoners that they can cope with minor grammar mistakes as long as meaning is preserved.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_60",
            "content": "Table 2 contains our results after correcting for these errors. Note that animal-domain models achieved performance comparable to the persondomain models on novel datasets in their own domain, while person-domain models saw minimal inter-domain improvement. Cross-domain models also saw almost no improvement, suggesting that having seen correct explanations in both domains eliminated this kind of formatting error.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_61",
            "content": "Lossy errors",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "454-ARR_v1_62",
            "content": "The most important aspect of abduction in our datasets is identification of the correct attribute. The entity at the beginning of the explanation always matches that at the beginning of the observation; therefore, if the correct attribute is identified, the correct explanation can be reconstructed. Table 2: Improvement of all models on all test sets after allowing lossless errors.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_63",
            "content": "Table 3 contains our results after correcting for these errors. Note that every model achieved some useful results on every test set. Most inter-domain results improved to rival intra-domain results, although the Abduction-Person model continued to struggle. Intra-domain results saw minimal improvement, with none seeing a >2% point increase. The cross-domain models again saw no visible improvement, further suggesting that these inferior results were avoidable from seeing facts, rules, and explanations in different formats at training time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_64",
            "content": "Discussion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "454-ARR_v1_65",
            "content": "Our results show that models trained on our simplest datasets struggle to generalise to new complexity levels and domains, while those trained on our more complex datasets are better able to generalise but still perform suboptimally. Meanwhile, those trained on combined cross-domain datasets achieve performance superior to the sum of models trained on their parts and easily apply skills outside domains in which they were learned. It is also clear that models trained in the animal-domain achieve better intra-domain and inter-domain performance than person-domain models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_66",
            "content": "5.1 Animal-0.1 and Animal-0.2 Unsurprisingly, the models trained on our simplest datasets fare the worst. Our Animal-0.1 and -0.2 models perform similarly poorly, suggesting that Animal-0.2's additional complexity from randomised sentence orderings was of minimal importance. In fact, the Animal-0.2 model's performance on more complex datasets is worse than its simpler counterpart; examination of its results reveals a tendency to loop on unfamiliar inputs. Given the Animal-0.1 model's 99.3% correct (100% allowing lossy errors) performance on Animal-0.2, we treat these complexity levels as equivalent and the Animal-0.1 model as definitive.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_67",
            "content": "The Animal-0.1 model is approximately 1/3 as accurate on the person-domain when allowing lossless errors but only loses approximately 6% points when allowing lossy errors, suggesting that it fails to adapt to new formats but is mostly able to use the same techniques as in the animal-domain.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_68",
            "content": "These models' significant performance hit on higher complexity levels clearly indicates that they exploit the structure of their training set. However, it should be noted that the Animal-0.1 model drops each time by approximately a factor of 2. If this model only chose the penultimate attribute in a sentence containing the attribute in the question, its accuracy would drop by a factor of 3 with procedural rephrasing and again with confounding rules. Therefore, both models utilise some level of generalised abductive reasoning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_69",
            "content": "Animal-Simple and Person-Simple",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "454-ARR_v1_70",
            "content": "The Animal-Simple model significantly outperforms our simpler models; this makes sense since Table 3: Improvement of all models on all test sets after allowing lossy errors.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_71",
            "content": "Animal-0.1 and -0.2 can be thought of as special, unshuffled cases of Animal-Simple. 5 Similarly to the Animal-0.1 model, the Animal-Simple model performs about half as well on Animal as on Animal-Simple. This model also performs worse on Person-Simple than Animal when allowing lossless errors but better when allowing lossy errors, implying that it exploits the structure of Animal-Simple to some degree to identify correct attributes. Its performance drop from Person-Simple to Person is greater than from Animal-Simple to Animal, suggesting that changes in domain and complexity are more difficult to generalise when compounded.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_72",
            "content": "Our Person-Simple model also performs well but fails to generalise to higher complexity; this can be partially explained by the multi-entity facts in the animal-domain, as rules using these facts are not used to create confounding rules. This model gives almost no correct inter-domain explanations unless lossy errors are allowed, in which case it achieves similar inter-domain performance to the animal-domain models. Its performance drop on Animal can be compared to that of the Animal-Simple model from Person-Simple to Person, exacerbated by the person-domain models' poorer performance in general.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_73",
            "content": "Abduction-Animal and Abduction-Person",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "454-ARR_v1_74",
            "content": "The Animal model performs the best of all singledomain models, achieving >60% performance on all datasets except Person-Simple when allowing lossy errors. The drop from Person to Person-Simple is evidence of cheating, but its generalisability is superior to all other models and demonstrates some abductive ability. Surprisingly, it achieves worse intra-domain results on lower complexity levels than the Animal-Simple model, again indi-5 Because of this and their failure to train generalisable abduction, we do not include either Animal-0.1 or -0.2 in the public release of AbductionRules. The code we used for our experiments can be used to regenerate them if desired.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_75",
            "content": "cating that some of its performance is dependent on Animal's rule structure. Still, this performance drop is relatively small (being <10% in all cases), further reinforcing that while this model utilises some degree of both cheating and abduction (like all our models), its abductive capabilities generalise to a promising extent.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_76",
            "content": "By contrast, the Person model achieves the worst performance of any model, performing as well on Person-Simple as that dataset's model does on Person and achieving abysmal inter-domain performance, even on Animal. This model is the clearest indication that (our instantiations of) the two domains are not equivalent; the animal-domain's models are much better able to generalise.The multientity rules again offer some explanatory powerthe Animal model demonstrates some overtraining on the confounding rules and so performs more poorly in their absence, but still learned to explain observations using multi-entity rules that lacked confounding equivalents, making it robust to extraneous rules but not reliant on them. If this were a major determining factor, we would expect models trained on both maximally and minimally complex datasets to be even more robust and generalised.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_77",
            "content": "Cross-domain models",
            "ntype": "title",
            "meta": {
                "section": "5.4"
            }
        },
        {
            "ix": "454-ARR_v1_78",
            "content": "Our cross-domain models are our best-performing models by far, achieving superior performance on unseen datasets than the sum of models trained on their combined training sets' parts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_79",
            "content": "The Person+Animal-0.1 model, being trained on our simplest dataset and having its most complex training set come from the worse of our two training domains, is the worse of our two cross-domain models. Nonetheless, it reaches a remarkable level of performance, explaining >76% of all observations correctly on all test sets. Its performance in the face of unconfounded rephrased rules (something unprecedented in its training) is dependent on the domain. In the person-domain (i.e. on Person-Simple), where it received its most complex train-ing, it achieves its best result on a dataset it was not trained on (excepting Animal-0.2), while in the animal-domain (i.e. on Animal-Simple) it achieves its worst result, having not seen any rephrased animal rules at training time. Still, it demonstrates a greater ability than any single-domain model to generalise to these unfamiliar rule structures. It can also apply its training on confounded rules outside the domain in which it was learned, achieving far greater performance on Animal than any other dataset that it was not trained on.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_80",
            "content": "The Animal+Person-Simple model is our best and most promising, achieving >99% performance on every dataset and consistently adapting to all complexity levels in every domain. Like Person+Animal-0.1, it encounters unprecedented rule structures (singular single-entity animal rules, confounded person rules) and generalises almost perfectly to each. While our datasets remain somewhat limited in scope, we believe that this result demonstrates that Transformers can generalise abductive techniques beyond the domains in which those techniques were trained, provided the domain itself is not entirely novel.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_81",
            "content": "Extrapolating these cross-domain results, it seems likely that finetuning Transformers that have received extensive pretraining (such as GPT-3 (Brown et al., 2020)) on datasets covering more varied and complex examples of abduction would make these models capable of much more generalised natural-language abductive reasoning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_82",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "454-ARR_v1_83",
            "content": "We have presented the AbductionRules datasets and shown that pretrained T5 models finetuned on them exhibit generalised abductive reasoning. Our more complex datasets train abduction more generally reliably than our less complex datasets. Further, training in multiple domains is superior to training in only one domain, and we have clear evidence of generalisation of techniques from one domain to another. We have also made improvements to the generation of natural-language logic dataset generation, presenting a new middle-ground between the template-based PARARULE Plus (Bao, 2021) and the manually rephrased Pararules (Clark et al., 2020). We believe our results are promising and demonstrate the viability of Transformer-based abduction (and logical reasoning in general), but also indicate opportunities for improvement.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_84",
            "content": "Future Work",
            "ntype": "title",
            "meta": {
                "section": "6.1"
            }
        },
        {
            "ix": "454-ARR_v1_85",
            "content": "Future work in this area might explore:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_86",
            "content": "\u2022 Examining skill transfer between different kinds of logical reasoning. \u2022 Applying abductive techniques in real-world, as opposed to artificial, domains. \u2022 Generating probability distributions over multiple possible explanations. \u2022 Testing explanations by verifying that they allow the original observation to be deduced. \u2022 Explanations that include not only missing premises but the relevant rule(s) they satisfy.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_87",
            "content": "Plural? Specific? Also? Then/All? \u2022 Incorrect capitalisation, as in \"The anne is wealthy.\"",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_88",
            "content": "\u2022 Omission of spaces, as in \"Thebob is small.\"",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_89",
            "content": "C Abduction-Person-Simple example Label: Fiona is big . The model must output the explanation given the context and observation as input. Facts and rules used to explain the observation are bolded while relevant attributes are highlighted.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "454-ARR_v1_90",
            "content": "UNKNOWN, None, 2021, Pararule plus: A larger deep multistep reasoning dataset over natural language, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Pararule plus: A larger deep multistep reasoning dataset over natural language",
                "pub": null
            }
        },
        {
            "ix": "454-ARR_v1_91",
            "content": "Chandra Bhagavatula, Chaitanya Ronan Le Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Wen-Tau Downey, Yejin Yih,  Choi, Abductive commonsense reasoning, 2019, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Chandra Bhagavatula",
                    "Chaitanya Ronan Le Bras",
                    "Keisuke Malaviya",
                    "Ari Sakaguchi",
                    "Hannah Holtzman",
                    "Doug Rashkin",
                    "Wen-Tau Downey",
                    "Yejin Yih",
                    " Choi"
                ],
                "title": "Abductive commonsense reasoning",
                "pub_date": "2019",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "454-ARR_v1_92",
            "content": "Satwik Bhattamishra, Arkil Patel, Navin Goyal, On the computational power of transformers and its implications in sequence modeling, 2020, Proceedings of the 24th Conference on Computational Natural Language Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Satwik Bhattamishra",
                    "Arkil Patel",
                    "Navin Goyal"
                ],
                "title": "On the computational power of transformers and its implications in sequence modeling",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 24th Conference on Computational Natural Language Learning",
                "pub": null
            }
        },
        {
            "ix": "454-ARR_v1_93",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "454-ARR_v1_94",
            "content": "Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever, Generative pretraining from pixels, 2020, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Mark Chen",
                    "Alec Radford",
                    "Rewon Child",
                    "Jeffrey Wu",
                    "Heewoo Jun",
                    "David Luan",
                    "Ilya Sutskever"
                ],
                "title": "Generative pretraining from pixels",
                "pub_date": "2020",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "454-ARR_v1_95",
            "content": "Peter Clark, Oyvind Tafjord, Kyle Richardson, Transformers as soft reasoners over language, 2020, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI-20), .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Peter Clark",
                    "Oyvind Tafjord",
                    "Kyle Richardson"
                ],
                "title": "Transformers as soft reasoners over language",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI-20)",
                "pub": null
            }
        },
        {
            "ix": "454-ARR_v1_96",
            "content": "Nicolas Gontier, Koustuv Sinha, Siva Reddy, and Chris Pal. 2020. Measuring systematic generalization in neural proof generation with transformers, , Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Nicolas Gontier",
                    "Koustuv Sinha"
                ],
                "title": "Siva Reddy, and Chris Pal. 2020. Measuring systematic generalization in neural proof generation with transformers",
                "pub_date": null,
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "454-ARR_v1_97",
            "content": "Alexey Ignatiev, Nina Narodytska, Joao Marques-Silva, Abduction-based explanations for machine learning models, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Alexey Ignatiev",
                    "Nina Narodytska",
                    "Joao Marques-Silva"
                ],
                "title": "Abduction-based explanations for machine learning models",
                "pub_date": "2019",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "454-ARR_v1_98",
            "content": "UNKNOWN, None, 2020, The chess transformer: Mastering play using generative language models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "The chess transformer: Mastering play using generative language models",
                "pub": null
            }
        },
        {
            "ix": "454-ARR_v1_99",
            "content": "Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, Dustin Tran, Image transformer, 2018, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Niki Parmar",
                    "Ashish Vaswani",
                    "Jakob Uszkoreit",
                    "Lukasz Kaiser",
                    "Noam Shazeer",
                    "Alexander Ku",
                    "Dustin Tran"
                ],
                "title": "Image transformer",
                "pub_date": "2018",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "454-ARR_v1_100",
            "content": "UNKNOWN, None, 2020, Generative language modeling for automated theorem proving, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Generative language modeling for automated theorem proving",
                "pub": null
            }
        },
        {
            "ix": "454-ARR_v1_101",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Colin Raffel",
                    "Noam Shazeer",
                    "Adam Roberts",
                    "Katherine Lee",
                    "Sharan Narang",
                    "Michael Matena",
                    "Yanqi Zhou",
                    "Wei Li",
                    "Peter J Liu"
                ],
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "pub_date": "2020",
                "pub_title": "Journal of Machine Learning Research",
                "pub": null
            }
        },
        {
            "ix": "454-ARR_v1_102",
            "content": "Oliver Ray, Automated abduction in scientific discovery, 2007, Model-Based Reasoning in Science, Technology, and Medicine, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Oliver Ray"
                ],
                "title": "Automated abduction in scientific discovery",
                "pub_date": "2007",
                "pub_title": "Model-Based Reasoning in Science, Technology, and Medicine",
                "pub": "Springer"
            }
        },
        {
            "ix": "454-ARR_v1_103",
            "content": "Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, Mohit Bansal, Prover: Proof generation for interpretable reasoning over rules, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Swarnadeep Saha",
                    "Sayan Ghosh",
                    "Shashank Srivastava",
                    "Mohit Bansal"
                ],
                "title": "Prover: Proof generation for interpretable reasoning over rules",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "454-ARR_v1_104",
            "content": "Swarnadeep Saha, Prateek Yadav, Mohit Bansal, 2021. multiPRover: Generating multiple proofs for improved interpretability in rule reasoning, , Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Swarnadeep Saha",
                    "Prateek Yadav",
                    "Mohit Bansal"
                ],
                "title": "2021. multiPRover: Generating multiple proofs for improved interpretability in rule reasoning",
                "pub_date": null,
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "454-ARR_v1_105",
            "content": "UNKNOWN, None, 2021, A generative symbolic model for more general natural language understanding and reasoning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "A generative symbolic model for more general natural language understanding and reasoning",
                "pub": null
            }
        },
        {
            "ix": "454-ARR_v1_106",
            "content": "Oyvind Tafjord, Bhavana Dalvi, Peter Clark, ProofWriter: Generating implications, proofs, and abductive statements over natural language, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Oyvind Tafjord",
                    "Bhavana Dalvi",
                    "Peter Clark"
                ],
                "title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "454-ARR_v1_107",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "454-ARR_v1_108",
            "content": "UNKNOWN, None, 2021, Naturalproofs: Mathematical theorem proving in natural language, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Naturalproofs: Mathematical theorem proving in natural language",
                "pub": null
            }
        },
        {
            "ix": "454-ARR_v1_109",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger,  Drame, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Thomas Wolf",
                    "Lysandre Debut",
                    "Victor Sanh",
                    "Julien Chaumond",
                    "Clement Delangue",
                    "Anthony Moi",
                    "Pierric Cistac",
                    "Tim Rault",
                    "Remi Louf",
                    "Morgan Funtowicz",
                    "Joe Davison",
                    "Sam Shleifer",
                    "Clara Patrick Von Platen",
                    "Yacine Ma",
                    "Julien Jernite",
                    "Canwen Plu",
                    "Teven Xu",
                    "Sylvain Scao",
                    "Mariama Gugger",
                    " Drame"
                ],
                "title": "Transformers: State-of-the-art natural language processing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "454-ARR_v1_0@0",
            "content": "AbductionRules: Training Transformers to Explain Unexpected Inputs",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_0",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_2@0",
            "content": "Transformers have recently been shown to be capable of reliably performing logical reasoning over facts and rules expressed in natural language, but abductive reasoning -inference to the best explanation of an unexpected observation -has been underexplored despite significant applications to scientific discovery, common-sense reasoning, and model interpretability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_2",
            "start": 0,
            "end": 365,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_3@0",
            "content": "This paper presents AbductionRules, a group of natural language datasets designed to train and test generalisable abduction over naturallanguage knowledge bases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_3",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_3@1",
            "content": "We use these datasets to finetune pretrained Transformers and discuss their performance, finding that our models learned generalisable abductive techniques but also learned to exploit the structure of our data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_3",
            "start": 162,
            "end": 371,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_3@2",
            "content": "Finally, we discuss the viability of this approach to abductive reasoning and ways in which it may be improved in future work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_3",
            "start": 373,
            "end": 498,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_4@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_4",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_5@0",
            "content": "Since its introduction, models based on the Transformer (Vaswani et al., 2017) have, due to their learning ability and Turing-completeness (Bhattamishra et al., 2020), sparked research into their use in many applications beyond their original purpose of natural language processing (NLP), including image processing and generation (Parmar et al., 2018;Chen et al., 2020), theorem proving (Polu and Sutskever, 2020;Welleck et al., 2021), and chess (Noever et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_5",
            "start": 0,
            "end": 468,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_6@0",
            "content": "One such task is logical inference -reasoning over first-order logic (FOL) knowledge bases (collections of facts and rules).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_6",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_6@1",
            "content": "Given a knowledge base, one may attempt to find logical implications (deduction), discover rules that extrapolate patterns in known facts (induction), or infer facts that would explain surprising observations (abduction).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_6",
            "start": 125,
            "end": 345,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_6@2",
            "content": "More specifically, if a newly observed fact p cannot be deduced from an existing knowledge base, abduction is the process of finding one or more facts that, if added to the knowledge base, would allow p to be deduced from existing rules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_6",
            "start": 347,
            "end": 583,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_6@3",
            "content": "Figure 1 demonstrates the difference between these three kinds of inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_6",
            "start": 585,
            "end": 660,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_7@0",
            "content": "Traditionally, FOL is represented using a formal mathematical syntax, with facts resembling HUMAN(SOCRATES) and rules resembling \u2200X : HUMAN(X) =\u21d2 MORTAL(X).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_7",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_7@1",
            "content": "Clark et al. (2020) recently pioneered an alternative approach we call natural-language logic, which might represent these as \"Socrates is human\" and \"Humans are mortal\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_7",
            "start": 157,
            "end": 326,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_7@2",
            "content": "This approach, properly followed, retains the precision of the mathematical syntax while also taking advantage of Transformers' NLP aptitude and pretraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_7",
            "start": 328,
            "end": 484,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_7@3",
            "content": "This approach also allows reasoning over texts not written in formal representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_7",
            "start": 486,
            "end": 570,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_7@4",
            "content": "Clark et al. (2020) examined their models' potential for deduction only.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_7",
            "start": 572,
            "end": 643,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_7@5",
            "content": "Tafjord et al. (2021) extended this work to explore abduction but retained a focus on deduction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_7",
            "start": 645,
            "end": 740,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_8@0",
            "content": "Our goal is to use the natural-language logic approach to train Transformers to perform abductive reasoning with the following properties:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_8",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_9@0",
            "content": "\u2022 Generalisable: Be able to apply techniques outside domains in which they were learned. \u2022 Generative: Produce explanations rather than labelling them as sufficient or insufficient. \u2022 Single-hop: Produce direct explanations. Instead of \"plants are green because chlorophyll is green because green light is not used in photosynthesis\", prefer \"plants are green because chlorophyll is green\". If further explanation is desired, abduction can be applied again. \u2022 Discerning: Prefer simpler explanations. \u2022 Explicit: Use given knowledge bases rather than relying on pretraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_9",
            "start": 0,
            "end": 573,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_10@0",
            "content": "Our efforts to train abduction in this way are motivated by multiple potential applications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_10",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_10@1",
            "content": "\u2022 Ray (2007) describes the use of automated abduction in scientific discovery.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_10",
            "start": 93,
            "end": 170,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_10@2",
            "content": "Since much scientific knowledge exists in the form of natural language rather than formal representations, advances in natural-language abduction would greatly assist in automating the scientific method by helping to explain experimental observations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_10",
            "start": 172,
            "end": 422,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_11@0",
            "content": "\u2022 Ignatiev et al. (2019) describe the use of abduction to interpret deep learning models similar to Transformers, which are infamously difficult to interpret. \u2022 Abduction may also help solve the longstanding problem of automating common-sense reasoning. Transformers excel at memorising common knowledge but routinely fail to capture any underlying reasoning. Training these models to explain their own outputs may remedy this problem by providing a way to integrate this fractured knowledge into a more connected model of reality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_11",
            "start": 0,
            "end": 530,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_12@0",
            "content": "We present the following contributions:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_12",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_13@0",
            "content": "\u2022 A collection of datasets for training and testing natural-language abduction. \u2022 A method of synthetically generating more realistic natural-language logic datasets. \u2022 Experimental results showing that Transformers can perform abductive reasoning without additional architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_13",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_14@0",
            "content": "2 Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_14",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_15@0",
            "content": "Natural-language logic",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_15",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_16@0",
            "content": "Our work builds on the RuleTaker line of research on natural-language logic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_16",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_16@1",
            "content": "This line began with Clark et al. (2020), who developed RuleTakers to reason deductively over FOL knowledge bases expressed in natural language, judging given facts to be true or false.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_16",
            "start": 77,
            "end": 261,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_16@2",
            "content": "These achieved promising results but failed to accurately explain their reasoning or generalise to inferences requiring more steps than were seen at training time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_16",
            "start": 263,
            "end": 425,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_16@3",
            "content": "PRover (Saha et al., 2020) achieved greater explainability by generating proofs of its answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_16",
            "start": 427,
            "end": 521,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_16@4",
            "content": "Similarly, the Iterative variant of ProofWriter (Tafjord et al., 2021) chained single-hop deductions rather than reasoning through multi-hop deductions all at once, making its reasoning transparent and easily generalisable to unseen depths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_16",
            "start": 523,
            "end": 762,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_16@5",
            "content": "multiPRover (Saha et al., 2021) also made use of this iterative approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_16",
            "start": 764,
            "end": 836,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_17@0",
            "content": "The generalisability and interpretability of iterative single-hop reasoning are why we seek to train single-hop abduction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_17",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_17@1",
            "content": "Tafjord et al. (2021) also adapted their deductionbased datasets to train abductive reasoning, achieving success but training multi-hop abduction only, and also requiring models to output every possible explanation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_17",
            "start": 123,
            "end": 337,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_17@2",
            "content": "By contrast, we seek to train models to discern between simpler and more complex explanations -for example, to prefer explanations requiring fewer unknown facts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_17",
            "start": 339,
            "end": 499,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_17@3",
            "content": "Bhagavatula et al. (2019) presented two more abduction-based datasets: \u03b1-NLI, which tests models' ability to choose which of two hypotheses better explains an observation, and \u03b1-NLG, a generative version of the same dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_17",
            "start": 501,
            "end": 724,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_17@4",
            "content": "These datasets do not give supporting knowledge bases -all background information must come from pretraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_17",
            "start": 726,
            "end": 834,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_17@5",
            "content": "While this is a valuable approach, we seek to investigate how well Transformers can reason over given knowledge bases to incorporate explicit background knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_17",
            "start": 836,
            "end": 998,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_17@6",
            "content": "Gontier et al. (2020) investigated Transformers' ability to perform inductive reasoning in natural language, finding them able to extrapolate patterns in given proofs but again unable to generalise to more complex proofs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_17",
            "start": 1000,
            "end": 1220,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_17@7",
            "content": "Saparov and Mitchell (2021) developed an alternative approach to classifying the ProofWriter datasets that does not reason over natural language, instead using a symbolic, Bayesian approach and using abductive reasoning to satisfy constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_17",
            "start": 1222,
            "end": 1464,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_18@0",
            "content": "Other adjacent work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_18",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_19@0",
            "content": "Their models' superior performance demonstrates that while Transformers are effective at logical reasoning, they may benefit from more specialised architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_19",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_20@0",
            "content": "Methodology",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_20",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_21@0",
            "content": "Prior to our work, there existed no dataset capable of training or testing the kind of abductive reasoning we seek.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_21",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_21@1",
            "content": "We therefore present Abduction-Rules, a natural-language logic dataset designed for this task, and use it to train and test several models based on a pretrained Text-to-Text Transfer Transformer, or T5 (Raffel et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_21",
            "start": 116,
            "end": 339,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_22@0",
            "content": "Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_22",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_23@0",
            "content": "AbductionRules has three main predecessors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_23",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_24@0",
            "content": "Rule Reasoning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_24",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_25@0",
            "content": "The Rule Reasoning dataset developed by Clark et al. (2020) was, to our knowledge, the first naturallanguage logic dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_25",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_26@0",
            "content": "To create this dataset, FOL predicates (e.g. BIG(LION)) were procedurally generated, entities (LION) and attributes (BIG(X)) were extracted, and templates (\"The {entity} is {attribute}\") were used to create natural-language logic translations (\"The lion is big\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_26",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_26@1",
            "content": "Rules were created similarly (e.g. \u2200X : BIG(X) =\u21d2 BLUE(X) became \"If something is big then it is blue\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_26",
            "start": 264,
            "end": 367,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_26@2",
            "content": "Facts and rules were grouped into knowledge bases, each with several questions; the model's task is to label each question true or false.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_26",
            "start": 369,
            "end": 505,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_27@0",
            "content": "The Rule Reasoning dataset includes knowledge bases in several domains; those in the animaldomain use animals as entities while those in the person-domain use peoples' names.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_27",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_27@1",
            "content": "All subsequent datasets similarly use these two domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_27",
            "start": 175,
            "end": 230,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_27@2",
            "content": "The animal-domain includes multi-entity facts (CHASES(LION, MOUSE), or \"the lion chases the mouse\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_27",
            "start": 232,
            "end": 331,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_27@3",
            "content": "For our purposes, we consider the lion to be the main entity and \"chases the mouse\" to be an attribute of the lion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_27",
            "start": 333,
            "end": 447,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_28@0",
            "content": "ParaRules",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_28",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_29@0",
            "content": "Recognising that their translations of mathematical syntax into natural language were strict and unrealistic (e.g. \"Charlie is green. Charlie is rough.\"), Clark et al. (2020) also produced ParaRules, which contained knowledge bases and questions similar to those in the Rule Reasoning dataset, but were paraphrased into more colloquial language (e.g. \"Charlie has green teeth and rough skin.\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_29",
            "start": 0,
            "end": 393,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_29@1",
            "content": "This approach much better prepares Transformers to reason logically over naturally-occurring texts but requires large amounts of human labour to produce.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_29",
            "start": 395,
            "end": 547,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_29@2",
            "content": "For this reason, ParaRules is much smaller than the Rule Reasoning dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_29",
            "start": 549,
            "end": 623,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_30@0",
            "content": "PARARULE Plus",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_30",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_31@0",
            "content": "Seeing the value in RuleTaker's size and ease of production as well as the greater utility of ParaRules, Bao (2021) produced PARARULE Plus, a compromise between the Rule Reasoning dataset and ParaRules that procedurally rephrases all rules during generation by using various templates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_31",
            "start": 0,
            "end": 284,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_31@1",
            "content": "PARARULE Plus also avoids being entirely context-free by pooling related attributes (such as \"big\", \"strong\", \"high\" and \"huge\") and only giving entities attributes from one pool.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_31",
            "start": 286,
            "end": 464,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_31@2",
            "content": "While PARARULE Plus falls short of ParaRules' variety, its greater collection of rephrased rules is highly valuable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_31",
            "start": 466,
            "end": 581,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_32@0",
            "content": "AbductionRules",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_32",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_33@0",
            "content": "We adapt the open-source code used to generate PARARULE Plus to create AbductionRules 1 , making the following changes:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_33",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_34@0",
            "content": "\u2022 Instead of labelling questions (for our purposes, \"observations\") with \"true\" or \"false\", we use the lone fact (or \"explanation\") that would prove or disprove it. \u2022 We ensure that no two knowledge bases in the same dataset give the same attributes to the same entities to avoid repeats. This reduces the size of the datasets; to compensate, we increase the number of entities. \u2022 While each rule has a single condition in PARARULE Plus (\"If something is cute, then...\"), we give three (\"If something is cute, funny, and adorable, then...\"), with an entity that satisfies exactly two conditions; the model must identify the third.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_34",
            "start": 0,
            "end": 629,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_35@0",
            "content": "After making these changes, we produce datasets with increasing levels of complexity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_35",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_36@0",
            "content": "\u2022 The first complexity level contains no further changes from PARARULE Plus and yields the dataset Abduction-Animal-0.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_36",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_37@0",
            "content": "Facts: The squirrel is quiet .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_37",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_37@1",
            "content": "The leopard is slow.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_37",
            "start": 31,
            "end": 50,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_37@2",
            "content": "The dog is adorable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_37",
            "start": 52,
            "end": 71,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_37@3",
            "content": "The crocodile is heavy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_37",
            "start": 73,
            "end": 95,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_38@0",
            "content": "The leopard is boring.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_38",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_38@1",
            "content": "The leopard is angry.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_38",
            "start": 23,
            "end": 43,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_38@2",
            "content": "The crocodile is awful.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_38",
            "start": 45,
            "end": 67,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_38@3",
            "content": "The leopard attacks the squirrel.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_38",
            "start": 69,
            "end": 101,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_39@0",
            "content": "The dog is small.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_39",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_39@1",
            "content": "The dog is cute.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_39",
            "start": 18,
            "end": 33,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_39@2",
            "content": "The squirrel is nice.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_39",
            "start": 35,
            "end": 55,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_39@3",
            "content": "The crocodile likes the dog.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_39",
            "start": 57,
            "end": 84,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_39@4",
            "content": "The squirrel is kind .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_39",
            "start": 86,
            "end": 107,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_40@0",
            "content": "Rules: If something is cute, is adorable, and is furry, then it is also lovely.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_40",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_40@1",
            "content": "All animals that are obese, are awful, and are heavy, are big.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_40",
            "start": 80,
            "end": 141,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_40@2",
            "content": "If an animal is fierce, sees the squirrel, and likes the dog, it is tired.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_40",
            "start": 143,
            "end": 216,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_40@3",
            "content": "Things that are smart , are kind , and are quiet , are also round .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_40",
            "start": 218,
            "end": 284,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_40@4",
            "content": "If an animal chases the dog, is boring, and attacks the squirrel, then it is also strong.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_40",
            "start": 286,
            "end": 374,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_40@5",
            "content": "All things that are slow, are sleepy, and are angry, are rough.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_40",
            "start": 376,
            "end": 438,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_41@0",
            "content": "Observation: The squirrel is round .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_41",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_42@0",
            "content": "Explanation: The squirrel is smart .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_42",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_42@1",
            "content": "\u2022 At the second complexity level, we shuffle all knowledge bases to prevent models from exploiting the constant position of all sentences and attributes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_42",
            "start": 37,
            "end": 189,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_42@2",
            "content": "This yields the dataset Abduction-Animal-0.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_42",
            "start": 191,
            "end": 235,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_43@0",
            "content": "\u2022 At the third complexity level, we procedurally rephrase rules with random variations instead of using the same templates as PARARULE Plus. For example, the animal-domain FOL rule \u2200X : (BIG(X) \u2227 HEAVY(X) \u2227 FIERCE(X)) =\u21d2 STRONG(X) might be rephrased as \"All animals that are big, are heavy, and are fierce, are also strong\" or \"If something is heavy, is fierce, and is big, it is strong\", among many other similar variations. Notably, this rephrasing process involves reordering all attributes so that attributes contained in correct abductions might be first, second, or third. This yields the datasets Abduction-Animal-Simple and Abduction-Person-Simple. Figure 2 contains an example item from Animal-Simple. 2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_43",
            "start": 0,
            "end": 711,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_44@0",
            "content": "2 For brevity, we omit the \"Abduction-\" prefix when dis-This method of procedural rule rephrasing represents a useful iteration on the naturallanguage logic approach and leaves room for further improvement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_44",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_44@1",
            "content": "Concentrated work in this line of research may produce synthetic natural-language logic datasets that are larger yet exhibit much wider variety, making this approach more powerful and robust.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_44",
            "start": 207,
            "end": 397,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_45@0",
            "content": "\u2022 At the fourth and final complexity level, we add extraneous confounding rules to knowledge bases. While lower complexity levels only ever have one rule that could explain a given observation, here we create two variations of every (single-entity) rule; one replaces a satisfied condition with an unsatisfied condition, while the other replaces all three conditions. All replacements come from different pools. This yields the datasets Abduction-Animal and Abduction-Person.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_45",
            "start": 0,
            "end": 474,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_46@0",
            "content": "Figure 3 contains simplified examples of data from each complexity level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_46",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_47@0",
            "content": "We intend each successive complexity level to remove additional idiosyncrasies that might be exploited in lieu of using abduction (i.e. used to \"cheat\"), so that this exploitation can be detected.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_47",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_47@1",
            "content": "We also intend the fourth to train models to favour simpler explanations when strictly more complex explanations are available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_47",
            "start": 197,
            "end": 323,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_48@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_48",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_49@0",
            "content": "We use AbductionRules to train 8 models based on the pretrained T5 implementation from the Hug-gingFace Transformers library (Wolf et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_49",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_49@1",
            "content": "3 We first use each training set to train 1 model, yielding 6 models trained at 4 complexity levels across 2 domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_49",
            "start": 146,
            "end": 262,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_49@2",
            "content": "To compare domains and complexity levels, we test all models on all test sets, giving us intra-domain results (isolating the effect of the complexity), and inter-domain results (some isolating the effect of the domain).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_49",
            "start": 264,
            "end": 482,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_49@3",
            "content": "We expect each successive complexity level to train a better-quality model and the two domains to be mostly comparable with some variation attributable to the animaldomain's multi-entity facts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_49",
            "start": 484,
            "end": 676,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_50@0",
            "content": "If our approach were adapted to models extensively trained to reason on many domains, we expect that teaching abduction in every domain would cussing the AbductionRules datasets within this paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_50",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_51@0",
            "content": "3 All code used for experiments in this paper can be found at https://anonymous.4open.science/r/ AbductionRules-D89D. be prohibitively expensive.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_51",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_51@1",
            "content": "Therefore, we seek to investigate Transformers' ability to transfer abductive reasoning techniques to domains where these techniques have not been taught but are nonetheless familiar to the Transformer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_51",
            "start": 146,
            "end": 347,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_51@2",
            "content": "To this end, we train two more cross-domain models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_51",
            "start": 349,
            "end": 399,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_52@0",
            "content": "\u2022 We train one model on our simplest dataset and our most complex dataset in another domain, i.e. Animal-0.1 and Person. We name this model Person+Animal-0.1. \u2022 We train another model on the simplest person-domain dataset and the most complex animal-domain dataset, i.e. Person-Simple and Animal, to compare the two domains. We name this model Animal+Person-Simple.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_52",
            "start": 0,
            "end": 364,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_53@0",
            "content": "While we are interested in these cross-domain models' performance on all datasets, we are particularly interested in their results on the most complex dataset on which they were not trained (Abduction-Animal and Abduction-Person, respectively).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_53",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_53@1",
            "content": "We treat performance on these datasets as a proxy for Transformers' ability to apply abductive reasoning outside the domains in which it was trained.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_53",
            "start": 245,
            "end": 393,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_54@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_54",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_55@0",
            "content": "Table 1 contains our results, showing the percentage of abductions correctly performed by each model on each test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_55",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_55@1",
            "content": "4 Note that no model ever gave a correct explanation outside the domain(s) in which it was trained.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_55",
            "start": 119,
            "end": 217,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_55@2",
            "content": "On the surface, this would suggest that our models were unable to generalise to new domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_55",
            "start": 219,
            "end": 310,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_55@3",
            "content": "However, inspection of inter-domain results shows that this is not entirely accurate; many explanations contain errors but nonetheless identify the ground-truth 4 All our results can be found at https://anonymous.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_55",
            "start": 312,
            "end": 524,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_55@4",
            "content": "4open.science/r/AbductionRules-D89D.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_55",
            "start": 526,
            "end": 561,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_56@0",
            "content": "explanation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_56",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_56@1",
            "content": "For example, the animal models commonly appended \"The\" to correct explanations, as in \"The Bob is small\"; while this is incorrect, it nonetheless indicates the correct explanation in a way that suggests the model still performed the correct abduction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_56",
            "start": 13,
            "end": 263,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_56@2",
            "content": "We distinguish between two kinds of errors in correct-yet-useful explanations: lossless errors and lossy errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_56",
            "start": 265,
            "end": 376,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_57@0",
            "content": "Lossless errors",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_57",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_58@0",
            "content": "Explanations with lossless errors failed to match the correct explanation character-for-character but allowed it to be reliably identified.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_58",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_59@0",
            "content": "We found several ways in which recognisably correct explanations differed from the ground-truth, such as extra words (\"The Bob is small\", \"The lion is attacks the mouse\"), looping (\"The dog is is is is is small\"), and incorrect grammar (\"The anne is wealthy\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_59",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_59@1",
            "content": "While these errors point towards flaws in training, it is a strength of natural-language logic and soft reasoners that they can cope with minor grammar mistakes as long as meaning is preserved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_59",
            "start": 261,
            "end": 453,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_60@0",
            "content": "Table 2 contains our results after correcting for these errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_60",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_60@1",
            "content": "Note that animal-domain models achieved performance comparable to the persondomain models on novel datasets in their own domain, while person-domain models saw minimal inter-domain improvement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_60",
            "start": 64,
            "end": 256,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_60@2",
            "content": "Cross-domain models also saw almost no improvement, suggesting that having seen correct explanations in both domains eliminated this kind of formatting error.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_60",
            "start": 258,
            "end": 415,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_61@0",
            "content": "Lossy errors",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_61",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_62@0",
            "content": "The most important aspect of abduction in our datasets is identification of the correct attribute.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_62",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_62@1",
            "content": "The entity at the beginning of the explanation always matches that at the beginning of the observation; therefore, if the correct attribute is identified, the correct explanation can be reconstructed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_62",
            "start": 99,
            "end": 298,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_62@2",
            "content": "Table 2: Improvement of all models on all test sets after allowing lossless errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_62",
            "start": 300,
            "end": 382,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_63@0",
            "content": "Table 3 contains our results after correcting for these errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_63",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_63@1",
            "content": "Note that every model achieved some useful results on every test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_63",
            "start": 64,
            "end": 132,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_63@2",
            "content": "Most inter-domain results improved to rival intra-domain results, although the Abduction-Person model continued to struggle.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_63",
            "start": 134,
            "end": 257,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_63@3",
            "content": "Intra-domain results saw minimal improvement, with none seeing a >2% point increase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_63",
            "start": 259,
            "end": 342,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_63@4",
            "content": "The cross-domain models again saw no visible improvement, further suggesting that these inferior results were avoidable from seeing facts, rules, and explanations in different formats at training time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_63",
            "start": 344,
            "end": 544,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_64@0",
            "content": "Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_64",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_65@0",
            "content": "Our results show that models trained on our simplest datasets struggle to generalise to new complexity levels and domains, while those trained on our more complex datasets are better able to generalise but still perform suboptimally.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_65",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_65@1",
            "content": "Meanwhile, those trained on combined cross-domain datasets achieve performance superior to the sum of models trained on their parts and easily apply skills outside domains in which they were learned.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_65",
            "start": 234,
            "end": 432,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_65@2",
            "content": "It is also clear that models trained in the animal-domain achieve better intra-domain and inter-domain performance than person-domain models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_65",
            "start": 434,
            "end": 574,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_66@0",
            "content": "5.1 Animal-0.1 and Animal-0.2 Unsurprisingly, the models trained on our simplest datasets fare the worst.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_66",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_66@1",
            "content": "Our Animal-0.1 and -0.2 models perform similarly poorly, suggesting that Animal-0.2's additional complexity from randomised sentence orderings was of minimal importance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_66",
            "start": 106,
            "end": 274,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_66@2",
            "content": "In fact, the Animal-0.2 model's performance on more complex datasets is worse than its simpler counterpart; examination of its results reveals a tendency to loop on unfamiliar inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_66",
            "start": 276,
            "end": 458,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_66@3",
            "content": "Given the Animal-0.1 model's 99.3% correct (100% allowing lossy errors) performance on Animal-0.2, we treat these complexity levels as equivalent and the Animal-0.1 model as definitive.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_66",
            "start": 460,
            "end": 644,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_67@0",
            "content": "The Animal-0.1 model is approximately 1/3 as accurate on the person-domain when allowing lossless errors but only loses approximately 6% points when allowing lossy errors, suggesting that it fails to adapt to new formats but is mostly able to use the same techniques as in the animal-domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_67",
            "start": 0,
            "end": 290,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_68@0",
            "content": "These models' significant performance hit on higher complexity levels clearly indicates that they exploit the structure of their training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_68",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_68@1",
            "content": "However, it should be noted that the Animal-0.1 model drops each time by approximately a factor of 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_68",
            "start": 143,
            "end": 243,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_68@2",
            "content": "If this model only chose the penultimate attribute in a sentence containing the attribute in the question, its accuracy would drop by a factor of 3 with procedural rephrasing and again with confounding rules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_68",
            "start": 245,
            "end": 452,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_68@3",
            "content": "Therefore, both models utilise some level of generalised abductive reasoning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_68",
            "start": 454,
            "end": 530,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_69@0",
            "content": "Animal-Simple and Person-Simple",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_69",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_70@0",
            "content": "The Animal-Simple model significantly outperforms our simpler models; this makes sense since Table 3: Improvement of all models on all test sets after allowing lossy errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_70",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_71@0",
            "content": "Animal-0.1 and -0.2 can be thought of as special, unshuffled cases of Animal-Simple.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_71",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_71@1",
            "content": "5 Similarly to the Animal-0.1 model, the Animal-Simple model performs about half as well on Animal as on Animal-Simple.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_71",
            "start": 85,
            "end": 203,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_71@2",
            "content": "This model also performs worse on Person-Simple than Animal when allowing lossless errors but better when allowing lossy errors, implying that it exploits the structure of Animal-Simple to some degree to identify correct attributes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_71",
            "start": 205,
            "end": 436,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_71@3",
            "content": "Its performance drop from Person-Simple to Person is greater than from Animal-Simple to Animal, suggesting that changes in domain and complexity are more difficult to generalise when compounded.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_71",
            "start": 438,
            "end": 631,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_72@0",
            "content": "Our Person-Simple model also performs well but fails to generalise to higher complexity; this can be partially explained by the multi-entity facts in the animal-domain, as rules using these facts are not used to create confounding rules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_72",
            "start": 0,
            "end": 236,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_72@1",
            "content": "This model gives almost no correct inter-domain explanations unless lossy errors are allowed, in which case it achieves similar inter-domain performance to the animal-domain models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_72",
            "start": 238,
            "end": 418,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_72@2",
            "content": "Its performance drop on Animal can be compared to that of the Animal-Simple model from Person-Simple to Person, exacerbated by the person-domain models' poorer performance in general.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_72",
            "start": 420,
            "end": 602,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_73@0",
            "content": "Abduction-Animal and Abduction-Person",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_73",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_74@0",
            "content": "The Animal model performs the best of all singledomain models, achieving >60% performance on all datasets except Person-Simple when allowing lossy errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_74",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_74@1",
            "content": "The drop from Person to Person-Simple is evidence of cheating, but its generalisability is superior to all other models and demonstrates some abductive ability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_74",
            "start": 155,
            "end": 314,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_74@2",
            "content": "Surprisingly, it achieves worse intra-domain results on lower complexity levels than the Animal-Simple model, again indi-5 Because of this and their failure to train generalisable abduction, we do not include either Animal-0.1 or -0.2 in the public release of AbductionRules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_74",
            "start": 316,
            "end": 590,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_74@3",
            "content": "The code we used for our experiments can be used to regenerate them if desired.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_74",
            "start": 592,
            "end": 670,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_75@0",
            "content": "cating that some of its performance is dependent on Animal's rule structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_75",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_75@1",
            "content": "Still, this performance drop is relatively small (being <10% in all cases), further reinforcing that while this model utilises some degree of both cheating and abduction (like all our models), its abductive capabilities generalise to a promising extent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_75",
            "start": 77,
            "end": 329,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_76@0",
            "content": "By contrast, the Person model achieves the worst performance of any model, performing as well on Person-Simple as that dataset's model does on Person and achieving abysmal inter-domain performance, even on Animal.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_76",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_76@1",
            "content": "This model is the clearest indication that (our instantiations of) the two domains are not equivalent; the animal-domain's models are much better able to generalise.The multientity rules again offer some explanatory powerthe Animal model demonstrates some overtraining on the confounding rules and so performs more poorly in their absence, but still learned to explain observations using multi-entity rules that lacked confounding equivalents, making it robust to extraneous rules but not reliant on them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_76",
            "start": 214,
            "end": 718,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_76@2",
            "content": "If this were a major determining factor, we would expect models trained on both maximally and minimally complex datasets to be even more robust and generalised.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_76",
            "start": 720,
            "end": 879,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_77@0",
            "content": "Cross-domain models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_77",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_78@0",
            "content": "Our cross-domain models are our best-performing models by far, achieving superior performance on unseen datasets than the sum of models trained on their combined training sets' parts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_78",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_79@0",
            "content": "The Person+Animal-0.1 model, being trained on our simplest dataset and having its most complex training set come from the worse of our two training domains, is the worse of our two cross-domain models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_79",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_79@1",
            "content": "Nonetheless, it reaches a remarkable level of performance, explaining >76% of all observations correctly on all test sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_79",
            "start": 202,
            "end": 323,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_79@2",
            "content": "Its performance in the face of unconfounded rephrased rules (something unprecedented in its training) is dependent on the domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_79",
            "start": 325,
            "end": 453,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_79@3",
            "content": "In the person-domain (i.e. on Person-Simple), where it received its most complex train-ing, it achieves its best result on a dataset it was not trained on (excepting Animal-0.2), while in the animal-domain (i.e. on Animal-Simple) it achieves its worst result, having not seen any rephrased animal rules at training time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_79",
            "start": 455,
            "end": 774,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_79@4",
            "content": "Still, it demonstrates a greater ability than any single-domain model to generalise to these unfamiliar rule structures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_79",
            "start": 776,
            "end": 895,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_79@5",
            "content": "It can also apply its training on confounded rules outside the domain in which it was learned, achieving far greater performance on Animal than any other dataset that it was not trained on.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_79",
            "start": 897,
            "end": 1085,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_80@0",
            "content": "The Animal+Person-Simple model is our best and most promising, achieving >99% performance on every dataset and consistently adapting to all complexity levels in every domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_80",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_80@1",
            "content": "Like Person+Animal-0.1, it encounters unprecedented rule structures (singular single-entity animal rules, confounded person rules) and generalises almost perfectly to each.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_80",
            "start": 175,
            "end": 346,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_80@2",
            "content": "While our datasets remain somewhat limited in scope, we believe that this result demonstrates that Transformers can generalise abductive techniques beyond the domains in which those techniques were trained, provided the domain itself is not entirely novel.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_80",
            "start": 348,
            "end": 603,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_81@0",
            "content": "Extrapolating these cross-domain results, it seems likely that finetuning Transformers that have received extensive pretraining (such as GPT-3 (Brown et al., 2020)) on datasets covering more varied and complex examples of abduction would make these models capable of much more generalised natural-language abductive reasoning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_81",
            "start": 0,
            "end": 325,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_82@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_82",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_83@0",
            "content": "We have presented the AbductionRules datasets and shown that pretrained T5 models finetuned on them exhibit generalised abductive reasoning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_83",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_83@1",
            "content": "Our more complex datasets train abduction more generally reliably than our less complex datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_83",
            "start": 141,
            "end": 237,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_83@2",
            "content": "Further, training in multiple domains is superior to training in only one domain, and we have clear evidence of generalisation of techniques from one domain to another.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_83",
            "start": 239,
            "end": 406,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_83@3",
            "content": "We have also made improvements to the generation of natural-language logic dataset generation, presenting a new middle-ground between the template-based PARARULE Plus (Bao, 2021) and the manually rephrased Pararules (Clark et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_83",
            "start": 408,
            "end": 644,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_83@4",
            "content": "We believe our results are promising and demonstrate the viability of Transformer-based abduction (and logical reasoning in general), but also indicate opportunities for improvement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_83",
            "start": 646,
            "end": 827,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_84@0",
            "content": "Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_84",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_85@0",
            "content": "Future work in this area might explore:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_85",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_86@0",
            "content": "\u2022 Examining skill transfer between different kinds of logical reasoning. \u2022 Applying abductive techniques in real-world, as opposed to artificial, domains. \u2022 Generating probability distributions over multiple possible explanations. \u2022 Testing explanations by verifying that they allow the original observation to be deduced. \u2022 Explanations that include not only missing premises but the relevant rule(s) they satisfy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_86",
            "start": 0,
            "end": 414,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_87@0",
            "content": "Plural?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_87",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_87@1",
            "content": "Specific? Also?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_87",
            "start": 8,
            "end": 22,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_87@2",
            "content": "Then/All? \u2022 Incorrect capitalisation, as in \"The anne is wealthy.\"",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_87",
            "start": 24,
            "end": 89,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_88@0",
            "content": "\u2022 Omission of spaces, as in \"Thebob is small.\"",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_88",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_89@0",
            "content": "C Abduction-Person-Simple example Label: Fiona is big .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_89",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_89@1",
            "content": "The model must output the explanation given the context and observation as input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_89",
            "start": 56,
            "end": 136,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_89@2",
            "content": "Facts and rules used to explain the observation are bolded while relevant attributes are highlighted.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_89",
            "start": 138,
            "end": 238,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_90@0",
            "content": "UNKNOWN, None, 2021, Pararule plus: A larger deep multistep reasoning dataset over natural language, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_90",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_91@0",
            "content": "Chandra Bhagavatula, Chaitanya Ronan Le Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Wen-Tau Downey, Yejin Yih,  Choi, Abductive commonsense reasoning, 2019, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_91",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_92@0",
            "content": "Satwik Bhattamishra, Arkil Patel, Navin Goyal, On the computational power of transformers and its implications in sequence modeling, 2020, Proceedings of the 24th Conference on Computational Natural Language Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_92",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_93@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_93",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_94@0",
            "content": "Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever, Generative pretraining from pixels, 2020, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_94",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_95@0",
            "content": "Peter Clark, Oyvind Tafjord, Kyle Richardson, Transformers as soft reasoners over language, 2020, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI-20), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_95",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_96@0",
            "content": "Nicolas Gontier, Koustuv Sinha, Siva Reddy, and Chris Pal. 2020. Measuring systematic generalization in neural proof generation with transformers, , Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_96",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_97@0",
            "content": "Alexey Ignatiev, Nina Narodytska, Joao Marques-Silva, Abduction-based explanations for machine learning models, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_97",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_98@0",
            "content": "UNKNOWN, None, 2020, The chess transformer: Mastering play using generative language models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_98",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_99@0",
            "content": "Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, Dustin Tran, Image transformer, 2018, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_99",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_100@0",
            "content": "UNKNOWN, None, 2020, Generative language modeling for automated theorem proving, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_100",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_101@0",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_101",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_102@0",
            "content": "Oliver Ray, Automated abduction in scientific discovery, 2007, Model-Based Reasoning in Science, Technology, and Medicine, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_102",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_103@0",
            "content": "Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, Mohit Bansal, Prover: Proof generation for interpretable reasoning over rules, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_103",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_104@0",
            "content": "Swarnadeep Saha, Prateek Yadav, Mohit Bansal, 2021. multiPRover: Generating multiple proofs for improved interpretability in rule reasoning, , Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_104",
            "start": 0,
            "end": 336,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_105@0",
            "content": "UNKNOWN, None, 2021, A generative symbolic model for more general natural language understanding and reasoning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_105",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_106@0",
            "content": "Oyvind Tafjord, Bhavana Dalvi, Peter Clark, ProofWriter: Generating implications, proofs, and abductive statements over natural language, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_106",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_107@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_107",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_108@0",
            "content": "UNKNOWN, None, 2021, Naturalproofs: Mathematical theorem proving in natural language, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_108",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "454-ARR_v1_109@0",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger,  Drame, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "454-ARR_v1_109",
            "start": 0,
            "end": 463,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "454-ARR_v1_0",
            "tgt_ix": "454-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_0",
            "tgt_ix": "454-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_1",
            "tgt_ix": "454-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_1",
            "tgt_ix": "454-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_1",
            "tgt_ix": "454-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_2",
            "tgt_ix": "454-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_0",
            "tgt_ix": "454-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_3",
            "tgt_ix": "454-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_5",
            "tgt_ix": "454-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_6",
            "tgt_ix": "454-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_7",
            "tgt_ix": "454-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_8",
            "tgt_ix": "454-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_10",
            "tgt_ix": "454-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_12",
            "tgt_ix": "454-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_4",
            "tgt_ix": "454-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_4",
            "tgt_ix": "454-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_4",
            "tgt_ix": "454-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_4",
            "tgt_ix": "454-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_4",
            "tgt_ix": "454-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_4",
            "tgt_ix": "454-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_4",
            "tgt_ix": "454-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_4",
            "tgt_ix": "454-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_4",
            "tgt_ix": "454-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_4",
            "tgt_ix": "454-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_4",
            "tgt_ix": "454-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_0",
            "tgt_ix": "454-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_14",
            "tgt_ix": "454-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_16",
            "tgt_ix": "454-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_15",
            "tgt_ix": "454-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_15",
            "tgt_ix": "454-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_15",
            "tgt_ix": "454-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_0",
            "tgt_ix": "454-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_17",
            "tgt_ix": "454-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_18",
            "tgt_ix": "454-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_18",
            "tgt_ix": "454-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_0",
            "tgt_ix": "454-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_19",
            "tgt_ix": "454-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_20",
            "tgt_ix": "454-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_20",
            "tgt_ix": "454-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_20",
            "tgt_ix": "454-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_21",
            "tgt_ix": "454-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_22",
            "tgt_ix": "454-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_22",
            "tgt_ix": "454-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_20",
            "tgt_ix": "454-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_23",
            "tgt_ix": "454-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_25",
            "tgt_ix": "454-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_26",
            "tgt_ix": "454-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_24",
            "tgt_ix": "454-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_24",
            "tgt_ix": "454-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_24",
            "tgt_ix": "454-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_24",
            "tgt_ix": "454-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_20",
            "tgt_ix": "454-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_27",
            "tgt_ix": "454-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_28",
            "tgt_ix": "454-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_28",
            "tgt_ix": "454-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_20",
            "tgt_ix": "454-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_29",
            "tgt_ix": "454-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_30",
            "tgt_ix": "454-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_30",
            "tgt_ix": "454-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_20",
            "tgt_ix": "454-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_31",
            "tgt_ix": "454-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_33",
            "tgt_ix": "454-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_35",
            "tgt_ix": "454-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_32",
            "tgt_ix": "454-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_32",
            "tgt_ix": "454-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_32",
            "tgt_ix": "454-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_32",
            "tgt_ix": "454-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_32",
            "tgt_ix": "454-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_37",
            "tgt_ix": "454-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_38",
            "tgt_ix": "454-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_39",
            "tgt_ix": "454-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_40",
            "tgt_ix": "454-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_41",
            "tgt_ix": "454-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_42",
            "tgt_ix": "454-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_44",
            "tgt_ix": "454-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_46",
            "tgt_ix": "454-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_32",
            "tgt_ix": "454-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_32",
            "tgt_ix": "454-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_32",
            "tgt_ix": "454-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_32",
            "tgt_ix": "454-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_32",
            "tgt_ix": "454-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_32",
            "tgt_ix": "454-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_32",
            "tgt_ix": "454-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_32",
            "tgt_ix": "454-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_32",
            "tgt_ix": "454-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_32",
            "tgt_ix": "454-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_32",
            "tgt_ix": "454-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_20",
            "tgt_ix": "454-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_47",
            "tgt_ix": "454-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_49",
            "tgt_ix": "454-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_50",
            "tgt_ix": "454-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_51",
            "tgt_ix": "454-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_48",
            "tgt_ix": "454-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_48",
            "tgt_ix": "454-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_48",
            "tgt_ix": "454-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_48",
            "tgt_ix": "454-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_48",
            "tgt_ix": "454-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_48",
            "tgt_ix": "454-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_0",
            "tgt_ix": "454-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_53",
            "tgt_ix": "454-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_55",
            "tgt_ix": "454-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_54",
            "tgt_ix": "454-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_54",
            "tgt_ix": "454-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_54",
            "tgt_ix": "454-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_54",
            "tgt_ix": "454-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_56",
            "tgt_ix": "454-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_58",
            "tgt_ix": "454-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_59",
            "tgt_ix": "454-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_57",
            "tgt_ix": "454-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_57",
            "tgt_ix": "454-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_57",
            "tgt_ix": "454-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_57",
            "tgt_ix": "454-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_54",
            "tgt_ix": "454-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_60",
            "tgt_ix": "454-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_62",
            "tgt_ix": "454-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_61",
            "tgt_ix": "454-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_61",
            "tgt_ix": "454-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_61",
            "tgt_ix": "454-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_0",
            "tgt_ix": "454-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_63",
            "tgt_ix": "454-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_65",
            "tgt_ix": "454-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_66",
            "tgt_ix": "454-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_67",
            "tgt_ix": "454-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_64",
            "tgt_ix": "454-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_64",
            "tgt_ix": "454-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_64",
            "tgt_ix": "454-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_64",
            "tgt_ix": "454-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_64",
            "tgt_ix": "454-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_64",
            "tgt_ix": "454-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_68",
            "tgt_ix": "454-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_70",
            "tgt_ix": "454-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_71",
            "tgt_ix": "454-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_69",
            "tgt_ix": "454-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_69",
            "tgt_ix": "454-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_69",
            "tgt_ix": "454-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_69",
            "tgt_ix": "454-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_64",
            "tgt_ix": "454-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_72",
            "tgt_ix": "454-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_74",
            "tgt_ix": "454-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_75",
            "tgt_ix": "454-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_73",
            "tgt_ix": "454-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_73",
            "tgt_ix": "454-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_73",
            "tgt_ix": "454-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_73",
            "tgt_ix": "454-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_64",
            "tgt_ix": "454-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_76",
            "tgt_ix": "454-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_78",
            "tgt_ix": "454-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_79",
            "tgt_ix": "454-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_80",
            "tgt_ix": "454-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_77",
            "tgt_ix": "454-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_77",
            "tgt_ix": "454-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_77",
            "tgt_ix": "454-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_77",
            "tgt_ix": "454-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_77",
            "tgt_ix": "454-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_0",
            "tgt_ix": "454-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_81",
            "tgt_ix": "454-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_82",
            "tgt_ix": "454-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_82",
            "tgt_ix": "454-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_82",
            "tgt_ix": "454-ARR_v1_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_83",
            "tgt_ix": "454-ARR_v1_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_85",
            "tgt_ix": "454-ARR_v1_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_87",
            "tgt_ix": "454-ARR_v1_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_84",
            "tgt_ix": "454-ARR_v1_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_84",
            "tgt_ix": "454-ARR_v1_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_84",
            "tgt_ix": "454-ARR_v1_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_84",
            "tgt_ix": "454-ARR_v1_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_84",
            "tgt_ix": "454-ARR_v1_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_84",
            "tgt_ix": "454-ARR_v1_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "454-ARR_v1_0",
            "tgt_ix": "454-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_1",
            "tgt_ix": "454-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_2",
            "tgt_ix": "454-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_3",
            "tgt_ix": "454-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_3",
            "tgt_ix": "454-ARR_v1_3@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_3",
            "tgt_ix": "454-ARR_v1_3@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_4",
            "tgt_ix": "454-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_5",
            "tgt_ix": "454-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_6",
            "tgt_ix": "454-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_6",
            "tgt_ix": "454-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_6",
            "tgt_ix": "454-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_6",
            "tgt_ix": "454-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_7",
            "tgt_ix": "454-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_7",
            "tgt_ix": "454-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_7",
            "tgt_ix": "454-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_7",
            "tgt_ix": "454-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_7",
            "tgt_ix": "454-ARR_v1_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_7",
            "tgt_ix": "454-ARR_v1_7@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_8",
            "tgt_ix": "454-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_9",
            "tgt_ix": "454-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_10",
            "tgt_ix": "454-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_10",
            "tgt_ix": "454-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_10",
            "tgt_ix": "454-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_11",
            "tgt_ix": "454-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_12",
            "tgt_ix": "454-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_13",
            "tgt_ix": "454-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_14",
            "tgt_ix": "454-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_15",
            "tgt_ix": "454-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_16",
            "tgt_ix": "454-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_16",
            "tgt_ix": "454-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_16",
            "tgt_ix": "454-ARR_v1_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_16",
            "tgt_ix": "454-ARR_v1_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_16",
            "tgt_ix": "454-ARR_v1_16@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_16",
            "tgt_ix": "454-ARR_v1_16@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_17",
            "tgt_ix": "454-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_17",
            "tgt_ix": "454-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_17",
            "tgt_ix": "454-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_17",
            "tgt_ix": "454-ARR_v1_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_17",
            "tgt_ix": "454-ARR_v1_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_17",
            "tgt_ix": "454-ARR_v1_17@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_17",
            "tgt_ix": "454-ARR_v1_17@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_17",
            "tgt_ix": "454-ARR_v1_17@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_18",
            "tgt_ix": "454-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_19",
            "tgt_ix": "454-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_20",
            "tgt_ix": "454-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_21",
            "tgt_ix": "454-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_21",
            "tgt_ix": "454-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_22",
            "tgt_ix": "454-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_23",
            "tgt_ix": "454-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_24",
            "tgt_ix": "454-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_25",
            "tgt_ix": "454-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_26",
            "tgt_ix": "454-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_26",
            "tgt_ix": "454-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_26",
            "tgt_ix": "454-ARR_v1_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_27",
            "tgt_ix": "454-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_27",
            "tgt_ix": "454-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_27",
            "tgt_ix": "454-ARR_v1_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_27",
            "tgt_ix": "454-ARR_v1_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_28",
            "tgt_ix": "454-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_29",
            "tgt_ix": "454-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_29",
            "tgt_ix": "454-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_29",
            "tgt_ix": "454-ARR_v1_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_30",
            "tgt_ix": "454-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_31",
            "tgt_ix": "454-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_31",
            "tgt_ix": "454-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_31",
            "tgt_ix": "454-ARR_v1_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_32",
            "tgt_ix": "454-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_33",
            "tgt_ix": "454-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_34",
            "tgt_ix": "454-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_35",
            "tgt_ix": "454-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_36",
            "tgt_ix": "454-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_37",
            "tgt_ix": "454-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_37",
            "tgt_ix": "454-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_37",
            "tgt_ix": "454-ARR_v1_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_37",
            "tgt_ix": "454-ARR_v1_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_38",
            "tgt_ix": "454-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_38",
            "tgt_ix": "454-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_38",
            "tgt_ix": "454-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_38",
            "tgt_ix": "454-ARR_v1_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_39",
            "tgt_ix": "454-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_39",
            "tgt_ix": "454-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_39",
            "tgt_ix": "454-ARR_v1_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_39",
            "tgt_ix": "454-ARR_v1_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_39",
            "tgt_ix": "454-ARR_v1_39@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_40",
            "tgt_ix": "454-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_40",
            "tgt_ix": "454-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_40",
            "tgt_ix": "454-ARR_v1_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_40",
            "tgt_ix": "454-ARR_v1_40@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_40",
            "tgt_ix": "454-ARR_v1_40@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_40",
            "tgt_ix": "454-ARR_v1_40@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_41",
            "tgt_ix": "454-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_42",
            "tgt_ix": "454-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_42",
            "tgt_ix": "454-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_42",
            "tgt_ix": "454-ARR_v1_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_43",
            "tgt_ix": "454-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_44",
            "tgt_ix": "454-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_44",
            "tgt_ix": "454-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_45",
            "tgt_ix": "454-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_46",
            "tgt_ix": "454-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_47",
            "tgt_ix": "454-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_47",
            "tgt_ix": "454-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_48",
            "tgt_ix": "454-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_49",
            "tgt_ix": "454-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_49",
            "tgt_ix": "454-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_49",
            "tgt_ix": "454-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_49",
            "tgt_ix": "454-ARR_v1_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_50",
            "tgt_ix": "454-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_51",
            "tgt_ix": "454-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_51",
            "tgt_ix": "454-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_51",
            "tgt_ix": "454-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_52",
            "tgt_ix": "454-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_53",
            "tgt_ix": "454-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_53",
            "tgt_ix": "454-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_54",
            "tgt_ix": "454-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_55",
            "tgt_ix": "454-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_55",
            "tgt_ix": "454-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_55",
            "tgt_ix": "454-ARR_v1_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_55",
            "tgt_ix": "454-ARR_v1_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_55",
            "tgt_ix": "454-ARR_v1_55@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_56",
            "tgt_ix": "454-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_56",
            "tgt_ix": "454-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_56",
            "tgt_ix": "454-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_57",
            "tgt_ix": "454-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_58",
            "tgt_ix": "454-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_59",
            "tgt_ix": "454-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_59",
            "tgt_ix": "454-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_60",
            "tgt_ix": "454-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_60",
            "tgt_ix": "454-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_60",
            "tgt_ix": "454-ARR_v1_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_61",
            "tgt_ix": "454-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_62",
            "tgt_ix": "454-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_62",
            "tgt_ix": "454-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_62",
            "tgt_ix": "454-ARR_v1_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_63",
            "tgt_ix": "454-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_63",
            "tgt_ix": "454-ARR_v1_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_63",
            "tgt_ix": "454-ARR_v1_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_63",
            "tgt_ix": "454-ARR_v1_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_63",
            "tgt_ix": "454-ARR_v1_63@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_64",
            "tgt_ix": "454-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_65",
            "tgt_ix": "454-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_65",
            "tgt_ix": "454-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_65",
            "tgt_ix": "454-ARR_v1_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_66",
            "tgt_ix": "454-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_66",
            "tgt_ix": "454-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_66",
            "tgt_ix": "454-ARR_v1_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_66",
            "tgt_ix": "454-ARR_v1_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_67",
            "tgt_ix": "454-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_68",
            "tgt_ix": "454-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_68",
            "tgt_ix": "454-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_68",
            "tgt_ix": "454-ARR_v1_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_68",
            "tgt_ix": "454-ARR_v1_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_69",
            "tgt_ix": "454-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_70",
            "tgt_ix": "454-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_71",
            "tgt_ix": "454-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_71",
            "tgt_ix": "454-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_71",
            "tgt_ix": "454-ARR_v1_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_71",
            "tgt_ix": "454-ARR_v1_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_72",
            "tgt_ix": "454-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_72",
            "tgt_ix": "454-ARR_v1_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_72",
            "tgt_ix": "454-ARR_v1_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_73",
            "tgt_ix": "454-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_74",
            "tgt_ix": "454-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_74",
            "tgt_ix": "454-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_74",
            "tgt_ix": "454-ARR_v1_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_74",
            "tgt_ix": "454-ARR_v1_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_75",
            "tgt_ix": "454-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_75",
            "tgt_ix": "454-ARR_v1_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_76",
            "tgt_ix": "454-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_76",
            "tgt_ix": "454-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_76",
            "tgt_ix": "454-ARR_v1_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_77",
            "tgt_ix": "454-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_78",
            "tgt_ix": "454-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_79",
            "tgt_ix": "454-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_79",
            "tgt_ix": "454-ARR_v1_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_79",
            "tgt_ix": "454-ARR_v1_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_79",
            "tgt_ix": "454-ARR_v1_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_79",
            "tgt_ix": "454-ARR_v1_79@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_79",
            "tgt_ix": "454-ARR_v1_79@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_80",
            "tgt_ix": "454-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_80",
            "tgt_ix": "454-ARR_v1_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_80",
            "tgt_ix": "454-ARR_v1_80@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_81",
            "tgt_ix": "454-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_82",
            "tgt_ix": "454-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_83",
            "tgt_ix": "454-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_83",
            "tgt_ix": "454-ARR_v1_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_83",
            "tgt_ix": "454-ARR_v1_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_83",
            "tgt_ix": "454-ARR_v1_83@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_83",
            "tgt_ix": "454-ARR_v1_83@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_84",
            "tgt_ix": "454-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_85",
            "tgt_ix": "454-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_86",
            "tgt_ix": "454-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_87",
            "tgt_ix": "454-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_87",
            "tgt_ix": "454-ARR_v1_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_87",
            "tgt_ix": "454-ARR_v1_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_88",
            "tgt_ix": "454-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_89",
            "tgt_ix": "454-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_89",
            "tgt_ix": "454-ARR_v1_89@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_89",
            "tgt_ix": "454-ARR_v1_89@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_90",
            "tgt_ix": "454-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_91",
            "tgt_ix": "454-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_92",
            "tgt_ix": "454-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_93",
            "tgt_ix": "454-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_94",
            "tgt_ix": "454-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_95",
            "tgt_ix": "454-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_96",
            "tgt_ix": "454-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_97",
            "tgt_ix": "454-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_98",
            "tgt_ix": "454-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_99",
            "tgt_ix": "454-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_100",
            "tgt_ix": "454-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_101",
            "tgt_ix": "454-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_102",
            "tgt_ix": "454-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_103",
            "tgt_ix": "454-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_104",
            "tgt_ix": "454-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_105",
            "tgt_ix": "454-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_106",
            "tgt_ix": "454-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_107",
            "tgt_ix": "454-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_108",
            "tgt_ix": "454-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "454-ARR_v1_109",
            "tgt_ix": "454-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1331,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "454-ARR",
        "version": 1
    }
}