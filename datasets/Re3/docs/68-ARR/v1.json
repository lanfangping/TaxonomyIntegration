{
    "nodes": [
        {
            "ix": "68-ARR_v1_0",
            "content": "Learning Disentangled Semantic Representations for Zero-Shot Cross-Lingual Transfer in Multilingual Machine Reading Comprehension",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_2",
            "content": "Multilingual pre-trained models are able to zero-shot transfer knowledge from richresource to low-resource languages in machine reading comprehension (MRC). However, inherent linguistic discrepancies in different languages could make answer spans predicted by zero-shot transfer violate syntactic constraints of the target language. In this paper, we propose a novel multilingual MRC framework equipped with a Siamese Semantic Disentanglement Model (S 2 DM) to disassociate semantics from syntax in representations learned by multilingual pre-trained models. To explicitly transfer only semantic knowledge to the target language, we propose two groups of losses tailored for semantic and syntactic encoding and disentanglement. Moreover, we theoretically analyze the generalization of our decoupling model, even for low-resource languages without training corpus. Experimental results on three multilingual MRC datasets (i.e., XQuAD, MLQA, and TyDi QA) demonstrate the effectiveness of our proposed approach over models based on mBERT and XLM-100.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "68-ARR_v1_4",
            "content": "Multilingual pre-trained language models (PLMs) (Devlin et al., 2019;Conneau and Lample, 2019;Conneau et al., 2020) have been widely utilized in cross-lingual understanding tasks. However, the zero-shot transfer method based on multilingual PLM has limited effectiveness for low-resource languages MRC tasks. This type of multilingual MRC model could roughly detect answer spans but fail to predict the precise boundaries of answers (Yuan et al., 2020). Moreover, the recent development in multilingual MRC evaluation datasets (Artetxe et al., 2020;Lewis et al., 2020;Clark et al., 2020) trigger research interests in multilingual and cross-lingual MRC (Hsu et al., 2019;Cui et al., 2019;Yuan et al., 2020;Liu et al., 2020;Huang et al., 2021;Wu et al., 2021). : Relations between answer spans and syntactic constituents. (a) An example from XQuAD (Artetxe et al., 2020) where the ground-truth answer is a syntactic constituent. (b) A case from BiPaR (Jing et al., 2019) where the answer predicted by the model transferred from English to Chinese violates syntactic constituent boundaries.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_5",
            "content": "For the inaccuracy of boundary detection, existing methods mainly solve this issue by introducing external knowledge. Based on the finding that 70% of the answer spans are language-specific phrases in MLQA (Lewis et al., 2020) dataset, Yuan et al. (2020) proposes an additional language-specific knowledge phrase masking (LAKM) task to enhance the boundary detection performance for lowresource languages. Liang et al. (2021) propose a separate model for boundary calibration based on the output of a base zero-shot transfer model, introducing a phrase boundary recovery task to pretrain calibration module on large-scale multilingual datasets synthesized from Wikipedia documents. The two works rely on external corpora, which are hard to be accessed widely.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_6",
            "content": "As illustrated in Figure 1(b), the transfer model may violate syntactic constraints for answer spans on the target language (e.g., the predicted answer \"\u6708\u5149\u4e0d\u4f4f\" crossing the boundaries of two subtrees). An intuitive assumption is that the majority of answer spans respect syntactic constituency boundaries (i.e., syntactic constraint, illustrated by the case in Figure 1(a)). On four multilingual MRC evaluation datasets, we use Stanford CoreNLP 1 to collect syntax parse trees and calculate the percentages of ground-truth answers that respect syntactic constituent boundaries. As shown in Table 1, over 87% of answer spans respect syntactic constraint.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_7",
            "content": "On the bilingual parallel MRC corpus BiPaR (Jing et al., 2019), we have compared two MRC models: a monolingual MRC model trained on the Chinese data of BiPaR vs. an mBERT-based MRC model trained on the English data of BiPaR and adapted to Chinese via zero-shot transfer. For answers that the monolingual model correctly predicts and respect syntactic constraint, 23.15% of them, which are incorrectly predicted by the transfer model, violate the syntactic constraint, illustrated by the case in Figure 1(b). It suggests that the source language syntax may have a negative impact on the answer boundary detection on the target language during zero-shot transfer, due to the linguistic discrepancies between the two languages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_8",
            "content": "However, the linguistic discrepancies are diverse and impossible to learn. We propose to decouple semantics from syntax in pre-trained models for multilingual MRC, transforming the learning of linguistic discrepancies into universal semantic information. Specifically, we propose a Siamese Semantic Disentanglement Model (S 2 DM) that utilises two latent variables to learn semantic and syntactic vectors in multilingual pre-trained representations. As shown in Figure 2(a), stacking a linear output layer for MRC over the disentangled semantic representation layer, we can finetune the multilingual PLMs on the rich-resource source language and transfer only disentangled semantic knowledge into the target language MRC. Our model aims to reduce the negative impact of the source language syntax on answer boundary detection in the target language.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_9",
            "content": "Besides, the multilingual pre-trained models have learned an amount of linguistic structure of multiple languages (Chi et al., 2020). The semantic can not directly disentangle from complex syntactic information in PLMs. To disassociate semantic and syntactic information well, we introduce objective functions of learning cross-lingual reconstruction and semantic discrimination and losses of incorporating word order information and syntax structure information (Part-of-Speech tags and syntax parse trees), respectively. We use a publicly available multilingual sentence-level parallel corpus with syntactic labels to train S 2 DM.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_10",
            "content": "To summarize, our main contributions are as follows.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_11",
            "content": "\u2022 We propose a multilingual MRC framework that explicitly transfers semantic knowledge of the source language to the target language to reduce the negative impact of source syntax on answer span detection in the target language MRC. \u2022 We propose a siamese semantic disentanglement model that can effectively separate semantic from syntactic information of multilingual PLMs with semantics/syntax-oriented losses. \u2022 We demonstrate the generalization of S 2 DM through theoretical analysis and experimental verification. In other words, our model is suitable even for low-resource languages without training data.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_12",
            "content": "Finally, experimental results on three multilingual MRC datasets ( XQuAD, MLQA, and TyDi QA) demonstrate that our model can significantly improve performance over two strong baselines (averagely 3.13 and 2.53 EM points on three datasets, respectively).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_13",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "68-ARR_v1_14",
            "content": "Cross-lingual/Multilingual Machine Reading Comprehension Hsu et al. (2019) investigate cross-lingual transfer capability of multilingual BERT (mBERT) on MRC tasks and find that zeroshot learning based on PLM is feasible, even between distant languages, such as English and Chinese. Various approaches have been proposed on top of multilingual MRC based on PLMs. Cui et al. (2019) propose a method that combines multilingual BERT and back-translation for cross-lingual MRC. In order to effectively leverage translation Different from the above studies, we mainly consider the impact of syntactic divergences between the source and target language in zero-shot crosslingual transfer based on multilingual PLMs, and attempt to disassociate semantics from syntax and only transfer semantic to the target language.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_15",
            "content": "Disentangled Representation Learning Recently, there has been a growing amount of work on learning disentangled latent representations in NLP tasks (Zhang et al., 2019;Hu et al., 2017;Yin et al., 2018). In this aspect, the most related work to our syntax-semantics decoupling method is the vMF-Gaussian Variational Autoencoder (VGVAE) model proposed by Chen et al. (2019). It is a generative model using two latent variables to represent semantics and syntax of the sentence, developed for monolingual setting and trained with paraphrases. It uses paraphrase reconstruction loss and a discriminative paraphrase loss to learn semantic representations and word order information for syntactic representations. We adapt this model to multilingual syntax-semantics disentanglement. We use bilingual sentence pairs to train our model with a cross-lingual reconstruction loss and semantic discrimination loss. To better disentangle semantics from complex and diverse syntax in multilingual PLMs, we introduce two additional syntax-related losses for incorporating POS tags and syntax trees.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_16",
            "content": "Approach",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "68-ARR_v1_17",
            "content": "Figure 2 shows the architecture of our multilingual MRC framework with the proposed siamese semantic disentanglement model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_18",
            "content": "Multilingual MRC Framework",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "68-ARR_v1_19",
            "content": "Our multilingual MRC framework consists of three essential components: the multilingual PLM layer, the siamese semantic disentanglement module, and the linear output layer. The output representations from the multilingual PLM are fed into S 2 DM to disassociate semantic and syntactic information. Only the disentangled semantic representations are input to the linear output layer for predicting answer spans in passages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_20",
            "content": "In order to facilitate the zero-shot cross-lingual transfer of only semantic knowledge from the richresource source language to the low-resource target language, we take a two-stage training strategy. First, we pre-train S 2 DM with parallel data (see Section 3.2) while the parameters of the multilingual PLM are frozen. Once S 2 DM is trained, only the output of source language MLP network input to linear output layer for MRC. In the second step, we freeze the parameters of the S 2 DM and finetune the entire multilingual MRC framework on MRC data of the source language.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_21",
            "content": "Siamese Semantic Disentanglement Model",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "68-ARR_v1_22",
            "content": "In S 2 DM, we assume that a sentence x is generated by a semantic and syntactic variable, i.e., y and z, independently. We follow VGVAE Chen et al. (2019) to use the von Mises-Fisher (vMF) distribution for the semantic variable and the Gaussian distribution for the syntactic variable. Formally, the joint probability of the sentence and its two latent variables can be factorized as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_23",
            "content": "p \u03b8 (x, y, z) = p \u03b8 (y)p \u03b8 (z)p \u03b8 (x|y, z)(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_24",
            "content": "where p \u03b8 (x|y, z) is a generative model. In our model, we use a bag-of-words decoder as the generator.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_25",
            "content": "The variational inference process of VG-VAE uses a factorized approximated posterior q \u03c6 (y|x)q \u03c6 (z|x) = q \u03c6 (y, z|x) with the objective function that maximizes a lower bound of the marginal log-likelihood:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_26",
            "content": "LV GV AE = LRL + KL(q \u03c6 (z|x)||p \u03b8 (z)) + KL(q \u03c6 (y|x)||p \u03b8 (y)),(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_27",
            "content": "LRL = E y\u223cq \u03c6 (y|x) z\u223cq \u03c6 (z|x) \u2212 log p \u03b8 (x|y, z)(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_28",
            "content": "where q \u03c6 (y|x) follows vMF(\u00b5 \u03b1 (x), \u03ba \u03b1 (x)) and q \u03c6 (z|x) follows N (\u00b5 \u03b2 (x), diag(\u03ba \u03b2 (x))). The prior p \u03b8 (y) and p \u03b8 (z) follows the uniform distribution vMF(\u2022, 0) and a standard Gaussian distribution respectively. Eq.( 3) is the reconstruction loss (RL) of the generator. In our model, we adopt a multilayer perceptron (MLP) network to learn the mean (\u00b5) and variance (\u03ba) of two distributions. The pretrained representations of sentence are contextuallyencoded token vectors, so the latent variable vectors obtained by sampling from the distributions need to be averaged, then output sentence-level semantic vector and syntactic vector. Since S 2 DM uses a Siamese network for both the source and target language, the disentanglement between semantics and syntax is conducted for the two languages simultaneously with two parametershared subnetworks, as shown in Figure 2(b).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_29",
            "content": "We attempt to extract rich semantic information from multilingual representations which is universal for multiple languages and contains less syntactic information. Except for the conventional reconstruction loss, we propose two additional losses on parallel data to encourage the latent variable y to capture semantic information: a Cross-lingual Reconstruction Loss (CRL) and Semantic Discrimination Loss (SDL). The former estimates the cross-entropy loss when we use the semantic representation y t of the target language to reconstruct the source input and use the source semantic representation y s for target reconstruction. The latter is used to force the learned source semantic representation y s to be as close as possible to the target semantic representation y t since the semantic meanings of the parallel source and target sentence is equivalent to each other. The two losses are estimated as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_30",
            "content": "LCRL = E y t \u223cq \u03c6 (y|x t ) zs \u223cq \u03c6 (z|xs) \u2212 log p \u03b8 (xs|yt, zs) + E ys\u223cq \u03c6 (y|xs) z t \u223cq \u03c6 (z|x t ) \u2212 log p \u03b8 (xt|ys, zt) ,(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_31",
            "content": "LSDL = max 0, \u03b4 \u2212 sim(ys, yt) + sim(ys, nt)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_32",
            "content": "+ max 0, \u03b4 \u2212 sim(ys, yt) + sim(ns, yt)(5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_33",
            "content": "where sim(\u2022, \u2022) is a cosine similarity score function.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_34",
            "content": "The margin \u03b4 is a hyperparameter to control the gap between parallel sentence pair (y s , y t ) and two nonparallel sentence pairs (y s , n t ) and (n s , y t ). n s is the semantic vector of a negative sample, which has the highest cosine similarity to y s . Specially, as partial sentences in our corpus are parallel in more than two languages, we limit the data range of negative sampling to only 2-way parallel pairs. n t are obtained in the similar way to n s .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_35",
            "content": "In order to guide S 2 DM to disassociate syntactic information into the syntactic latent variable z, we also define three types of losses tailored for capturing different types of syntactic information. First, we employ Word Position Loss (WPL) , as follow:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_36",
            "content": "LW P L = E z\u223cq \u03c6 (z|x) \u2212 i log softmax(f (hi))i , (6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_37",
            "content": "where softmax(\u2022) i indicates the probability of the ith word at position i, and f (\u2022) is a three-layer feedforward neural network with input h i = [e i ; z] that from the concatenation of the syntactic variable z and the embedding vector e i of the multilingual PLM for the ith token in the input sentence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_38",
            "content": "In addition, we define a Part-of-Speech and syntax tree loss to encourage S 2 DM to isolate deeper syntactic information from pre-trained representations. POS tagging is a sequence labeling task, which can be regarded as a multi-class classification problem for each token in a sentence. Hence, we define Part-of-Speech (POS) Loss as a crossentropy style loss as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_39",
            "content": "LP OS = i m j=1 \u2212log softmax(g(hi))j (7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_40",
            "content": "where g(\u2022) is a linear layer, softmax(\u2022) j estimates the probability of POS tag j, m is the number of different POS tags.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_41",
            "content": "For learning structural information, we design Syntax Tree Loss (STL). Many studies have found that PLMs can encode syntactic structures of sentences (Hewitt and Manning (2019); Chi et al. (2020)). Inspired by Hewitt and Manning (2019), we formulate syntactic parsing from pretrained word representations as two independent tasks: depth prediction of a word and distance prediction of two words in the parse tree. Given a matrix B \u2208 R k\u00d7m as a linear transformation, the losses of these two subtasks are defined as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_42",
            "content": "L depth = i ( wi \u2212 Bhi 2 2 ),(8)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_43",
            "content": "L distance = i,j dT (wi, wj) \u2212 dB(hi, hj)(9)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_44",
            "content": "where w i is the parse depth of a word defined as the number of edges from the root of the parse tree to w i , and Bh i 2 is the tree depth L2 norm of the vector space under the linear transformation. d T (w i , w j ) is the number of edges in the path between the ith and jth word in the parse tree T . As for d B (h i , h j ), it can be defined as the squared L 2 distance after transformation by B:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_45",
            "content": "dB(hi, hj) = (B(hi \u2212 hj)) T (B(hi \u2212 hj))(10)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_46",
            "content": "To induce parse trees, we minimize the summation of the above two losses L depth and L distance , and L ST L is defined as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_47",
            "content": "LST L = L depth + L distance (11)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_48",
            "content": "According to the different syntactic tasks, we train two S 2 DM variants: S 2 DM_POS and S 2 DM_SP (SP for syntactic parsing), where their training objectives are defined as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_49",
            "content": "L1 = LV GV AE + LCRL + LSDL + LW P L + LP OS , L2 = LV GV AE + LCRL + LSDL + LW P L + LST L",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_50",
            "content": "Analysis",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "68-ARR_v1_51",
            "content": "In this section, we analyze the generalization of our decoupling-based multilingual MRC model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_52",
            "content": "By two reconstruction losses Eq.(3) and Eq.( 4), we will prove that the syntactic and semantic vectors obtained by S 2 DM are language-agnostic. Since the structure of Eq.(3) and Eq.( 4) are the same, we take one part of Eq.(4) for analysis. Due to z s and y t are independent of each other, p \u03b8 (x s , z s |y t ) = p \u03b8 (x s , z s ). We obtain: Since minimizing KL(q \u03c6 (z|x)||p \u03b8 (z)) and KL(q \u03c6 (y|x)||p \u03b8 (y)), p \u03b8 (x s , z s ) and p \u03b8 (x t , z t ) will eventually get the same fitted distribution. In the same way, p \u03b8 (x s , y s ) and p \u03b8 (x t , y t ) also fit to the same distribution, no matter what the target language is. This is consistent with our motivation to use the siamese network.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_53",
            "content": "E y t \u223cq \u03c6 (y|x t )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_54",
            "content": "Furthermore, the semantic discrimination loss Eq.( 5) guarantees that the semantic vectors of the source language and the target language are similar. Minimizing Eq.( 5) can be equivalent to: sim(ys, yt) > sim(ys, nt) + \u03b4 sim(ys, yt) > sim(ns, yt) + \u03b4 which is to maximize sim(y s , y t ) to encourages the target semantic vector to approach parallel source semantic vector.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_55",
            "content": "In summary, S 2 DM can obtain languageagnostic semantic and syntactic vectors. Therefore, our multilingual MRC model is suitable even for low-resource languages without training data of decoupling model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_56",
            "content": "Experiment 4.1 Datasets",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "68-ARR_v1_57",
            "content": "To verify the effectiveness of our multilingual M-RC model, we conducted experiments on three multilingual question answering benchmarks:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_58",
            "content": "XQuAD (Artetxe et al., 2020) consists of 11 languages corpus translated from the SQuAD v1.1 (Rajpurkar et al., 2016) development set, include : Spanish (es), German (de), Greek (el), Russian (ru), Turkish (tr), Arabic (ar), Vietnamese (vi), Thai (th), Chinese (zh), Hindi (hi), and Romanian (ro).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_59",
            "content": "MLQA (Lewis et al., 2020) consists of over 5K extractive MRC instances in 7 languages: English (en), Arabic (ar), German (de), Spanish (es), Hindi (hi), Vietnamese (vi) and Chinese (zh). MLQA is also highly parallel, with MRC instances parallel across 4 different languages on average.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_60",
            "content": "TyDi QA-GoldP is the gold passage task in Ty-Di QA (Clark et al., 2020) covers 9 typologically diverse languages: Arabic (ar), Bengali (bg), English (en), Finnish (fi), Indonesian (id), Korean (ko), Russian (ru), Swahili (sw), Telugu (te). It is a more challenging MRC benchmark as questions have been written without seeing the answers, leading to 3 and 2 times less lexical overlap than XQuAD and MLQA, respectively (Hu et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_61",
            "content": "Baseline Models",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "68-ARR_v1_62",
            "content": "We used the following two multilingual PLMs to build our MRC model to conduct experiments:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_63",
            "content": "mBERT is the multilingual version of BERT Devlin et al. (2019), with 177M parameters, is pretrained on the Wikipedia of 104 languages to optimize the masked language modeling objective.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_64",
            "content": "XLM-100 uses a pre-training objective similar to that of mBERT but with a larger number of parameters (578M) and a larger shared vocabulary than mBERT, and is trained on the same Wikipedia data covering 100 languages as mBERT.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_65",
            "content": "Setup",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "68-ARR_v1_66",
            "content": "For S 2 DM, we collected approximately 26,000+ labelled parallel sentence pairs from the Universal Dependencies (UD 2.7) Corpus (Zeman et al., 2020) as the training set. The training set covers 20 languages and overlap with 13 languages of three MRC evaluation datasets. We used Universal POS tags and HEAD tags in UD 2.7 for the POS tagging and syntactic parsing task. We chose data from the Chinese semantic textual similarity (STS) task (Tang et al., 2016) as the development set. For hyper-parameters in S 2 DM, the learning rate was set to 5e-5, the margin \u03b4 was 0.4, and the latent variable dimensions was 200.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_67",
            "content": "For our multilingual MRC models and two baseline models, we fine-tuned them on the SQuAD v1.1 (Rajpurkar et al., 2016) and evaluated the test data of the three multilingual MRC datasets. For models based on mBERT, we fine-tuned them for 3 epochs with a training batch size of 32 and a learning rate of 2e-5. We fine-tuned models based on XLM-100 for 2 epochs with a training batch size of 16 and a learning rate of 3e-5.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_68",
            "content": "Experiment Results",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "68-ARR_v1_69",
            "content": "The overall experimental results are shown in Table 2. All our tests were conducted under the conditions of zero-shot transfer. Our models (S 2 DM_POS, S 2 DM_SP combined with XLM-100 or mBERT) significantly outperform both XLM-100 and mBERT baselines on three datasets. S 2 DM_SP achieves the best performance, indicating that the learning of deeper syntax information is compelling. Especially, compared with baselines on TyDi QA-Gold datasets, S 2 DM_SP based on XLM-100 and mBERT gains 4.1%, 4.2% EM respectively improvements on average across 9 languages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_70",
            "content": "The results of 12 languages in XQuAD and M-LQA are shown in Table 3. For cross-lingual transfer performance, our models are better than the two baselines on EM or F1 of all 11 low-resource target languages. The TyDi QA-GoldP dataset is more challenging than XQuAD and MLQA. The results of TyDi QA-GoldP are shown in Table 4, and our models are superior to the baselines on EM or F1 of all 8 low-resource target languages. Significantly, XLM+S 2 DM_SP outperforms the XLM-100 baselines by 8.4%, 9.5% in EM for Finnish (fi), Russian(ru), respectively. The language families of these two languages are different from that of English. The evaluation results on these three datasets verify the effectiveness of our proposed method.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_71",
            "content": "In Section 3.3, we theoretically analyze the generalization of our model. The results in three datasets show the effectiveness of five languages not included in the training target languages for S 2 DM. The five languages are Romanian (ro), Vietnamese (vi) in XQuAD and Bengali (bg), Swahili (sw), Telugu (te) in TyDi QA-GoldP, which are resource-scarce and have different language families from English. Significantly, mBERT+S 2 DM_SP outperforms the mBERT baseline by 13.6% in EM for Swahili (sw).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_72",
            "content": "Analysis",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "68-ARR_v1_73",
            "content": "Ablation Study",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "68-ARR_v1_74",
            "content": "We further conducted an ablation study based on the mBERT and VGVAE model with different combinations of losses (introduced in the Section.3.2). The results are reported in Table 3. Our mBERT+S 2 DM_SP MRC model achieves the strongest performance among all variants, surpassing the model w/ all losses. According to the results shown in Figure 3, we can summarize that each loss is essential and suitable to our model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_75",
            "content": "The results without POS and STL loss (e.g., w/ CRL+SDL+WPL) on the MLQA dataset validate the effectiveness of our losses (POS or STL loss) tailored for capturing syntactic information. The performance of models that only contain two losses in CRL, SDL, and WPL drops significantly compared with the w/ CRL+SDL+WPL model. The results of models that only contain one of the losses in CRL, SDL drop slightly, but the EM of the model with only WPL is better than w/ CRL+WPL and w/ SDL+WPL, which further demonstrates the importance of the syntax-oriented loss. All ablation models do not exceed our best model, illustrating the importance of all our losses.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_76",
            "content": "5.2 Why Use a Siamese Network in S 2 DM?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_77",
            "content": "In order to separate semantic information from PLMs, an alternative way is to train a single net- Corresponding to S 2 DM, there are also two single-network variants: S 2 DM_single_POS and S 2 DM_single_SP. Since there is no explicit semantics learning across the source and target language, we conjecture that the single-network S 2 DM will affect the quality of learned semantic vectors and the degree of semantics-syntax decoupling. As shown in Table 5, the performance of the single-network S 2 DM is worse than the siamese-network model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_78",
            "content": "Why S 2 DM Works?",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "68-ARR_v1_79",
            "content": "Our method mainly aims to reduce the potential negative impact of syntactic differences of languages in the zero-shot transfer process by explicitly isolating semantics from syntax in representations from multilingual pre-trained models. There- fore, we hope to obtain multilingual semantic representations with rich semantic information to guide the machine to read and understand texts. In order to examine (1) whether semantic vectors y in S 2 DM encode rich semantic information, and (2) whether semantics is sufficiently separated from syntax, and (3) whether semantic disentanglement can improve predicted answer spans in matching syntactic structures of the target language, we conducted additional experiments and analyses.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_80",
            "content": "Here we used three datasets of cross-lingual semantic textual similarity (STS) in SemEval-2017 2 to evaluate the quality of semantic vectors learned by S 2 DM. The three datasets are for Arabic to English (ar-en), Spanish to English (es-en), and Turkish to English (tr-en) cross-lingual STS. We report the results of our models in Figure 5 based on m-BERT. We also evaluated learned syntactic vectors in cross-lingual STS, hoping that the performance gap between semantic vectors (i.e., y in S 2 DM) and syntactic vectors (i.e., z in S 2 DM) is as large as possible. As shown in Figure 5, disentangled semantic representations significantly improve Pearson correlation over the baseline in ar-en, es-en, and tr-en by 11.46%, 3.40%, 4.98%, respectively. Additionally, disentangled syntactic representations are negatively correlated to STS in most cases. These results suggest that disentangled semantic vectors indeed learn rich universal semantic information.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_81",
            "content": "We visualize hidden representations of the last layer of mBERT and semantic representations of mBERT+S 2 DM_POS and mBERT+S 2 DM_SP in Figure 6, in which the parallel sentences are from a 15-way parallel corpus (Conneau et al., 2018). It is clear to see that disentangled semantic representations learned by S 2 DM make parallel sentences in 15 languages (semantically equivalent to each other) closer to one another in space, blending language boundaries clearly seen from mBERT Finally, we evaluated the degree of consistency to syntactic constituents of predicted answer spans. As described in Section 1, 23.15% of the nontransfer predicted correct answers violate syntactic constraint of the target language during the raw zero-shot cross-lingual transfer on BiPaR. By contrast, S 2 DM_POS and S 2 DM_SP drop this percentage to 12.98% and 6.60%, respectively. Moreover, on the entire test set of BiPaR (Jing et al., 2019) in Chinese, 93.27% answers predicted by S 2 DM_SP exactly span syntactic constituents, which is 8.14% higher than the mBERT model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_82",
            "content": "Conclusions",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "68-ARR_v1_83",
            "content": "In this paper, we have presented a novel multilingual MRC model for zero-shot cross-lingual transfer, which can disentangle semantic from syntactic representations and explicitly transfer semantic information from rich-resource language to low-resource languages, reducing the influence of syntactic differences between languages on the answer span prediction of the target language. To disassociate semantics from syntax in multilingual pre-trained representations, we propose the siamese semantic disentanglement model that semantics/syntax-oriented losses to guide latent variables to learn corresponding information. For low-resource languages without training data of decoupling model, we theoretically analyze and experiment verify the generalization of our multilingual MRC model. Further in-depth analyses suggest that the proposed S 2 DM can efficiently disentangle semantics from syntax and significantly improve syntactic consistency of answer predictions on the target language after zero-shot crosslingual transfer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "68-ARR_v1_84",
            "content": "Mikel Artetxe, Sebastian Ruder, Dani Yogatama, On the cross-lingual transferability of monolingual representations, 2020, Proceedings of the 58th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Mikel Artetxe",
                    "Sebastian Ruder",
                    "Dani Yogatama"
                ],
                "title": "On the cross-lingual transferability of monolingual representations",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_85",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics, ACL 2020",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_86",
            "content": "Mingda Chen, Qingming Tang, Sam Wiseman, Kevin Gimpel, A multi-task approach for disentangling syntax and semantics in sentence representations, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Mingda Chen",
                    "Qingming Tang",
                    "Sam Wiseman",
                    "Kevin Gimpel"
                ],
                "title": "A multi-task approach for disentangling syntax and semantics in sentence representations",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_87",
            "content": "Ethan Chi, John Hewitt, Christopher Manning, Finding universal grammatical relations in multilingual BERT, 2020, Proceedings of the 58th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Ethan Chi",
                    "John Hewitt",
                    "Christopher Manning"
                ],
                "title": "Finding universal grammatical relations in multilingual BERT",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_88",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics, ACL 2020",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_89",
            "content": "Jonathan Clark, Jennimaria Palomaki, Vitaly Nikolaev, Eunsol Choi, Dan Garrette, Michael Collins, Tom Kwiatkowski, Tydi QA: A benchmark for information-seeking question answering in typologically diverse languages, 2020, Trans. Assoc. Comput. Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Jonathan Clark",
                    "Jennimaria Palomaki",
                    "Vitaly Nikolaev",
                    "Eunsol Choi",
                    "Dan Garrette",
                    "Michael Collins",
                    "Tom Kwiatkowski"
                ],
                "title": "Tydi QA: A benchmark for information-seeking question answering in typologically diverse languages",
                "pub_date": "2020",
                "pub_title": "Trans. Assoc. Comput. Linguistics",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_90",
            "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Unsupervised cross-lingual representation learning at scale, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Alexis Conneau",
                    "Kartikay Khandelwal",
                    "Naman Goyal",
                    "Vishrav Chaudhary",
                    "Guillaume Wenzek",
                    "Francisco Guzm\u00e1n",
                    "Edouard Grave",
                    "Myle Ott",
                    "Luke Zettlemoyer",
                    "Veselin Stoyanov"
                ],
                "title": "Unsupervised cross-lingual representation learning at scale",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_91",
            "content": "Alexis Conneau, Guillaume Lample, Crosslingual language model pretraining, 2019, Proceedings of the 2019 Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Alexis Conneau",
                    "Guillaume Lample"
                ],
                "title": "Crosslingual language model pretraining",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_92",
            "content": "Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, Veselin Stoyanov, XNLI: Evaluating cross-lingual sentence representations, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Alexis Conneau",
                    "Ruty Rinott",
                    "Guillaume Lample",
                    "Adina Williams",
                    "Samuel Bowman",
                    "Holger Schwenk",
                    "Veselin Stoyanov"
                ],
                "title": "XNLI: Evaluating cross-lingual sentence representations",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_93",
            "content": "Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu, Cross-lingual machine reading comprehension, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Yiming Cui",
                    "Wanxiang Che",
                    "Ting Liu",
                    "Bing Qin",
                    "Shijin Wang",
                    "Guoping Hu"
                ],
                "title": "Cross-lingual machine reading comprehension",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_94",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_95",
            "content": "John Hewitt, Christopher Manning, A structural probe for finding syntax in word representations, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "John Hewitt",
                    "Christopher Manning"
                ],
                "title": "A structural probe for finding syntax in word representations",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_96",
            "content": "Tsung-Yuan Hsu, Chi-Liang Liu, Hung-Yi Lee, Zero-shot reading comprehension by crosslingual transfer learning with multi-lingual language representation model, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Tsung-Yuan Hsu",
                    "Chi-Liang Liu",
                    "Hung-Yi Lee"
                ],
                "title": "Zero-shot reading comprehension by crosslingual transfer learning with multi-lingual language representation model",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_97",
            "content": "Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, Melvin Johnson, X-TREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation, 2020, Proceedings of the 37th International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Junjie Hu",
                    "Sebastian Ruder",
                    "Aditya Siddhant",
                    "Graham Neubig",
                    "Orhan Firat",
                    "Melvin Johnson"
                ],
                "title": "X-TREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 37th International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_98",
            "content": "Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric Xing, Toward controlled generation of text, 2017, Proceedings of the 34th International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Zhiting Hu",
                    "Zichao Yang",
                    "Xiaodan Liang",
                    "Ruslan Salakhutdinov",
                    "Eric Xing"
                ],
                "title": "Toward controlled generation of text",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 34th International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_99",
            "content": "UNKNOWN, None, 2021, Improving cross-lingual reading comprehension with self-training, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Improving cross-lingual reading comprehension with self-training",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_100",
            "content": "Yimin Jing, Deyi Xiong, Yan Zhen, Bi-PaR: A bilingual parallel dataset for multilingual and cross-lingual reading comprehension on novels, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Yimin Jing",
                    "Deyi Xiong",
                    "Yan Zhen"
                ],
                "title": "Bi-PaR: A bilingual parallel dataset for multilingual and cross-lingual reading comprehension on novels",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_101",
            "content": "S Patrick, Barlas Lewis, Ruty Oguz,  Rinott, MLQA: Evaluating cross-lingual extractive question answering, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "S Patrick",
                    "Barlas Lewis",
                    "Ruty Oguz",
                    " Rinott"
                ],
                "title": "MLQA: Evaluating cross-lingual extractive question answering",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_102",
            "content": "Shining Liang, Linjun Shou, Jian Pei, Ming Gong, Wanli Zuo, Daxin Jiang, Calibrenet: Calibration networks for multilingual sequence labeling, 2021-03-08, WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Shining Liang",
                    "Linjun Shou",
                    "Jian Pei",
                    "Ming Gong",
                    "Wanli Zuo",
                    "Daxin Jiang"
                ],
                "title": "Calibrenet: Calibration networks for multilingual sequence labeling",
                "pub_date": "2021-03-08",
                "pub_title": "WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_103",
            "content": "Junhao Liu, Linjun Shou, Jian Pei, Ming Gong, Min Yang, Daxin Jiang, Cross-lingual machine reading comprehension with language branch knowledge distillation, 2020, Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Junhao Liu",
                    "Linjun Shou",
                    "Jian Pei",
                    "Ming Gong",
                    "Min Yang",
                    "Daxin Jiang"
                ],
                "title": "Cross-lingual machine reading comprehension with language branch knowledge distillation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_104",
            "content": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, SQuAD: 100, 000+ questions for machine comprehension of text, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Pranav Rajpurkar",
                    "Jian Zhang",
                    "Konstantin Lopyrev",
                    "Percy Liang"
                ],
                "title": "SQuAD: 100, 000+ questions for machine comprehension of text",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_105",
            "content": "UNKNOWN, None, 2016, Chinese semantic text similarity trainning dataset, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Chinese semantic text similarity trainning dataset",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_106",
            "content": "UNKNOWN, None, 2021, Improving low-resource reading comprehension via cross-lingual transposition rethinking, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Improving low-resource reading comprehension via cross-lingual transposition rethinking",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_107",
            "content": "Pengcheng Yin, Chunting Zhou, Junxian He, Graham Neubig, StructVAE: Tree-structured latent variable models for semi-supervised semantic parsing, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Pengcheng Yin",
                    "Chunting Zhou",
                    "Junxian He",
                    "Graham Neubig"
                ],
                "title": "StructVAE: Tree-structured latent variable models for semi-supervised semantic parsing",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_108",
            "content": "Fei Yuan, Linjun Shou, Xuanyu Bai, Ming Gong, Yaobo Liang, Nan Duan, Yan Fu, Daxin Jiang, Enhancing answer boundary detection for multilingual machine reading comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Fei Yuan",
                    "Linjun Shou",
                    "Xuanyu Bai",
                    "Ming Gong",
                    "Yaobo Liang",
                    "Nan Duan",
                    "Yan Fu",
                    "Daxin Jiang"
                ],
                "title": "Enhancing answer boundary detection for multilingual machine reading comprehension",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_109",
            "content": "UNKNOWN, None, , Mitchell Abrams, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Mitchell Abrams",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_110",
            "content": "UNKNOWN, None, , LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (\u00daFAL), Faculty of Mathematics and Physics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (\u00daFAL), Faculty of Mathematics and Physics",
                "pub": null
            }
        },
        {
            "ix": "68-ARR_v1_111",
            "content": "Xinyuan Zhang, Yi Yang, Siyang Yuan, Dinghan Shen, Lawrence Carin, Syntax-infused variational autoencoder for text generation, 2019, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Xinyuan Zhang",
                    "Yi Yang",
                    "Siyang Yuan",
                    "Dinghan Shen",
                    "Lawrence Carin"
                ],
                "title": "Syntax-infused variational autoencoder for text generation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "68-ARR_v1_0@0",
            "content": "Learning Disentangled Semantic Representations for Zero-Shot Cross-Lingual Transfer in Multilingual Machine Reading Comprehension",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_0",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_2@0",
            "content": "Multilingual pre-trained models are able to zero-shot transfer knowledge from richresource to low-resource languages in machine reading comprehension (MRC).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_2",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_2@1",
            "content": "However, inherent linguistic discrepancies in different languages could make answer spans predicted by zero-shot transfer violate syntactic constraints of the target language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_2",
            "start": 157,
            "end": 331,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_2@2",
            "content": "In this paper, we propose a novel multilingual MRC framework equipped with a Siamese Semantic Disentanglement Model (S 2 DM) to disassociate semantics from syntax in representations learned by multilingual pre-trained models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_2",
            "start": 333,
            "end": 557,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_2@3",
            "content": "To explicitly transfer only semantic knowledge to the target language, we propose two groups of losses tailored for semantic and syntactic encoding and disentanglement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_2",
            "start": 559,
            "end": 726,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_2@4",
            "content": "Moreover, we theoretically analyze the generalization of our decoupling model, even for low-resource languages without training corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_2",
            "start": 728,
            "end": 862,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_2@5",
            "content": "Experimental results on three multilingual MRC datasets (i.e., XQuAD, MLQA, and TyDi QA) demonstrate the effectiveness of our proposed approach over models based on mBERT and XLM-100.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_2",
            "start": 864,
            "end": 1046,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_4@0",
            "content": "Multilingual pre-trained language models (PLMs) (Devlin et al., 2019;Conneau and Lample, 2019;Conneau et al., 2020) have been widely utilized in cross-lingual understanding tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_4",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_4@1",
            "content": "However, the zero-shot transfer method based on multilingual PLM has limited effectiveness for low-resource languages MRC tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_4",
            "start": 180,
            "end": 307,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_4@2",
            "content": "This type of multilingual MRC model could roughly detect answer spans but fail to predict the precise boundaries of answers (Yuan et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_4",
            "start": 309,
            "end": 452,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_4@3",
            "content": "Moreover, the recent development in multilingual MRC evaluation datasets (Artetxe et al., 2020;Lewis et al., 2020;Clark et al., 2020) trigger research interests in multilingual and cross-lingual MRC (Hsu et al., 2019;Cui et al., 2019;Yuan et al., 2020;Liu et al., 2020;Huang et al., 2021;Wu et al., 2021). : Relations between answer spans and syntactic constituents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_4",
            "start": 454,
            "end": 819,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_4@4",
            "content": "(a) An example from XQuAD (Artetxe et al., 2020) where the ground-truth answer is a syntactic constituent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_4",
            "start": 821,
            "end": 926,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_4@5",
            "content": "(b) A case from BiPaR (Jing et al., 2019) where the answer predicted by the model transferred from English to Chinese violates syntactic constituent boundaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_4",
            "start": 928,
            "end": 1087,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_5@0",
            "content": "For the inaccuracy of boundary detection, existing methods mainly solve this issue by introducing external knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_5",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_5@1",
            "content": "Based on the finding that 70% of the answer spans are language-specific phrases in MLQA (Lewis et al., 2020) dataset, Yuan et al. (2020) proposes an additional language-specific knowledge phrase masking (LAKM) task to enhance the boundary detection performance for lowresource languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_5",
            "start": 118,
            "end": 404,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_5@2",
            "content": "Liang et al. (2021) propose a separate model for boundary calibration based on the output of a base zero-shot transfer model, introducing a phrase boundary recovery task to pretrain calibration module on large-scale multilingual datasets synthesized from Wikipedia documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_5",
            "start": 406,
            "end": 680,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_5@3",
            "content": "The two works rely on external corpora, which are hard to be accessed widely.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_5",
            "start": 682,
            "end": 758,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_6@0",
            "content": "As illustrated in Figure 1(b), the transfer model may violate syntactic constraints for answer spans on the target language (e.g., the predicted answer \"\u6708\u5149\u4e0d\u4f4f\" crossing the boundaries of two subtrees). An intuitive assumption is that the majority of answer spans respect syntactic constituency boundaries (i.e., syntactic constraint, illustrated by the case in Figure 1(a)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_6",
            "start": 0,
            "end": 372,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_6@1",
            "content": "On four multilingual MRC evaluation datasets, we use Stanford CoreNLP 1 to collect syntax parse trees and calculate the percentages of ground-truth answers that respect syntactic constituent boundaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_6",
            "start": 374,
            "end": 575,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_6@2",
            "content": "As shown in Table 1, over 87% of answer spans respect syntactic constraint.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_6",
            "start": 577,
            "end": 651,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_7@0",
            "content": "On the bilingual parallel MRC corpus BiPaR (Jing et al., 2019), we have compared two MRC models: a monolingual MRC model trained on the Chinese data of BiPaR vs. an mBERT-based MRC model trained on the English data of BiPaR and adapted to Chinese via zero-shot transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_7",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_7@1",
            "content": "For answers that the monolingual model correctly predicts and respect syntactic constraint, 23.15% of them, which are incorrectly predicted by the transfer model, violate the syntactic constraint, illustrated by the case in Figure 1(b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_7",
            "start": 271,
            "end": 506,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_7@2",
            "content": "It suggests that the source language syntax may have a negative impact on the answer boundary detection on the target language during zero-shot transfer, due to the linguistic discrepancies between the two languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_7",
            "start": 508,
            "end": 723,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_8@0",
            "content": "However, the linguistic discrepancies are diverse and impossible to learn.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_8",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_8@1",
            "content": "We propose to decouple semantics from syntax in pre-trained models for multilingual MRC, transforming the learning of linguistic discrepancies into universal semantic information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_8",
            "start": 75,
            "end": 253,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_8@2",
            "content": "Specifically, we propose a Siamese Semantic Disentanglement Model (S 2 DM) that utilises two latent variables to learn semantic and syntactic vectors in multilingual pre-trained representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_8",
            "start": 255,
            "end": 448,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_8@3",
            "content": "As shown in Figure 2(a), stacking a linear output layer for MRC over the disentangled semantic representation layer, we can finetune the multilingual PLMs on the rich-resource source language and transfer only disentangled semantic knowledge into the target language MRC.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_8",
            "start": 450,
            "end": 720,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_8@4",
            "content": "Our model aims to reduce the negative impact of the source language syntax on answer boundary detection in the target language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_8",
            "start": 722,
            "end": 848,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_9@0",
            "content": "Besides, the multilingual pre-trained models have learned an amount of linguistic structure of multiple languages (Chi et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_9",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_9@1",
            "content": "The semantic can not directly disentangle from complex syntactic information in PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_9",
            "start": 134,
            "end": 218,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_9@2",
            "content": "To disassociate semantic and syntactic information well, we introduce objective functions of learning cross-lingual reconstruction and semantic discrimination and losses of incorporating word order information and syntax structure information (Part-of-Speech tags and syntax parse trees), respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_9",
            "start": 220,
            "end": 521,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_9@3",
            "content": "We use a publicly available multilingual sentence-level parallel corpus with syntactic labels to train S 2 DM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_9",
            "start": 523,
            "end": 632,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_10@0",
            "content": "To summarize, our main contributions are as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_10",
            "start": 0,
            "end": 51,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_11@0",
            "content": "\u2022 We propose a multilingual MRC framework that explicitly transfers semantic knowledge of the source language to the target language to reduce the negative impact of source syntax on answer span detection in the target language MRC. \u2022 We propose a siamese semantic disentanglement model that can effectively separate semantic from syntactic information of multilingual PLMs with semantics/syntax-oriented losses. \u2022 We demonstrate the generalization of S 2 DM through theoretical analysis and experimental verification. In other words, our model is suitable even for low-resource languages without training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_11",
            "start": 0,
            "end": 610,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_12@0",
            "content": "Finally, experimental results on three multilingual MRC datasets ( XQuAD, MLQA, and TyDi QA) demonstrate that our model can significantly improve performance over two strong baselines (averagely 3.13 and 2.53 EM points on three datasets, respectively).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_12",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_13@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_13",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_14@0",
            "content": "Cross-lingual/Multilingual Machine Reading Comprehension Hsu et al. (2019) investigate cross-lingual transfer capability of multilingual BERT (mBERT) on MRC tasks and find that zeroshot learning based on PLM is feasible, even between distant languages, such as English and Chinese.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_14",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_14@1",
            "content": "Various approaches have been proposed on top of multilingual MRC based on PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_14",
            "start": 282,
            "end": 360,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_14@2",
            "content": "Cui et al. (2019) propose a method that combines multilingual BERT and back-translation for cross-lingual MRC.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_14",
            "start": 362,
            "end": 471,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_14@3",
            "content": "In order to effectively leverage translation Different from the above studies, we mainly consider the impact of syntactic divergences between the source and target language in zero-shot crosslingual transfer based on multilingual PLMs, and attempt to disassociate semantics from syntax and only transfer semantic to the target language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_14",
            "start": 473,
            "end": 808,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_15@0",
            "content": "Disentangled Representation Learning Recently, there has been a growing amount of work on learning disentangled latent representations in NLP tasks (Zhang et al., 2019;Hu et al., 2017;Yin et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_15",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_15@1",
            "content": "In this aspect, the most related work to our syntax-semantics decoupling method is the vMF-Gaussian Variational Autoencoder (VGVAE) model proposed by Chen et al. (2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_15",
            "start": 203,
            "end": 371,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_15@2",
            "content": "It is a generative model using two latent variables to represent semantics and syntax of the sentence, developed for monolingual setting and trained with paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_15",
            "start": 373,
            "end": 538,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_15@3",
            "content": "It uses paraphrase reconstruction loss and a discriminative paraphrase loss to learn semantic representations and word order information for syntactic representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_15",
            "start": 540,
            "end": 706,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_15@4",
            "content": "We adapt this model to multilingual syntax-semantics disentanglement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_15",
            "start": 708,
            "end": 776,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_15@5",
            "content": "We use bilingual sentence pairs to train our model with a cross-lingual reconstruction loss and semantic discrimination loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_15",
            "start": 778,
            "end": 902,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_15@6",
            "content": "To better disentangle semantics from complex and diverse syntax in multilingual PLMs, we introduce two additional syntax-related losses for incorporating POS tags and syntax trees.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_15",
            "start": 904,
            "end": 1083,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_16@0",
            "content": "Approach",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_16",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_17@0",
            "content": "Figure 2 shows the architecture of our multilingual MRC framework with the proposed siamese semantic disentanglement model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_17",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_18@0",
            "content": "Multilingual MRC Framework",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_18",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_19@0",
            "content": "Our multilingual MRC framework consists of three essential components: the multilingual PLM layer, the siamese semantic disentanglement module, and the linear output layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_19",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_19@1",
            "content": "The output representations from the multilingual PLM are fed into S 2 DM to disassociate semantic and syntactic information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_19",
            "start": 173,
            "end": 296,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_19@2",
            "content": "Only the disentangled semantic representations are input to the linear output layer for predicting answer spans in passages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_19",
            "start": 298,
            "end": 421,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_20@0",
            "content": "In order to facilitate the zero-shot cross-lingual transfer of only semantic knowledge from the richresource source language to the low-resource target language, we take a two-stage training strategy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_20",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_20@1",
            "content": "First, we pre-train S 2 DM with parallel data (see Section 3.2) while the parameters of the multilingual PLM are frozen.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_20",
            "start": 201,
            "end": 320,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_20@2",
            "content": "Once S 2 DM is trained, only the output of source language MLP network input to linear output layer for MRC.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_20",
            "start": 322,
            "end": 429,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_20@3",
            "content": "In the second step, we freeze the parameters of the S 2 DM and finetune the entire multilingual MRC framework on MRC data of the source language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_20",
            "start": 431,
            "end": 575,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_21@0",
            "content": "Siamese Semantic Disentanglement Model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_21",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_22@0",
            "content": "In S 2 DM, we assume that a sentence x is generated by a semantic and syntactic variable, i.e., y and z, independently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_22",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_22@1",
            "content": "We follow VGVAE Chen et al. (2019) to use the von Mises-Fisher (vMF) distribution for the semantic variable and the Gaussian distribution for the syntactic variable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_22",
            "start": 120,
            "end": 284,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_22@2",
            "content": "Formally, the joint probability of the sentence and its two latent variables can be factorized as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_22",
            "start": 286,
            "end": 383,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_23@0",
            "content": "p \u03b8 (x, y, z) = p \u03b8 (y)p \u03b8 (z)p \u03b8 (x|y, z)(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_23",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_24@0",
            "content": "where p \u03b8 (x|y, z) is a generative model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_24",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_24@1",
            "content": "In our model, we use a bag-of-words decoder as the generator.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_24",
            "start": 42,
            "end": 102,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_25@0",
            "content": "The variational inference process of VG-VAE uses a factorized approximated posterior q \u03c6 (y|x)q \u03c6 (z|x) = q \u03c6 (y, z|x) with the objective function that maximizes a lower bound of the marginal log-likelihood:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_25",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_26@0",
            "content": "LV GV AE = LRL + KL(q \u03c6 (z|x)||p \u03b8 (z)) + KL(q \u03c6 (y|x)||p \u03b8 (y)),(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_26",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_27@0",
            "content": "LRL = E y\u223cq \u03c6 (y|x) z\u223cq \u03c6 (z|x) \u2212 log p \u03b8 (x|y, z)(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_27",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_28@0",
            "content": "where q \u03c6 (y|x) follows vMF(\u00b5 \u03b1 (x), \u03ba \u03b1 (x)) and q \u03c6 (z|x) follows N (\u00b5 \u03b2 (x), diag(\u03ba \u03b2 (x))).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_28",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_28@1",
            "content": "The prior p \u03b8 (y) and p \u03b8 (z) follows the uniform distribution vMF(\u2022, 0) and a standard Gaussian distribution respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_28",
            "start": 96,
            "end": 218,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_28@2",
            "content": "Eq.( 3) is the reconstruction loss (RL) of the generator.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_28",
            "start": 220,
            "end": 276,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_28@3",
            "content": "In our model, we adopt a multilayer perceptron (MLP) network to learn the mean (\u00b5) and variance (\u03ba) of two distributions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_28",
            "start": 278,
            "end": 398,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_28@4",
            "content": "The pretrained representations of sentence are contextuallyencoded token vectors, so the latent variable vectors obtained by sampling from the distributions need to be averaged, then output sentence-level semantic vector and syntactic vector.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_28",
            "start": 400,
            "end": 641,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_28@5",
            "content": "Since S 2 DM uses a Siamese network for both the source and target language, the disentanglement between semantics and syntax is conducted for the two languages simultaneously with two parametershared subnetworks, as shown in Figure 2(b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_28",
            "start": 643,
            "end": 880,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_29@0",
            "content": "We attempt to extract rich semantic information from multilingual representations which is universal for multiple languages and contains less syntactic information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_29",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_29@1",
            "content": "Except for the conventional reconstruction loss, we propose two additional losses on parallel data to encourage the latent variable y to capture semantic information: a Cross-lingual Reconstruction Loss (CRL) and Semantic Discrimination Loss (SDL).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_29",
            "start": 165,
            "end": 412,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_29@2",
            "content": "The former estimates the cross-entropy loss when we use the semantic representation y t of the target language to reconstruct the source input and use the source semantic representation y s for target reconstruction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_29",
            "start": 414,
            "end": 629,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_29@3",
            "content": "The latter is used to force the learned source semantic representation y s to be as close as possible to the target semantic representation y t since the semantic meanings of the parallel source and target sentence is equivalent to each other.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_29",
            "start": 631,
            "end": 873,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_29@4",
            "content": "The two losses are estimated as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_29",
            "start": 875,
            "end": 914,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_30@0",
            "content": "LCRL = E y t \u223cq \u03c6 (y|x t ) zs \u223cq \u03c6 (z|xs) \u2212 log p \u03b8 (xs|yt, zs) + E ys\u223cq \u03c6 (y|xs) z t \u223cq \u03c6 (z|x t ) \u2212 log p \u03b8 (xt|ys, zt) ,(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_30",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_31@0",
            "content": "LSDL = max 0, \u03b4 \u2212 sim(ys, yt) + sim(ys, nt)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_31",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_32@0",
            "content": "+ max 0, \u03b4 \u2212 sim(ys, yt) + sim(ns, yt)(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_32",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_33@0",
            "content": "where sim(\u2022, \u2022) is a cosine similarity score function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_33",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_34@0",
            "content": "The margin \u03b4 is a hyperparameter to control the gap between parallel sentence pair (y s , y t ) and two nonparallel sentence pairs (y s , n t ) and (n s , y t ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_34",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_34@1",
            "content": "n s is the semantic vector of a negative sample, which has the highest cosine similarity to y s .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_34",
            "start": 162,
            "end": 258,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_34@2",
            "content": "Specially, as partial sentences in our corpus are parallel in more than two languages, we limit the data range of negative sampling to only 2-way parallel pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_34",
            "start": 260,
            "end": 420,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_34@3",
            "content": "n t are obtained in the similar way to n s .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_34",
            "start": 422,
            "end": 465,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_35@0",
            "content": "In order to guide S 2 DM to disassociate syntactic information into the syntactic latent variable z, we also define three types of losses tailored for capturing different types of syntactic information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_35",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_35@1",
            "content": "First, we employ Word Position Loss (WPL) , as follow:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_35",
            "start": 203,
            "end": 256,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_36@0",
            "content": "LW P L = E z\u223cq \u03c6 (z|x) \u2212 i log softmax(f (hi))i , (6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_36",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_37@0",
            "content": "where softmax(\u2022) i indicates the probability of the ith word at position i, and f (\u2022) is a three-layer feedforward neural network with input h i = [e i ; z] that from the concatenation of the syntactic variable z and the embedding vector e i of the multilingual PLM for the ith token in the input sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_37",
            "start": 0,
            "end": 305,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_38@0",
            "content": "In addition, we define a Part-of-Speech and syntax tree loss to encourage S 2 DM to isolate deeper syntactic information from pre-trained representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_38",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_38@1",
            "content": "POS tagging is a sequence labeling task, which can be regarded as a multi-class classification problem for each token in a sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_38",
            "start": 155,
            "end": 286,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_38@2",
            "content": "Hence, we define Part-of-Speech (POS) Loss as a crossentropy style loss as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_38",
            "start": 288,
            "end": 370,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_39@0",
            "content": "LP OS = i m j=1 \u2212log softmax(g(hi))j (7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_39",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_40@0",
            "content": "where g(\u2022) is a linear layer, softmax(\u2022) j estimates the probability of POS tag j, m is the number of different POS tags.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_40",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_41@0",
            "content": "For learning structural information, we design Syntax Tree Loss (STL).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_41",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_41@1",
            "content": "Many studies have found that PLMs can encode syntactic structures of sentences (Hewitt and Manning (2019); Chi et al. (2020)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_41",
            "start": 71,
            "end": 196,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_41@2",
            "content": "Inspired by Hewitt and Manning (2019), we formulate syntactic parsing from pretrained word representations as two independent tasks: depth prediction of a word and distance prediction of two words in the parse tree.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_41",
            "start": 198,
            "end": 412,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_41@3",
            "content": "Given a matrix B \u2208 R k\u00d7m as a linear transformation, the losses of these two subtasks are defined as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_41",
            "start": 414,
            "end": 514,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_42@0",
            "content": "L depth = i ( wi \u2212 Bhi 2 2 ),(8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_42",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_43@0",
            "content": "L distance = i,j dT (wi, wj) \u2212 dB(hi, hj)(9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_43",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_44@0",
            "content": "where w i is the parse depth of a word defined as the number of edges from the root of the parse tree to w i , and Bh i 2 is the tree depth L2 norm of the vector space under the linear transformation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_44",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_44@1",
            "content": "d T (w i , w j ) is the number of edges in the path between the ith and jth word in the parse tree T .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_44",
            "start": 201,
            "end": 302,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_44@2",
            "content": "As for d B (h i , h j ), it can be defined as the squared L 2 distance after transformation by B:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_44",
            "start": 304,
            "end": 400,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_45@0",
            "content": "dB(hi, hj) = (B(hi \u2212 hj)) T (B(hi \u2212 hj))(10)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_45",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_46@0",
            "content": "To induce parse trees, we minimize the summation of the above two losses L depth and L distance , and L ST L is defined as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_46",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_47@0",
            "content": "LST L = L depth + L distance (11)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_47",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_48@0",
            "content": "According to the different syntactic tasks, we train two S 2 DM variants: S 2 DM_POS and S 2 DM_SP (SP for syntactic parsing), where their training objectives are defined as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_48",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_49@0",
            "content": "L1 = LV GV AE + LCRL + LSDL + LW P L + LP OS , L2 = LV GV AE + LCRL + LSDL + LW P L + LST L",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_49",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_50@0",
            "content": "Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_50",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_51@0",
            "content": "In this section, we analyze the generalization of our decoupling-based multilingual MRC model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_51",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_52@0",
            "content": "By two reconstruction losses Eq.(3) and Eq.( 4), we will prove that the syntactic and semantic vectors obtained by S 2 DM are language-agnostic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_52",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_52@1",
            "content": "Since the structure of Eq.(3) and Eq.( 4) are the same, we take one part of Eq.(4) for analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_52",
            "start": 145,
            "end": 240,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_52@2",
            "content": "Due to z s and y t are independent of each other, p \u03b8 (x s , z s |y t ) = p \u03b8 (x s , z s ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_52",
            "start": 242,
            "end": 332,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_52@3",
            "content": "We obtain: Since minimizing KL(q \u03c6 (z|x)||p \u03b8 (z)) and KL(q \u03c6 (y|x)||p \u03b8 (y)), p \u03b8 (x s , z s ) and p \u03b8 (x t , z t ) will eventually get the same fitted distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_52",
            "start": 334,
            "end": 499,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_52@4",
            "content": "In the same way, p \u03b8 (x s , y s ) and p \u03b8 (x t , y t ) also fit to the same distribution, no matter what the target language is.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_52",
            "start": 501,
            "end": 628,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_52@5",
            "content": "This is consistent with our motivation to use the siamese network.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_52",
            "start": 630,
            "end": 695,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_53@0",
            "content": "E y t \u223cq \u03c6 (y|x t )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_53",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_54@0",
            "content": "Furthermore, the semantic discrimination loss Eq.( 5) guarantees that the semantic vectors of the source language and the target language are similar.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_54",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_54@1",
            "content": "Minimizing Eq.( 5) can be equivalent to: sim(ys, yt) > sim(ys, nt) + \u03b4 sim(ys, yt) > sim(ns, yt) + \u03b4 which is to maximize sim(y s , y t ) to encourages the target semantic vector to approach parallel source semantic vector.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_54",
            "start": 151,
            "end": 373,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_55@0",
            "content": "In summary, S 2 DM can obtain languageagnostic semantic and syntactic vectors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_55",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_55@1",
            "content": "Therefore, our multilingual MRC model is suitable even for low-resource languages without training data of decoupling model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_55",
            "start": 79,
            "end": 202,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_56@0",
            "content": "Experiment 4.1 Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_56",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_57@0",
            "content": "To verify the effectiveness of our multilingual M-RC model, we conducted experiments on three multilingual question answering benchmarks:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_57",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_58@0",
            "content": "XQuAD (Artetxe et al., 2020) consists of 11 languages corpus translated from the SQuAD v1.1 (Rajpurkar et al., 2016) development set, include : Spanish (es), German (de), Greek (el), Russian (ru), Turkish (tr), Arabic (ar), Vietnamese (vi), Thai (th), Chinese (zh), Hindi (hi), and Romanian (ro).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_58",
            "start": 0,
            "end": 295,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_59@0",
            "content": "MLQA (Lewis et al., 2020) consists of over 5K extractive MRC instances in 7 languages: English (en), Arabic (ar), German (de), Spanish (es), Hindi (hi), Vietnamese (vi) and Chinese (zh).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_59",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_59@1",
            "content": "MLQA is also highly parallel, with MRC instances parallel across 4 different languages on average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_59",
            "start": 187,
            "end": 284,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_60@0",
            "content": "TyDi QA-GoldP is the gold passage task in Ty-Di QA (Clark et al., 2020) covers 9 typologically diverse languages: Arabic (ar), Bengali (bg), English (en), Finnish (fi), Indonesian (id), Korean (ko), Russian (ru), Swahili (sw), Telugu (te).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_60",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_60@1",
            "content": "It is a more challenging MRC benchmark as questions have been written without seeing the answers, leading to 3 and 2 times less lexical overlap than XQuAD and MLQA, respectively (Hu et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_60",
            "start": 240,
            "end": 435,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_61@0",
            "content": "Baseline Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_61",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_62@0",
            "content": "We used the following two multilingual PLMs to build our MRC model to conduct experiments:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_62",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_63@0",
            "content": "mBERT is the multilingual version of BERT Devlin et al. (2019), with 177M parameters, is pretrained on the Wikipedia of 104 languages to optimize the masked language modeling objective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_63",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_64@0",
            "content": "XLM-100 uses a pre-training objective similar to that of mBERT but with a larger number of parameters (578M) and a larger shared vocabulary than mBERT, and is trained on the same Wikipedia data covering 100 languages as mBERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_64",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_65@0",
            "content": "Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_65",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_66@0",
            "content": "For S 2 DM, we collected approximately 26,000+ labelled parallel sentence pairs from the Universal Dependencies (UD 2.7) Corpus (Zeman et al., 2020) as the training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_66",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_66@1",
            "content": "The training set covers 20 languages and overlap with 13 languages of three MRC evaluation datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_66",
            "start": 170,
            "end": 269,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_66@2",
            "content": "We used Universal POS tags and HEAD tags in UD 2.7 for the POS tagging and syntactic parsing task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_66",
            "start": 271,
            "end": 368,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_66@3",
            "content": "We chose data from the Chinese semantic textual similarity (STS) task (Tang et al., 2016) as the development set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_66",
            "start": 370,
            "end": 482,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_66@4",
            "content": "For hyper-parameters in S 2 DM, the learning rate was set to 5e-5, the margin \u03b4 was 0.4, and the latent variable dimensions was 200.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_66",
            "start": 484,
            "end": 615,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_67@0",
            "content": "For our multilingual MRC models and two baseline models, we fine-tuned them on the SQuAD v1.1 (Rajpurkar et al., 2016) and evaluated the test data of the three multilingual MRC datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_67",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_67@1",
            "content": "For models based on mBERT, we fine-tuned them for 3 epochs with a training batch size of 32 and a learning rate of 2e-5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_67",
            "start": 187,
            "end": 306,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_67@2",
            "content": "We fine-tuned models based on XLM-100 for 2 epochs with a training batch size of 16 and a learning rate of 3e-5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_67",
            "start": 308,
            "end": 419,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_68@0",
            "content": "Experiment Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_68",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_69@0",
            "content": "The overall experimental results are shown in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_69",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_69@1",
            "content": "All our tests were conducted under the conditions of zero-shot transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_69",
            "start": 55,
            "end": 126,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_69@2",
            "content": "Our models (S 2 DM_POS, S 2 DM_SP combined with XLM-100 or mBERT) significantly outperform both XLM-100 and mBERT baselines on three datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_69",
            "start": 128,
            "end": 269,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_69@3",
            "content": "S 2 DM_SP achieves the best performance, indicating that the learning of deeper syntax information is compelling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_69",
            "start": 271,
            "end": 383,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_69@4",
            "content": "Especially, compared with baselines on TyDi QA-Gold datasets, S 2 DM_SP based on XLM-100 and mBERT gains 4.1%, 4.2% EM respectively improvements on average across 9 languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_69",
            "start": 385,
            "end": 559,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_70@0",
            "content": "The results of 12 languages in XQuAD and M-LQA are shown in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_70",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_70@1",
            "content": "For cross-lingual transfer performance, our models are better than the two baselines on EM or F1 of all 11 low-resource target languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_70",
            "start": 69,
            "end": 205,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_70@2",
            "content": "The TyDi QA-GoldP dataset is more challenging than XQuAD and MLQA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_70",
            "start": 207,
            "end": 272,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_70@3",
            "content": "The results of TyDi QA-GoldP are shown in Table 4, and our models are superior to the baselines on EM or F1 of all 8 low-resource target languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_70",
            "start": 274,
            "end": 420,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_70@4",
            "content": "Significantly, XLM+S 2 DM_SP outperforms the XLM-100 baselines by 8.4%, 9.5% in EM for Finnish (fi), Russian(ru), respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_70",
            "start": 422,
            "end": 548,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_70@5",
            "content": "The language families of these two languages are different from that of English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_70",
            "start": 550,
            "end": 629,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_70@6",
            "content": "The evaluation results on these three datasets verify the effectiveness of our proposed method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_70",
            "start": 631,
            "end": 725,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_71@0",
            "content": "In Section 3.3, we theoretically analyze the generalization of our model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_71",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_71@1",
            "content": "The results in three datasets show the effectiveness of five languages not included in the training target languages for S 2 DM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_71",
            "start": 74,
            "end": 201,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_71@2",
            "content": "The five languages are Romanian (ro), Vietnamese (vi) in XQuAD and Bengali (bg), Swahili (sw), Telugu (te) in TyDi QA-GoldP, which are resource-scarce and have different language families from English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_71",
            "start": 203,
            "end": 403,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_71@3",
            "content": "Significantly, mBERT+S 2 DM_SP outperforms the mBERT baseline by 13.6% in EM for Swahili (sw).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_71",
            "start": 405,
            "end": 498,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_72@0",
            "content": "Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_72",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_73@0",
            "content": "Ablation Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_73",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_74@0",
            "content": "We further conducted an ablation study based on the mBERT and VGVAE model with different combinations of losses (introduced in the Section.3.2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_74",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_74@1",
            "content": "The results are reported in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_74",
            "start": 145,
            "end": 180,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_74@2",
            "content": "Our mBERT+S 2 DM_SP MRC model achieves the strongest performance among all variants, surpassing the model w/ all losses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_74",
            "start": 182,
            "end": 301,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_74@3",
            "content": "According to the results shown in Figure 3, we can summarize that each loss is essential and suitable to our model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_74",
            "start": 303,
            "end": 417,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_75@0",
            "content": "The results without POS and STL loss (e.g., w/ CRL+SDL+WPL) on the MLQA dataset validate the effectiveness of our losses (POS or STL loss) tailored for capturing syntactic information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_75",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_75@1",
            "content": "The performance of models that only contain two losses in CRL, SDL, and WPL drops significantly compared with the w/ CRL+SDL+WPL model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_75",
            "start": 185,
            "end": 319,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_75@2",
            "content": "The results of models that only contain one of the losses in CRL, SDL drop slightly, but the EM of the model with only WPL is better than w/ CRL+WPL and w/ SDL+WPL, which further demonstrates the importance of the syntax-oriented loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_75",
            "start": 321,
            "end": 555,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_75@3",
            "content": "All ablation models do not exceed our best model, illustrating the importance of all our losses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_75",
            "start": 557,
            "end": 652,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_76@0",
            "content": "5.2 Why Use a Siamese Network in S 2 DM?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_76",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_77@0",
            "content": "In order to separate semantic information from PLMs, an alternative way is to train a single net- Corresponding to S 2 DM, there are also two single-network variants: S 2 DM_single_POS and S 2 DM_single_SP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_77",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_77@1",
            "content": "Since there is no explicit semantics learning across the source and target language, we conjecture that the single-network S 2 DM will affect the quality of learned semantic vectors and the degree of semantics-syntax decoupling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_77",
            "start": 207,
            "end": 434,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_77@2",
            "content": "As shown in Table 5, the performance of the single-network S 2 DM is worse than the siamese-network model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_77",
            "start": 436,
            "end": 541,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_78@0",
            "content": "Why S 2 DM Works?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_78",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_79@0",
            "content": "Our method mainly aims to reduce the potential negative impact of syntactic differences of languages in the zero-shot transfer process by explicitly isolating semantics from syntax in representations from multilingual pre-trained models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_79",
            "start": 0,
            "end": 236,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_79@1",
            "content": "There- fore, we hope to obtain multilingual semantic representations with rich semantic information to guide the machine to read and understand texts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_79",
            "start": 238,
            "end": 387,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_79@2",
            "content": "In order to examine (1) whether semantic vectors y in S 2 DM encode rich semantic information, and (2) whether semantics is sufficiently separated from syntax, and (3) whether semantic disentanglement can improve predicted answer spans in matching syntactic structures of the target language, we conducted additional experiments and analyses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_79",
            "start": 389,
            "end": 730,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_80@0",
            "content": "Here we used three datasets of cross-lingual semantic textual similarity (STS) in SemEval-2017 2 to evaluate the quality of semantic vectors learned by S 2 DM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_80",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_80@1",
            "content": "The three datasets are for Arabic to English (ar-en), Spanish to English (es-en), and Turkish to English (tr-en) cross-lingual STS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_80",
            "start": 160,
            "end": 290,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_80@2",
            "content": "We report the results of our models in Figure 5 based on m-BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_80",
            "start": 292,
            "end": 355,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_80@3",
            "content": "We also evaluated learned syntactic vectors in cross-lingual STS, hoping that the performance gap between semantic vectors (i.e., y in S 2 DM) and syntactic vectors (i.e., z in S 2 DM) is as large as possible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_80",
            "start": 357,
            "end": 565,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_80@4",
            "content": "As shown in Figure 5, disentangled semantic representations significantly improve Pearson correlation over the baseline in ar-en, es-en, and tr-en by 11.46%, 3.40%, 4.98%, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_80",
            "start": 567,
            "end": 751,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_80@5",
            "content": "Additionally, disentangled syntactic representations are negatively correlated to STS in most cases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_80",
            "start": 753,
            "end": 852,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_80@6",
            "content": "These results suggest that disentangled semantic vectors indeed learn rich universal semantic information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_80",
            "start": 854,
            "end": 959,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_81@0",
            "content": "We visualize hidden representations of the last layer of mBERT and semantic representations of mBERT+S 2 DM_POS and mBERT+S 2 DM_SP in Figure 6, in which the parallel sentences are from a 15-way parallel corpus (Conneau et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_81",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_81@1",
            "content": "It is clear to see that disentangled semantic representations learned by S 2 DM make parallel sentences in 15 languages (semantically equivalent to each other) closer to one another in space, blending language boundaries clearly seen from mBERT Finally, we evaluated the degree of consistency to syntactic constituents of predicted answer spans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_81",
            "start": 235,
            "end": 579,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_81@2",
            "content": "As described in Section 1, 23.15% of the nontransfer predicted correct answers violate syntactic constraint of the target language during the raw zero-shot cross-lingual transfer on BiPaR. By contrast, S 2 DM_POS and S 2 DM_SP drop this percentage to 12.98% and 6.60%, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_81",
            "start": 581,
            "end": 862,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_81@3",
            "content": "Moreover, on the entire test set of BiPaR (Jing et al., 2019) in Chinese, 93.27% answers predicted by S 2 DM_SP exactly span syntactic constituents, which is 8.14% higher than the mBERT model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_81",
            "start": 864,
            "end": 1055,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_82@0",
            "content": "Conclusions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_82",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_83@0",
            "content": "In this paper, we have presented a novel multilingual MRC model for zero-shot cross-lingual transfer, which can disentangle semantic from syntactic representations and explicitly transfer semantic information from rich-resource language to low-resource languages, reducing the influence of syntactic differences between languages on the answer span prediction of the target language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_83",
            "start": 0,
            "end": 382,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_83@1",
            "content": "To disassociate semantics from syntax in multilingual pre-trained representations, we propose the siamese semantic disentanglement model that semantics/syntax-oriented losses to guide latent variables to learn corresponding information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_83",
            "start": 384,
            "end": 619,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_83@2",
            "content": "For low-resource languages without training data of decoupling model, we theoretically analyze and experiment verify the generalization of our multilingual MRC model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_83",
            "start": 621,
            "end": 786,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_83@3",
            "content": "Further in-depth analyses suggest that the proposed S 2 DM can efficiently disentangle semantics from syntax and significantly improve syntactic consistency of answer predictions on the target language after zero-shot crosslingual transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_83",
            "start": 788,
            "end": 1027,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_84@0",
            "content": "Mikel Artetxe, Sebastian Ruder, Dani Yogatama, On the cross-lingual transferability of monolingual representations, 2020, Proceedings of the 58th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_84",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_85@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_85",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_86@0",
            "content": "Mingda Chen, Qingming Tang, Sam Wiseman, Kevin Gimpel, A multi-task approach for disentangling syntax and semantics in sentence representations, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_86",
            "start": 0,
            "end": 311,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_87@0",
            "content": "Ethan Chi, John Hewitt, Christopher Manning, Finding universal grammatical relations in multilingual BERT, 2020, Proceedings of the 58th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_87",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_88@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_88",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_89@0",
            "content": "Jonathan Clark, Jennimaria Palomaki, Vitaly Nikolaev, Eunsol Choi, Dan Garrette, Michael Collins, Tom Kwiatkowski, Tydi QA: A benchmark for information-seeking question answering in typologically diverse languages, 2020, Trans. Assoc. Comput. Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_89",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_90@0",
            "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Unsupervised cross-lingual representation learning at scale, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_90",
            "start": 0,
            "end": 322,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_91@0",
            "content": "Alexis Conneau, Guillaume Lample, Crosslingual language model pretraining, 2019, Proceedings of the 2019 Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_91",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_92@0",
            "content": "Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, Veselin Stoyanov, XNLI: Evaluating cross-lingual sentence representations, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_92",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_93@0",
            "content": "Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu, Cross-lingual machine reading comprehension, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_93",
            "start": 0,
            "end": 284,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_94@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_94",
            "start": 0,
            "end": 310,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_95@0",
            "content": "John Hewitt, Christopher Manning, A structural probe for finding syntax in word representations, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_95",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_96@0",
            "content": "Tsung-Yuan Hsu, Chi-Liang Liu, Hung-Yi Lee, Zero-shot reading comprehension by crosslingual transfer learning with multi-lingual language representation model, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_96",
            "start": 0,
            "end": 328,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_97@0",
            "content": "Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, Melvin Johnson, X-TREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation, 2020, Proceedings of the 37th International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_97",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_98@0",
            "content": "Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric Xing, Toward controlled generation of text, 2017, Proceedings of the 34th International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_98",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_99@0",
            "content": "UNKNOWN, None, 2021, Improving cross-lingual reading comprehension with self-training, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_99",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_100@0",
            "content": "Yimin Jing, Deyi Xiong, Yan Zhen, Bi-PaR: A bilingual parallel dataset for multilingual and cross-lingual reading comprehension on novels, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_100",
            "start": 0,
            "end": 307,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_101@0",
            "content": "S Patrick, Barlas Lewis, Ruty Oguz,  Rinott, MLQA: Evaluating cross-lingual extractive question answering, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_101",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_102@0",
            "content": "Shining Liang, Linjun Shou, Jian Pei, Ming Gong, Wanli Zuo, Daxin Jiang, Calibrenet: Calibration networks for multilingual sequence labeling, 2021-03-08, WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_102",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_103@0",
            "content": "Junhao Liu, Linjun Shou, Jian Pei, Ming Gong, Min Yang, Daxin Jiang, Cross-lingual machine reading comprehension with language branch knowledge distillation, 2020, Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_103",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_104@0",
            "content": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, SQuAD: 100, 000+ questions for machine comprehension of text, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_104",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_105@0",
            "content": "UNKNOWN, None, 2016, Chinese semantic text similarity trainning dataset, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_105",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_106@0",
            "content": "UNKNOWN, None, 2021, Improving low-resource reading comprehension via cross-lingual transposition rethinking, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_106",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_107@0",
            "content": "Pengcheng Yin, Chunting Zhou, Junxian He, Graham Neubig, StructVAE: Tree-structured latent variable models for semi-supervised semantic parsing, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_107",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_108@0",
            "content": "Fei Yuan, Linjun Shou, Xuanyu Bai, Ming Gong, Yaobo Liang, Nan Duan, Yan Fu, Daxin Jiang, Enhancing answer boundary detection for multilingual machine reading comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_108",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_109@0",
            "content": "UNKNOWN, None, , Mitchell Abrams, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_109",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_110@0",
            "content": "UNKNOWN, None, , LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (\u00daFAL), Faculty of Mathematics and Physics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_110",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "68-ARR_v1_111@0",
            "content": "Xinyuan Zhang, Yi Yang, Siyang Yuan, Dinghan Shen, Lawrence Carin, Syntax-infused variational autoencoder for text generation, 2019, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "68-ARR_v1_111",
            "start": 0,
            "end": 228,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "68-ARR_v1_0",
            "tgt_ix": "68-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_0",
            "tgt_ix": "68-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_1",
            "tgt_ix": "68-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_1",
            "tgt_ix": "68-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_0",
            "tgt_ix": "68-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_2",
            "tgt_ix": "68-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_4",
            "tgt_ix": "68-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_5",
            "tgt_ix": "68-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_6",
            "tgt_ix": "68-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_7",
            "tgt_ix": "68-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_8",
            "tgt_ix": "68-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_9",
            "tgt_ix": "68-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_10",
            "tgt_ix": "68-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_3",
            "tgt_ix": "68-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_3",
            "tgt_ix": "68-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_3",
            "tgt_ix": "68-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_3",
            "tgt_ix": "68-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_3",
            "tgt_ix": "68-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_3",
            "tgt_ix": "68-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_3",
            "tgt_ix": "68-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_3",
            "tgt_ix": "68-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_3",
            "tgt_ix": "68-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_3",
            "tgt_ix": "68-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_0",
            "tgt_ix": "68-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_12",
            "tgt_ix": "68-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_14",
            "tgt_ix": "68-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_13",
            "tgt_ix": "68-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_13",
            "tgt_ix": "68-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_13",
            "tgt_ix": "68-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_0",
            "tgt_ix": "68-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_15",
            "tgt_ix": "68-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_16",
            "tgt_ix": "68-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_16",
            "tgt_ix": "68-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_16",
            "tgt_ix": "68-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_17",
            "tgt_ix": "68-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_19",
            "tgt_ix": "68-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_18",
            "tgt_ix": "68-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_18",
            "tgt_ix": "68-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_18",
            "tgt_ix": "68-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_16",
            "tgt_ix": "68-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_20",
            "tgt_ix": "68-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_22",
            "tgt_ix": "68-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_23",
            "tgt_ix": "68-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_24",
            "tgt_ix": "68-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_25",
            "tgt_ix": "68-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_26",
            "tgt_ix": "68-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_27",
            "tgt_ix": "68-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_28",
            "tgt_ix": "68-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_29",
            "tgt_ix": "68-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_30",
            "tgt_ix": "68-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_31",
            "tgt_ix": "68-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_32",
            "tgt_ix": "68-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_33",
            "tgt_ix": "68-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_34",
            "tgt_ix": "68-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_35",
            "tgt_ix": "68-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_36",
            "tgt_ix": "68-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_37",
            "tgt_ix": "68-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_38",
            "tgt_ix": "68-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_39",
            "tgt_ix": "68-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_40",
            "tgt_ix": "68-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_41",
            "tgt_ix": "68-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_42",
            "tgt_ix": "68-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_43",
            "tgt_ix": "68-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_44",
            "tgt_ix": "68-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_45",
            "tgt_ix": "68-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_46",
            "tgt_ix": "68-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_47",
            "tgt_ix": "68-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_48",
            "tgt_ix": "68-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_16",
            "tgt_ix": "68-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_49",
            "tgt_ix": "68-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_51",
            "tgt_ix": "68-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_52",
            "tgt_ix": "68-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_53",
            "tgt_ix": "68-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_54",
            "tgt_ix": "68-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_50",
            "tgt_ix": "68-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_50",
            "tgt_ix": "68-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_50",
            "tgt_ix": "68-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_50",
            "tgt_ix": "68-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_50",
            "tgt_ix": "68-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_50",
            "tgt_ix": "68-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_0",
            "tgt_ix": "68-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_55",
            "tgt_ix": "68-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_57",
            "tgt_ix": "68-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_58",
            "tgt_ix": "68-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_59",
            "tgt_ix": "68-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_56",
            "tgt_ix": "68-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_56",
            "tgt_ix": "68-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_56",
            "tgt_ix": "68-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_56",
            "tgt_ix": "68-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_56",
            "tgt_ix": "68-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_56",
            "tgt_ix": "68-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_60",
            "tgt_ix": "68-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_62",
            "tgt_ix": "68-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_63",
            "tgt_ix": "68-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_61",
            "tgt_ix": "68-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_61",
            "tgt_ix": "68-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_61",
            "tgt_ix": "68-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_61",
            "tgt_ix": "68-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_56",
            "tgt_ix": "68-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_64",
            "tgt_ix": "68-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_66",
            "tgt_ix": "68-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_65",
            "tgt_ix": "68-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_65",
            "tgt_ix": "68-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_65",
            "tgt_ix": "68-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_56",
            "tgt_ix": "68-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_67",
            "tgt_ix": "68-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_69",
            "tgt_ix": "68-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_70",
            "tgt_ix": "68-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_68",
            "tgt_ix": "68-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_68",
            "tgt_ix": "68-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_68",
            "tgt_ix": "68-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_68",
            "tgt_ix": "68-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_0",
            "tgt_ix": "68-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_71",
            "tgt_ix": "68-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_72",
            "tgt_ix": "68-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_72",
            "tgt_ix": "68-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_74",
            "tgt_ix": "68-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_75",
            "tgt_ix": "68-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_76",
            "tgt_ix": "68-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_73",
            "tgt_ix": "68-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_73",
            "tgt_ix": "68-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_73",
            "tgt_ix": "68-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_73",
            "tgt_ix": "68-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_73",
            "tgt_ix": "68-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_72",
            "tgt_ix": "68-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_77",
            "tgt_ix": "68-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_79",
            "tgt_ix": "68-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_80",
            "tgt_ix": "68-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_78",
            "tgt_ix": "68-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_78",
            "tgt_ix": "68-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_78",
            "tgt_ix": "68-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_78",
            "tgt_ix": "68-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_0",
            "tgt_ix": "68-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_81",
            "tgt_ix": "68-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_82",
            "tgt_ix": "68-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_82",
            "tgt_ix": "68-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "68-ARR_v1_0",
            "tgt_ix": "68-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_1",
            "tgt_ix": "68-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_2",
            "tgt_ix": "68-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_2",
            "tgt_ix": "68-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_2",
            "tgt_ix": "68-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_2",
            "tgt_ix": "68-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_2",
            "tgt_ix": "68-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_2",
            "tgt_ix": "68-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_3",
            "tgt_ix": "68-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_4",
            "tgt_ix": "68-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_4",
            "tgt_ix": "68-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_4",
            "tgt_ix": "68-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_4",
            "tgt_ix": "68-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_4",
            "tgt_ix": "68-ARR_v1_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_4",
            "tgt_ix": "68-ARR_v1_4@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_5",
            "tgt_ix": "68-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_5",
            "tgt_ix": "68-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_5",
            "tgt_ix": "68-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_5",
            "tgt_ix": "68-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_6",
            "tgt_ix": "68-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_6",
            "tgt_ix": "68-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_6",
            "tgt_ix": "68-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_7",
            "tgt_ix": "68-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_7",
            "tgt_ix": "68-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_7",
            "tgt_ix": "68-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_8",
            "tgt_ix": "68-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_8",
            "tgt_ix": "68-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_8",
            "tgt_ix": "68-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_8",
            "tgt_ix": "68-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_8",
            "tgt_ix": "68-ARR_v1_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_9",
            "tgt_ix": "68-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_9",
            "tgt_ix": "68-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_9",
            "tgt_ix": "68-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_9",
            "tgt_ix": "68-ARR_v1_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_10",
            "tgt_ix": "68-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_11",
            "tgt_ix": "68-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_12",
            "tgt_ix": "68-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_13",
            "tgt_ix": "68-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_14",
            "tgt_ix": "68-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_14",
            "tgt_ix": "68-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_14",
            "tgt_ix": "68-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_14",
            "tgt_ix": "68-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_15",
            "tgt_ix": "68-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_15",
            "tgt_ix": "68-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_15",
            "tgt_ix": "68-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_15",
            "tgt_ix": "68-ARR_v1_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_15",
            "tgt_ix": "68-ARR_v1_15@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_15",
            "tgt_ix": "68-ARR_v1_15@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_15",
            "tgt_ix": "68-ARR_v1_15@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_16",
            "tgt_ix": "68-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_17",
            "tgt_ix": "68-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_18",
            "tgt_ix": "68-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_19",
            "tgt_ix": "68-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_19",
            "tgt_ix": "68-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_19",
            "tgt_ix": "68-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_20",
            "tgt_ix": "68-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_20",
            "tgt_ix": "68-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_20",
            "tgt_ix": "68-ARR_v1_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_20",
            "tgt_ix": "68-ARR_v1_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_21",
            "tgt_ix": "68-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_22",
            "tgt_ix": "68-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_22",
            "tgt_ix": "68-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_22",
            "tgt_ix": "68-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_23",
            "tgt_ix": "68-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_24",
            "tgt_ix": "68-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_24",
            "tgt_ix": "68-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_25",
            "tgt_ix": "68-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_26",
            "tgt_ix": "68-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_27",
            "tgt_ix": "68-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_28",
            "tgt_ix": "68-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_28",
            "tgt_ix": "68-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_28",
            "tgt_ix": "68-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_28",
            "tgt_ix": "68-ARR_v1_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_28",
            "tgt_ix": "68-ARR_v1_28@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_28",
            "tgt_ix": "68-ARR_v1_28@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_29",
            "tgt_ix": "68-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_29",
            "tgt_ix": "68-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_29",
            "tgt_ix": "68-ARR_v1_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_29",
            "tgt_ix": "68-ARR_v1_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_29",
            "tgt_ix": "68-ARR_v1_29@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_30",
            "tgt_ix": "68-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_31",
            "tgt_ix": "68-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_32",
            "tgt_ix": "68-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_33",
            "tgt_ix": "68-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_34",
            "tgt_ix": "68-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_34",
            "tgt_ix": "68-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_34",
            "tgt_ix": "68-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_34",
            "tgt_ix": "68-ARR_v1_34@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_35",
            "tgt_ix": "68-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_35",
            "tgt_ix": "68-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_36",
            "tgt_ix": "68-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_37",
            "tgt_ix": "68-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_38",
            "tgt_ix": "68-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_38",
            "tgt_ix": "68-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_38",
            "tgt_ix": "68-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_39",
            "tgt_ix": "68-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_40",
            "tgt_ix": "68-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_41",
            "tgt_ix": "68-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_41",
            "tgt_ix": "68-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_41",
            "tgt_ix": "68-ARR_v1_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_41",
            "tgt_ix": "68-ARR_v1_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_42",
            "tgt_ix": "68-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_43",
            "tgt_ix": "68-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_44",
            "tgt_ix": "68-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_44",
            "tgt_ix": "68-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_44",
            "tgt_ix": "68-ARR_v1_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_45",
            "tgt_ix": "68-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_46",
            "tgt_ix": "68-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_47",
            "tgt_ix": "68-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_48",
            "tgt_ix": "68-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_49",
            "tgt_ix": "68-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_50",
            "tgt_ix": "68-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_51",
            "tgt_ix": "68-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_52",
            "tgt_ix": "68-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_52",
            "tgt_ix": "68-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_52",
            "tgt_ix": "68-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_52",
            "tgt_ix": "68-ARR_v1_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_52",
            "tgt_ix": "68-ARR_v1_52@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_52",
            "tgt_ix": "68-ARR_v1_52@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_53",
            "tgt_ix": "68-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_54",
            "tgt_ix": "68-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_54",
            "tgt_ix": "68-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_55",
            "tgt_ix": "68-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_55",
            "tgt_ix": "68-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_56",
            "tgt_ix": "68-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_57",
            "tgt_ix": "68-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_58",
            "tgt_ix": "68-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_59",
            "tgt_ix": "68-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_59",
            "tgt_ix": "68-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_60",
            "tgt_ix": "68-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_60",
            "tgt_ix": "68-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_61",
            "tgt_ix": "68-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_62",
            "tgt_ix": "68-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_63",
            "tgt_ix": "68-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_64",
            "tgt_ix": "68-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_65",
            "tgt_ix": "68-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_66",
            "tgt_ix": "68-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_66",
            "tgt_ix": "68-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_66",
            "tgt_ix": "68-ARR_v1_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_66",
            "tgt_ix": "68-ARR_v1_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_66",
            "tgt_ix": "68-ARR_v1_66@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_67",
            "tgt_ix": "68-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_67",
            "tgt_ix": "68-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_67",
            "tgt_ix": "68-ARR_v1_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_68",
            "tgt_ix": "68-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_69",
            "tgt_ix": "68-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_69",
            "tgt_ix": "68-ARR_v1_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_69",
            "tgt_ix": "68-ARR_v1_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_69",
            "tgt_ix": "68-ARR_v1_69@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_69",
            "tgt_ix": "68-ARR_v1_69@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_70",
            "tgt_ix": "68-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_70",
            "tgt_ix": "68-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_70",
            "tgt_ix": "68-ARR_v1_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_70",
            "tgt_ix": "68-ARR_v1_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_70",
            "tgt_ix": "68-ARR_v1_70@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_70",
            "tgt_ix": "68-ARR_v1_70@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_70",
            "tgt_ix": "68-ARR_v1_70@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_71",
            "tgt_ix": "68-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_71",
            "tgt_ix": "68-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_71",
            "tgt_ix": "68-ARR_v1_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_71",
            "tgt_ix": "68-ARR_v1_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_72",
            "tgt_ix": "68-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_73",
            "tgt_ix": "68-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_74",
            "tgt_ix": "68-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_74",
            "tgt_ix": "68-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_74",
            "tgt_ix": "68-ARR_v1_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_74",
            "tgt_ix": "68-ARR_v1_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_75",
            "tgt_ix": "68-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_75",
            "tgt_ix": "68-ARR_v1_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_75",
            "tgt_ix": "68-ARR_v1_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_75",
            "tgt_ix": "68-ARR_v1_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_76",
            "tgt_ix": "68-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_77",
            "tgt_ix": "68-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_77",
            "tgt_ix": "68-ARR_v1_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_77",
            "tgt_ix": "68-ARR_v1_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_78",
            "tgt_ix": "68-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_79",
            "tgt_ix": "68-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_79",
            "tgt_ix": "68-ARR_v1_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_79",
            "tgt_ix": "68-ARR_v1_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_80",
            "tgt_ix": "68-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_80",
            "tgt_ix": "68-ARR_v1_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_80",
            "tgt_ix": "68-ARR_v1_80@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_80",
            "tgt_ix": "68-ARR_v1_80@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_80",
            "tgt_ix": "68-ARR_v1_80@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_80",
            "tgt_ix": "68-ARR_v1_80@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_80",
            "tgt_ix": "68-ARR_v1_80@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_81",
            "tgt_ix": "68-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_81",
            "tgt_ix": "68-ARR_v1_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_81",
            "tgt_ix": "68-ARR_v1_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_81",
            "tgt_ix": "68-ARR_v1_81@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_82",
            "tgt_ix": "68-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_83",
            "tgt_ix": "68-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_83",
            "tgt_ix": "68-ARR_v1_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_83",
            "tgt_ix": "68-ARR_v1_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_83",
            "tgt_ix": "68-ARR_v1_83@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_84",
            "tgt_ix": "68-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_85",
            "tgt_ix": "68-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_86",
            "tgt_ix": "68-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_87",
            "tgt_ix": "68-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_88",
            "tgt_ix": "68-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_89",
            "tgt_ix": "68-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_90",
            "tgt_ix": "68-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_91",
            "tgt_ix": "68-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_92",
            "tgt_ix": "68-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_93",
            "tgt_ix": "68-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_94",
            "tgt_ix": "68-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_95",
            "tgt_ix": "68-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_96",
            "tgt_ix": "68-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_97",
            "tgt_ix": "68-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_98",
            "tgt_ix": "68-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_99",
            "tgt_ix": "68-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_100",
            "tgt_ix": "68-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_101",
            "tgt_ix": "68-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_102",
            "tgt_ix": "68-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_103",
            "tgt_ix": "68-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_104",
            "tgt_ix": "68-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_105",
            "tgt_ix": "68-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_106",
            "tgt_ix": "68-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_107",
            "tgt_ix": "68-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_108",
            "tgt_ix": "68-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_109",
            "tgt_ix": "68-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_110",
            "tgt_ix": "68-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "68-ARR_v1_111",
            "tgt_ix": "68-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1222,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "68-ARR",
        "version": 1
    }
}