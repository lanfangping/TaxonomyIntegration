{
    "nodes": [
        {
            "ix": "345-ARR_v2_0",
            "content": "A Sentence is Worth 128 Pseudo Tokens: A Semantic-Aware Contrastive Learning Framework for Sentence Embeddings",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_2",
            "content": "Contrastive learning has shown great potential in unsupervised sentence embedding tasks, e.g., SimCSE (Gao et al., 2021). However, We find that these existing solutions are heavily affected by superficial features like the length of sentences or syntactic structures. In this paper, we propose a semantics-aware contrastive learning framework for sentence embeddings, termed Pseudo-Token BERT (PT-BERT), which is able to exploit the pseudotoken space (i.e., latent semantic space) representation of a sentence while eliminating the impact of superficial features such as sentence length and syntax. Specifically, we introduce an additional pseudo token embedding layer independent of the BERT encoder to map each sentence into a sequence of pseudo tokens in a fixed length. Leveraging these pseudo sequences, we are able to construct same-length positive and negative pairs based on the attention mechanism to perform contrastive learning. In addition, we utilize both the gradientupdating and momentum-updating encoders to encode instances while dynamically maintaining an additional queue to store the representation of sentence embeddings, enhancing the encoder's learning performance for negative examples. Experiments show that our model outperforms the state-of-the-art baselines on six standard semantic textual similarity (STS) tasks. Furthermore, experiments on alignments and uniformity losses, as well as hard examples with different sentence lengths and syntax, consistently verify the effectiveness of our method.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "345-ARR_v2_4",
            "content": "Sentence embedding serves as an essential technique in a wide range of applications, including semantic search, text clustering, text classification, etc. (Kiros et al., 2015;Logeswaran and Lee, 2018;Conneau et al., 2017;Cer et al., 2018;Reimers and Gurevych, 2019;Gao et al., 2021) Discrete augmentation (CLEAR, etc.) Continuous augmentation (SimCSE, etc.) Figure 1: A realistic scenario is described at the top, negative examples have the same length and structure, while positive examples act in the opposite way. In comparison, discrete augmentation obtains positive instances with word deletion or reordering Meng et al., 2021), which may misinterpret the meaning. The continuous method treats embeddings of the same original sentence as positive examples and augments sentences with the different encoding functions (Carlsson et al., 2021;Gao et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_5",
            "content": "learning works on learning representations such that similar examples stay close whereas dissimilar ones are far apart, and thus is suitable for sentence embeddings due to its natural availability of similar examples. Incorporating contrastive learning in sentence embeddings improves the efficiency of semantic information learning in an unsupervised manner and has been shown to be effective on a variety of tasks (Reimers and Gurevych, 2019;Gao et al., 2021;Zhang et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_6",
            "content": "In contrastive learning for sentence embeddings, a key challenge is constructing positive instances. Both discrete and continuous augmentation methods have been studied recently. Methods in Wu et al. (2018); Meng et al. (2021) perform discrete operations directly on the original sentences, such as word deletion and sentence shuffling, to get positive samples. However, these methods may lead to unacceptable semantic distortions or even complete misinterpretations of the original statement. In contrast, the SimCSE method (Gao et al., 2021) obtains two different embeddings in the continuous embedding space as a positive pair for one sentence through different dropout masks (Srivastava et al., 2014) in the neural network for representation learning. Nonetheless, this method overly relies on superficial features existing in the dataset like sentence lengths and syntactic structures and may pay less reflection on meaningful semantic information. As an illustrative example, the sentencepair in Fig. 1 \"A caterpillar was caught by me.\" and \"I caught a caterpillar.\" appear to organize differently in expression but convey exactly the same semantics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_7",
            "content": "To overcome these drawbacks, in this paper, we propose a semantic-aware contrastive learning framework for sentence embeddings, termed Pseudo-Token BERT (PT-BERT), that is able to capture the pseudo-token space (i.e., latent semantic space) representation while ignoring effects of superficial features like sentence lengths and syntactic structures. Inspired by previous works on prompt learning and sentence selection (Li and Liang, 2021;Liu et al., 2021;Humeau et al., 2020), which create a pseudo-sequence and have it serve the downstream tasks, we present PT-BERT to train pseudo token representations and then to map sentences into pseudo token spaces based on an attention mechanism.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_8",
            "content": "In particular, we train additional 128 pseudo token embeddings, together with sentence embeddings extracted from the BERT model (i.e., gradient-encoder), and then use the attention mechanism (Vaswani et al., 2017) to map the sentence embedding to the pseudo token space (i.e., semantic space). We use another BERT model (i.e., momentum-encoder ) to encode the original sentence, adopt a similar attention mechanism with the pseudo token embeddings, and finally output a continuously augmented version of the sentence embedding. We treat the representations of the original sentence encoded by the gradientencoder and the momentum-encoder as a positive pair. In addition, the momentum-encoder also generates negative examples, dynamically maintains a queue to store these negative examples, and updates them over time. By projecting all sentences onto the same pseudo sentence, the model greatly reduces the dependence on sentence length and syntax when making judgments and makes the model more focused on the semantic level information.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_9",
            "content": "In our experiments, we compare our results with the previous state-of-the-art work. We train PT-BERT on 10 6 randomly sampled sentences from English Wikipedia and evaluate on seven standard semantic textual similarity (STS) tasks (Agirre et al., 2012(Agirre et al., , 2013(Agirre et al., , 2014(Agirre et al., , 2015(Agirre et al., , 2016 (Marelli et al., 2014). Besides, we also compare our approach with a framework based on an advanced discrete augmentation we proposed. We obtain a new stateof-the-art on standard semantic textual similarity tasks with our PT-BERT, which achieves 77.74% of Spearman's correlation. To show the effectiveness of pseudo tokens, we calculate the align-loss and uniformity loss (Wang and Isola, 2020) and verify our approach on a sub-dataset with hard examples sampled from STS-(2012STS-( -2016. We have released our source code 1 to facilitate future work.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_10",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "345-ARR_v2_11",
            "content": "In this section, we discuss related studies with repect to the contrastive learning framework and sentence embedding.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_12",
            "content": "Contrastive Learning for Sentence Embedding",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "345-ARR_v2_13",
            "content": "Contrastive learning. Contrastive learning (Hadsell et al., 2006) has been used with much success in both natural language processing and computer vision (Yang et al., 2019;Klein and Nabi, 2020;Gao et al., 2021). In contrast to generative learning, contrastive learning requires learning to distinguish and match data at the abstract semantic level of the feature space. It focuses on learning common features between similar examples and distinguishing differences between non-similar examples. In order to compare the instances with more negative examples and less computation, memory bank (Wu et al., 2018) is proposed to enhance the performance under the contrastive learning framework. While with a large capacity to store more samples, the memory bank is not consistent enough, which could not update the negative examples during comparison. Momentum-Contrast (MoCo) uses a queue to maintain the dictionary of samples which allows the model to compare the query with more keys for each step and ensure the consistency of the framework. It updates the parameter of the dictionary in a momentum way.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_14",
            "content": "Discrete and continuous augmentation. By equipping discrete augmentation that modifies sentences directly on token level with contrastive learning, significant success has been achieved in obtaining sentence embeddings. Such methods include word omission (Yang et al., 2019), entity replacement (Xiong et al., 2020), trigger words (Klein and Nabi, 2020) and traditional augmentations such as deletion, reorder and substitution Meng et al., 2021). Examples with diverse expressions can be learned during training, making the model more robust to expressions of different sentence lengths and styles. However, these approaches are limited because there are huge difficulties in augmenting sentences precisely since a few changes can make the meaning completely different or even opposite.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_15",
            "content": "Researchers have also explored the possibility of building sentences continuously, which instead applies operation in embedding space. CT-BERT (Carlsson et al., 2021) encodes the same sentence with two different encoders. Unsup-SimCSE (Gao et al., 2021) compares the representations of the same sentence with different dropout masks among the mini-batch. These approaches continuously augment sentences while retaining the original meaning. However, positive pairs seen by SimCSE always have the same length and structure, whereas negative samples are likely to act oppositely. As a result, sentence length and structure are highly correlated to the similarity score of examples. During training, the model has never seen positive samples with diverse expressions, so that in real test scenarios, the model would be more inclined to classify the synonymous pairs with different expressions as negatives, and those sentences with the same length and structures are more likely to be grouped as positive pairs. This may cause a biased encoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_16",
            "content": "Pseudo Tokens",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "345-ARR_v2_17",
            "content": "In the domain of prompt learning (Liu et al., 2021;Jiang et al., 2020;Li and Liang, 2021;Gao et al., 2020), the way to create prompt can be divided into two types, namely discrete and continuous ways. Discrete methods usually search the natural language template as the prompt (Davison et al., Petroni et al., 2019), while the continuous way always directly works on the embedding space with \"pseudo tokens\" (Liu et al., 2021;Li and Liang, 2021). In retrieval and dialogue tasks, the current approach adopts \"pseudo tokens\", namely \"poly codes\" (Humeau et al., 2020), to jointly encode the query and response precisely and ensure the inference time when compared with the Cross-Encoders and Bi-Encoders (Wolf et al., 2019;Mazar\u00e9 et al., 2018;Dinan et al., 2019). The essence of these methods is to create a pseudo-sequence and have it serve the downstream tasks without the need for humans to understand the exact meaning. The parameters of these pseudo tokens are independent of the natural language embeddings, and can be tuned based on a specific downstream task. In the following sections, we will show the idea to weaken the model's consideration of sentence length and structures by introducing additional pseudo token embeddings on top of the BERT encoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_18",
            "content": "Methods",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "345-ARR_v2_19",
            "content": "In this section, we introduce PT-BERT, which provides novel contributions on combining advantages of both discrete and continuous augmentations to advance the state-of-art of sentence embeddings. We first present the setup of problems with a thorough analysis on the bias introduced by the textual similarity theoretically and experimentally. Then we show the details of Pseudo-Token representation and our model's architecture.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_20",
            "content": "Preliminary",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "345-ARR_v2_21",
            "content": "Consider a sentence s, we say that the augmentation is continuous if s is augmented by different encoding functions, f (\u2022) and f \u2032 (\u2022). Sentence embeddings h = f (s) and h \u2032 = f \u2032 (s) are obtained by these two functions. With a slight change of the encoding function (e.g., encoders with different dropout masks), h \u2032 can be seen as a more precisely augmented version of h compared with the discrete augmentation. Semantic information of h \u2032 should be the same as h. Therefore, h and h \u2032 are a pair of positive examples and we could randomly sample a sentence to construct negative example pairs. Previous state-of-the-art models (Gao et al., 2021) adopt the continuous strategy that augments sentences with dropout (Srivastava et al., 2014). It is obvious that all the positive examples in SimCSE have the same length and structure while negative examples act oppositely. In this way, SimCSE will inevitably take these two factors as hints during test. To further verify this conjecture, we sort out the positive pairs with a length difference of more than five words and negative pairs of less than two words from STS-(2012STS-( -2016.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_22",
            "content": "Table 1 shows that the performance of SimCSE plummets on this dataset. Besides, we also find that SimCSE truncates all training corpus into 32 tokens, which shortens the discrepancy of the sentence's length. After we scale the max length that SimCSE could accept from 32 to 64 and 128, the performance degrades significantly during the test even though the model is supposed to learn more from the complete version of sentences(See Table 2). The reason for this result may lie in the fact that, without truncation, all positive pairs still have the same length, whereas the difference in length between the negative and positive ones is enlarged. Therefore, the encoder will rely more on sentence length and make the wrong decision.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_23",
            "content": "Pseudo-Token BERT",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "345-ARR_v2_24",
            "content": "We realize it is vital to train an unbiased encoder that captures the semantics and also would not introduce intermediate errors. This motivates us to propose the PT-BERT, as evidence shows that the encoder may fail to make predictions when trained on a biased dataset with same-length positive pairs, by learning the spurious correlations that work only well on the training dataset (Arjovsky et al., 2019;Nam et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_25",
            "content": "Pseudo-Token representations. The idea of PT-BERT is to reduce the model's excessive dependence on textual similarity when making predic-tions. Discrete augmentation achieves this goal by providing both positive and negative examples with diverse expressions. Therefore the model does not jump to conclusions based on sentence length and syntactic structure during the test.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_26",
            "content": "Note that we achieve this same purpose in a seemingly opposite way: mapping the representations of both positive and negative examples to a pseudo sentence with the same length and structure. We take an additional embedding layer outside the BERT encoder to represent a pseudo sentence {0, 1, ..., m} with fixed length m and syntax. This embedding layer is fully independent of the BERT encoder, including the parameters and corresponding vocabulary. Random initialization is applied to this layer, and each parameter will be updated during training. The size of this layer depends on the vocabulary of pseudo tokens(length of pseudo sentences). Besides, adopting the attention mechanism (Vaswani et al., 2017;Bahdanau et al., 2015;Gehring et al., 2017), we take the pseudo sentence embeddings as the query states of cross attention while key and value states are the sentence embeddings obtained from the BERT encoder. This allows the pseudo sentence to attend to the core part and the redundant part of original sentence while keeping the fixed length and structure.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_27",
            "content": "Fig. 2 illustrates the framework of PT-BERT. Denoting the pseudo sentence embedding as P and the sentence embedding encoded by BERT as Y, we obtain the weighted pseudo sentence embedding of each sentence by mapping the sentence embedding to the pseudo tokens with attention:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_28",
            "content": "Z \u2032 i = Attention(PW Q , Y i W K , Y i W V ) (1) Attention(Q, K, V) = softmax( QK T \u221a d k )V,(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_29",
            "content": "where d k is the dimension of the model, W Q , W K , W V are the learnable parameters with R d k \u00d7d k , i denotes the i-th sentence in the dataset. Then we obtain the final embedding h i with the same attention layer by mapping pseudo sentences back to original sentence embeddings:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_30",
            "content": "h i = Attention(Y i W Q , Z \u2032 i W K , Z \u2032 i W V ). (3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_31",
            "content": "Finally, we compare the cosine similarities between the obtained embeddings of h and h \u2032 using Eq. 4 , where h \u2032 are the samples encoded by the momentum-encoder and stored in a queue.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_32",
            "content": "Model architecture. Instead of inputting the same sentence twice to the same encoder, we follow the architecture proposed in Momentum-Contrast (MoCo) such that PT-BERT can efficiently learn from more negative examples. Samples in PT-BERT are encoded into vectors with two encoders: gradient-update encoder (the upper encoder in Fig. 2) and momentum-update encoder (the momentum encoder in Fig. 2). We dynamically maintain a queue to store the sentence representations from momentum-update encoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_33",
            "content": "This mechanism allows us to store as much negative samples as possible without re-computation. Once the queue is full, we replace the \"oldest\" negative sample with a \"fresh\" one encoded by the momentum-encoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_34",
            "content": "Similar to the works based on continuous augmentation, at the very beginning of the framework, PT-BERT takes input sentence s and obtains h i and h \u2032 i with two different encoder functions. We measure the loss function with:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_35",
            "content": "\u2113 i = \u2212 log e sim(h i ,h \u2032 i )/\u03c4 M j=1 e sim(h i ,h j \u2032 )/\u03c4 ,(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_36",
            "content": "where h i denotes the representations extracted from the gradient-update encoder, h \u2032 i represents the sentence embedding in the queue, and M is the queue size. Our gradient-update and momentumupdate encoder are based on the pre-trained language model with the same structure and dimensions as BERT-base-uncased (Devlin et al., 2019). The momentum encoder will update its parameters similar to MoCo:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_37",
            "content": "\u03b8 k \u2190 \u03bb\u03b8 k + (1 \u2212 \u03bb)\u03b8 q , (5",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_38",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_39",
            "content": "where \u03b8 k is the parameter of the momentumcontrast encoder that maintains the dictionary, \u03b8 q is the query encoder that updates the parameters with gradients, and \u03bb is a hyperparameter used to control the updating process.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_40",
            "content": "Relationship with prompt learning. Rather than directly perform soft prompting in the embedding space (Li and Liang, 2021;Qin and Eisner, 2021;Liu et al., 2021) embedding precisely without adding extra computation. In some tasks, fixed-LM tuning (Li and Liang, 2021) in soft prompting becomes competitive only when the language models been scaled to big enough (Lester et al., 2021). While the prompt+LM (Ben-David et al., 2021;Liu et al., 2021) tuning adds more burdens for both the period of training and inference. Both prompt+LM and fixed-LM prompt tuning require storing separate copies of soft prompts for different tasks, while our approach only saves the trained BERT model, which draws on some ideas in prompt learning and makes our considerations in computational and memory efficiency and generality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_41",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "345-ARR_v2_42",
            "content": "In this section, we perform the standard semantic textual similarity (STS) (Agirre et al., 2012(Agirre et al., , 2013(Agirre et al., , 2014(Agirre et al., , 2015(Agirre et al., , 2016 tasks to test our model. For all tasks, we measure the Spearman's correlation to compare our performance with the previous stateof-the-art SimCSE (Gao et al., 2021). In the following, we will describe the training procedure in detail.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_43",
            "content": "Training Data and Settings",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "345-ARR_v2_44",
            "content": "Datasets. Following SimCSE, We train our model on 1-million sentences randomly sampled from English Wikipedia, and evaluate the model every 125 steps to find the best checkpoints. Note that we do not fine-tune our model on any dataset, which indicates that our method is completely unsupervised.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_45",
            "content": "Hardware and schedule. We train our model on the machine with one NVIDIA V100s GPU. Following the settings of SimCSE (Gao et al., 2021), it takes 50 minutes to run an epoch.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_46",
            "content": "Implementations",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "345-ARR_v2_47",
            "content": "We implement PT-BERT based on Huggingface transformers (Wolf et al., 2020) and initialize it with the released BERT base (Devlin et al., 2019). We initialize a new embedding for pseudo tokens with 128\u00d7768. During training, we create a pseudo sentence {0, 1, 2, ..., 127} for every input and map the original sentence to this pseudo sentence by attention. With batches of 64 sentences and an additional dynamically maintained queue of 256 sentences, each sentence has one positive sample and 255 negative samples. Adam (Kingma and Ba, 2014) optimizer is used to update the model parameters. We also take the original dropout strategy of BERT with rate p = 0.1. We set the momentum for the momentum-encoder with \u03bb = 0.885.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_48",
            "content": "Evaluation Setup",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "345-ARR_v2_49",
            "content": "We evaluate the fine-tuned BERT encoder on STS-B development sets every 125 steps to select the best checkpoints. We report all the checkpoints based on the evaluation results reported in Table 4. The training process is fully unsupervised since no training corpus from STS is used. During the evaluation, we also calculate the trends of alignment-loss and uniformity-loss. Losses were compared with SimCSE (Gao et al., 2021) under the same experimental settings. After training and evaluation, we test models on 7 STS tasks: STS 2012-2016 (Agirre et al., 2012(Agirre et al., , 2013(Agirre et al., , 2014(Agirre et al., , 2015(Agirre et al., , 2016, STS Benchmark (Cer et al., 2017) and SICK-Relatedness (Marelli et al., 2014). We report the result of Spearman's correlation for all the experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_50",
            "content": "Main Results and Analysis",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "345-ARR_v2_51",
            "content": "We first compare PT-BERT with our baseline: MoCo framework + BERT encoder (MoCo-BERT).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_52",
            "content": "MoCo-BERT could be seen as a version of PT-BERT without pseudo token embeddings. Then we apply traditional discrete augmentations such as reorder, duplication, and deletion on this framework. We also compare our work with CLEAR that substitutes and deletes the token spans. Besides, we argue that the performance of these methods is too weak. We additionally propose an advanced discrete augmentation approach that produces positive examples with the guidance of Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002;Palmer et al., 2010) positive examples. In addition to the work based on discrete approaches, we also compare with Sim-CSE (Gao et al., 2021) which continuously augment sentences with dropout. In Table 3, PT-BERT with 128 pseudo tokens further pushed the state-ofthe-art results to 77.74% and significantly outperformed SimCSE over six datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_53",
            "content": "In Fig 3, we observe that PT-BERT also achieves better alignment and uniformity against SimCSE, which indicates that pseudo tokens really help the learning of sentence representations. In detail, alignment and uniformity are proposed by (Wang and Isola, 2020) to evaluate the quality of representations in contrastive learning. The calculation of these two metrics are shown in the following formulas: L unif ormity = log",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_54",
            "content": "L alignment = E (x,x + )\u223cppos ||f (x) \u2212 f (x + )|| 2 , (6) Method STS12 STS13 STS14 STS15 STS16 STS-B SICK-R Avg.(",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_55",
            "content": "E (x,y)\u223cp data e \u22122||f (x)\u2212f (y)|| 2 ,(7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_56",
            "content": "where (x, x + ) is the positive pair, (x, y) is the pair consisting of any two different sentences in the whole sentence set, f (x) is the normalized representation of x. We employ the final embedding h to calculate these scores.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_57",
            "content": "According to the above formulas, lower alignment loss means a shorter distance between the positive samples, and low uniformity loss implies the diversity of embeddings of all sentences. Both are our expectations for the representations based on contrastive learning. To evaluate our model's performance on alignment and uniformity, we compare it with SimCSE on the STS-benchmark dataset (Cer et al., 2017), and the result is shown in Figure 3. The result demonstrates that PT-BERT outperforms SimCSE on these two metrics: our model has a lower alignment and uniformity than SimCSE in almost all the training steps, which indicates that the representations produced by our model are more in line with the goal of the contrastive learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_58",
            "content": "Analysis",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "345-ARR_v2_59",
            "content": "Ablation Studies",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "345-ARR_v2_60",
            "content": "In this section, we first investigate the impact of different sizes of pseudo token embeddings. Then we would like to report the performance difference caused by queue size under the MoCo framework.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_61",
            "content": "Pseudo Sentence Length Different lengths of pseudo tokens can affect the ability of the model to express the sentence representations. By mapping the original sentences to various lengths of pseudo tokens, the performance of PT-BERT could be different. In this section, we keep all the parts except the pseudo tokens and their embeddings unchanged. We scale the pseudo sequence length from 64 to 360. Table 5(a) shows a comparison between different lengths of pseudo sequence in PT-BERT. We find that during training, PT-BERT performs better when attending to pseudo sequences with 128 tokens. Too few pseudo tokens do not fully explain the semantics of the original sentence, while too many pseudo tokens increase the number of parameters and over-express the sentence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_62",
            "content": "Queue Size The introduction of more negative samples would make the model's training more reliable. By training with different queue sizes, we report the result of PT-BERT with different performances due to the number of negative samples. In Table 5(b), queue size q = 4 performs best. However, the difference in performance between the three sets of experiments is not large, suggesting that the model can learn well as long as it can see enough negative samples.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_63",
            "content": "Exploration on Hard Examples with Different Length",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "345-ARR_v2_64",
            "content": "To prove the effectiveness of PT-BERT that could weaken the hints caused by textual similarity, we further test PT-BERT on the sub-dataset introduced in Sec. 3.1. We sorted out the positive pairs with a length difference of more than five words and negative pairs of less than two words from STS-(2012STS-( -2016. PT-BERT significantly outperforms SimCSE with 3.36% Spearman's correlation, indicating that PT-BERT could handle these hard examples better than SimCSE. This further proves that PT-BERT could debias the spurious correlation introduced by sentence length and syntax, and focus more on the semantics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_65",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "345-ARR_v2_66",
            "content": "In this paper, we propose a semantic-aware contrastive learning framework for sentence embeddings, termed PT-BERT. Our proposed PT-BERT approach is able to weaken textual similarity information, such as sentence length and syntactic structures, by mapping the original sentence to a fixed pseudo sentence embedding. We provide analysis of these factors on methods based on continuous and discrete augmentation, showing that PT-BERT augments sentences more accurately than discrete methods while considering more semantics instead of textual similarity than continuous approaches. Lower uniformity loss and alignment loss prove the effectiveness of PT-BERT and further experiments also show that PT-BERT could handle hard examples better than existing approaches.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_67",
            "content": "Providing a new perspective to the continuous data augmentation in sentence embeddings, we believe our proposed PT-BERT has great potential to be applied in broader downstream applications, such as text classification, text clustering, and sentiment analysis.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "345-ARR_v2_68",
            "content": "Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, I\u00f1igo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, Janyce Wiebe, and pilot on interpretability, 2015, Proceedings of the 9th International Workshop on Semantic Evaluation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Eneko Agirre",
                    "Carmen Banea",
                    "Claire Cardie",
                    "Daniel Cer",
                    "Mona Diab",
                    "Aitor Gonzalez-Agirre",
                    "Weiwei Guo",
                    "I\u00f1igo Lopez-Gazpio",
                    "Montse Maritxalar",
                    "Rada Mihalcea",
                    "German Rigau",
                    "Larraitz Uria",
                    "Janyce Wiebe"
                ],
                "title": "and pilot on interpretability",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 9th International Workshop on Semantic Evaluation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "345-ARR_v2_69",
            "content": "Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, Janyce Wiebe, SemEval-2014 task 10: Multilingual semantic textual similarity, 2014, Proceedings of the 8th International Workshop on Semantic Evaluation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Eneko Agirre",
                    "Carmen Banea",
                    "Claire Cardie",
                    "Daniel Cer",
                    "Mona Diab",
                    "Aitor Gonzalez-Agirre",
                    "Weiwei Guo",
                    "Rada Mihalcea",
                    "German Rigau",
                    "Janyce Wiebe"
                ],
                "title": "SemEval-2014 task 10: Multilingual semantic textual similarity",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 8th International Workshop on Semantic Evaluation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "345-ARR_v2_70",
            "content": "Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, Janyce Wiebe, SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation, 2016, Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Eneko Agirre",
                    "Carmen Banea",
                    "Daniel Cer",
                    "Mona Diab",
                    "Aitor Gonzalez-Agirre",
                    "Rada Mihalcea",
                    "German Rigau",
                    "Janyce Wiebe"
                ],
                "title": "SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "345-ARR_v2_71",
            "content": "Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, SemEval-2012 task 6: A pilot on semantic textual similarity, 2012, *SEM 2012: The First Joint Conference on Lexical and Computational Semantics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Eneko Agirre",
                    "Daniel Cer",
                    "Mona Diab",
                    "Aitor Gonzalez-Agirre"
                ],
                "title": "SemEval-2012 task 6: A pilot on semantic textual similarity",
                "pub_date": "2012",
                "pub_title": "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_72",
            "content": "Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, *SEM 2013 shared task: Semantic textual similarity, 2013, Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Eneko Agirre",
                    "Daniel Cer",
                    "Mona Diab",
                    "Aitor Gonzalez-Agirre",
                    "Weiwei Guo"
                ],
                "title": "*SEM 2013 shared task: Semantic textual similarity",
                "pub_date": "2013",
                "pub_title": "Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "345-ARR_v2_73",
            "content": "UNKNOWN, None, 1907, Invariant risk minimization. ArXiv, abs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": "1907",
                "pub_title": "Invariant risk minimization. ArXiv, abs",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_74",
            "content": "UNKNOWN, None, 2015, Neural machine translation by jointly learning to align and translate, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Neural machine translation by jointly learning to align and translate",
                "pub": "CoRR"
            }
        },
        {
            "ix": "345-ARR_v2_75",
            "content": "UNKNOWN, None, 2021, PADA: example-based prompt learning for on-the-fly adaptation to unseen domains, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "PADA: example-based prompt learning for on-the-fly adaptation to unseen domains",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_76",
            "content": "Fredrik Carlsson, Evangelia Amaru Cuba Gyllensten,  Gogoulou, Erik Ylip\u00e4\u00e4 Hellqvist, and Magnus Sahlgren. 2021. Semantic re-tuning with contrastive tension, , ICLR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Fredrik Carlsson",
                    "Evangelia Amaru Cuba Gyllensten",
                    " Gogoulou"
                ],
                "title": "Erik Ylip\u00e4\u00e4 Hellqvist, and Magnus Sahlgren. 2021. Semantic re-tuning with contrastive tension",
                "pub_date": null,
                "pub_title": "ICLR",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_77",
            "content": "Daniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-Gazpio, Lucia Specia, SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation, 2017, Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Daniel Cer",
                    "Mona Diab",
                    "Eneko Agirre",
                    "I\u00f1igo Lopez-Gazpio",
                    "Lucia Specia"
                ],
                "title": "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "345-ARR_v2_78",
            "content": "Daniel Cer, Yinfei Yang, Sheng-Yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St, Noah John, Mario Constant, Steve Guajardo-Cespedes, Chris Yuan, Brian Tar, Ray Strope,  Kurzweil, Universal sentence encoder for English, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Daniel Cer",
                    "Yinfei Yang",
                    "Sheng-Yi Kong",
                    "Nan Hua",
                    "Nicole Limtiaco",
                    "Rhomni St",
                    "Noah John",
                    "Mario Constant",
                    "Steve Guajardo-Cespedes",
                    "Chris Yuan",
                    "Brian Tar",
                    "Ray Strope",
                    " Kurzweil"
                ],
                "title": "Universal sentence encoder for English",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "345-ARR_v2_79",
            "content": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, A simple framework for contrastive learning of visual representations, 2020, Proceedings of the 37th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Ting Chen",
                    "Simon Kornblith",
                    "Mohammad Norouzi",
                    "Geoffrey Hinton"
                ],
                "title": "A simple framework for contrastive learning of visual representations",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 37th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "345-ARR_v2_80",
            "content": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc Barrault, Antoine Bordes, Supervised learning of universal sentence representations from natural language inference data, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Alexis Conneau",
                    "Douwe Kiela",
                    "Holger Schwenk",
                    "Lo\u00efc Barrault",
                    "Antoine Bordes"
                ],
                "title": "Supervised learning of universal sentence representations from natural language inference data",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_81",
            "content": "Joe Davison, Joshua Feldman, Alexander Rush, Commonsense knowledge mining from pretrained models, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Joe Davison",
                    "Joshua Feldman",
                    "Alexander Rush"
                ],
                "title": "Commonsense knowledge mining from pretrained models",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "345-ARR_v2_82",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "345-ARR_v2_83",
            "content": "Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston, Wizard of Wikipedia: Knowledge-powered conversational agents, 2019, Proceedings of the International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Emily Dinan",
                    "Stephen Roller",
                    "Kurt Shuster",
                    "Angela Fan",
                    "Michael Auli",
                    "Jason Weston"
                ],
                "title": "Wizard of Wikipedia: Knowledge-powered conversational agents",
                "pub_date": "2019",
                "pub_title": "Proceedings of the International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_84",
            "content": "UNKNOWN, None, 2020, Making pre-trained language models better few-shot learners, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Making pre-trained language models better few-shot learners",
                "pub": "CoRR"
            }
        },
        {
            "ix": "345-ARR_v2_85",
            "content": "Tianyu Gao, Xingcheng Yao, Danqi Chen, SimCSE: Simple contrastive learning of sentence embeddings, 2021, Empirical Methods in Natural Language Processing, EMNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Tianyu Gao",
                    "Xingcheng Yao",
                    "Danqi Chen"
                ],
                "title": "SimCSE: Simple contrastive learning of sentence embeddings",
                "pub_date": "2021",
                "pub_title": "Empirical Methods in Natural Language Processing",
                "pub": "EMNLP"
            }
        },
        {
            "ix": "345-ARR_v2_86",
            "content": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann Dauphin, Convolutional sequence to sequence learning, 2017, Proceedings of the 34th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Jonas Gehring",
                    "Michael Auli",
                    "David Grangier",
                    "Denis Yarats",
                    "Yann Dauphin"
                ],
                "title": "Convolutional sequence to sequence learning",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 34th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "345-ARR_v2_87",
            "content": "Daniel Gildea, Daniel Jurafsky, Automatic labeling of semantic roles, 2002, Comput. Linguist, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Daniel Gildea",
                    "Daniel Jurafsky"
                ],
                "title": "Automatic labeling of semantic roles",
                "pub_date": "2002",
                "pub_title": "Comput. Linguist",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_88",
            "content": "R Hadsell, S Chopra, Y Lecun, Dimensionality reduction by learning an invariant mapping, 2006, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "R Hadsell",
                    "S Chopra",
                    "Y Lecun"
                ],
                "title": "Dimensionality reduction by learning an invariant mapping",
                "pub_date": "2006",
                "pub_title": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_89",
            "content": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Momentum contrast for unsupervised visual representation learning, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Kaiming He",
                    "Haoqi Fan",
                    "Yuxin Wu",
                    "Saining Xie",
                    "Ross Girshick"
                ],
                "title": "Momentum contrast for unsupervised visual representation learning",
                "pub_date": "2020",
                "pub_title": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_90",
            "content": "Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, Jason Weston, Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Samuel Humeau",
                    "Kurt Shuster",
                    "Marie-Anne Lachaux",
                    "Jason Weston"
                ],
                "title": "Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_91",
            "content": "Zhengbao Jiang, Frank Xu, How Can We Know What Language Models Know?, 2020-06, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Zhengbao Jiang",
                    "Frank Xu"
                ],
                "title": "How Can We Know What Language Models Know?",
                "pub_date": "2020-06",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_92",
            "content": "UNKNOWN, None, 2014, Adam: A method for stochastic optimization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": null,
                "title": null,
                "pub_date": "2014",
                "pub_title": "Adam: A method for stochastic optimization",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_93",
            "content": "Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler, 2015, Proceedings of the 28th International Conference on Neural Information Processing Systems, MIT Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Ryan Kiros",
                    "Yukun Zhu",
                    "Ruslan Salakhutdinov",
                    "Richard Zemel",
                    "Antonio Torralba"
                ],
                "title": "Raquel Urtasun, and Sanja Fidler",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 28th International Conference on Neural Information Processing Systems",
                "pub": "MIT Press"
            }
        },
        {
            "ix": "345-ARR_v2_94",
            "content": "Tassilo Klein, Moin Nabi, Contrastive selfsupervised learning for commonsense reasoning, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Tassilo Klein",
                    "Moin Nabi"
                ],
                "title": "Contrastive selfsupervised learning for commonsense reasoning",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_95",
            "content": "Brian Lester, Rami Al-Rfou, Noah Constant, The power of scale for parameter-efficient prompt tuning, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Brian Lester",
                    "Rami Al-Rfou",
                    "Noah Constant"
                ],
                "title": "The power of scale for parameter-efficient prompt tuning",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_96",
            "content": "UNKNOWN, None, 2021, Prefix-tuning: Optimizing continuous prompts for generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Prefix-tuning: Optimizing continuous prompts for generation",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_97",
            "content": "UNKNOWN, None, , Zhilin Yang, and Jie Tang. 2021. Gpt understands, too, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Zhilin Yang, and Jie Tang. 2021. Gpt understands, too",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_98",
            "content": "Lajanugen Logeswaran, Honglak Lee, An efficient framework for learning sentence representations, 2018, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Lajanugen Logeswaran",
                    "Honglak Lee"
                ],
                "title": "An efficient framework for learning sentence representations",
                "pub_date": "2018",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_99",
            "content": "Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, Roberto Zamparelli, A SICK cure for the evaluation of compositional distributional semantic models, 2014-05-26, Proceedings of the Ninth International Conference on Language Resources and Evaluation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Marco Marelli",
                    "Stefano Menini",
                    "Marco Baroni",
                    "Luisa Bentivogli",
                    "Raffaella Bernardi",
                    "Roberto Zamparelli"
                ],
                "title": "A SICK cure for the evaluation of compositional distributional semantic models",
                "pub_date": "2014-05-26",
                "pub_title": "Proceedings of the Ninth International Conference on Language Resources and Evaluation",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_100",
            "content": "Pierre-Emmanuel Mazar\u00e9, Samuel Humeau, Martin Raison, Antoine Bordes, Training millions of personalized dialogue agents, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Pierre-Emmanuel Mazar\u00e9",
                    "Samuel Humeau",
                    "Martin Raison",
                    "Antoine Bordes"
                ],
                "title": "Training millions of personalized dialogue agents",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "345-ARR_v2_101",
            "content": "UNKNOWN, None, 2021, Cocolm: Correcting and contrasting text sequences for language model pretraining, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Cocolm: Correcting and contrasting text sequences for language model pretraining",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_102",
            "content": "Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, Jinwoo Shin, Learning from failure: Training debiased classifier from biased classifier, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Junhyun Nam",
                    "Hyuntak Cha",
                    "Sungsoo Ahn",
                    "Jaeho Lee",
                    "Jinwoo Shin"
                ],
                "title": "Learning from failure: Training debiased classifier from biased classifier",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_103",
            "content": "Martha Palmer, Daniel Gildea, Nianwen Xue, Semantic role labeling, 2010, Synthesis Lectures on Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Martha Palmer",
                    "Daniel Gildea",
                    "Nianwen Xue"
                ],
                "title": "Semantic role labeling",
                "pub_date": "2010",
                "pub_title": "Synthesis Lectures on Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_104",
            "content": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Language models as knowledge bases?, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Fabio Petroni",
                    "Tim Rockt\u00e4schel",
                    "Sebastian Riedel",
                    "Patrick Lewis",
                    "Anton Bakhtin",
                    "Yuxiang Wu",
                    "Alexander Miller"
                ],
                "title": "Language models as knowledge bases?",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "345-ARR_v2_105",
            "content": "UNKNOWN, None, 2021, Learning how to ask: Querying lms with mixtures of soft prompts, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Learning how to ask: Querying lms with mixtures of soft prompts",
                "pub": "CoRR"
            }
        },
        {
            "ix": "345-ARR_v2_106",
            "content": "Nils Reimers, Iryna Gurevych, Sentence-bert: Sentence embeddings using siamese bert-networks, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Nils Reimers",
                    "Iryna Gurevych"
                ],
                "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_107",
            "content": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, Dropout: A simple way to prevent neural networks from overfitting, 2014, J. Mach. Learn. Res, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Nitish Srivastava",
                    "Geoffrey Hinton",
                    "Alex Krizhevsky",
                    "Ilya Sutskever",
                    "Ruslan Salakhutdinov"
                ],
                "title": "Dropout: A simple way to prevent neural networks from overfitting",
                "pub_date": "2014",
                "pub_title": "J. Mach. Learn. Res",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_108",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Illia Kaiser,  Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "Illia Kaiser",
                    " Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "345-ARR_v2_109",
            "content": "Tongzhou Wang, Phillip Isola, Understanding contrastive representation learning through alignment and uniformity on the hypersphere, 2020, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Tongzhou Wang",
                    "Phillip Isola"
                ],
                "title": "Understanding contrastive representation learning through alignment and uniformity on the hypersphere",
                "pub_date": "2020",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "345-ARR_v2_110",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest,  Rush, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Thomas Wolf",
                    "Lysandre Debut",
                    "Victor Sanh",
                    "Julien Chaumond",
                    "Clement Delangue",
                    "Anthony Moi",
                    "Pierric Cistac",
                    "Tim Rault",
                    "R\u00e9mi Louf",
                    "Morgan Funtowicz",
                    "Joe Davison",
                    "Sam Shleifer",
                    "Clara Patrick Von Platen",
                    "Yacine Ma",
                    "Julien Jernite",
                    "Canwen Plu",
                    "Teven Xu",
                    "Sylvain Scao",
                    "Mariama Gugger",
                    "Quentin Drame",
                    "Alexander Lhoest",
                    " Rush"
                ],
                "title": "Transformers: State-of-the-art natural language processing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "345-ARR_v2_111",
            "content": "UNKNOWN, None, 2019, Transfertransfo: A transfer learning approach for neural network based conversational agents, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Transfertransfo: A transfer learning approach for neural network based conversational agents",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_112",
            "content": "UNKNOWN, None, 2018, Unsupervised feature learning via nonparametric instance-level discrimination, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Unsupervised feature learning via nonparametric instance-level discrimination",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_113",
            "content": "UNKNOWN, None, 2020, Clear: Contrastive learning for sentence representation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Clear: Contrastive learning for sentence representation",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_114",
            "content": "Wenhan Xiong, Jingfei Du, William Wang, Veselin Stoyanov, Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Wenhan Xiong",
                    "Jingfei Du",
                    "William Wang",
                    "Veselin Stoyanov"
                ],
                "title": "Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model",
                "pub_date": "2020-04-26",
                "pub_title": "8th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "345-ARR_v2_115",
            "content": "Zonghan Yang, Yong Cheng, Yang Liu, Maosong Sun, Reducing word omission errors in neural machine translation: A contrastive learning approach, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Zonghan Yang",
                    "Yong Cheng",
                    "Yang Liu",
                    "Maosong Sun"
                ],
                "title": "Reducing word omission errors in neural machine translation: A contrastive learning approach",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "345-ARR_v2_116",
            "content": "Yan Zhang, Ruidan He, Zuozhu Liu, Hui Kwan, Lidong Lim,  Bing, An unsupervised sentence embedding method by mutual information maximization, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Yan Zhang",
                    "Ruidan He",
                    "Zuozhu Liu",
                    "Hui Kwan",
                    "Lidong Lim",
                    " Bing"
                ],
                "title": "An unsupervised sentence embedding method by mutual information maximization",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "345-ARR_v2_0@0",
            "content": "A Sentence is Worth 128 Pseudo Tokens: A Semantic-Aware Contrastive Learning Framework for Sentence Embeddings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_0",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_2@0",
            "content": "Contrastive learning has shown great potential in unsupervised sentence embedding tasks, e.g., SimCSE (Gao et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_2",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_2@1",
            "content": "However, We find that these existing solutions are heavily affected by superficial features like the length of sentences or syntactic structures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_2",
            "start": 122,
            "end": 266,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_2@2",
            "content": "In this paper, we propose a semantics-aware contrastive learning framework for sentence embeddings, termed Pseudo-Token BERT (PT-BERT), which is able to exploit the pseudotoken space (i.e., latent semantic space) representation of a sentence while eliminating the impact of superficial features such as sentence length and syntax.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_2",
            "start": 268,
            "end": 597,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_2@3",
            "content": "Specifically, we introduce an additional pseudo token embedding layer independent of the BERT encoder to map each sentence into a sequence of pseudo tokens in a fixed length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_2",
            "start": 599,
            "end": 772,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_2@4",
            "content": "Leveraging these pseudo sequences, we are able to construct same-length positive and negative pairs based on the attention mechanism to perform contrastive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_2",
            "start": 774,
            "end": 938,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_2@5",
            "content": "In addition, we utilize both the gradientupdating and momentum-updating encoders to encode instances while dynamically maintaining an additional queue to store the representation of sentence embeddings, enhancing the encoder's learning performance for negative examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_2",
            "start": 940,
            "end": 1209,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_2@6",
            "content": "Experiments show that our model outperforms the state-of-the-art baselines on six standard semantic textual similarity (STS) tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_2",
            "start": 1211,
            "end": 1341,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_2@7",
            "content": "Furthermore, experiments on alignments and uniformity losses, as well as hard examples with different sentence lengths and syntax, consistently verify the effectiveness of our method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_2",
            "start": 1343,
            "end": 1525,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_4@0",
            "content": "Sentence embedding serves as an essential technique in a wide range of applications, including semantic search, text clustering, text classification, etc. (Kiros et al., 2015;Logeswaran and Lee, 2018;Conneau et al., 2017;Cer et al., 2018;Reimers and Gurevych, 2019;Gao et al., 2021) Discrete augmentation (CLEAR, etc.) Continuous augmentation (SimCSE, etc.) Figure 1: A realistic scenario is described at the top, negative examples have the same length and structure, while positive examples act in the opposite way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_4",
            "start": 0,
            "end": 515,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_4@1",
            "content": "In comparison, discrete augmentation obtains positive instances with word deletion or reordering Meng et al., 2021), which may misinterpret the meaning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_4",
            "start": 517,
            "end": 668,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_4@2",
            "content": "The continuous method treats embeddings of the same original sentence as positive examples and augments sentences with the different encoding functions (Carlsson et al., 2021;Gao et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_4",
            "start": 670,
            "end": 862,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_5@0",
            "content": "learning works on learning representations such that similar examples stay close whereas dissimilar ones are far apart, and thus is suitable for sentence embeddings due to its natural availability of similar examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_5",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_5@1",
            "content": "Incorporating contrastive learning in sentence embeddings improves the efficiency of semantic information learning in an unsupervised manner and has been shown to be effective on a variety of tasks (Reimers and Gurevych, 2019;Gao et al., 2021;Zhang et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_5",
            "start": 218,
            "end": 480,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_6@0",
            "content": "In contrastive learning for sentence embeddings, a key challenge is constructing positive instances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_6",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_6@1",
            "content": "Both discrete and continuous augmentation methods have been studied recently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_6",
            "start": 101,
            "end": 177,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_6@2",
            "content": "Methods in Wu et al. (2018); Meng et al. (2021) perform discrete operations directly on the original sentences, such as word deletion and sentence shuffling, to get positive samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_6",
            "start": 179,
            "end": 360,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_6@3",
            "content": "However, these methods may lead to unacceptable semantic distortions or even complete misinterpretations of the original statement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_6",
            "start": 362,
            "end": 492,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_6@4",
            "content": "In contrast, the SimCSE method (Gao et al., 2021) obtains two different embeddings in the continuous embedding space as a positive pair for one sentence through different dropout masks (Srivastava et al., 2014) in the neural network for representation learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_6",
            "start": 494,
            "end": 754,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_6@5",
            "content": "Nonetheless, this method overly relies on superficial features existing in the dataset like sentence lengths and syntactic structures and may pay less reflection on meaningful semantic information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_6",
            "start": 756,
            "end": 952,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_6@6",
            "content": "As an illustrative example, the sentencepair in Fig. 1 \"A caterpillar was caught by me.\" and \"I caught a caterpillar.\" appear to organize differently in expression but convey exactly the same semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_6",
            "start": 954,
            "end": 1155,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_7@0",
            "content": "To overcome these drawbacks, in this paper, we propose a semantic-aware contrastive learning framework for sentence embeddings, termed Pseudo-Token BERT (PT-BERT), that is able to capture the pseudo-token space (i.e., latent semantic space) representation while ignoring effects of superficial features like sentence lengths and syntactic structures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_7",
            "start": 0,
            "end": 349,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_7@1",
            "content": "Inspired by previous works on prompt learning and sentence selection (Li and Liang, 2021;Liu et al., 2021;Humeau et al., 2020), which create a pseudo-sequence and have it serve the downstream tasks, we present PT-BERT to train pseudo token representations and then to map sentences into pseudo token spaces based on an attention mechanism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_7",
            "start": 351,
            "end": 689,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_8@0",
            "content": "In particular, we train additional 128 pseudo token embeddings, together with sentence embeddings extracted from the BERT model (i.e., gradient-encoder), and then use the attention mechanism (Vaswani et al., 2017) to map the sentence embedding to the pseudo token space (i.e., semantic space).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_8",
            "start": 0,
            "end": 292,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_8@1",
            "content": "We use another BERT model (i.e., momentum-encoder ) to encode the original sentence, adopt a similar attention mechanism with the pseudo token embeddings, and finally output a continuously augmented version of the sentence embedding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_8",
            "start": 294,
            "end": 526,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_8@2",
            "content": "We treat the representations of the original sentence encoded by the gradientencoder and the momentum-encoder as a positive pair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_8",
            "start": 528,
            "end": 656,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_8@3",
            "content": "In addition, the momentum-encoder also generates negative examples, dynamically maintains a queue to store these negative examples, and updates them over time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_8",
            "start": 658,
            "end": 816,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_8@4",
            "content": "By projecting all sentences onto the same pseudo sentence, the model greatly reduces the dependence on sentence length and syntax when making judgments and makes the model more focused on the semantic level information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_8",
            "start": 818,
            "end": 1036,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_9@0",
            "content": "In our experiments, we compare our results with the previous state-of-the-art work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_9",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_9@1",
            "content": "We train PT-BERT on 10 6 randomly sampled sentences from English Wikipedia and evaluate on seven standard semantic textual similarity (STS) tasks (Agirre et al., 2012(Agirre et al., , 2013(Agirre et al., , 2014(Agirre et al., , 2015(Agirre et al., , 2016 (Marelli et al., 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_9",
            "start": 84,
            "end": 361,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_9@2",
            "content": "Besides, we also compare our approach with a framework based on an advanced discrete augmentation we proposed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_9",
            "start": 363,
            "end": 472,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_9@3",
            "content": "We obtain a new stateof-the-art on standard semantic textual similarity tasks with our PT-BERT, which achieves 77.74% of Spearman's correlation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_9",
            "start": 474,
            "end": 617,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_9@4",
            "content": "To show the effectiveness of pseudo tokens, we calculate the align-loss and uniformity loss (Wang and Isola, 2020) and verify our approach on a sub-dataset with hard examples sampled from STS-(2012STS-( -2016.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_9",
            "start": 619,
            "end": 827,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_9@5",
            "content": "We have released our source code 1 to facilitate future work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_9",
            "start": 829,
            "end": 889,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_10@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_10",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_11@0",
            "content": "In this section, we discuss related studies with repect to the contrastive learning framework and sentence embedding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_11",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_12@0",
            "content": "Contrastive Learning for Sentence Embedding",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_12",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_13@0",
            "content": "Contrastive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_13",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_13@1",
            "content": "Contrastive learning (Hadsell et al., 2006) has been used with much success in both natural language processing and computer vision (Yang et al., 2019;Klein and Nabi, 2020;Gao et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_13",
            "start": 22,
            "end": 211,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_13@2",
            "content": "In contrast to generative learning, contrastive learning requires learning to distinguish and match data at the abstract semantic level of the feature space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_13",
            "start": 213,
            "end": 369,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_13@3",
            "content": "It focuses on learning common features between similar examples and distinguishing differences between non-similar examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_13",
            "start": 371,
            "end": 494,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_13@4",
            "content": "In order to compare the instances with more negative examples and less computation, memory bank (Wu et al., 2018) is proposed to enhance the performance under the contrastive learning framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_13",
            "start": 496,
            "end": 689,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_13@5",
            "content": "While with a large capacity to store more samples, the memory bank is not consistent enough, which could not update the negative examples during comparison.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_13",
            "start": 691,
            "end": 846,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_13@6",
            "content": "Momentum-Contrast (MoCo) uses a queue to maintain the dictionary of samples which allows the model to compare the query with more keys for each step and ensure the consistency of the framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_13",
            "start": 848,
            "end": 1040,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_13@7",
            "content": "It updates the parameter of the dictionary in a momentum way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_13",
            "start": 1042,
            "end": 1102,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_14@0",
            "content": "Discrete and continuous augmentation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_14",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_14@1",
            "content": "By equipping discrete augmentation that modifies sentences directly on token level with contrastive learning, significant success has been achieved in obtaining sentence embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_14",
            "start": 38,
            "end": 218,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_14@2",
            "content": "Such methods include word omission (Yang et al., 2019), entity replacement (Xiong et al., 2020), trigger words (Klein and Nabi, 2020) and traditional augmentations such as deletion, reorder and substitution Meng et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_14",
            "start": 220,
            "end": 445,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_14@3",
            "content": "Examples with diverse expressions can be learned during training, making the model more robust to expressions of different sentence lengths and styles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_14",
            "start": 447,
            "end": 597,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_14@4",
            "content": "However, these approaches are limited because there are huge difficulties in augmenting sentences precisely since a few changes can make the meaning completely different or even opposite.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_14",
            "start": 599,
            "end": 785,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_15@0",
            "content": "Researchers have also explored the possibility of building sentences continuously, which instead applies operation in embedding space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_15",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_15@1",
            "content": "CT-BERT (Carlsson et al., 2021) encodes the same sentence with two different encoders.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_15",
            "start": 135,
            "end": 220,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_15@2",
            "content": "Unsup-SimCSE (Gao et al., 2021) compares the representations of the same sentence with different dropout masks among the mini-batch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_15",
            "start": 222,
            "end": 353,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_15@3",
            "content": "These approaches continuously augment sentences while retaining the original meaning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_15",
            "start": 355,
            "end": 439,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_15@4",
            "content": "However, positive pairs seen by SimCSE always have the same length and structure, whereas negative samples are likely to act oppositely.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_15",
            "start": 441,
            "end": 576,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_15@5",
            "content": "As a result, sentence length and structure are highly correlated to the similarity score of examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_15",
            "start": 578,
            "end": 678,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_15@6",
            "content": "During training, the model has never seen positive samples with diverse expressions, so that in real test scenarios, the model would be more inclined to classify the synonymous pairs with different expressions as negatives, and those sentences with the same length and structures are more likely to be grouped as positive pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_15",
            "start": 680,
            "end": 1007,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_15@7",
            "content": "This may cause a biased encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_15",
            "start": 1009,
            "end": 1040,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_16@0",
            "content": "Pseudo Tokens",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_16",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_17@0",
            "content": "In the domain of prompt learning (Liu et al., 2021;Jiang et al., 2020;Li and Liang, 2021;Gao et al., 2020), the way to create prompt can be divided into two types, namely discrete and continuous ways.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_17",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_17@1",
            "content": "Discrete methods usually search the natural language template as the prompt (Davison et al., Petroni et al., 2019), while the continuous way always directly works on the embedding space with \"pseudo tokens\" (Liu et al., 2021;Li and Liang, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_17",
            "start": 201,
            "end": 445,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_17@2",
            "content": "In retrieval and dialogue tasks, the current approach adopts \"pseudo tokens\", namely \"poly codes\" (Humeau et al., 2020), to jointly encode the query and response precisely and ensure the inference time when compared with the Cross-Encoders and Bi-Encoders (Wolf et al., 2019;Mazar\u00e9 et al., 2018;Dinan et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_17",
            "start": 447,
            "end": 761,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_17@3",
            "content": "The essence of these methods is to create a pseudo-sequence and have it serve the downstream tasks without the need for humans to understand the exact meaning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_17",
            "start": 763,
            "end": 921,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_17@4",
            "content": "The parameters of these pseudo tokens are independent of the natural language embeddings, and can be tuned based on a specific downstream task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_17",
            "start": 923,
            "end": 1065,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_17@5",
            "content": "In the following sections, we will show the idea to weaken the model's consideration of sentence length and structures by introducing additional pseudo token embeddings on top of the BERT encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_17",
            "start": 1067,
            "end": 1262,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_18@0",
            "content": "Methods",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_18",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_19@0",
            "content": "In this section, we introduce PT-BERT, which provides novel contributions on combining advantages of both discrete and continuous augmentations to advance the state-of-art of sentence embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_19",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_19@1",
            "content": "We first present the setup of problems with a thorough analysis on the bias introduced by the textual similarity theoretically and experimentally.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_19",
            "start": 196,
            "end": 341,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_19@2",
            "content": "Then we show the details of Pseudo-Token representation and our model's architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_19",
            "start": 343,
            "end": 427,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_20@0",
            "content": "Preliminary",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_20",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_21@0",
            "content": "Consider a sentence s, we say that the augmentation is continuous if s is augmented by different encoding functions, f (\u2022) and f \u2032 (\u2022).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_21",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_21@1",
            "content": "Sentence embeddings h = f (s) and h \u2032 = f \u2032 (s) are obtained by these two functions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_21",
            "start": 136,
            "end": 219,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_21@2",
            "content": "With a slight change of the encoding function (e.g., encoders with different dropout masks), h \u2032 can be seen as a more precisely augmented version of h compared with the discrete augmentation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_21",
            "start": 221,
            "end": 412,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_21@3",
            "content": "Semantic information of h \u2032 should be the same as h. Therefore, h and h \u2032 are a pair of positive examples and we could randomly sample a sentence to construct negative example pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_21",
            "start": 414,
            "end": 595,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_21@4",
            "content": "Previous state-of-the-art models (Gao et al., 2021) adopt the continuous strategy that augments sentences with dropout (Srivastava et al., 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_21",
            "start": 597,
            "end": 741,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_21@5",
            "content": "It is obvious that all the positive examples in SimCSE have the same length and structure while negative examples act oppositely.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_21",
            "start": 743,
            "end": 871,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_21@6",
            "content": "In this way, SimCSE will inevitably take these two factors as hints during test.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_21",
            "start": 873,
            "end": 952,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_21@7",
            "content": "To further verify this conjecture, we sort out the positive pairs with a length difference of more than five words and negative pairs of less than two words from STS-(2012STS-( -2016.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_21",
            "start": 954,
            "end": 1136,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_22@0",
            "content": "Table 1 shows that the performance of SimCSE plummets on this dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_22",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_22@1",
            "content": "Besides, we also find that SimCSE truncates all training corpus into 32 tokens, which shortens the discrepancy of the sentence's length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_22",
            "start": 71,
            "end": 206,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_22@2",
            "content": "After we scale the max length that SimCSE could accept from 32 to 64 and 128, the performance degrades significantly during the test even though the model is supposed to learn more from the complete version of sentences(See Table 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_22",
            "start": 208,
            "end": 440,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_22@3",
            "content": "The reason for this result may lie in the fact that, without truncation, all positive pairs still have the same length, whereas the difference in length between the negative and positive ones is enlarged.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_22",
            "start": 442,
            "end": 645,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_22@4",
            "content": "Therefore, the encoder will rely more on sentence length and make the wrong decision.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_22",
            "start": 647,
            "end": 731,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_23@0",
            "content": "Pseudo-Token BERT",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_23",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_24@0",
            "content": "We realize it is vital to train an unbiased encoder that captures the semantics and also would not introduce intermediate errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_24",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_24@1",
            "content": "This motivates us to propose the PT-BERT, as evidence shows that the encoder may fail to make predictions when trained on a biased dataset with same-length positive pairs, by learning the spurious correlations that work only well on the training dataset (Arjovsky et al., 2019;Nam et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_24",
            "start": 130,
            "end": 424,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_25@0",
            "content": "Pseudo-Token representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_25",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_25@1",
            "content": "The idea of PT-BERT is to reduce the model's excessive dependence on textual similarity when making predic-tions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_25",
            "start": 30,
            "end": 142,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_25@2",
            "content": "Discrete augmentation achieves this goal by providing both positive and negative examples with diverse expressions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_25",
            "start": 144,
            "end": 258,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_25@3",
            "content": "Therefore the model does not jump to conclusions based on sentence length and syntactic structure during the test.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_25",
            "start": 260,
            "end": 373,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_26@0",
            "content": "Note that we achieve this same purpose in a seemingly opposite way: mapping the representations of both positive and negative examples to a pseudo sentence with the same length and structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_26",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_26@1",
            "content": "We take an additional embedding layer outside the BERT encoder to represent a pseudo sentence {0, 1, ..., m} with fixed length m and syntax.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_26",
            "start": 192,
            "end": 331,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_26@2",
            "content": "This embedding layer is fully independent of the BERT encoder, including the parameters and corresponding vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_26",
            "start": 333,
            "end": 449,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_26@3",
            "content": "Random initialization is applied to this layer, and each parameter will be updated during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_26",
            "start": 451,
            "end": 549,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_26@4",
            "content": "The size of this layer depends on the vocabulary of pseudo tokens(length of pseudo sentences).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_26",
            "start": 551,
            "end": 644,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_26@5",
            "content": "Besides, adopting the attention mechanism (Vaswani et al., 2017;Bahdanau et al., 2015;Gehring et al., 2017), we take the pseudo sentence embeddings as the query states of cross attention while key and value states are the sentence embeddings obtained from the BERT encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_26",
            "start": 646,
            "end": 918,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_26@6",
            "content": "This allows the pseudo sentence to attend to the core part and the redundant part of original sentence while keeping the fixed length and structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_26",
            "start": 920,
            "end": 1067,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_27@0",
            "content": "Fig. 2 illustrates the framework of PT-BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_27",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_27@1",
            "content": "Denoting the pseudo sentence embedding as P and the sentence embedding encoded by BERT as Y, we obtain the weighted pseudo sentence embedding of each sentence by mapping the sentence embedding to the pseudo tokens with attention:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_27",
            "start": 45,
            "end": 273,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_28@0",
            "content": "Z \u2032 i = Attention(PW Q , Y i W K , Y i W V ) (1) Attention(Q, K, V) = softmax( QK T \u221a d k )V,(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_28",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_29@0",
            "content": "where d k is the dimension of the model, W Q , W K , W V are the learnable parameters with R d k \u00d7d k , i denotes the i-th sentence in the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_29",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_29@1",
            "content": "Then we obtain the final embedding h i with the same attention layer by mapping pseudo sentences back to original sentence embeddings:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_29",
            "start": 148,
            "end": 281,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_30@0",
            "content": "h i = Attention(Y i W Q , Z \u2032 i W K , Z \u2032 i W V ). (3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_30",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_31@0",
            "content": "Finally, we compare the cosine similarities between the obtained embeddings of h and h \u2032 using Eq. 4 , where h \u2032 are the samples encoded by the momentum-encoder and stored in a queue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_31",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_32@0",
            "content": "Model architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_32",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_32@1",
            "content": "Instead of inputting the same sentence twice to the same encoder, we follow the architecture proposed in Momentum-Contrast (MoCo) such that PT-BERT can efficiently learn from more negative examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_32",
            "start": 20,
            "end": 217,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_32@2",
            "content": "Samples in PT-BERT are encoded into vectors with two encoders: gradient-update encoder (the upper encoder in Fig. 2) and momentum-update encoder (the momentum encoder in Fig. 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_32",
            "start": 219,
            "end": 396,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_32@3",
            "content": "We dynamically maintain a queue to store the sentence representations from momentum-update encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_32",
            "start": 398,
            "end": 496,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_33@0",
            "content": "This mechanism allows us to store as much negative samples as possible without re-computation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_33",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_33@1",
            "content": "Once the queue is full, we replace the \"oldest\" negative sample with a \"fresh\" one encoded by the momentum-encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_33",
            "start": 95,
            "end": 209,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_34@0",
            "content": "Similar to the works based on continuous augmentation, at the very beginning of the framework, PT-BERT takes input sentence s and obtains h i and h \u2032 i with two different encoder functions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_34",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_34@1",
            "content": "We measure the loss function with:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_34",
            "start": 190,
            "end": 223,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_35@0",
            "content": "\u2113 i = \u2212 log e sim(h i ,h \u2032 i )/\u03c4 M j=1 e sim(h i ,h j \u2032 )/\u03c4 ,(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_35",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_36@0",
            "content": "where h i denotes the representations extracted from the gradient-update encoder, h \u2032 i represents the sentence embedding in the queue, and M is the queue size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_36",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_36@1",
            "content": "Our gradient-update and momentumupdate encoder are based on the pre-trained language model with the same structure and dimensions as BERT-base-uncased (Devlin et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_36",
            "start": 161,
            "end": 333,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_36@2",
            "content": "The momentum encoder will update its parameters similar to MoCo:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_36",
            "start": 335,
            "end": 398,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_37@0",
            "content": "\u03b8 k \u2190 \u03bb\u03b8 k + (1 \u2212 \u03bb)\u03b8 q , (5",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_37",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_38@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_38",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_39@0",
            "content": "where \u03b8 k is the parameter of the momentumcontrast encoder that maintains the dictionary, \u03b8 q is the query encoder that updates the parameters with gradients, and \u03bb is a hyperparameter used to control the updating process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_39",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_40@0",
            "content": "Relationship with prompt learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_40",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_40@1",
            "content": "Rather than directly perform soft prompting in the embedding space (Li and Liang, 2021;Qin and Eisner, 2021;Liu et al., 2021) embedding precisely without adding extra computation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_40",
            "start": 35,
            "end": 213,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_40@2",
            "content": "In some tasks, fixed-LM tuning (Li and Liang, 2021) in soft prompting becomes competitive only when the language models been scaled to big enough (Lester et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_40",
            "start": 215,
            "end": 382,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_40@3",
            "content": "While the prompt+LM (Ben-David et al., 2021;Liu et al., 2021) tuning adds more burdens for both the period of training and inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_40",
            "start": 384,
            "end": 516,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_40@4",
            "content": "Both prompt+LM and fixed-LM prompt tuning require storing separate copies of soft prompts for different tasks, while our approach only saves the trained BERT model, which draws on some ideas in prompt learning and makes our considerations in computational and memory efficiency and generality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_40",
            "start": 518,
            "end": 810,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_41@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_41",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_42@0",
            "content": "In this section, we perform the standard semantic textual similarity (STS) (Agirre et al., 2012(Agirre et al., , 2013(Agirre et al., , 2014(Agirre et al., , 2015(Agirre et al., , 2016 tasks to test our model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_42",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_42@1",
            "content": "For all tasks, we measure the Spearman's correlation to compare our performance with the previous stateof-the-art SimCSE (Gao et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_42",
            "start": 209,
            "end": 348,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_42@2",
            "content": "In the following, we will describe the training procedure in detail.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_42",
            "start": 350,
            "end": 417,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_43@0",
            "content": "Training Data and Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_43",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_44@0",
            "content": "Datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_44",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_44@1",
            "content": "Following SimCSE, We train our model on 1-million sentences randomly sampled from English Wikipedia, and evaluate the model every 125 steps to find the best checkpoints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_44",
            "start": 10,
            "end": 178,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_44@2",
            "content": "Note that we do not fine-tune our model on any dataset, which indicates that our method is completely unsupervised.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_44",
            "start": 180,
            "end": 294,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_45@0",
            "content": "Hardware and schedule.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_45",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_45@1",
            "content": "We train our model on the machine with one NVIDIA V100s GPU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_45",
            "start": 23,
            "end": 82,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_45@2",
            "content": "Following the settings of SimCSE (Gao et al., 2021), it takes 50 minutes to run an epoch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_45",
            "start": 84,
            "end": 172,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_46@0",
            "content": "Implementations",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_46",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_47@0",
            "content": "We implement PT-BERT based on Huggingface transformers (Wolf et al., 2020) and initialize it with the released BERT base (Devlin et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_47",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_47@1",
            "content": "We initialize a new embedding for pseudo tokens with 128\u00d7768.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_47",
            "start": 144,
            "end": 204,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_47@2",
            "content": "During training, we create a pseudo sentence {0, 1, 2, ..., 127} for every input and map the original sentence to this pseudo sentence by attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_47",
            "start": 206,
            "end": 353,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_47@3",
            "content": "With batches of 64 sentences and an additional dynamically maintained queue of 256 sentences, each sentence has one positive sample and 255 negative samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_47",
            "start": 355,
            "end": 511,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_47@4",
            "content": "Adam (Kingma and Ba, 2014) optimizer is used to update the model parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_47",
            "start": 513,
            "end": 588,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_47@5",
            "content": "We also take the original dropout strategy of BERT with rate p = 0.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_47",
            "start": 590,
            "end": 658,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_47@6",
            "content": "We set the momentum for the momentum-encoder with \u03bb = 0.885.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_47",
            "start": 660,
            "end": 719,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_48@0",
            "content": "Evaluation Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_48",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_49@0",
            "content": "We evaluate the fine-tuned BERT encoder on STS-B development sets every 125 steps to select the best checkpoints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_49",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_49@1",
            "content": "We report all the checkpoints based on the evaluation results reported in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_49",
            "start": 114,
            "end": 195,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_49@2",
            "content": "The training process is fully unsupervised since no training corpus from STS is used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_49",
            "start": 197,
            "end": 281,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_49@3",
            "content": "During the evaluation, we also calculate the trends of alignment-loss and uniformity-loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_49",
            "start": 283,
            "end": 372,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_49@4",
            "content": "Losses were compared with SimCSE (Gao et al., 2021) under the same experimental settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_49",
            "start": 374,
            "end": 462,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_49@5",
            "content": "After training and evaluation, we test models on 7 STS tasks: STS 2012-2016 (Agirre et al., 2012(Agirre et al., , 2013(Agirre et al., , 2014(Agirre et al., , 2015(Agirre et al., , 2016, STS Benchmark (Cer et al., 2017) and SICK-Relatedness (Marelli et al., 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_49",
            "start": 464,
            "end": 726,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_49@6",
            "content": "We report the result of Spearman's correlation for all the experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_49",
            "start": 728,
            "end": 798,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_50@0",
            "content": "Main Results and Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_50",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_51@0",
            "content": "We first compare PT-BERT with our baseline: MoCo framework + BERT encoder (MoCo-BERT).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_51",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_52@0",
            "content": "MoCo-BERT could be seen as a version of PT-BERT without pseudo token embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_52",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_52@1",
            "content": "Then we apply traditional discrete augmentations such as reorder, duplication, and deletion on this framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_52",
            "start": 81,
            "end": 190,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_52@2",
            "content": "We also compare our work with CLEAR that substitutes and deletes the token spans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_52",
            "start": 192,
            "end": 272,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_52@3",
            "content": "Besides, we argue that the performance of these methods is too weak.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_52",
            "start": 274,
            "end": 341,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_52@4",
            "content": "We additionally propose an advanced discrete augmentation approach that produces positive examples with the guidance of Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002;Palmer et al., 2010) positive examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_52",
            "start": 343,
            "end": 557,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_52@5",
            "content": "In addition to the work based on discrete approaches, we also compare with Sim-CSE (Gao et al., 2021) which continuously augment sentences with dropout.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_52",
            "start": 559,
            "end": 710,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_52@6",
            "content": "In Table 3, PT-BERT with 128 pseudo tokens further pushed the state-ofthe-art results to 77.74% and significantly outperformed SimCSE over six datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_52",
            "start": 712,
            "end": 863,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_53@0",
            "content": "In Fig 3, we observe that PT-BERT also achieves better alignment and uniformity against SimCSE, which indicates that pseudo tokens really help the learning of sentence representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_53",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_53@1",
            "content": "In detail, alignment and uniformity are proposed by (Wang and Isola, 2020) to evaluate the quality of representations in contrastive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_53",
            "start": 185,
            "end": 326,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_53@2",
            "content": "The calculation of these two metrics are shown in the following formulas: L unif ormity = log",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_53",
            "start": 328,
            "end": 420,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_54@0",
            "content": "L alignment = E (x,x + )\u223cppos ||f (x) \u2212 f (x + )|| 2 , (6) Method STS12 STS13 STS14 STS15 STS16 STS-B SICK-R Avg.(",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_54",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_55@0",
            "content": "E (x,y)\u223cp data e \u22122||f (x)\u2212f (y)|| 2 ,(7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_55",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_56@0",
            "content": "where (x, x + ) is the positive pair, (x, y) is the pair consisting of any two different sentences in the whole sentence set, f (x) is the normalized representation of x. We employ the final embedding h to calculate these scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_56",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_57@0",
            "content": "According to the above formulas, lower alignment loss means a shorter distance between the positive samples, and low uniformity loss implies the diversity of embeddings of all sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_57",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_57@1",
            "content": "Both are our expectations for the representations based on contrastive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_57",
            "start": 187,
            "end": 266,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_57@2",
            "content": "To evaluate our model's performance on alignment and uniformity, we compare it with SimCSE on the STS-benchmark dataset (Cer et al., 2017), and the result is shown in Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_57",
            "start": 268,
            "end": 443,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_57@3",
            "content": "The result demonstrates that PT-BERT outperforms SimCSE on these two metrics: our model has a lower alignment and uniformity than SimCSE in almost all the training steps, which indicates that the representations produced by our model are more in line with the goal of the contrastive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_57",
            "start": 445,
            "end": 737,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_58@0",
            "content": "Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_58",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_59@0",
            "content": "Ablation Studies",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_59",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_60@0",
            "content": "In this section, we first investigate the impact of different sizes of pseudo token embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_60",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_60@1",
            "content": "Then we would like to report the performance difference caused by queue size under the MoCo framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_60",
            "start": 96,
            "end": 197,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_61@0",
            "content": "Pseudo Sentence Length Different lengths of pseudo tokens can affect the ability of the model to express the sentence representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_61",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_61@1",
            "content": "By mapping the original sentences to various lengths of pseudo tokens, the performance of PT-BERT could be different.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_61",
            "start": 135,
            "end": 251,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_61@2",
            "content": "In this section, we keep all the parts except the pseudo tokens and their embeddings unchanged.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_61",
            "start": 253,
            "end": 347,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_61@3",
            "content": "We scale the pseudo sequence length from 64 to 360.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_61",
            "start": 349,
            "end": 399,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_61@4",
            "content": "Table 5(a) shows a comparison between different lengths of pseudo sequence in PT-BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_61",
            "start": 401,
            "end": 486,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_61@5",
            "content": "We find that during training, PT-BERT performs better when attending to pseudo sequences with 128 tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_61",
            "start": 488,
            "end": 592,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_61@6",
            "content": "Too few pseudo tokens do not fully explain the semantics of the original sentence, while too many pseudo tokens increase the number of parameters and over-express the sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_61",
            "start": 594,
            "end": 769,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_62@0",
            "content": "Queue Size The introduction of more negative samples would make the model's training more reliable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_62",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_62@1",
            "content": "By training with different queue sizes, we report the result of PT-BERT with different performances due to the number of negative samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_62",
            "start": 100,
            "end": 237,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_62@2",
            "content": "In Table 5(b), queue size q = 4 performs best.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_62",
            "start": 239,
            "end": 284,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_62@3",
            "content": "However, the difference in performance between the three sets of experiments is not large, suggesting that the model can learn well as long as it can see enough negative samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_62",
            "start": 286,
            "end": 463,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_63@0",
            "content": "Exploration on Hard Examples with Different Length",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_63",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_64@0",
            "content": "To prove the effectiveness of PT-BERT that could weaken the hints caused by textual similarity, we further test PT-BERT on the sub-dataset introduced in Sec. 3.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_64",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_64@1",
            "content": "We sorted out the positive pairs with a length difference of more than five words and negative pairs of less than two words from STS-(2012STS-( -2016.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_64",
            "start": 163,
            "end": 312,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_64@2",
            "content": "PT-BERT significantly outperforms SimCSE with 3.36% Spearman's correlation, indicating that PT-BERT could handle these hard examples better than SimCSE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_64",
            "start": 314,
            "end": 465,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_64@3",
            "content": "This further proves that PT-BERT could debias the spurious correlation introduced by sentence length and syntax, and focus more on the semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_64",
            "start": 467,
            "end": 611,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_65@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_65",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_66@0",
            "content": "In this paper, we propose a semantic-aware contrastive learning framework for sentence embeddings, termed PT-BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_66",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_66@1",
            "content": "Our proposed PT-BERT approach is able to weaken textual similarity information, such as sentence length and syntactic structures, by mapping the original sentence to a fixed pseudo sentence embedding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_66",
            "start": 115,
            "end": 314,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_66@2",
            "content": "We provide analysis of these factors on methods based on continuous and discrete augmentation, showing that PT-BERT augments sentences more accurately than discrete methods while considering more semantics instead of textual similarity than continuous approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_66",
            "start": 316,
            "end": 578,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_66@3",
            "content": "Lower uniformity loss and alignment loss prove the effectiveness of PT-BERT and further experiments also show that PT-BERT could handle hard examples better than existing approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_66",
            "start": 580,
            "end": 761,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_67@0",
            "content": "Providing a new perspective to the continuous data augmentation in sentence embeddings, we believe our proposed PT-BERT has great potential to be applied in broader downstream applications, such as text classification, text clustering, and sentiment analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_67",
            "start": 0,
            "end": 258,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_68@0",
            "content": "Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, I\u00f1igo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, Janyce Wiebe, and pilot on interpretability, 2015, Proceedings of the 9th International Workshop on Semantic Evaluation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_68",
            "start": 0,
            "end": 346,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_69@0",
            "content": "Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, Janyce Wiebe, SemEval-2014 task 10: Multilingual semantic textual similarity, 2014, Proceedings of the 8th International Workshop on Semantic Evaluation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_69",
            "start": 0,
            "end": 325,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_70@0",
            "content": "Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, Janyce Wiebe, SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation, 2016, Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_70",
            "start": 0,
            "end": 342,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_71@0",
            "content": "Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, SemEval-2012 task 6: A pilot on semantic textual similarity, 2012, *SEM 2012: The First Joint Conference on Lexical and Computational Semantics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_71",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_72@0",
            "content": "Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, *SEM 2013 shared task: Semantic textual similarity, 2013, Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_72",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_73@0",
            "content": "UNKNOWN, None, 1907, Invariant risk minimization. ArXiv, abs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_73",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_74@0",
            "content": "UNKNOWN, None, 2015, Neural machine translation by jointly learning to align and translate, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_74",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_75@0",
            "content": "UNKNOWN, None, 2021, PADA: example-based prompt learning for on-the-fly adaptation to unseen domains, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_75",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_76@0",
            "content": "Fredrik Carlsson, Evangelia Amaru Cuba Gyllensten,  Gogoulou, Erik Ylip\u00e4\u00e4 Hellqvist, and Magnus Sahlgren. 2021. Semantic re-tuning with contrastive tension, , ICLR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_76",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_77@0",
            "content": "Daniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-Gazpio, Lucia Specia, SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation, 2017, Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_77",
            "start": 0,
            "end": 303,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_78@0",
            "content": "Daniel Cer, Yinfei Yang, Sheng-Yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St, Noah John, Mario Constant, Steve Guajardo-Cespedes, Chris Yuan, Brian Tar, Ray Strope,  Kurzweil, Universal sentence encoder for English, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_78",
            "start": 0,
            "end": 373,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_79@0",
            "content": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, A simple framework for contrastive learning of visual representations, 2020, Proceedings of the 37th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_79",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_80@0",
            "content": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc Barrault, Antoine Bordes, Supervised learning of universal sentence representations from natural language inference data, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_80",
            "start": 0,
            "end": 266,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_81@0",
            "content": "Joe Davison, Joshua Feldman, Alexander Rush, Commonsense knowledge mining from pretrained models, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_81",
            "start": 0,
            "end": 322,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_82@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_82",
            "start": 0,
            "end": 335,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_83@0",
            "content": "Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston, Wizard of Wikipedia: Knowledge-powered conversational agents, 2019, Proceedings of the International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_83",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_84@0",
            "content": "UNKNOWN, None, 2020, Making pre-trained language models better few-shot learners, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_84",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_85@0",
            "content": "Tianyu Gao, Xingcheng Yao, Danqi Chen, SimCSE: Simple contrastive learning of sentence embeddings, 2021, Empirical Methods in Natural Language Processing, EMNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_85",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_86@0",
            "content": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann Dauphin, Convolutional sequence to sequence learning, 2017, Proceedings of the 34th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_86",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_87@0",
            "content": "Daniel Gildea, Daniel Jurafsky, Automatic labeling of semantic roles, 2002, Comput. Linguist, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_87",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_88@0",
            "content": "R Hadsell, S Chopra, Y Lecun, Dimensionality reduction by learning an invariant mapping, 2006, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_88",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_89@0",
            "content": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Momentum contrast for unsupervised visual representation learning, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_89",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_90@0",
            "content": "Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, Jason Weston, Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_90",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_91@0",
            "content": "Zhengbao Jiang, Frank Xu, How Can We Know What Language Models Know?, 2020-06, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_91",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_92@0",
            "content": "UNKNOWN, None, 2014, Adam: A method for stochastic optimization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_92",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_93@0",
            "content": "Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler, 2015, Proceedings of the 28th International Conference on Neural Information Processing Systems, MIT Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_93",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_94@0",
            "content": "Tassilo Klein, Moin Nabi, Contrastive selfsupervised learning for commonsense reasoning, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_94",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_95@0",
            "content": "Brian Lester, Rami Al-Rfou, Noah Constant, The power of scale for parameter-efficient prompt tuning, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_95",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_96@0",
            "content": "UNKNOWN, None, 2021, Prefix-tuning: Optimizing continuous prompts for generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_96",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_97@0",
            "content": "UNKNOWN, None, , Zhilin Yang, and Jie Tang. 2021. Gpt understands, too, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_97",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_98@0",
            "content": "Lajanugen Logeswaran, Honglak Lee, An efficient framework for learning sentence representations, 2018, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_98",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_99@0",
            "content": "Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, Roberto Zamparelli, A SICK cure for the evaluation of compositional distributional semantic models, 2014-05-26, Proceedings of the Ninth International Conference on Language Resources and Evaluation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_99",
            "start": 0,
            "end": 283,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_100@0",
            "content": "Pierre-Emmanuel Mazar\u00e9, Samuel Humeau, Martin Raison, Antoine Bordes, Training millions of personalized dialogue agents, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_100",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_101@0",
            "content": "UNKNOWN, None, 2021, Cocolm: Correcting and contrasting text sequences for language model pretraining, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_101",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_102@0",
            "content": "Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, Jinwoo Shin, Learning from failure: Training debiased classifier from biased classifier, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_102",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_103@0",
            "content": "Martha Palmer, Daniel Gildea, Nianwen Xue, Semantic role labeling, 2010, Synthesis Lectures on Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_103",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_104@0",
            "content": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Language models as knowledge bases?, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_104",
            "start": 0,
            "end": 371,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_105@0",
            "content": "UNKNOWN, None, 2021, Learning how to ask: Querying lms with mixtures of soft prompts, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_105",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_106@0",
            "content": "Nils Reimers, Iryna Gurevych, Sentence-bert: Sentence embeddings using siamese bert-networks, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_106",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_107@0",
            "content": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, Dropout: A simple way to prevent neural networks from overfitting, 2014, J. Mach. Learn. Res, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_107",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_108@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Illia Kaiser,  Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_108",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_109@0",
            "content": "Tongzhou Wang, Phillip Isola, Understanding contrastive representation learning through alignment and uniformity on the hypersphere, 2020, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_109",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_110@0",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest,  Rush, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_110",
            "start": 0,
            "end": 536,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_111@0",
            "content": "UNKNOWN, None, 2019, Transfertransfo: A transfer learning approach for neural network based conversational agents, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_111",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_112@0",
            "content": "UNKNOWN, None, 2018, Unsupervised feature learning via nonparametric instance-level discrimination, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_112",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_113@0",
            "content": "UNKNOWN, None, 2020, Clear: Contrastive learning for sentence representation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_113",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_114@0",
            "content": "Wenhan Xiong, Jingfei Du, William Wang, Veselin Stoyanov, Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_114",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_115@0",
            "content": "Zonghan Yang, Yong Cheng, Yang Liu, Maosong Sun, Reducing word omission errors in neural machine translation: A contrastive learning approach, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_115",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "345-ARR_v2_116@0",
            "content": "Yan Zhang, Ruidan He, Zuozhu Liu, Hui Kwan, Lidong Lim,  Bing, An unsupervised sentence embedding method by mutual information maximization, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "345-ARR_v2_116",
            "start": 0,
            "end": 292,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "345-ARR_v2_0",
            "tgt_ix": "345-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_0",
            "tgt_ix": "345-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_1",
            "tgt_ix": "345-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_1",
            "tgt_ix": "345-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_0",
            "tgt_ix": "345-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_2",
            "tgt_ix": "345-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_4",
            "tgt_ix": "345-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_5",
            "tgt_ix": "345-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_6",
            "tgt_ix": "345-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_7",
            "tgt_ix": "345-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_8",
            "tgt_ix": "345-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_3",
            "tgt_ix": "345-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_3",
            "tgt_ix": "345-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_3",
            "tgt_ix": "345-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_3",
            "tgt_ix": "345-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_3",
            "tgt_ix": "345-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_3",
            "tgt_ix": "345-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_3",
            "tgt_ix": "345-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_0",
            "tgt_ix": "345-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_9",
            "tgt_ix": "345-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_10",
            "tgt_ix": "345-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_10",
            "tgt_ix": "345-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_10",
            "tgt_ix": "345-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_11",
            "tgt_ix": "345-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_13",
            "tgt_ix": "345-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_14",
            "tgt_ix": "345-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_12",
            "tgt_ix": "345-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_12",
            "tgt_ix": "345-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_12",
            "tgt_ix": "345-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_12",
            "tgt_ix": "345-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_10",
            "tgt_ix": "345-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_15",
            "tgt_ix": "345-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_16",
            "tgt_ix": "345-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_16",
            "tgt_ix": "345-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_0",
            "tgt_ix": "345-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_17",
            "tgt_ix": "345-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_18",
            "tgt_ix": "345-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_18",
            "tgt_ix": "345-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_18",
            "tgt_ix": "345-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_19",
            "tgt_ix": "345-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_21",
            "tgt_ix": "345-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_20",
            "tgt_ix": "345-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_20",
            "tgt_ix": "345-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_20",
            "tgt_ix": "345-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_18",
            "tgt_ix": "345-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_22",
            "tgt_ix": "345-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_24",
            "tgt_ix": "345-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_25",
            "tgt_ix": "345-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_26",
            "tgt_ix": "345-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_27",
            "tgt_ix": "345-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_28",
            "tgt_ix": "345-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_29",
            "tgt_ix": "345-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_30",
            "tgt_ix": "345-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_31",
            "tgt_ix": "345-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_32",
            "tgt_ix": "345-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_33",
            "tgt_ix": "345-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_34",
            "tgt_ix": "345-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_35",
            "tgt_ix": "345-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_36",
            "tgt_ix": "345-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_37",
            "tgt_ix": "345-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_38",
            "tgt_ix": "345-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_39",
            "tgt_ix": "345-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_0",
            "tgt_ix": "345-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_40",
            "tgt_ix": "345-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_41",
            "tgt_ix": "345-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_41",
            "tgt_ix": "345-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_41",
            "tgt_ix": "345-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_42",
            "tgt_ix": "345-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_44",
            "tgt_ix": "345-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_43",
            "tgt_ix": "345-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_43",
            "tgt_ix": "345-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_43",
            "tgt_ix": "345-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_41",
            "tgt_ix": "345-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_45",
            "tgt_ix": "345-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_46",
            "tgt_ix": "345-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_46",
            "tgt_ix": "345-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_41",
            "tgt_ix": "345-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_47",
            "tgt_ix": "345-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_48",
            "tgt_ix": "345-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_48",
            "tgt_ix": "345-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_41",
            "tgt_ix": "345-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_49",
            "tgt_ix": "345-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_51",
            "tgt_ix": "345-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_52",
            "tgt_ix": "345-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_53",
            "tgt_ix": "345-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_54",
            "tgt_ix": "345-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_55",
            "tgt_ix": "345-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_56",
            "tgt_ix": "345-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_50",
            "tgt_ix": "345-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_50",
            "tgt_ix": "345-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_50",
            "tgt_ix": "345-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_50",
            "tgt_ix": "345-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_50",
            "tgt_ix": "345-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_50",
            "tgt_ix": "345-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_50",
            "tgt_ix": "345-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_50",
            "tgt_ix": "345-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_0",
            "tgt_ix": "345-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_57",
            "tgt_ix": "345-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_58",
            "tgt_ix": "345-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_58",
            "tgt_ix": "345-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_60",
            "tgt_ix": "345-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_61",
            "tgt_ix": "345-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_59",
            "tgt_ix": "345-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_59",
            "tgt_ix": "345-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_59",
            "tgt_ix": "345-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_59",
            "tgt_ix": "345-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_58",
            "tgt_ix": "345-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_62",
            "tgt_ix": "345-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_63",
            "tgt_ix": "345-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_63",
            "tgt_ix": "345-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_0",
            "tgt_ix": "345-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_64",
            "tgt_ix": "345-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_66",
            "tgt_ix": "345-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_65",
            "tgt_ix": "345-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_65",
            "tgt_ix": "345-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_65",
            "tgt_ix": "345-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "345-ARR_v2_0",
            "tgt_ix": "345-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_1",
            "tgt_ix": "345-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_2",
            "tgt_ix": "345-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_2",
            "tgt_ix": "345-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_2",
            "tgt_ix": "345-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_2",
            "tgt_ix": "345-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_2",
            "tgt_ix": "345-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_2",
            "tgt_ix": "345-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_2",
            "tgt_ix": "345-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_2",
            "tgt_ix": "345-ARR_v2_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_3",
            "tgt_ix": "345-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_4",
            "tgt_ix": "345-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_4",
            "tgt_ix": "345-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_4",
            "tgt_ix": "345-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_5",
            "tgt_ix": "345-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_5",
            "tgt_ix": "345-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_6",
            "tgt_ix": "345-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_6",
            "tgt_ix": "345-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_6",
            "tgt_ix": "345-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_6",
            "tgt_ix": "345-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_6",
            "tgt_ix": "345-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_6",
            "tgt_ix": "345-ARR_v2_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_6",
            "tgt_ix": "345-ARR_v2_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_7",
            "tgt_ix": "345-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_7",
            "tgt_ix": "345-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_8",
            "tgt_ix": "345-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_8",
            "tgt_ix": "345-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_8",
            "tgt_ix": "345-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_8",
            "tgt_ix": "345-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_8",
            "tgt_ix": "345-ARR_v2_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_9",
            "tgt_ix": "345-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_9",
            "tgt_ix": "345-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_9",
            "tgt_ix": "345-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_9",
            "tgt_ix": "345-ARR_v2_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_9",
            "tgt_ix": "345-ARR_v2_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_9",
            "tgt_ix": "345-ARR_v2_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_10",
            "tgt_ix": "345-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_11",
            "tgt_ix": "345-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_12",
            "tgt_ix": "345-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_13",
            "tgt_ix": "345-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_13",
            "tgt_ix": "345-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_13",
            "tgt_ix": "345-ARR_v2_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_13",
            "tgt_ix": "345-ARR_v2_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_13",
            "tgt_ix": "345-ARR_v2_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_13",
            "tgt_ix": "345-ARR_v2_13@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_13",
            "tgt_ix": "345-ARR_v2_13@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_13",
            "tgt_ix": "345-ARR_v2_13@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_14",
            "tgt_ix": "345-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_14",
            "tgt_ix": "345-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_14",
            "tgt_ix": "345-ARR_v2_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_14",
            "tgt_ix": "345-ARR_v2_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_14",
            "tgt_ix": "345-ARR_v2_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_15",
            "tgt_ix": "345-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_15",
            "tgt_ix": "345-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_15",
            "tgt_ix": "345-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_15",
            "tgt_ix": "345-ARR_v2_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_15",
            "tgt_ix": "345-ARR_v2_15@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_15",
            "tgt_ix": "345-ARR_v2_15@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_15",
            "tgt_ix": "345-ARR_v2_15@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_15",
            "tgt_ix": "345-ARR_v2_15@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_16",
            "tgt_ix": "345-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_17",
            "tgt_ix": "345-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_17",
            "tgt_ix": "345-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_17",
            "tgt_ix": "345-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_17",
            "tgt_ix": "345-ARR_v2_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_17",
            "tgt_ix": "345-ARR_v2_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_17",
            "tgt_ix": "345-ARR_v2_17@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_18",
            "tgt_ix": "345-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_19",
            "tgt_ix": "345-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_19",
            "tgt_ix": "345-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_19",
            "tgt_ix": "345-ARR_v2_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_20",
            "tgt_ix": "345-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_21",
            "tgt_ix": "345-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_21",
            "tgt_ix": "345-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_21",
            "tgt_ix": "345-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_21",
            "tgt_ix": "345-ARR_v2_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_21",
            "tgt_ix": "345-ARR_v2_21@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_21",
            "tgt_ix": "345-ARR_v2_21@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_21",
            "tgt_ix": "345-ARR_v2_21@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_21",
            "tgt_ix": "345-ARR_v2_21@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_22",
            "tgt_ix": "345-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_22",
            "tgt_ix": "345-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_22",
            "tgt_ix": "345-ARR_v2_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_22",
            "tgt_ix": "345-ARR_v2_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_22",
            "tgt_ix": "345-ARR_v2_22@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_23",
            "tgt_ix": "345-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_24",
            "tgt_ix": "345-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_24",
            "tgt_ix": "345-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_25",
            "tgt_ix": "345-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_25",
            "tgt_ix": "345-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_25",
            "tgt_ix": "345-ARR_v2_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_25",
            "tgt_ix": "345-ARR_v2_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_26",
            "tgt_ix": "345-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_26",
            "tgt_ix": "345-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_26",
            "tgt_ix": "345-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_26",
            "tgt_ix": "345-ARR_v2_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_26",
            "tgt_ix": "345-ARR_v2_26@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_26",
            "tgt_ix": "345-ARR_v2_26@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_26",
            "tgt_ix": "345-ARR_v2_26@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_27",
            "tgt_ix": "345-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_27",
            "tgt_ix": "345-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_28",
            "tgt_ix": "345-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_29",
            "tgt_ix": "345-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_29",
            "tgt_ix": "345-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_30",
            "tgt_ix": "345-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_31",
            "tgt_ix": "345-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_32",
            "tgt_ix": "345-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_32",
            "tgt_ix": "345-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_32",
            "tgt_ix": "345-ARR_v2_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_32",
            "tgt_ix": "345-ARR_v2_32@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_33",
            "tgt_ix": "345-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_33",
            "tgt_ix": "345-ARR_v2_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_34",
            "tgt_ix": "345-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_34",
            "tgt_ix": "345-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_35",
            "tgt_ix": "345-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_36",
            "tgt_ix": "345-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_36",
            "tgt_ix": "345-ARR_v2_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_36",
            "tgt_ix": "345-ARR_v2_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_37",
            "tgt_ix": "345-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_38",
            "tgt_ix": "345-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_39",
            "tgt_ix": "345-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_40",
            "tgt_ix": "345-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_40",
            "tgt_ix": "345-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_40",
            "tgt_ix": "345-ARR_v2_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_40",
            "tgt_ix": "345-ARR_v2_40@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_40",
            "tgt_ix": "345-ARR_v2_40@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_41",
            "tgt_ix": "345-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_42",
            "tgt_ix": "345-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_42",
            "tgt_ix": "345-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_42",
            "tgt_ix": "345-ARR_v2_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_43",
            "tgt_ix": "345-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_44",
            "tgt_ix": "345-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_44",
            "tgt_ix": "345-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_44",
            "tgt_ix": "345-ARR_v2_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_45",
            "tgt_ix": "345-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_45",
            "tgt_ix": "345-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_45",
            "tgt_ix": "345-ARR_v2_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_46",
            "tgt_ix": "345-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_47",
            "tgt_ix": "345-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_47",
            "tgt_ix": "345-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_47",
            "tgt_ix": "345-ARR_v2_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_47",
            "tgt_ix": "345-ARR_v2_47@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_47",
            "tgt_ix": "345-ARR_v2_47@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_47",
            "tgt_ix": "345-ARR_v2_47@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_47",
            "tgt_ix": "345-ARR_v2_47@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_48",
            "tgt_ix": "345-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_49",
            "tgt_ix": "345-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_49",
            "tgt_ix": "345-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_49",
            "tgt_ix": "345-ARR_v2_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_49",
            "tgt_ix": "345-ARR_v2_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_49",
            "tgt_ix": "345-ARR_v2_49@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_49",
            "tgt_ix": "345-ARR_v2_49@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_49",
            "tgt_ix": "345-ARR_v2_49@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_50",
            "tgt_ix": "345-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_51",
            "tgt_ix": "345-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_52",
            "tgt_ix": "345-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_52",
            "tgt_ix": "345-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_52",
            "tgt_ix": "345-ARR_v2_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_52",
            "tgt_ix": "345-ARR_v2_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_52",
            "tgt_ix": "345-ARR_v2_52@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_52",
            "tgt_ix": "345-ARR_v2_52@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_52",
            "tgt_ix": "345-ARR_v2_52@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_53",
            "tgt_ix": "345-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_53",
            "tgt_ix": "345-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_53",
            "tgt_ix": "345-ARR_v2_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_54",
            "tgt_ix": "345-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_55",
            "tgt_ix": "345-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_56",
            "tgt_ix": "345-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_57",
            "tgt_ix": "345-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_57",
            "tgt_ix": "345-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_57",
            "tgt_ix": "345-ARR_v2_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_57",
            "tgt_ix": "345-ARR_v2_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_58",
            "tgt_ix": "345-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_59",
            "tgt_ix": "345-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_60",
            "tgt_ix": "345-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_60",
            "tgt_ix": "345-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_61",
            "tgt_ix": "345-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_61",
            "tgt_ix": "345-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_61",
            "tgt_ix": "345-ARR_v2_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_61",
            "tgt_ix": "345-ARR_v2_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_61",
            "tgt_ix": "345-ARR_v2_61@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_61",
            "tgt_ix": "345-ARR_v2_61@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_61",
            "tgt_ix": "345-ARR_v2_61@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_62",
            "tgt_ix": "345-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_62",
            "tgt_ix": "345-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_62",
            "tgt_ix": "345-ARR_v2_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_62",
            "tgt_ix": "345-ARR_v2_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_63",
            "tgt_ix": "345-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_64",
            "tgt_ix": "345-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_64",
            "tgt_ix": "345-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_64",
            "tgt_ix": "345-ARR_v2_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_64",
            "tgt_ix": "345-ARR_v2_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_65",
            "tgt_ix": "345-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_66",
            "tgt_ix": "345-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_66",
            "tgt_ix": "345-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_66",
            "tgt_ix": "345-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_66",
            "tgt_ix": "345-ARR_v2_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_67",
            "tgt_ix": "345-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_68",
            "tgt_ix": "345-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_69",
            "tgt_ix": "345-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_70",
            "tgt_ix": "345-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_71",
            "tgt_ix": "345-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_72",
            "tgt_ix": "345-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_73",
            "tgt_ix": "345-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_74",
            "tgt_ix": "345-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_75",
            "tgt_ix": "345-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_76",
            "tgt_ix": "345-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_77",
            "tgt_ix": "345-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_78",
            "tgt_ix": "345-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_79",
            "tgt_ix": "345-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_80",
            "tgt_ix": "345-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_81",
            "tgt_ix": "345-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_82",
            "tgt_ix": "345-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_83",
            "tgt_ix": "345-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_84",
            "tgt_ix": "345-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_85",
            "tgt_ix": "345-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_86",
            "tgt_ix": "345-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_87",
            "tgt_ix": "345-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_88",
            "tgt_ix": "345-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_89",
            "tgt_ix": "345-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_90",
            "tgt_ix": "345-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_91",
            "tgt_ix": "345-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_92",
            "tgt_ix": "345-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_93",
            "tgt_ix": "345-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_94",
            "tgt_ix": "345-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_95",
            "tgt_ix": "345-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_96",
            "tgt_ix": "345-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_97",
            "tgt_ix": "345-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_98",
            "tgt_ix": "345-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_99",
            "tgt_ix": "345-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_100",
            "tgt_ix": "345-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_101",
            "tgt_ix": "345-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_102",
            "tgt_ix": "345-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_103",
            "tgt_ix": "345-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_104",
            "tgt_ix": "345-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_105",
            "tgt_ix": "345-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_106",
            "tgt_ix": "345-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_107",
            "tgt_ix": "345-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_108",
            "tgt_ix": "345-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_109",
            "tgt_ix": "345-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_110",
            "tgt_ix": "345-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_111",
            "tgt_ix": "345-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_112",
            "tgt_ix": "345-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_113",
            "tgt_ix": "345-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_114",
            "tgt_ix": "345-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_115",
            "tgt_ix": "345-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "345-ARR_v2_116",
            "tgt_ix": "345-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 788,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "345-ARR",
        "version": 2
    }
}