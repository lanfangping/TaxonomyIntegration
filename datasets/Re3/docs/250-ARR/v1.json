{
    "nodes": [
        {
            "ix": "250-ARR_v1_0",
            "content": "RAIL-KD: RAndom Intermediate Layer Mapping for Knowledge Distillation",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_2",
            "content": "Intermediate layer knowledge distillation (KD) can improve the standard KD technique (which only targets the output of teacher and student models) especially over large pre-trained language models. However, intermediate layer distillation suffers from excessive computational burdens and engineering efforts required for setting up a proper layer mapping. To address these problems, we propose a RAndom Intermediate Layer Knowledge Distillation (RAIL-KD) approach in which, intermediate layers from the teacher model are selected randomly to be distilled into the intermediate layers of the student model. This randomized selection enforces that all teacher layers are taken into account in the training process, while reducing the computational cost of intermediate layer distillation. Also, we show that it acts as a regularizer for improving the generalizability of the student model. We perform extensive experiments on GLUE tasks as well as on out-of-domain test sets. We show that our proposed RAIL-KD approach outperforms other state-of-the-art intermediate layer KD methods considerably in both performance and training-time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "250-ARR_v1_4",
            "content": "Pre-trained Language Models (PLMs), such as BERT (Devlin et al., 2019), RoBERTa and XLNet (Yang et al., 2019) have shown remarkable abilities to match and even surpass human performances on many Natural Languages Understanding (NLU) tasks (Rajpurkar et al., 2018;Wang et al., 2018. However, the deployment of these models in real world applications (e.g. edge devices) come with challenges, mainly due to large model size and inference time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_5",
            "content": "In this regard, several model compression techniques such as quantization (Shen et al., 2019;Zafrir et al., 2019), pruning (Guo et al., 2019;Gordon et al., 2020;Michel et al., 2019), optimizing the Transformer architecture (Fan et al., 2019;Wu et al., 2020b;Lu et al., 2020), and knowledge distillation (Sanh et al., 2019a;Jiao et al., 2019;Sun et al., 2020b;Rashid et al., 2021;Kamalloo et al., 2021) have been developed to reduce the model size and latency, while maintaining comparable performance to the original model. KD, which is the main focus of this work, is a neural model compression approach that involves training a small student model with the guidance of a large pre-trained teacher model. In the original KD technique (Bucilu\u01ce et al., 2006;Hinton et al., 2014;Turc et al., 2019), the teacher output predictions are used as soft labels for supervising the training of the student. There has been several attempts in the literature to reduce the teacherstudent performance gap by leveraging data augmentation (Fu et al., 2020;Li et al., 2021;Jiao et al., 2019), adversarial training (Zaharia et al., 2021;Rashid et al., 2020Rashid et al., , 2021, and intermediate layer distillation (ILD) (Wang et al., 2020b,a;Ji et al., 2021;.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_6",
            "content": "When it comes to BERT compression, ILD leads to clear gains in performances (Sanh et al., 2019a;Jiao et al., 2019; due to its ability to enhance the knowledge transfer beyond logits matching. This is done by mapping intermediate layer representations of both models to a common space 1 , and then matching them via regression (Sun et al., 2019) or cosine similarity (Sanh et al., 2019a) losses. One major problem with ILD is the absence of an appropriate strategy to select layers to be matched on both sides, reacting to the skip and search problem . Some solutions in the literature mostly rely on layer combination (Wu et al., 2020a), attention-based layer",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_7",
            "content": "Complexity Limitation PKD (Sun et al., 2019) Extra Hyperparameter O(m) Extensive Search CKD (Wu et al., 2020a) Extra Hyperparameter O(m) Extensive Search ALP-KD projection and contrastive learning (Sun et al., 2020a). While these solutions are all effective to some extent, to the best of our knowledge, there is no work in the literature doing a comprehensive evaluation of these techniques in terms of both efficiency and performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_8",
            "content": "A case in point is that the aforementioned solutions to the layer skip and search problem do not scale to very deep networks. We propose RAIL-KD (RAndom Intermediate Layer KD), a simple yet effective method for intermediate layer mapping which randomly selects k out of n intermediate layers of the teacher at each epoch to be distilled to the corresponding student layers. Since the layer selection is done randomly, all the intermediate layers of the teacher will have a chance to be selected for distillation. Our method adds no computational cost to the training, still outperforming all aforementioned methods on the GLUE benchmark (Wang et al., 2018). Moreover, we observe larger gains when distilling from large teacher models, as well as when student models are evaluated on out-ofdomain datasets. Last, we report the results on 5 random seeds in order to verify the contribution of the random selection process, thus making the comparison fair with previous methods. The main contributions of our paper are as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_9",
            "content": "\u2022 We introduce RAIL-KD, a more efficient and scalable intermediate layer distillation approach. \u2022 To the best of our knowledge, we are the first to perform a comprehensive study of the ILD techniques in terms of both efficiency and performance. \u2022 We consider the distillation of models such as BERT and RoBERTa, and compare different up-to-date distillation techniques on out-ofdomain test sets.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_10",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "250-ARR_v1_11",
            "content": "Recent years, have seen a wide range of methods have witnessed to expand knowledge transfer of transformer-based (Vaswani et al., 2017) NLU models beyond logits matching. DistillBERT (Sanh et al., 2019a) added a cosine similarity loss between teacher and student embeddings layers. Tiny-BERT (Jiao et al., 2019), MobileBERT (Sun et al., 2020b), and MiniLM (Wang et al., 2020b) matched the intermediate layers representations and selfattention distributions of the teacher and the student.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_12",
            "content": "In PKD, Sun et al. (2019) used deterministic mapping strategies to distill a 12-layer BERT teacher to a 6-layer BERT student. PKD-LAST and PKD-SKIP refer to matching layers {1 \u2212 5} of the student with layers {7 \u2212 11} and {2, 4, 6, 8, 10} of the teacher respectively. However, these works ignored the impact of layer selection, as they used a fixed layer-wise mapping. 2 Researchers have found that tuning the layer mapping scheme can significantly improve the performance of ILD techniques (Sun et al., 2019). Nevertheless, finding the optimal mapping can be challenging, which is referred to as the layer skip and search problems by . To address the layer skip problem, CKD (Wu et al., 2020a) is built on top of PKD by partitioning all the intermediate layers of the teacher to the number of student layers. Then, the combined representation of the layers of each partition is distilled into a number of subset corresponding to the number of student layers. However, finding the optimal partitioning scheme requires running exhaustive experiments. Given teacher and student BERT models with n and m layers respectively (where n >> m), it is not trivial to choose the teacher layers that can be incorporated in the distillation process and how we should map them to the student layers (search). ALP-KD overcomes this issue by computing attention weights between each student layer and all the intermediate layers of the teacher. The learned attention weights for each student layer are used to obtain a weighted representation of all teacher layers. Although ALP-KD has shown promising results on 12-layer BERT-based compression, attending to all layers of the teacher adds considerable computational overhead to the training phase. This can become computationally prohibitive when scaling to very large models such as RoBERTa-large or GPT-2 (Radford et al., 2019). Alternatively, CODIR (Sun et al., 2020a) exploited contrastive learning (Tian et al., 2019) to perform intermediate layers matching between the teacher and the student models with no deterministic mapping. Similar to ALP-KD, this approach also requires excessive training time due to the contrastive loss calculation and the use of negative samples from a memory bank.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_13",
            "content": "Table 1 summarizes the main characteristics of the existing state-of-the-art intermediate layer distillation techniques (PKD (Sun et al., 2019), CKD (Wu et al., 2020a), and CoDIR) used for pre-trained language models compared with our proposed RAIL-KD. First, PKD and CKD treat the mapping as an extra hyperparameter that requires extensive experiments in order to find the optimal mapping. Second, ALP-KD and CoDIR (Sun et al., 2020a) use the attention mechanism and contrastive learning respectively to address the issue, but at the expense of extra computational cost.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_14",
            "content": "Our proposed RAIL-KD method does not add any computational cost to the distillation process, while empirically outperforming previous methods. For instance, RAIL-KD is roughly twice faster than CoDIR in a 24 to 6 layer compression. In addition, it does not require extensive experiments to find the optimal mapping scheme. In this work, we position ourselves to works that tackle the skip and search problem 3 . Otherwise said, we don't compare with works like TinyBERT (Jiao et al., 2019) or MiniLM (Wang et al., 2020b), which use extra losses like self-attention distribution matching. However, we expect that these methods, as well as state-of-the-art (Rashid et al., 2021;He et al., 2021) ones can take full advantage of RAIL-KD, since they use a deterministic layer mapping scheme.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_15",
            "content": "RAIL-KD",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "250-ARR_v1_16",
            "content": "The RAIL-KD method is sketched in Figure 1",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_17",
            "content": "= (x 0 , \u2022 \u2022 \u2022 , x L\u22121 )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_18",
            "content": "which is a sequence of L tokens and y its corresponding label. In Figure 1, our Random Selection operator is applied to the intermediate layers of the teacher to randomly select m out of n layers. The intermediate layer representations of the m selected layers of the teacher and the student model corresponding to the X input can be described as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_19",
            "content": "H T X = {H T 1,X , \u2022 \u2022 \u2022 , H T m,X } and H S \u03b8 X = {H S \u03b8 1,X , \u2022 \u2022 \u2022 , H S \u03b8 m,X } respectively, where H T i,X = \u222a L\u22121 k=0 {h T i,x k } \u2208 R L\u00d7d 1 and H S \u03b8 i,X = \u222a L\u22121 k=0 {h S \u03b8 i,x k } \u2208 R L\u00d7d 2 .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_20",
            "content": "Here, d 1 and d 2 indicate the hidden dimension of the layers of the teacher and the student models respectively. To obtain H T i,X and H S j,X , we need to find the individual representation of each token x k at each layer i, which is indicated as h T i,x k and h S i,x k for the teacher and student networks respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_21",
            "content": "At this stage, we need to obtain an aggregated representation of the sequence X at each layer. In this regard, one can either use the <CLS> token representation or use the mean-pooling of the sequence representations of the layer. Since in (Sun et al., 2020a), the mean-pooling representation shows better results, we adopt it to compute the sentence representation of each layer. Meanpooling is a row-wise average over Sun et al., 2020a):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_22",
            "content": "H T i,X , H S \u03b8 i,X to get hT i,X \u2208 R d 1 hS \u03b8 i,X \u2208 R d 2 (",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_23",
            "content": "hT i,X = 1 L L\u22121 k=0 h T i,x k ; hS \u03b8 i,X = 1 L L\u22121 k=0 h S \u03b8 i,x k (1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_24",
            "content": "After obtaining aggregated layer representations for both the student and teacher networks, our RAIL-KD proposal is to randomly select m layer representations from the teacher through training to perform the intermediate layer distillation (ILD). RAIL-KD does ILD in two different forms: using layer-wise distillation (see Fig. 1(a)) or by concatenating layer representations (see Fig. 1(b)) which are described in the following two sub-sections.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_25",
            "content": "Layer-wise RAIL-KD",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "250-ARR_v1_26",
            "content": "In this setting, the representations hT i,X \u2208 R d 1 and hS \u03b8 i,X \u2208 R d 2 are projected into a same-size lowerdimensional space \u0125T i,X , \u0125S \u03b8 i,X \u2208 R u using (d 1 \u00d7 u) and (d 2 \u00d7 u) linear mappings respectively. Assume that the set A = {a \u03ba |a \u03ba \u223c {1, 2, ..., n}, 1 \u2264 \u03ba \u2264 m} contains indices of selected m layers from the teacher, then to calculate the layer-wise loss we have:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_27",
            "content": "L RAIL-KD l = X\u2208X i\u2208A \u03b1 i || \u0125T i,X || \u0125T i,X || 2 \u2212 \u0125S \u03b8 i,X || \u0125S \u03b8 i,X || 2 || 2 2 (2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_28",
            "content": "where X denotes the training set, and \u03b1 i is a hyperparameter to assign a custom weights to the layerwise distillation loss. It is worth mentioning that in our experiments we set \u03b1 i = 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_29",
            "content": "Concatenated RAIL-KD",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "250-ARR_v1_30",
            "content": "In this setting, intermediate layer representations are concatenated and then distilled:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_31",
            "content": "hT X = [ hT i,X ] i\u2208A , hS \u03b8 X = [ hS \u03b8 j,X",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_32",
            "content": "] m j=1 which are further mapped into the same lower-dimensional space \u0125T X , \u0125S \u03b8 X \u2208 R u using (md 1 \u00d7 u) and (md 2 \u00d7 u) linear mappings to calculate the concatenated distillation loss.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_33",
            "content": "L RAIL-KD c = X\u2208X || \u0125T X || \u0125T X || 2 \u2212 \u0125S \u03b8 X || \u0125S \u03b8 X || 2 || 2 2 (3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_34",
            "content": "Any type of loss such as contrastive (Sun et al., 2020a), or mean-square-error (MSE) Sun et al., 2019) can be applied for our RAIL-KD approach.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_35",
            "content": "Training Loss",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "250-ARR_v1_36",
            "content": "The intermediate representation distillation loss L RAIL-KD is combined with the original KD loss L KD , which is used to distill the knowledge from the output logits of the teacher model T to the output logits of the student model S \u03b8 , and the original cross-entropy loss L CE . The total loss function for training the student model is:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_37",
            "content": "L = \u03bb 1 L CE + \u03bb 2 L KD + \u03bb 3 L RAIL-KD l/c(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_38",
            "content": "where \u03bb 1 , \u03bb 2 , and \u03bb 3 are hyper-parameters of our model to minimize the total loss, and 4 Experimental Protocol",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_39",
            "content": "\u03bb 1 + \u03bb 2 + \u03bb 3 = 1.",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_40",
            "content": "Datasets and Evaluation",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "250-ARR_v1_41",
            "content": "We evaluate RAIL-KD on 8 tasks from the GLUE benchmark (Wang et al., 2018): 2 single-sentence (CoLA and SST-2) and 5 sentence-pair (MRPC, RTE, QQP, QNLI, and MNLI) classification tasks, and 1 regression task (STS-B). Following prior works (Sun et al., 2019;Jiao et al., 2019;Sun et al., 2020a), we use the same metrics as the GLUE benchmark for evaluation. Moreover, to further show the generalization capability of our RAIL-KD method on out-of-domain (OOD) across tasks, we use Scitail (Khot et al., 2018), PAWS (Paraphrase Adversaries from Word Scrambling) (Zhang et al., 2019), and IMDb (Internet Movie Database) (Maas et al., 2011) test sets to evaluate the models fine-tuned on MNLI, QQP, and SST-2 tasks respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_42",
            "content": "Implementation Details",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "250-ARR_v1_43",
            "content": "We run extensive experiments on 3 different teachers in order to ensure a fair comparison with a wide range of prior works, and also to show the effectiveness of RAIL-KD. We experiment with the 12-layer BERT-base-uncased (Devlin et al., 2019) model as teacher (BERT 12 ) and the 6-layer DistilBERT (Sanh et al., 2019a) as student (DistillBERT 6 ) to compare with PKD (Sun et al., 2019) and ALP-KD . Also, we consider 24-layer RoBERTa-large and 6-layer Dis-tilRoberta (Sanh et al., 2019b) as the backbone for teacher (RoBERTa 24 ) and student (DistilRoberta 6 ) respectively to compare results when n >> m. Furthermore, we perform evaluation using the 12 layers RoBERTa-base (RoBERTa 12 ) model as a teacher to be able to directly compare our figures with the ones of CoDIR. Hyperparameters selection and other implementation details can be found in Appendix.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_44",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "250-ARR_v1_45",
            "content": "Table 2 shows the performances of models trained on GLUE tasks, and evaluated on their respective DEV and TEST sets for 12-layer to 6-layer distillation. BERT 12 and DistilBERT 6 are used as backbone for the teacher and student models respectively. The baselines are fine-tuned without KD (w/o KD) and with Vanilla KD. Moreover, we directly compare RAIL-KD lc results with PKD and ALP-KD as more competing techniques. First, we observe that the performance gap between ILD methods and vanilla-KD is tight (0.8% and 0.3% on DEV and TEST sets respectively). Moreover, as we expect, ALP-KD performs better on DEV and similar on TEST compared to PKD with 0.2% improvement on the DEV results. Second, results show that RAIL-KD outperforms the best ILD methods by a margin of 0.5% and 0.3% on average on DEV and TEST sets respectively. We notice that, except on RTE TEST, our RAIL-KD variants obtained the highest per-task performances. Third, we observe that RAIL-KD variants perform very similarly, which indicates that our method is effective on concatenated as well as layer-wise distillation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_46",
            "content": "Similar trends are seen on the 24-to 6-layer model compression experiments, which are reported in Table 3. In this experiment, we used Roberta 24 and DistillRoberta 6 as teacher and students models respectively. Overall, RAIL-KD outperforms the best baseline by 1.2% and 0.3% on DEV and TEST sets respectively. Interestingly, the gap on DEV compared with PKD and ALP-KD is larger than the one reported on BERT 12 experiments, and PKD TEST socres are much lower from that of ALP and RAIL-KD. This might be because PKD skips a large number of intermediate layers on RoBERTa 24 , and the computational cost of ALP-KD attention weights over a large number of teacher layers might produce smaller weights on Roberta 24 compared to BERT 12 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_47",
            "content": "Furthermore, we demonstrate the effectiveness of RAIL-KD by directly comparing it with CoDIR (Sun et al., 2020a), the current state-ofthe-art ILD method. It uses the contrastive objective and a memory bank to extract a large number of negative samples for contrastive loss calculations. Table 4 shows GLUE test results of both approaches when distilling RoBERTa 12 to DistillRoberta 6 . CoDIR results are adopted from their paper, and we followed their experimental protocol by not reporting scores on STS-B. On average, RAIL-KD performs on par with CoDIR (+0.2%) and outperforms it on 5 out of 8 datasets, while being almost twice faster as shown in the next section.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_48",
            "content": "Training Speed up",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "250-ARR_v1_49",
            "content": "Table 5 shows the training time speed up against the teacher of different techniques on 8 GLUE tasks. We measured the speed up by calculating the student_train_time/teacher_train_time using RoBERTa 2 4 and DistilRoBERTa 6 as backbone for teacher and student respectively. We used this configuration because CODIR pretrained student models are not available and we can only run CODIR code out-of-the-box.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_50",
            "content": "Our results indicate that random layer mapping not only delivers consistently better results than the deterministic mapping technique such as PKD, but it has less computational overhead during training",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_51",
            "content": "Model Teacher PKD ALP CODIR RAIL l RAIL c Speed-up 1.00\u00d7 1.89\u00d7 1.75\u00d7 1\u00d7 1.89\u00d7 1.96\u00d7",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_52",
            "content": "Table 5: Training time speedup against the teacher for different techniques using the same setting of Table 4.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_53",
            "content": "(twice faster than CODIR), while avoiding extensive search experiments to find an optimal mapping. Furthermore, using attention for layer selection (ALP-KD) or contrastive learning (CoDIR) leads to slightly worse performance result than random selection.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_54",
            "content": "Impact of Random Layer Selection",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "250-ARR_v1_55",
            "content": "To evaluate the impact of random layer selection on the performance of RAIL-KD, we report the standard deviation of the DistilBERT 6 student models (Table 2",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_56",
            "content": "Out-of-Distribution Test",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "250-ARR_v1_57",
            "content": "We further validate the generalization ability of student models by measuring their robustness to in-domain and out-of-domain evaluation. We do so by evaluating models finetuned on MLI, QQP and SST-2 and then evaluated on SciTail, PAWS, and IMDB respectively. These datasets contains counterexamples to biases found in the training data (McCoy et al., 2019;Schuster et al., 2019;Clark et al., 2019). Performances of BERT 12 /Roberta 24 teacher and DistilBERT 6 /DistilRoBERTa 6 student variants are reported in Table 7. Also, we compute the unweighted average score of the three tasks. First, we notice high variability in models rank and some inconsistencies in performances across tasks when compared with in-domain results. This was also reported in prior works on out-of-domain training and evaluation (Clark et al., 2019;Mahabadi et al., 2020;Utama et al., 2020;Sanh et al., 2020). Still, RAIL-KD clearly outperforms all baselines across tasks. Surprisingly, we observe that PKD and ALP-KD perform poorly (on all three tasks) compared to the Vanilla KD baseline. Table 7: Out-of-domain performances of models trained on MNLI, QQP, SST-2 and evaluated on SciTail, PAWS, and IMDB respectively. BERT 12 /Roberta 24 and DistilBERT 6 /DistilRoBERTa 6 are used as backbone for the teacher and students respectively. For each setting, we report the unweighted average score on the 3 tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_58",
            "content": "Interestingly, we observe that RAIL-KD l performs consistently better (1.1% on average) than RAIL-KD c on Roberta 24 compression, while RAIL-KD c performs better (1.1% on average) on BERT 12 . These results suggest that layer-wise distillation approach is more effective than concatenated distillation when we have a large capacity gap (layer number) between the teacher and the student, and vice versa.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_59",
            "content": "Analysis",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "250-ARR_v1_60",
            "content": "We run extensive analysis to better understand why RAIL-KD performs better than the other baselines. We visualize the layer-wise cosine similarity between the intermediate representations of the teacher and the student networks. Figure 2 shows the cosine similarity score between three intermediate layer representations of BERT 12 teacher (i.e. layers 2, 4 and 6) and the first three layer representations of the student for PKD, ALP-KD, RAIL-KD l/c students on 100 samples randomly selected from the SST-2 dataset. Due to space constraints, we only plot the scores for the first three layers of We found that RAIL-KD allows the student to mimic teacher layers similar to PKD and much better than ALP-KD, despite that the mapping scheme varies at each epoch. Moreover, we observe that ALP-KD results in less similarity scores in the upper intermediate layers. PKD gives lower similarity scores in the lower layers while improving in the upper layers. In contrast, our approach gives more stable similarity scores for all layers while getting closer to the teacher representation in the upper layers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_61",
            "content": "We further investigate the attention weights learned by ALP-KD, and find out that they mostly focus on few layers (sparse attention). Figure 3 This is an indicator that ALP-KD overfits to the information driven from last layers. In contrast, the randomness in layer selection of RAIL-KD ensures a uniform focus on teacher layers. This may explain the poor performance of ALP-KD on out-4 Similar trends found on other datasets. of-domain evaluation compared with RAIL-KD.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_62",
            "content": "From Figure 3, we see clearly that ALP-KD mostly prefers the upper layers of the teacher. On the other hand, the deterministic nature of PDK allows it to match better particular layers of the teacher (e.g. bottom ones as shown in Figure 2), but PKD never sees the layers that are skipped by the mapping. Consequently, it is expected that even though PDK can mimic bottom layers well, it is worse overall because it completely ignores some layers of the teacher. Random layer selection allow RAIL-KD to mimic all teacher layers while delivering high performances.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_63",
            "content": "Conclusion and Future Work",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "250-ARR_v1_64",
            "content": "We introduced a novel, simple, and efficient intermediate layer KD approach that outperforms the conventional approaches with performance improvement and efficient training time. RAIL-KD selects random intermediate layers from the teacher which equals to the number of intermediate layers of the student model. The selected intermediate layers are then sorted to distill their representations into the student model. RAIL-KD yields better regularization, which helps performance. Furthermore, our approach shows better performance for larger model distillation with faster training time, which opens up an avenue to investigate our approach for super-large models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "250-ARR_v1_65",
            "content": "Cristian Bucilu\u01ce, Rich Caruana, Alexandru Niculescu-Mizil, Model compression, 2006, Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Cristian Bucilu\u01ce",
                    "Rich Caruana",
                    "Alexandru Niculescu-Mizil"
                ],
                "title": "Model compression",
                "pub_date": "2006",
                "pub_title": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_66",
            "content": "Christopher Clark, Mark Yatskar, Luke Zettlemoyer, Don't take the easy way out: Ensemble based methods for avoiding known dataset biases, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Christopher Clark",
                    "Mark Yatskar",
                    "Luke Zettlemoyer"
                ],
                "title": "Don't take the easy way out: Ensemble based methods for avoiding known dataset biases",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_67",
            "content": "UNKNOWN, None, 2019, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_68",
            "content": "UNKNOWN, None, 2019, Reducing transformer depth on demand with structured dropout, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Reducing transformer depth on demand with structured dropout",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_69",
            "content": "UNKNOWN, None, 2020, Role-wise data augmentation for knowledge distillation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Role-wise data augmentation for knowledge distillation",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_70",
            "content": "UNKNOWN, None, , and Nicholas Andrews. 2020. Compressing bert: Studying the effects of weight pruning on transfer learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "and Nicholas Andrews. 2020. Compressing bert: Studying the effects of weight pruning on transfer learning",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_71",
            "content": "UNKNOWN, None, 2019, Reweighted proximal pruning for large-scale language representation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Reweighted proximal pruning for large-scale language representation",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_72",
            "content": "UNKNOWN, None, , 2021. Generate, annotate, and learn: Generative models advance selftraining and knowledge distillation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "2021. Generate, annotate, and learn: Generative models advance selftraining and knowledge distillation",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_73",
            "content": "Geoffrey Hinton, Oriol Vinyals, Jeff Dean, Distilling the knowledge in a neural network, 2014, NIPS Workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Geoffrey Hinton",
                    "Oriol Vinyals",
                    "Jeff Dean"
                ],
                "title": "Distilling the knowledge in a neural network",
                "pub_date": "2014",
                "pub_title": "NIPS Workshop",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_74",
            "content": "Aref Jafari, Mehdi Rezagholizadeh, Pranav Sharma, Ali Ghodsi, Annealing knowledge distillation, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Aref Jafari",
                    "Mehdi Rezagholizadeh",
                    "Pranav Sharma",
                    "Ali Ghodsi"
                ],
                "title": "Annealing knowledge distillation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "250-ARR_v1_75",
            "content": "Mingi Ji, Byeongho Heo, Sungrae Park, 2021. Show, attend and distill: Knowledge distillation via attention-based feature matching, , Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Mingi Ji",
                    "Byeongho Heo",
                    "Sungrae Park"
                ],
                "title": "2021. Show, attend and distill: Knowledge distillation via attention-based feature matching",
                "pub_date": null,
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_76",
            "content": "UNKNOWN, None, 2019, Tinybert: Distilling bert for natural language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Tinybert: Distilling bert for natural language understanding",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_77",
            "content": "UNKNOWN, None, 2021, Not far away, not so close: Sample efficient nearest neighbour data augmentation via minimax, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Not far away, not so close: Sample efficient nearest neighbour data augmentation via minimax",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_78",
            "content": "Tushar Khot, Ashish Sabharwal, Peter Clark, Scitail: A textual entailment dataset from science question answering, 2018, AAAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Tushar Khot",
                    "Ashish Sabharwal",
                    "Peter Clark"
                ],
                "title": "Scitail: A textual entailment dataset from science question answering",
                "pub_date": "2018",
                "pub_title": "AAAI",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_79",
            "content": "UNKNOWN, None, 2021, How to select one among all? an extensive empirical study towards the robustness of knowledge distillation in natural language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "How to select one among all? an extensive empirical study towards the robustness of knowledge distillation in natural language understanding",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_80",
            "content": "UNKNOWN, None, 2020, Roberta: A robustly optimized fbertg pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Roberta: A robustly optimized fbertg pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_81",
            "content": "UNKNOWN, None, 2020, Dynabert: Dynamic bert with adaptive width and depth, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Dynabert: Dynamic bert with adaptive width and depth",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_82",
            "content": "Andrew Maas, Raymond Daly, Peter Pham, Dan Huang, Andrew Ng, Christopher Potts, Learning word vectors for sentiment analysis, 2011, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Andrew Maas",
                    "Raymond Daly",
                    "Peter Pham",
                    "Dan Huang",
                    "Andrew Ng",
                    "Christopher Potts"
                ],
                "title": "Learning word vectors for sentiment analysis",
                "pub_date": "2011",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_83",
            "content": "Yonatan Rabeeh Karimi Mahabadi, James Belinkov,  Henderson, End-to-end bias mitigation by modelling biases in corpora, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Yonatan Rabeeh Karimi Mahabadi",
                    "James Belinkov",
                    " Henderson"
                ],
                "title": "End-to-end bias mitigation by modelling biases in corpora",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "250-ARR_v1_84",
            "content": "Tom Mccoy, Ellie Pavlick, Tal Linzen, Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Tom Mccoy",
                    "Ellie Pavlick",
                    "Tal Linzen"
                ],
                "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_85",
            "content": "UNKNOWN, None, 2019, Are sixteen heads really better than one? in neurips, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Are sixteen heads really better than one? in neurips",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_86",
            "content": "Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, Hao Wu, Mixed precision training, 2018, In International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Paulius Micikevicius",
                    "Sharan Narang",
                    "Jonah Alben",
                    "Gregory Diamos",
                    "Erich Elsen",
                    "David Garcia",
                    "Boris Ginsburg",
                    "Michael Houston",
                    "Oleksii Kuchaiev",
                    "Ganesh Venkatesh",
                    "Hao Wu"
                ],
                "title": "Mixed precision training",
                "pub_date": "2018",
                "pub_title": "In International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_87",
            "content": "Peyman Passban, Yimeng Wu, Mehdi Rezagholizadeh, Qun Liu, ALP-KD: attention-based layer projection for knowledge distillation, 2021-02-02, Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Peyman Passban",
                    "Yimeng Wu",
                    "Mehdi Rezagholizadeh",
                    "Qun Liu"
                ],
                "title": "ALP-KD: attention-based layer projection for knowledge distillation",
                "pub_date": "2021-02-02",
                "pub_title": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "250-ARR_v1_88",
            "content": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Pytorch: An imperative style, high-performance deep learning library, 2019, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Adam Paszke",
                    "Sam Gross",
                    "Francisco Massa",
                    "Adam Lerer",
                    "James Bradbury",
                    "Gregory Chanan",
                    "Trevor Killeen",
                    "Zeming Lin",
                    "Natalia Gimelshein",
                    "Luca Antiga"
                ],
                "title": "Pytorch: An imperative style, high-performance deep learning library",
                "pub_date": "2019",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_89",
            "content": "UNKNOWN, None, 2019, Language models are unsupervised multitask learners, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Language models are unsupervised multitask learners",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_90",
            "content": "Pranav Rajpurkar, Robin Jia, Percy Liang, Know what you don't know: Unanswerable questions for squad, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Pranav Rajpurkar",
                    "Robin Jia",
                    "Percy Liang"
                ],
                "title": "Know what you don't know: Unanswerable questions for squad",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "250-ARR_v1_91",
            "content": "UNKNOWN, None, 2020, Towards zero-shot knowledge distillation for natural language processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Towards zero-shot knowledge distillation for natural language processing",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_92",
            "content": "UNKNOWN, None, , Vasileios Lioutas, and Mehdi Rezagholizadeh. 2021. Mate-kd: Masked adversarial text, a companion to knowledge distillation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Vasileios Lioutas, and Mehdi Rezagholizadeh. 2021. Mate-kd: Masked adversarial text, a companion to knowledge distillation",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_93",
            "content": "UNKNOWN, None, 2019, Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_94",
            "content": "UNKNOWN, None, 2019, Distilroberta, a distilled version of roberta: smaller, faster, cheaper and lighter, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Distilroberta, a distilled version of roberta: smaller, faster, cheaper and lighter",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_95",
            "content": "UNKNOWN, None, 2020, Learning from others' mistakes: Avoiding dataset biases without modeling them, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Learning from others' mistakes: Avoiding dataset biases without modeling them",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_96",
            "content": "Tal Schuster, Darsh Shah, Yun Jie Serene Yeo, Daniel Roberto Filizzola, Enrico Ortiz, Regina Santus,  Barzilay, Towards debiasing fact verification models, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Tal Schuster",
                    "Darsh Shah",
                    "Yun Jie Serene Yeo",
                    "Daniel Roberto Filizzola",
                    "Enrico Ortiz",
                    "Regina Santus",
                    " Barzilay"
                ],
                "title": "Towards debiasing fact verification models",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_97",
            "content": "UNKNOWN, None, 2019, Q-bert: Hessian based ultra low precision quantization of bert, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Q-bert: Hessian based ultra low precision quantization of bert",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_98",
            "content": "UNKNOWN, None, 2019, Patient knowledge distillation for bert model compression, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Patient knowledge distillation for bert model compression",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_99",
            "content": "Siqi Sun, Zhe Gan, Yu Cheng, Yuwei Fang, Shuohang Wang, Jingjing Liu, Contrastive distillation on intermediate representations for language model compression, 2020, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Siqi Sun",
                    "Zhe Gan",
                    "Yu Cheng",
                    "Yuwei Fang",
                    "Shuohang Wang",
                    "Jingjing Liu"
                ],
                "title": "Contrastive distillation on intermediate representations for language model compression",
                "pub_date": "2020",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_100",
            "content": "UNKNOWN, None, 2020, Mobilebert: a compact task-agnostic bert for resource-limited devices, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Mobilebert: a compact task-agnostic bert for resource-limited devices",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_101",
            "content": "UNKNOWN, None, 2019, Contrastive representation distillation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Contrastive representation distillation",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_102",
            "content": "UNKNOWN, None, 2019, Well-read students learn better: On the importance of pre-training compact models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Well-read students learn better: On the importance of pre-training compact models",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_103",
            "content": "Nafise Sadat Prasetya Ajie Utama, Iryna Moosavi,  Gurevych, Towards debiasing nlu models from unknown biases, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Nafise Sadat Prasetya Ajie Utama",
                    "Iryna Moosavi",
                    " Gurevych"
                ],
                "title": "Towards debiasing nlu models from unknown biases",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_104",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_105",
            "content": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, Superglue: A stickier benchmark for general-purpose language understanding systems, 2019, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Alex Wang",
                    "Yada Pruksachatkun",
                    "Nikita Nangia",
                    "Amanpreet Singh",
                    "Julian Michael",
                    "Felix Hill",
                    "Omer Levy",
                    "Samuel R Bowman"
                ],
                "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
                "pub_date": "2019",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_106",
            "content": "UNKNOWN, None, 2018, Glue: A multi-task benchmark and analysis platform for natural language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_107",
            "content": "UNKNOWN, None, 2020, Minilmv2: Multihead self-attention relation distillation for compressing pretrained transformers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Minilmv2: Multihead self-attention relation distillation for compressing pretrained transformers",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_108",
            "content": "UNKNOWN, None, 2020, Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_109",
            "content": "UNKNOWN, None, 2020, Why skip if you can combine: A simple knowledge distillation technique for intermediate layers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Why skip if you can combine: A simple knowledge distillation technique for intermediate layers",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_110",
            "content": "UNKNOWN, None, 2020, Lite transformer with long-short range attention, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Lite transformer with long-short range attention",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_111",
            "content": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, R Russ, Quoc V Salakhutdinov,  Le, Xlnet: Generalized autoregressive pretraining for language understanding, 2019, NeuRIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Zhilin Yang",
                    "Zihang Dai",
                    "Yiming Yang",
                    "Jaime Carbonell",
                    "R Russ",
                    "Quoc V Salakhutdinov",
                    " Le"
                ],
                "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
                "pub_date": "2019",
                "pub_title": "NeuRIPS",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_112",
            "content": "UNKNOWN, None, 2019, Q8bert: Quantized 8bit bert, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Q8bert: Quantized 8bit bert",
                "pub": null
            }
        },
        {
            "ix": "250-ARR_v1_113",
            "content": "George-Eduard Zaharia, Andrei-Marius Avram, Dumitru-Clementin Cercel, Traian Rebedea, Dialect identification through adversarial learning and knowledge distillation on romanian BERT, 2021-04-20, Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects, VarDial@EACL 2021, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "George-Eduard Zaharia",
                    "Andrei-Marius Avram",
                    "Dumitru-Clementin Cercel",
                    "Traian Rebedea"
                ],
                "title": "Dialect identification through adversarial learning and knowledge distillation on romanian BERT",
                "pub_date": "2021-04-20",
                "pub_title": "Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects, VarDial@EACL 2021",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "250-ARR_v1_114",
            "content": "UNKNOWN, None, 2019, Paws: Paraphrase adversaries from word scrambling, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Paws: Paraphrase adversaries from word scrambling",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "250-ARR_v1_0@0",
            "content": "RAIL-KD: RAndom Intermediate Layer Mapping for Knowledge Distillation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_0",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_2@0",
            "content": "Intermediate layer knowledge distillation (KD) can improve the standard KD technique (which only targets the output of teacher and student models) especially over large pre-trained language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_2",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_2@1",
            "content": "However, intermediate layer distillation suffers from excessive computational burdens and engineering efforts required for setting up a proper layer mapping.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_2",
            "start": 198,
            "end": 354,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_2@2",
            "content": "To address these problems, we propose a RAndom Intermediate Layer Knowledge Distillation (RAIL-KD) approach in which, intermediate layers from the teacher model are selected randomly to be distilled into the intermediate layers of the student model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_2",
            "start": 356,
            "end": 604,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_2@3",
            "content": "This randomized selection enforces that all teacher layers are taken into account in the training process, while reducing the computational cost of intermediate layer distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_2",
            "start": 606,
            "end": 785,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_2@4",
            "content": "Also, we show that it acts as a regularizer for improving the generalizability of the student model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_2",
            "start": 787,
            "end": 886,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_2@5",
            "content": "We perform extensive experiments on GLUE tasks as well as on out-of-domain test sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_2",
            "start": 888,
            "end": 972,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_2@6",
            "content": "We show that our proposed RAIL-KD approach outperforms other state-of-the-art intermediate layer KD methods considerably in both performance and training-time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_2",
            "start": 974,
            "end": 1132,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_4@0",
            "content": "Pre-trained Language Models (PLMs), such as BERT (Devlin et al., 2019), RoBERTa and XLNet (Yang et al., 2019) have shown remarkable abilities to match and even surpass human performances on many Natural Languages Understanding (NLU) tasks (Rajpurkar et al., 2018;Wang et al., 2018.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_4",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_4@1",
            "content": "However, the deployment of these models in real world applications (e.g. edge devices) come with challenges, mainly due to large model size and inference time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_4",
            "start": 282,
            "end": 440,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_5@0",
            "content": "In this regard, several model compression techniques such as quantization (Shen et al., 2019;Zafrir et al., 2019), pruning (Guo et al., 2019;Gordon et al., 2020;Michel et al., 2019), optimizing the Transformer architecture (Fan et al., 2019;Wu et al., 2020b;Lu et al., 2020), and knowledge distillation (Sanh et al., 2019a;Jiao et al., 2019;Sun et al., 2020b;Rashid et al., 2021;Kamalloo et al., 2021) have been developed to reduce the model size and latency, while maintaining comparable performance to the original model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_5",
            "start": 0,
            "end": 522,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_5@1",
            "content": "KD, which is the main focus of this work, is a neural model compression approach that involves training a small student model with the guidance of a large pre-trained teacher model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_5",
            "start": 524,
            "end": 704,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_5@2",
            "content": "In the original KD technique (Bucilu\u01ce et al., 2006;Hinton et al., 2014;Turc et al., 2019), the teacher output predictions are used as soft labels for supervising the training of the student.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_5",
            "start": 706,
            "end": 895,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_5@3",
            "content": "There has been several attempts in the literature to reduce the teacherstudent performance gap by leveraging data augmentation (Fu et al., 2020;Li et al., 2021;Jiao et al., 2019), adversarial training (Zaharia et al., 2021;Rashid et al., 2020Rashid et al., , 2021, and intermediate layer distillation (ILD) (Wang et al., 2020b,a;Ji et al., 2021;.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_5",
            "start": 897,
            "end": 1242,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_6@0",
            "content": "When it comes to BERT compression, ILD leads to clear gains in performances (Sanh et al., 2019a;Jiao et al., 2019; due to its ability to enhance the knowledge transfer beyond logits matching.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_6",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_6@1",
            "content": "This is done by mapping intermediate layer representations of both models to a common space 1 , and then matching them via regression (Sun et al., 2019) or cosine similarity (Sanh et al., 2019a) losses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_6",
            "start": 192,
            "end": 393,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_6@2",
            "content": "One major problem with ILD is the absence of an appropriate strategy to select layers to be matched on both sides, reacting to the skip and search problem .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_6",
            "start": 395,
            "end": 550,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_6@3",
            "content": "Some solutions in the literature mostly rely on layer combination (Wu et al., 2020a), attention-based layer",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_6",
            "start": 552,
            "end": 658,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_7@0",
            "content": "Complexity Limitation PKD (Sun et al., 2019) Extra Hyperparameter O(m) Extensive Search CKD (Wu et al., 2020a) Extra Hyperparameter O(m) Extensive Search ALP-KD projection and contrastive learning (Sun et al., 2020a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_7",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_7@1",
            "content": "While these solutions are all effective to some extent, to the best of our knowledge, there is no work in the literature doing a comprehensive evaluation of these techniques in terms of both efficiency and performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_7",
            "start": 218,
            "end": 435,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_8@0",
            "content": "A case in point is that the aforementioned solutions to the layer skip and search problem do not scale to very deep networks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_8",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_8@1",
            "content": "We propose RAIL-KD (RAndom Intermediate Layer KD), a simple yet effective method for intermediate layer mapping which randomly selects k out of n intermediate layers of the teacher at each epoch to be distilled to the corresponding student layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_8",
            "start": 126,
            "end": 372,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_8@2",
            "content": "Since the layer selection is done randomly, all the intermediate layers of the teacher will have a chance to be selected for distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_8",
            "start": 374,
            "end": 511,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_8@3",
            "content": "Our method adds no computational cost to the training, still outperforming all aforementioned methods on the GLUE benchmark (Wang et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_8",
            "start": 513,
            "end": 656,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_8@4",
            "content": "Moreover, we observe larger gains when distilling from large teacher models, as well as when student models are evaluated on out-ofdomain datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_8",
            "start": 658,
            "end": 804,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_8@5",
            "content": "Last, we report the results on 5 random seeds in order to verify the contribution of the random selection process, thus making the comparison fair with previous methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_8",
            "start": 806,
            "end": 974,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_8@6",
            "content": "The main contributions of our paper are as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_8",
            "start": 976,
            "end": 1026,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_9@0",
            "content": "\u2022 We introduce RAIL-KD, a more efficient and scalable intermediate layer distillation approach. \u2022 To the best of our knowledge, we are the first to perform a comprehensive study of the ILD techniques in terms of both efficiency and performance. \u2022 We consider the distillation of models such as BERT and RoBERTa, and compare different up-to-date distillation techniques on out-ofdomain test sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_9",
            "start": 0,
            "end": 394,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_10@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_10",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_11@0",
            "content": "Recent years, have seen a wide range of methods have witnessed to expand knowledge transfer of transformer-based (Vaswani et al., 2017) NLU models beyond logits matching.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_11",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_11@1",
            "content": "DistillBERT (Sanh et al., 2019a) added a cosine similarity loss between teacher and student embeddings layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_11",
            "start": 171,
            "end": 280,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_11@2",
            "content": "Tiny-BERT (Jiao et al., 2019), MobileBERT (Sun et al., 2020b), and MiniLM (Wang et al., 2020b) matched the intermediate layers representations and selfattention distributions of the teacher and the student.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_11",
            "start": 282,
            "end": 487,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_12@0",
            "content": "In PKD, Sun et al. (2019) used deterministic mapping strategies to distill a 12-layer BERT teacher to a 6-layer BERT student.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_12",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_12@1",
            "content": "PKD-LAST and PKD-SKIP refer to matching layers {1 \u2212 5} of the student with layers {7 \u2212 11} and {2, 4, 6, 8, 10} of the teacher respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_12",
            "start": 126,
            "end": 265,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_12@2",
            "content": "However, these works ignored the impact of layer selection, as they used a fixed layer-wise mapping.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_12",
            "start": 267,
            "end": 366,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_12@3",
            "content": "2 Researchers have found that tuning the layer mapping scheme can significantly improve the performance of ILD techniques (Sun et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_12",
            "start": 368,
            "end": 508,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_12@4",
            "content": "Nevertheless, finding the optimal mapping can be challenging, which is referred to as the layer skip and search problems by .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_12",
            "start": 510,
            "end": 634,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_12@5",
            "content": "To address the layer skip problem, CKD (Wu et al., 2020a) is built on top of PKD by partitioning all the intermediate layers of the teacher to the number of student layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_12",
            "start": 636,
            "end": 807,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_12@6",
            "content": "Then, the combined representation of the layers of each partition is distilled into a number of subset corresponding to the number of student layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_12",
            "start": 809,
            "end": 957,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_12@7",
            "content": "However, finding the optimal partitioning scheme requires running exhaustive experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_12",
            "start": 959,
            "end": 1047,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_12@8",
            "content": "Given teacher and student BERT models with n and m layers respectively (where n >> m), it is not trivial to choose the teacher layers that can be incorporated in the distillation process and how we should map them to the student layers (search).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_12",
            "start": 1049,
            "end": 1293,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_12@9",
            "content": "ALP-KD overcomes this issue by computing attention weights between each student layer and all the intermediate layers of the teacher.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_12",
            "start": 1295,
            "end": 1427,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_12@10",
            "content": "The learned attention weights for each student layer are used to obtain a weighted representation of all teacher layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_12",
            "start": 1429,
            "end": 1548,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_12@11",
            "content": "Although ALP-KD has shown promising results on 12-layer BERT-based compression, attending to all layers of the teacher adds considerable computational overhead to the training phase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_12",
            "start": 1550,
            "end": 1731,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_12@12",
            "content": "This can become computationally prohibitive when scaling to very large models such as RoBERTa-large or GPT-2 (Radford et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_12",
            "start": 1733,
            "end": 1864,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_12@13",
            "content": "Alternatively, CODIR (Sun et al., 2020a) exploited contrastive learning (Tian et al., 2019) to perform intermediate layers matching between the teacher and the student models with no deterministic mapping.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_12",
            "start": 1866,
            "end": 2070,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_12@14",
            "content": "Similar to ALP-KD, this approach also requires excessive training time due to the contrastive loss calculation and the use of negative samples from a memory bank.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_12",
            "start": 2072,
            "end": 2233,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_13@0",
            "content": "Table 1 summarizes the main characteristics of the existing state-of-the-art intermediate layer distillation techniques (PKD (Sun et al., 2019), CKD (Wu et al., 2020a), and CoDIR) used for pre-trained language models compared with our proposed RAIL-KD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_13",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_13@1",
            "content": "First, PKD and CKD treat the mapping as an extra hyperparameter that requires extensive experiments in order to find the optimal mapping.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_13",
            "start": 253,
            "end": 389,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_13@2",
            "content": "Second, ALP-KD and CoDIR (Sun et al., 2020a) use the attention mechanism and contrastive learning respectively to address the issue, but at the expense of extra computational cost.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_13",
            "start": 391,
            "end": 570,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_14@0",
            "content": "Our proposed RAIL-KD method does not add any computational cost to the distillation process, while empirically outperforming previous methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_14",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_14@1",
            "content": "For instance, RAIL-KD is roughly twice faster than CoDIR in a 24 to 6 layer compression.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_14",
            "start": 143,
            "end": 230,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_14@2",
            "content": "In addition, it does not require extensive experiments to find the optimal mapping scheme.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_14",
            "start": 232,
            "end": 321,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_14@3",
            "content": "In this work, we position ourselves to works that tackle the skip and search problem 3 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_14",
            "start": 323,
            "end": 410,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_14@4",
            "content": "Otherwise said, we don't compare with works like TinyBERT (Jiao et al., 2019) or MiniLM (Wang et al., 2020b), which use extra losses like self-attention distribution matching.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_14",
            "start": 412,
            "end": 586,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_14@5",
            "content": "However, we expect that these methods, as well as state-of-the-art (Rashid et al., 2021;He et al., 2021) ones can take full advantage of RAIL-KD, since they use a deterministic layer mapping scheme.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_14",
            "start": 588,
            "end": 785,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_15@0",
            "content": "RAIL-KD",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_15",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_16@0",
            "content": "The RAIL-KD method is sketched in Figure 1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_16",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_17@0",
            "content": "= (x 0 , \u2022 \u2022 \u2022 , x L\u22121 )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_17",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_18@0",
            "content": "which is a sequence of L tokens and y its corresponding label.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_18",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_18@1",
            "content": "In Figure 1, our Random Selection operator is applied to the intermediate layers of the teacher to randomly select m out of n layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_18",
            "start": 63,
            "end": 195,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_18@2",
            "content": "The intermediate layer representations of the m selected layers of the teacher and the student model corresponding to the X input can be described as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_18",
            "start": 197,
            "end": 345,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_19@0",
            "content": "H T X = {H T 1,X , \u2022 \u2022 \u2022 , H T m,X } and H S \u03b8 X = {H S \u03b8 1,X , \u2022 \u2022 \u2022 , H S \u03b8 m,X } respectively, where H T i,X = \u222a L\u22121 k=0 {h T i,x k } \u2208 R L\u00d7d 1 and H S \u03b8 i,X = \u222a L\u22121 k=0 {h S \u03b8 i,x k } \u2208 R L\u00d7d 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_19",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_20@0",
            "content": "Here, d 1 and d 2 indicate the hidden dimension of the layers of the teacher and the student models respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_20",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_20@1",
            "content": "To obtain H T i,X and H S j,X , we need to find the individual representation of each token x k at each layer i, which is indicated as h T i,x k and h S i,x k for the teacher and student networks respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_20",
            "start": 114,
            "end": 322,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_21@0",
            "content": "At this stage, we need to obtain an aggregated representation of the sequence X at each layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_21",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_21@1",
            "content": "In this regard, one can either use the <CLS> token representation or use the mean-pooling of the sequence representations of the layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_21",
            "start": 95,
            "end": 229,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_21@2",
            "content": "Since in (Sun et al., 2020a), the mean-pooling representation shows better results, we adopt it to compute the sentence representation of each layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_21",
            "start": 231,
            "end": 379,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_21@3",
            "content": "Meanpooling is a row-wise average over Sun et al., 2020a):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_21",
            "start": 381,
            "end": 438,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_22@0",
            "content": "H T i,X , H S \u03b8 i,X to get hT i,X \u2208 R d 1 hS \u03b8 i,X \u2208 R d 2 (",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_22",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_23@0",
            "content": "hT i,X = 1 L L\u22121 k=0 h T i,x k ; hS \u03b8 i,X = 1 L L\u22121 k=0 h S \u03b8 i,x k (1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_23",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_24@0",
            "content": "After obtaining aggregated layer representations for both the student and teacher networks, our RAIL-KD proposal is to randomly select m layer representations from the teacher through training to perform the intermediate layer distillation (ILD). RAIL-KD does ILD in two different forms: using layer-wise distillation (see Fig. 1(a)) or by concatenating layer representations (see Fig. 1(b)) which are described in the following two sub-sections.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_24",
            "start": 0,
            "end": 445,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_25@0",
            "content": "Layer-wise RAIL-KD",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_25",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_26@0",
            "content": "In this setting, the representations hT i,X \u2208 R d 1 and hS \u03b8 i,X \u2208 R d 2 are projected into a same-size lowerdimensional space \u0125T i,X , \u0125S \u03b8 i,X \u2208 R u using (d 1 \u00d7 u) and (d 2 \u00d7 u) linear mappings respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_26",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_26@1",
            "content": "Assume that the set A = {a \u03ba |a \u03ba \u223c {1, 2, ..., n}, 1 \u2264 \u03ba \u2264 m} contains indices of selected m layers from the teacher, then to calculate the layer-wise loss we have:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_26",
            "start": 211,
            "end": 375,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_27@0",
            "content": "L RAIL-KD l = X\u2208X i\u2208A \u03b1 i || \u0125T i,X || \u0125T i,X || 2 \u2212 \u0125S \u03b8 i,X || \u0125S \u03b8 i,X || 2 || 2 2 (2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_27",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_28@0",
            "content": "where X denotes the training set, and \u03b1 i is a hyperparameter to assign a custom weights to the layerwise distillation loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_28",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_28@1",
            "content": "It is worth mentioning that in our experiments we set \u03b1 i = 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_28",
            "start": 125,
            "end": 186,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_29@0",
            "content": "Concatenated RAIL-KD",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_29",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_30@0",
            "content": "In this setting, intermediate layer representations are concatenated and then distilled:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_30",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_31@0",
            "content": "hT X = [ hT i,X ] i\u2208A , hS \u03b8 X = [ hS \u03b8 j,X",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_31",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_32@0",
            "content": "] m j=1 which are further mapped into the same lower-dimensional space \u0125T X , \u0125S \u03b8 X \u2208 R u using (md 1 \u00d7 u) and (md 2 \u00d7 u) linear mappings to calculate the concatenated distillation loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_32",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_33@0",
            "content": "L RAIL-KD c = X\u2208X || \u0125T X || \u0125T X || 2 \u2212 \u0125S \u03b8 X || \u0125S \u03b8 X || 2 || 2 2 (3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_33",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_34@0",
            "content": "Any type of loss such as contrastive (Sun et al., 2020a), or mean-square-error (MSE) Sun et al., 2019) can be applied for our RAIL-KD approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_34",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_35@0",
            "content": "Training Loss",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_35",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_36@0",
            "content": "The intermediate representation distillation loss L RAIL-KD is combined with the original KD loss L KD , which is used to distill the knowledge from the output logits of the teacher model T to the output logits of the student model S \u03b8 , and the original cross-entropy loss L CE .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_36",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_36@1",
            "content": "The total loss function for training the student model is:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_36",
            "start": 281,
            "end": 338,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_37@0",
            "content": "L = \u03bb 1 L CE + \u03bb 2 L KD + \u03bb 3 L RAIL-KD l/c(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_37",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_38@0",
            "content": "where \u03bb 1 , \u03bb 2 , and \u03bb 3 are hyper-parameters of our model to minimize the total loss, and 4 Experimental Protocol",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_38",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_39@0",
            "content": "\u03bb 1 + \u03bb 2 + \u03bb 3 = 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_39",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_40@0",
            "content": "Datasets and Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_40",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_41@0",
            "content": "We evaluate RAIL-KD on 8 tasks from the GLUE benchmark (Wang et al., 2018): 2 single-sentence (CoLA and SST-2) and 5 sentence-pair (MRPC, RTE, QQP, QNLI, and MNLI) classification tasks, and 1 regression task (STS-B).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_41",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_41@1",
            "content": "Following prior works (Sun et al., 2019;Jiao et al., 2019;Sun et al., 2020a), we use the same metrics as the GLUE benchmark for evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_41",
            "start": 217,
            "end": 355,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_41@2",
            "content": "Moreover, to further show the generalization capability of our RAIL-KD method on out-of-domain (OOD) across tasks, we use Scitail (Khot et al., 2018), PAWS (Paraphrase Adversaries from Word Scrambling) (Zhang et al., 2019), and IMDb (Internet Movie Database) (Maas et al., 2011) test sets to evaluate the models fine-tuned on MNLI, QQP, and SST-2 tasks respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_41",
            "start": 357,
            "end": 722,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_42@0",
            "content": "Implementation Details",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_42",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_43@0",
            "content": "We run extensive experiments on 3 different teachers in order to ensure a fair comparison with a wide range of prior works, and also to show the effectiveness of RAIL-KD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_43",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_43@1",
            "content": "We experiment with the 12-layer BERT-base-uncased (Devlin et al., 2019) model as teacher (BERT 12 ) and the 6-layer DistilBERT (Sanh et al., 2019a) as student (DistillBERT 6 ) to compare with PKD (Sun et al., 2019) and ALP-KD .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_43",
            "start": 171,
            "end": 397,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_43@2",
            "content": "Also, we consider 24-layer RoBERTa-large and 6-layer Dis-tilRoberta (Sanh et al., 2019b) as the backbone for teacher (RoBERTa 24 ) and student (DistilRoberta 6 ) respectively to compare results when n >> m. Furthermore, we perform evaluation using the 12 layers RoBERTa-base (RoBERTa 12 ) model as a teacher to be able to directly compare our figures with the ones of CoDIR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_43",
            "start": 399,
            "end": 772,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_43@3",
            "content": "Hyperparameters selection and other implementation details can be found in Appendix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_43",
            "start": 774,
            "end": 857,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_44@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_44",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_45@0",
            "content": "Table 2 shows the performances of models trained on GLUE tasks, and evaluated on their respective DEV and TEST sets for 12-layer to 6-layer distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_45",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_45@1",
            "content": "BERT 12 and DistilBERT 6 are used as backbone for the teacher and student models respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_45",
            "start": 154,
            "end": 247,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_45@2",
            "content": "The baselines are fine-tuned without KD (w/o KD) and with Vanilla KD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_45",
            "start": 249,
            "end": 317,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_45@3",
            "content": "Moreover, we directly compare RAIL-KD lc results with PKD and ALP-KD as more competing techniques.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_45",
            "start": 319,
            "end": 416,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_45@4",
            "content": "First, we observe that the performance gap between ILD methods and vanilla-KD is tight (0.8% and 0.3% on DEV and TEST sets respectively).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_45",
            "start": 418,
            "end": 554,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_45@5",
            "content": "Moreover, as we expect, ALP-KD performs better on DEV and similar on TEST compared to PKD with 0.2% improvement on the DEV results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_45",
            "start": 556,
            "end": 686,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_45@6",
            "content": "Second, results show that RAIL-KD outperforms the best ILD methods by a margin of 0.5% and 0.3% on average on DEV and TEST sets respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_45",
            "start": 688,
            "end": 828,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_45@7",
            "content": "We notice that, except on RTE TEST, our RAIL-KD variants obtained the highest per-task performances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_45",
            "start": 830,
            "end": 929,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_45@8",
            "content": "Third, we observe that RAIL-KD variants perform very similarly, which indicates that our method is effective on concatenated as well as layer-wise distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_45",
            "start": 931,
            "end": 1090,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_46@0",
            "content": "Similar trends are seen on the 24-to 6-layer model compression experiments, which are reported in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_46",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_46@1",
            "content": "In this experiment, we used Roberta 24 and DistillRoberta 6 as teacher and students models respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_46",
            "start": 107,
            "end": 210,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_46@2",
            "content": "Overall, RAIL-KD outperforms the best baseline by 1.2% and 0.3% on DEV and TEST sets respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_46",
            "start": 212,
            "end": 309,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_46@3",
            "content": "Interestingly, the gap on DEV compared with PKD and ALP-KD is larger than the one reported on BERT 12 experiments, and PKD TEST socres are much lower from that of ALP and RAIL-KD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_46",
            "start": 311,
            "end": 489,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_46@4",
            "content": "This might be because PKD skips a large number of intermediate layers on RoBERTa 24 , and the computational cost of ALP-KD attention weights over a large number of teacher layers might produce smaller weights on Roberta 24 compared to BERT 12 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_46",
            "start": 491,
            "end": 734,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_47@0",
            "content": "Furthermore, we demonstrate the effectiveness of RAIL-KD by directly comparing it with CoDIR (Sun et al., 2020a), the current state-ofthe-art ILD method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_47",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_47@1",
            "content": "It uses the contrastive objective and a memory bank to extract a large number of negative samples for contrastive loss calculations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_47",
            "start": 154,
            "end": 285,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_47@2",
            "content": "Table 4 shows GLUE test results of both approaches when distilling RoBERTa 12 to DistillRoberta 6 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_47",
            "start": 287,
            "end": 385,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_47@3",
            "content": "CoDIR results are adopted from their paper, and we followed their experimental protocol by not reporting scores on STS-B. On average, RAIL-KD performs on par with CoDIR (+0.2%) and outperforms it on 5 out of 8 datasets, while being almost twice faster as shown in the next section.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_47",
            "start": 387,
            "end": 667,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_48@0",
            "content": "Training Speed up",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_48",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_49@0",
            "content": "Table 5 shows the training time speed up against the teacher of different techniques on 8 GLUE tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_49",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_49@1",
            "content": "We measured the speed up by calculating the student_train_time/teacher_train_time using RoBERTa 2 4 and DistilRoBERTa 6 as backbone for teacher and student respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_49",
            "start": 102,
            "end": 270,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_49@2",
            "content": "We used this configuration because CODIR pretrained student models are not available and we can only run CODIR code out-of-the-box.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_49",
            "start": 272,
            "end": 402,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_50@0",
            "content": "Our results indicate that random layer mapping not only delivers consistently better results than the deterministic mapping technique such as PKD, but it has less computational overhead during training",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_50",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_51@0",
            "content": "Model Teacher PKD ALP CODIR RAIL l RAIL c Speed-up 1.00\u00d7 1.89\u00d7 1.75\u00d7 1\u00d7 1.89\u00d7 1.96\u00d7",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_51",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_52@0",
            "content": "Table 5: Training time speedup against the teacher for different techniques using the same setting of Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_52",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_53@0",
            "content": "(twice faster than CODIR), while avoiding extensive search experiments to find an optimal mapping.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_53",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_53@1",
            "content": "Furthermore, using attention for layer selection (ALP-KD) or contrastive learning (CoDIR) leads to slightly worse performance result than random selection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_53",
            "start": 99,
            "end": 253,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_54@0",
            "content": "Impact of Random Layer Selection",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_54",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_55@0",
            "content": "To evaluate the impact of random layer selection on the performance of RAIL-KD, we report the standard deviation of the DistilBERT 6 student models (Table 2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_55",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_56@0",
            "content": "Out-of-Distribution Test",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_56",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_57@0",
            "content": "We further validate the generalization ability of student models by measuring their robustness to in-domain and out-of-domain evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_57",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_57@1",
            "content": "We do so by evaluating models finetuned on MLI, QQP and SST-2 and then evaluated on SciTail, PAWS, and IMDB respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_57",
            "start": 138,
            "end": 258,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_57@2",
            "content": "These datasets contains counterexamples to biases found in the training data (McCoy et al., 2019;Schuster et al., 2019;Clark et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_57",
            "start": 260,
            "end": 398,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_57@3",
            "content": "Performances of BERT 12 /Roberta 24 teacher and DistilBERT 6 /DistilRoBERTa 6 student variants are reported in Table 7.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_57",
            "start": 400,
            "end": 518,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_57@4",
            "content": "Also, we compute the unweighted average score of the three tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_57",
            "start": 520,
            "end": 584,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_57@5",
            "content": "First, we notice high variability in models rank and some inconsistencies in performances across tasks when compared with in-domain results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_57",
            "start": 586,
            "end": 725,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_57@6",
            "content": "This was also reported in prior works on out-of-domain training and evaluation (Clark et al., 2019;Mahabadi et al., 2020;Utama et al., 2020;Sanh et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_57",
            "start": 727,
            "end": 885,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_57@7",
            "content": "Still, RAIL-KD clearly outperforms all baselines across tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_57",
            "start": 887,
            "end": 948,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_57@8",
            "content": "Surprisingly, we observe that PKD and ALP-KD perform poorly (on all three tasks) compared to the Vanilla KD baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_57",
            "start": 950,
            "end": 1066,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_57@9",
            "content": "Table 7: Out-of-domain performances of models trained on MNLI, QQP, SST-2 and evaluated on SciTail, PAWS, and IMDB respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_57",
            "start": 1068,
            "end": 1195,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_57@10",
            "content": "BERT 12 /Roberta 24 and DistilBERT 6 /DistilRoBERTa 6 are used as backbone for the teacher and students respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_57",
            "start": 1197,
            "end": 1313,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_57@11",
            "content": "For each setting, we report the unweighted average score on the 3 tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_57",
            "start": 1315,
            "end": 1386,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_58@0",
            "content": "Interestingly, we observe that RAIL-KD l performs consistently better (1.1% on average) than RAIL-KD c on Roberta 24 compression, while RAIL-KD c performs better (1.1% on average) on BERT 12 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_58",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_58@1",
            "content": "These results suggest that layer-wise distillation approach is more effective than concatenated distillation when we have a large capacity gap (layer number) between the teacher and the student, and vice versa.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_58",
            "start": 193,
            "end": 402,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_59@0",
            "content": "Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_59",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_60@0",
            "content": "We run extensive analysis to better understand why RAIL-KD performs better than the other baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_60",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_60@1",
            "content": "We visualize the layer-wise cosine similarity between the intermediate representations of the teacher and the student networks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_60",
            "start": 101,
            "end": 227,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_60@2",
            "content": "Figure 2 shows the cosine similarity score between three intermediate layer representations of BERT 12 teacher (i.e. layers 2, 4 and 6) and the first three layer representations of the student for PKD, ALP-KD, RAIL-KD l/c students on 100 samples randomly selected from the SST-2 dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_60",
            "start": 229,
            "end": 515,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_60@3",
            "content": "Due to space constraints, we only plot the scores for the first three layers of We found that RAIL-KD allows the student to mimic teacher layers similar to PKD and much better than ALP-KD, despite that the mapping scheme varies at each epoch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_60",
            "start": 517,
            "end": 758,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_60@4",
            "content": "Moreover, we observe that ALP-KD results in less similarity scores in the upper intermediate layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_60",
            "start": 760,
            "end": 859,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_60@5",
            "content": "PKD gives lower similarity scores in the lower layers while improving in the upper layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_60",
            "start": 861,
            "end": 950,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_60@6",
            "content": "In contrast, our approach gives more stable similarity scores for all layers while getting closer to the teacher representation in the upper layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_60",
            "start": 952,
            "end": 1099,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_61@0",
            "content": "We further investigate the attention weights learned by ALP-KD, and find out that they mostly focus on few layers (sparse attention).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_61",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_61@1",
            "content": "Figure 3 This is an indicator that ALP-KD overfits to the information driven from last layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_61",
            "start": 134,
            "end": 227,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_61@2",
            "content": "In contrast, the randomness in layer selection of RAIL-KD ensures a uniform focus on teacher layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_61",
            "start": 229,
            "end": 328,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_61@3",
            "content": "This may explain the poor performance of ALP-KD on out-4 Similar trends found on other datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_61",
            "start": 330,
            "end": 425,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_61@4",
            "content": "of-domain evaluation compared with RAIL-KD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_61",
            "start": 427,
            "end": 469,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_62@0",
            "content": "From Figure 3, we see clearly that ALP-KD mostly prefers the upper layers of the teacher.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_62",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_62@1",
            "content": "On the other hand, the deterministic nature of PDK allows it to match better particular layers of the teacher (e.g. bottom ones as shown in Figure 2), but PKD never sees the layers that are skipped by the mapping.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_62",
            "start": 90,
            "end": 302,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_62@2",
            "content": "Consequently, it is expected that even though PDK can mimic bottom layers well, it is worse overall because it completely ignores some layers of the teacher.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_62",
            "start": 304,
            "end": 460,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_62@3",
            "content": "Random layer selection allow RAIL-KD to mimic all teacher layers while delivering high performances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_62",
            "start": 462,
            "end": 561,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_63@0",
            "content": "Conclusion and Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_63",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_64@0",
            "content": "We introduced a novel, simple, and efficient intermediate layer KD approach that outperforms the conventional approaches with performance improvement and efficient training time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_64",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_64@1",
            "content": "RAIL-KD selects random intermediate layers from the teacher which equals to the number of intermediate layers of the student model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_64",
            "start": 179,
            "end": 309,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_64@2",
            "content": "The selected intermediate layers are then sorted to distill their representations into the student model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_64",
            "start": 311,
            "end": 415,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_64@3",
            "content": "RAIL-KD yields better regularization, which helps performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_64",
            "start": 417,
            "end": 478,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_64@4",
            "content": "Furthermore, our approach shows better performance for larger model distillation with faster training time, which opens up an avenue to investigate our approach for super-large models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_64",
            "start": 480,
            "end": 663,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_65@0",
            "content": "Cristian Bucilu\u01ce, Rich Caruana, Alexandru Niculescu-Mizil, Model compression, 2006, Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_65",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_66@0",
            "content": "Christopher Clark, Mark Yatskar, Luke Zettlemoyer, Don't take the easy way out: Ensemble based methods for avoiding known dataset biases, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_66",
            "start": 0,
            "end": 321,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_67@0",
            "content": "UNKNOWN, None, 2019, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_67",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_68@0",
            "content": "UNKNOWN, None, 2019, Reducing transformer depth on demand with structured dropout, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_68",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_69@0",
            "content": "UNKNOWN, None, 2020, Role-wise data augmentation for knowledge distillation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_69",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_70@0",
            "content": "UNKNOWN, None, , and Nicholas Andrews. 2020. Compressing bert: Studying the effects of weight pruning on transfer learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_70",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_71@0",
            "content": "UNKNOWN, None, 2019, Reweighted proximal pruning for large-scale language representation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_71",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_72@0",
            "content": "UNKNOWN, None, , 2021. Generate, annotate, and learn: Generative models advance selftraining and knowledge distillation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_72",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_73@0",
            "content": "Geoffrey Hinton, Oriol Vinyals, Jeff Dean, Distilling the knowledge in a neural network, 2014, NIPS Workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_73",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_74@0",
            "content": "Aref Jafari, Mehdi Rezagholizadeh, Pranav Sharma, Ali Ghodsi, Annealing knowledge distillation, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_74",
            "start": 0,
            "end": 273,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_75@0",
            "content": "Mingi Ji, Byeongho Heo, Sungrae Park, 2021. Show, attend and distill: Knowledge distillation via attention-based feature matching, , Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_75",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_76@0",
            "content": "UNKNOWN, None, 2019, Tinybert: Distilling bert for natural language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_76",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_77@0",
            "content": "UNKNOWN, None, 2021, Not far away, not so close: Sample efficient nearest neighbour data augmentation via minimax, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_77",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_78@0",
            "content": "Tushar Khot, Ashish Sabharwal, Peter Clark, Scitail: A textual entailment dataset from science question answering, 2018, AAAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_78",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_79@0",
            "content": "UNKNOWN, None, 2021, How to select one among all? an extensive empirical study towards the robustness of knowledge distillation in natural language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_79",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_80@0",
            "content": "UNKNOWN, None, 2020, Roberta: A robustly optimized fbertg pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_80",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_81@0",
            "content": "UNKNOWN, None, 2020, Dynabert: Dynamic bert with adaptive width and depth, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_81",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_82@0",
            "content": "Andrew Maas, Raymond Daly, Peter Pham, Dan Huang, Andrew Ng, Christopher Potts, Learning word vectors for sentiment analysis, 2011, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_82",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_83@0",
            "content": "Yonatan Rabeeh Karimi Mahabadi, James Belinkov,  Henderson, End-to-end bias mitigation by modelling biases in corpora, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_83",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_84@0",
            "content": "Tom Mccoy, Ellie Pavlick, Tal Linzen, Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_84",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_85@0",
            "content": "UNKNOWN, None, 2019, Are sixteen heads really better than one? in neurips, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_85",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_86@0",
            "content": "Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, Hao Wu, Mixed precision training, 2018, In International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_86",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_87@0",
            "content": "Peyman Passban, Yimeng Wu, Mehdi Rezagholizadeh, Qun Liu, ALP-KD: attention-based layer projection for knowledge distillation, 2021-02-02, Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_87",
            "start": 0,
            "end": 408,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_88@0",
            "content": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Pytorch: An imperative style, high-performance deep learning library, 2019, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_88",
            "start": 0,
            "end": 273,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_89@0",
            "content": "UNKNOWN, None, 2019, Language models are unsupervised multitask learners, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_89",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_90@0",
            "content": "Pranav Rajpurkar, Robin Jia, Percy Liang, Know what you don't know: Unanswerable questions for squad, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_90",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_91@0",
            "content": "UNKNOWN, None, 2020, Towards zero-shot knowledge distillation for natural language processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_91",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_92@0",
            "content": "UNKNOWN, None, , Vasileios Lioutas, and Mehdi Rezagholizadeh. 2021. Mate-kd: Masked adversarial text, a companion to knowledge distillation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_92",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_93@0",
            "content": "UNKNOWN, None, 2019, Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_93",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_94@0",
            "content": "UNKNOWN, None, 2019, Distilroberta, a distilled version of roberta: smaller, faster, cheaper and lighter, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_94",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_95@0",
            "content": "UNKNOWN, None, 2020, Learning from others' mistakes: Avoiding dataset biases without modeling them, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_95",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_96@0",
            "content": "Tal Schuster, Darsh Shah, Yun Jie Serene Yeo, Daniel Roberto Filizzola, Enrico Ortiz, Regina Santus,  Barzilay, Towards debiasing fact verification models, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_96",
            "start": 0,
            "end": 339,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_97@0",
            "content": "UNKNOWN, None, 2019, Q-bert: Hessian based ultra low precision quantization of bert, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_97",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_98@0",
            "content": "UNKNOWN, None, 2019, Patient knowledge distillation for bert model compression, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_98",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_99@0",
            "content": "Siqi Sun, Zhe Gan, Yu Cheng, Yuwei Fang, Shuohang Wang, Jingjing Liu, Contrastive distillation on intermediate representations for language model compression, 2020, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_99",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_100@0",
            "content": "UNKNOWN, None, 2020, Mobilebert: a compact task-agnostic bert for resource-limited devices, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_100",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_101@0",
            "content": "UNKNOWN, None, 2019, Contrastive representation distillation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_101",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_102@0",
            "content": "UNKNOWN, None, 2019, Well-read students learn better: On the importance of pre-training compact models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_102",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_103@0",
            "content": "Nafise Sadat Prasetya Ajie Utama, Iryna Moosavi,  Gurevych, Towards debiasing nlu models from unknown biases, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_103",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_104@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_104",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_105@0",
            "content": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, Superglue: A stickier benchmark for general-purpose language understanding systems, 2019, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_105",
            "start": 0,
            "end": 260,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_106@0",
            "content": "UNKNOWN, None, 2018, Glue: A multi-task benchmark and analysis platform for natural language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_106",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_107@0",
            "content": "UNKNOWN, None, 2020, Minilmv2: Multihead self-attention relation distillation for compressing pretrained transformers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_107",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_108@0",
            "content": "UNKNOWN, None, 2020, Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_108",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_109@0",
            "content": "UNKNOWN, None, 2020, Why skip if you can combine: A simple knowledge distillation technique for intermediate layers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_109",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_110@0",
            "content": "UNKNOWN, None, 2020, Lite transformer with long-short range attention, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_110",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_111@0",
            "content": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, R Russ, Quoc V Salakhutdinov,  Le, Xlnet: Generalized autoregressive pretraining for language understanding, 2019, NeuRIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_111",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_112@0",
            "content": "UNKNOWN, None, 2019, Q8bert: Quantized 8bit bert, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_112",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_113@0",
            "content": "George-Eduard Zaharia, Andrei-Marius Avram, Dumitru-Clementin Cercel, Traian Rebedea, Dialect identification through adversarial learning and knowledge distillation on romanian BERT, 2021-04-20, Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects, VarDial@EACL 2021, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_113",
            "start": 0,
            "end": 344,
            "label": {}
        },
        {
            "ix": "250-ARR_v1_114@0",
            "content": "UNKNOWN, None, 2019, Paws: Paraphrase adversaries from word scrambling, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "250-ARR_v1_114",
            "start": 0,
            "end": 72,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "250-ARR_v1_0",
            "tgt_ix": "250-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_0",
            "tgt_ix": "250-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_1",
            "tgt_ix": "250-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_1",
            "tgt_ix": "250-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_0",
            "tgt_ix": "250-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_2",
            "tgt_ix": "250-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_4",
            "tgt_ix": "250-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_5",
            "tgt_ix": "250-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_3",
            "tgt_ix": "250-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_3",
            "tgt_ix": "250-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_3",
            "tgt_ix": "250-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_3",
            "tgt_ix": "250-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_7",
            "tgt_ix": "250-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_8",
            "tgt_ix": "250-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_3",
            "tgt_ix": "250-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_3",
            "tgt_ix": "250-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_3",
            "tgt_ix": "250-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_6",
            "tgt_ix": "250-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_0",
            "tgt_ix": "250-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_11",
            "tgt_ix": "250-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_12",
            "tgt_ix": "250-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_13",
            "tgt_ix": "250-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_10",
            "tgt_ix": "250-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_10",
            "tgt_ix": "250-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_10",
            "tgt_ix": "250-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_10",
            "tgt_ix": "250-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_10",
            "tgt_ix": "250-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_0",
            "tgt_ix": "250-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_14",
            "tgt_ix": "250-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_16",
            "tgt_ix": "250-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_17",
            "tgt_ix": "250-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_18",
            "tgt_ix": "250-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_19",
            "tgt_ix": "250-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_20",
            "tgt_ix": "250-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_21",
            "tgt_ix": "250-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_22",
            "tgt_ix": "250-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_23",
            "tgt_ix": "250-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_15",
            "tgt_ix": "250-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_15",
            "tgt_ix": "250-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_15",
            "tgt_ix": "250-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_15",
            "tgt_ix": "250-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_15",
            "tgt_ix": "250-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_15",
            "tgt_ix": "250-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_15",
            "tgt_ix": "250-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_15",
            "tgt_ix": "250-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_15",
            "tgt_ix": "250-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_15",
            "tgt_ix": "250-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_15",
            "tgt_ix": "250-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_24",
            "tgt_ix": "250-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_26",
            "tgt_ix": "250-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_27",
            "tgt_ix": "250-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_25",
            "tgt_ix": "250-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_25",
            "tgt_ix": "250-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_25",
            "tgt_ix": "250-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_25",
            "tgt_ix": "250-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_15",
            "tgt_ix": "250-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_28",
            "tgt_ix": "250-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_30",
            "tgt_ix": "250-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_31",
            "tgt_ix": "250-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_32",
            "tgt_ix": "250-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_33",
            "tgt_ix": "250-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_29",
            "tgt_ix": "250-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_29",
            "tgt_ix": "250-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_29",
            "tgt_ix": "250-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_29",
            "tgt_ix": "250-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_29",
            "tgt_ix": "250-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_29",
            "tgt_ix": "250-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_15",
            "tgt_ix": "250-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_34",
            "tgt_ix": "250-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_36",
            "tgt_ix": "250-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_37",
            "tgt_ix": "250-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_38",
            "tgt_ix": "250-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_35",
            "tgt_ix": "250-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_35",
            "tgt_ix": "250-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_35",
            "tgt_ix": "250-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_35",
            "tgt_ix": "250-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_35",
            "tgt_ix": "250-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_0",
            "tgt_ix": "250-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_39",
            "tgt_ix": "250-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_40",
            "tgt_ix": "250-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_40",
            "tgt_ix": "250-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_0",
            "tgt_ix": "250-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_41",
            "tgt_ix": "250-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_42",
            "tgt_ix": "250-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_42",
            "tgt_ix": "250-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_0",
            "tgt_ix": "250-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_43",
            "tgt_ix": "250-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_45",
            "tgt_ix": "250-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_46",
            "tgt_ix": "250-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_44",
            "tgt_ix": "250-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_44",
            "tgt_ix": "250-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_44",
            "tgt_ix": "250-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_44",
            "tgt_ix": "250-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_44",
            "tgt_ix": "250-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_47",
            "tgt_ix": "250-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_49",
            "tgt_ix": "250-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_50",
            "tgt_ix": "250-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_51",
            "tgt_ix": "250-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_52",
            "tgt_ix": "250-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_48",
            "tgt_ix": "250-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_48",
            "tgt_ix": "250-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_48",
            "tgt_ix": "250-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_48",
            "tgt_ix": "250-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_48",
            "tgt_ix": "250-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_48",
            "tgt_ix": "250-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_44",
            "tgt_ix": "250-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_53",
            "tgt_ix": "250-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_54",
            "tgt_ix": "250-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_54",
            "tgt_ix": "250-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_44",
            "tgt_ix": "250-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_55",
            "tgt_ix": "250-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_57",
            "tgt_ix": "250-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_56",
            "tgt_ix": "250-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_56",
            "tgt_ix": "250-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_56",
            "tgt_ix": "250-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_0",
            "tgt_ix": "250-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_58",
            "tgt_ix": "250-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_60",
            "tgt_ix": "250-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_61",
            "tgt_ix": "250-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_59",
            "tgt_ix": "250-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_59",
            "tgt_ix": "250-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_59",
            "tgt_ix": "250-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_59",
            "tgt_ix": "250-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_0",
            "tgt_ix": "250-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_62",
            "tgt_ix": "250-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_63",
            "tgt_ix": "250-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_63",
            "tgt_ix": "250-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "250-ARR_v1_0",
            "tgt_ix": "250-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_1",
            "tgt_ix": "250-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_2",
            "tgt_ix": "250-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_2",
            "tgt_ix": "250-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_2",
            "tgt_ix": "250-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_2",
            "tgt_ix": "250-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_2",
            "tgt_ix": "250-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_2",
            "tgt_ix": "250-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_2",
            "tgt_ix": "250-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_3",
            "tgt_ix": "250-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_4",
            "tgt_ix": "250-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_4",
            "tgt_ix": "250-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_5",
            "tgt_ix": "250-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_5",
            "tgt_ix": "250-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_5",
            "tgt_ix": "250-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_5",
            "tgt_ix": "250-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_6",
            "tgt_ix": "250-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_6",
            "tgt_ix": "250-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_6",
            "tgt_ix": "250-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_6",
            "tgt_ix": "250-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_7",
            "tgt_ix": "250-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_7",
            "tgt_ix": "250-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_8",
            "tgt_ix": "250-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_8",
            "tgt_ix": "250-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_8",
            "tgt_ix": "250-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_8",
            "tgt_ix": "250-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_8",
            "tgt_ix": "250-ARR_v1_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_8",
            "tgt_ix": "250-ARR_v1_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_8",
            "tgt_ix": "250-ARR_v1_8@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_9",
            "tgt_ix": "250-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_10",
            "tgt_ix": "250-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_11",
            "tgt_ix": "250-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_11",
            "tgt_ix": "250-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_11",
            "tgt_ix": "250-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_12",
            "tgt_ix": "250-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_12",
            "tgt_ix": "250-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_12",
            "tgt_ix": "250-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_12",
            "tgt_ix": "250-ARR_v1_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_12",
            "tgt_ix": "250-ARR_v1_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_12",
            "tgt_ix": "250-ARR_v1_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_12",
            "tgt_ix": "250-ARR_v1_12@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_12",
            "tgt_ix": "250-ARR_v1_12@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_12",
            "tgt_ix": "250-ARR_v1_12@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_12",
            "tgt_ix": "250-ARR_v1_12@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_12",
            "tgt_ix": "250-ARR_v1_12@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_12",
            "tgt_ix": "250-ARR_v1_12@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_12",
            "tgt_ix": "250-ARR_v1_12@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_12",
            "tgt_ix": "250-ARR_v1_12@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_12",
            "tgt_ix": "250-ARR_v1_12@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_13",
            "tgt_ix": "250-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_13",
            "tgt_ix": "250-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_13",
            "tgt_ix": "250-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_14",
            "tgt_ix": "250-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_14",
            "tgt_ix": "250-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_14",
            "tgt_ix": "250-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_14",
            "tgt_ix": "250-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_14",
            "tgt_ix": "250-ARR_v1_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_14",
            "tgt_ix": "250-ARR_v1_14@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_15",
            "tgt_ix": "250-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_16",
            "tgt_ix": "250-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_17",
            "tgt_ix": "250-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_18",
            "tgt_ix": "250-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_18",
            "tgt_ix": "250-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_18",
            "tgt_ix": "250-ARR_v1_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_19",
            "tgt_ix": "250-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_20",
            "tgt_ix": "250-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_20",
            "tgt_ix": "250-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_21",
            "tgt_ix": "250-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_21",
            "tgt_ix": "250-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_21",
            "tgt_ix": "250-ARR_v1_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_21",
            "tgt_ix": "250-ARR_v1_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_22",
            "tgt_ix": "250-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_23",
            "tgt_ix": "250-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_24",
            "tgt_ix": "250-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_25",
            "tgt_ix": "250-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_26",
            "tgt_ix": "250-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_26",
            "tgt_ix": "250-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_27",
            "tgt_ix": "250-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_28",
            "tgt_ix": "250-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_28",
            "tgt_ix": "250-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_29",
            "tgt_ix": "250-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_30",
            "tgt_ix": "250-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_31",
            "tgt_ix": "250-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_32",
            "tgt_ix": "250-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_33",
            "tgt_ix": "250-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_34",
            "tgt_ix": "250-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_35",
            "tgt_ix": "250-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_36",
            "tgt_ix": "250-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_36",
            "tgt_ix": "250-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_37",
            "tgt_ix": "250-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_38",
            "tgt_ix": "250-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_39",
            "tgt_ix": "250-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_40",
            "tgt_ix": "250-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_41",
            "tgt_ix": "250-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_41",
            "tgt_ix": "250-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_41",
            "tgt_ix": "250-ARR_v1_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_42",
            "tgt_ix": "250-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_43",
            "tgt_ix": "250-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_43",
            "tgt_ix": "250-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_43",
            "tgt_ix": "250-ARR_v1_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_43",
            "tgt_ix": "250-ARR_v1_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_44",
            "tgt_ix": "250-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_45",
            "tgt_ix": "250-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_45",
            "tgt_ix": "250-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_45",
            "tgt_ix": "250-ARR_v1_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_45",
            "tgt_ix": "250-ARR_v1_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_45",
            "tgt_ix": "250-ARR_v1_45@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_45",
            "tgt_ix": "250-ARR_v1_45@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_45",
            "tgt_ix": "250-ARR_v1_45@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_45",
            "tgt_ix": "250-ARR_v1_45@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_45",
            "tgt_ix": "250-ARR_v1_45@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_46",
            "tgt_ix": "250-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_46",
            "tgt_ix": "250-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_46",
            "tgt_ix": "250-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_46",
            "tgt_ix": "250-ARR_v1_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_46",
            "tgt_ix": "250-ARR_v1_46@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_47",
            "tgt_ix": "250-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_47",
            "tgt_ix": "250-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_47",
            "tgt_ix": "250-ARR_v1_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_47",
            "tgt_ix": "250-ARR_v1_47@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_48",
            "tgt_ix": "250-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_49",
            "tgt_ix": "250-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_49",
            "tgt_ix": "250-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_49",
            "tgt_ix": "250-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_50",
            "tgt_ix": "250-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_51",
            "tgt_ix": "250-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_52",
            "tgt_ix": "250-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_53",
            "tgt_ix": "250-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_53",
            "tgt_ix": "250-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_54",
            "tgt_ix": "250-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_55",
            "tgt_ix": "250-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_56",
            "tgt_ix": "250-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_57",
            "tgt_ix": "250-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_57",
            "tgt_ix": "250-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_57",
            "tgt_ix": "250-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_57",
            "tgt_ix": "250-ARR_v1_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_57",
            "tgt_ix": "250-ARR_v1_57@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_57",
            "tgt_ix": "250-ARR_v1_57@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_57",
            "tgt_ix": "250-ARR_v1_57@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_57",
            "tgt_ix": "250-ARR_v1_57@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_57",
            "tgt_ix": "250-ARR_v1_57@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_57",
            "tgt_ix": "250-ARR_v1_57@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_57",
            "tgt_ix": "250-ARR_v1_57@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_57",
            "tgt_ix": "250-ARR_v1_57@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_58",
            "tgt_ix": "250-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_58",
            "tgt_ix": "250-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_59",
            "tgt_ix": "250-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_60",
            "tgt_ix": "250-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_60",
            "tgt_ix": "250-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_60",
            "tgt_ix": "250-ARR_v1_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_60",
            "tgt_ix": "250-ARR_v1_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_60",
            "tgt_ix": "250-ARR_v1_60@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_60",
            "tgt_ix": "250-ARR_v1_60@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_60",
            "tgt_ix": "250-ARR_v1_60@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_61",
            "tgt_ix": "250-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_61",
            "tgt_ix": "250-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_61",
            "tgt_ix": "250-ARR_v1_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_61",
            "tgt_ix": "250-ARR_v1_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_61",
            "tgt_ix": "250-ARR_v1_61@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_62",
            "tgt_ix": "250-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_62",
            "tgt_ix": "250-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_62",
            "tgt_ix": "250-ARR_v1_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_62",
            "tgt_ix": "250-ARR_v1_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_63",
            "tgt_ix": "250-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_64",
            "tgt_ix": "250-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_64",
            "tgt_ix": "250-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_64",
            "tgt_ix": "250-ARR_v1_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_64",
            "tgt_ix": "250-ARR_v1_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_64",
            "tgt_ix": "250-ARR_v1_64@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_65",
            "tgt_ix": "250-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_66",
            "tgt_ix": "250-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_67",
            "tgt_ix": "250-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_68",
            "tgt_ix": "250-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_69",
            "tgt_ix": "250-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_70",
            "tgt_ix": "250-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_71",
            "tgt_ix": "250-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_72",
            "tgt_ix": "250-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_73",
            "tgt_ix": "250-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_74",
            "tgt_ix": "250-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_75",
            "tgt_ix": "250-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_76",
            "tgt_ix": "250-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_77",
            "tgt_ix": "250-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_78",
            "tgt_ix": "250-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_79",
            "tgt_ix": "250-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_80",
            "tgt_ix": "250-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_81",
            "tgt_ix": "250-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_82",
            "tgt_ix": "250-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_83",
            "tgt_ix": "250-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_84",
            "tgt_ix": "250-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_85",
            "tgt_ix": "250-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_86",
            "tgt_ix": "250-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_87",
            "tgt_ix": "250-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_88",
            "tgt_ix": "250-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_89",
            "tgt_ix": "250-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_90",
            "tgt_ix": "250-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_91",
            "tgt_ix": "250-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_92",
            "tgt_ix": "250-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_93",
            "tgt_ix": "250-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_94",
            "tgt_ix": "250-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_95",
            "tgt_ix": "250-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_96",
            "tgt_ix": "250-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_97",
            "tgt_ix": "250-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_98",
            "tgt_ix": "250-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_99",
            "tgt_ix": "250-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_100",
            "tgt_ix": "250-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_101",
            "tgt_ix": "250-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_102",
            "tgt_ix": "250-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_103",
            "tgt_ix": "250-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_104",
            "tgt_ix": "250-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_105",
            "tgt_ix": "250-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_106",
            "tgt_ix": "250-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_107",
            "tgt_ix": "250-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_108",
            "tgt_ix": "250-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_109",
            "tgt_ix": "250-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_110",
            "tgt_ix": "250-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_111",
            "tgt_ix": "250-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_112",
            "tgt_ix": "250-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_113",
            "tgt_ix": "250-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "250-ARR_v1_114",
            "tgt_ix": "250-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1121,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "250-ARR",
        "version": 1
    }
}