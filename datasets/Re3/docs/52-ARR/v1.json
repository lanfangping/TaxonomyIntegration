{
    "nodes": [
        {
            "ix": "52-ARR_v1_0",
            "content": "CoMPM: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion Recognition in Conversation",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_2",
            "content": "As the use of interactive machines grow, the task of Emotion Recognition in Conversation (ERC) became more important. If the machine-generated sentences reflect emotion, more human-like sympathetic conversations are possible. Since emotion recognition in conversation is inaccurate if the previous utterances are not taken into account, many studies reflect the dialogue context to improve the performances. Many recent approaches show performance improvement by combining knowledge into modules learned from external structured data. However, structured data is difficult to access in non-English languages, making it difficult to extend to other languages. Therefore, we extract the pre-trained memory using the pre-trained language model as an extractor of external knowledge. We introduce CoMPM, which combines the speaker's pre-trained memory with the context model, and find that the pre-trained memory significantly improves the performance of the context model. CoMPM achieves the first or second performance on all data and is state-of-the-art among systems that do not leverage structured data. In addition, our method shows that it can be extended to other languages because structured knowledge is not required, unlike previous methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "52-ARR_v1_4",
            "content": "As the number of applications such as interactive chatbots or social media that are used by many users has recently increased dramatically, Emotion Recognition in Conversation (ERC) plays a more important role in natural language processing, and as a proof, a lot of research Ghosal et al., 2020;Jiao et al., 2020) has been conducted on the task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_5",
            "content": "The ERC module increases the quality of empathetic conversations with the users and can be utilized when sending tailored push messages to the users (Shin et al., 2019;Zandie and Mahoor, 2020;Lin et al., 2020). In addition, emotion recognition Figure 1: An example of MELD dataset can be effectively used for opinion mining, recommender systems, and healthcare systems where it can improve the service qualities by providing personalized results. As these interactive machines increase, the ERC module plays an increasingly important role. Figure 1 is an example of a conversation in which two speakers are angry at each other. The emotion of speaker B's utterance (\"How'd you get to that?\") is angry. If the system does not take into account previous utterances, it is difficult to properly recognize emotions. Like the previous studies (Ghosal et al., 2020), we show that the utterance-level emotion recognition, which does not consider the previous utterance, have limitations and experiments result in poor performances.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_6",
            "content": "Therefore, recent studies are attempting to recognize emotions while taking into account the previous utterances. Representatively, Dia-logueRNN recognizes the present emotion by tracking context from the previous utterances and the speaker's emotion. AGHMN (Jiao et al., 2020) considers the previous utterances through memory summarizing using GRU with attention.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_7",
            "content": "Many recent studies use external knowledge to improve the ERC performance. However, this external knowledge is often only available in English. In order to utilize the previous methods in languages of other countries, it is expensive and difficult to utilize because external knowledge data must be newly constructed. In recent NLP studies, due to the effectiveness of the pre-trained language model, it has already been developed in many countries. Since pre-trained language models are trained by unsupervised learning, these models are relatively usable approaches regardless of language types. Petroni et al. (2019) introduces that these language models can be used as knowledge bases and have many advantages over the structured knowledge bases. Based on these studies, we eliminate the dependence on structured external data used in cutting-edge systems and use a pre-trained language model as a feature extractor of knowledge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_8",
            "content": "CoMPM, introduced in this paper, is composed of two modules that take into account previous utterances in dialogue. (1) The first is a context embedding module (CoM) that reflects all previous utterances as context. CoM is an auto-regressive model that predicts the current emotion through attention between the previous utterances of the conversation and the current utterance. (2) The second is a pre-trained memory module (PM) that extracts memory from utterances. We use the output of the pre-trained language model as the memory embedding where the utterances are passed into the language model. We use the PM to help predict the emotion of the speaker by taking into account the speaker's linguistic preferences and characteristics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_9",
            "content": "We experiment on 4 different English ERC datasets. Multi-party datasets are MELD and EmoryNLP (Zahiri and Choi, 2018), and dyadic datasets are IEMOCAP (Busso et al., 2008) and DailyDialog (Li et al., 2017). CoMPM achieves the first or second performance according to the evaluation metric compared to all previous systems. We perform an ablation study on each module to show that the proposed approach is effective. Further experiments also show that our approach can be used in other languages and show the performance of CoMPM when the number of data is limited.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_10",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "52-ARR_v1_11",
            "content": "Many recent studies use external knowledge to improve the ERC performance. KET (Zhong et al., 2019) is used as external knowledge based on ConceptNet (Speer et al., 2017) and emotion lexicon NRC_VAD (Mohammad, 2018) as the commonsense knowledge. ConceptNet is a knowledge graph that connects words and phrases in natural language using labeled edges. NRC_VAD Lexicon has human ratings of valence, arousal, and dominance for more than 20,000 English words. COSMIC (Ghosal et al., 2020) and Psychological (Li et al., 2021) improve the performance of emotion recognition by extracting commonsense knowledge of the previous utterances. Commonsense knowledge feature is extracted and leveraged with COMET (Bosselut et al., 2019) trained with ATOMIC (The Atlas of Machine Commonsense) . ATOMIC has 9 sentence relation types with inferential if-then commonsense knowledge expressed in text. ToDKAT (Zhu et al., 2021) improves performance by combining commonsense knowledge using COMET and topic discovery using VHRED (Serban et al., 2017) to the model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_12",
            "content": "Ekman (Ekman, 1992) constructs taxonomy of six common emotions (Joy, Sadness, Fear, Anger, Surprise, and Disgust) from human facial expressions. In addition, Ekman explains that a multimodal view is important for multiple emotions recognition. The multi-modal data such as MELD and IEMOCAP are some of the available standard datasets for emotion recognition and they are composed of text, speech and vision-based data. Datcu and Rothkrantz (2014) uses speech and visual information to recognize emotions, and (Alm et al., 2005) attempts to recognize emotions based on text information. MELD and ICON (Hazarika et al., 2018a) show that the more multi-modal information is used, the better the performance and the text information plays the most important role. Multimodal information is not always given in most social media, especially in chatbot systems where they are mainly composed of text-based systems. In this work, we design and introduce a text-based emotion recognition system using neural networks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_13",
            "content": "In the previous studies, such as Hazarika et al. (2018b); Zadeh et al. (2017); , most works focused on dyadic-party conversation. However, as the multi-party conversation datasets including MELD and EmoryNLP have become available, a lot of recent research is being conducted on multi-party dialogues such as ; Jiao et al. (2020); Ghosal et al. (2020). In general, the multi-party conversations have higher speaker dependency than the dyadic-party dialogues, therefore have more conditions to consider and result in poor performance. Zhou et al. (2018); Zhang et al. (2018a) shows that commonsense knowledge is important for understanding conversations and generating appropriate responses. reports that the lack of external knowledge makes it difficult to classify implicit emotions from the conversation history. EDA (Bothe et al., 2020) expands the multi-modal emotion datasets by extracting dialog acts from MELD and IEMOCAP and finds out that there is a correlation between dialogue acts and emotion labels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_14",
            "content": "Approach",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "52-ARR_v1_15",
            "content": "Problem Statement",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "52-ARR_v1_16",
            "content": "In a conversation, M sequential utterances are given as [(u 1 , p u 1 ), (u 2 , p u 2 ), ..., (u M , p u M )]. u i is the utterance which the speaker p u i uttered, where p u i is one of the conversation participants. While p u i and p u j (i \u0338 = j) can be the same speaker, the minimum number of the unique conversation participants should be 2 or more. The ERC is a task of predicting the emotion e t of u t , the utterance of the t-th turn, given the previous utterances h t = {u 1 , ..., u t\u22121 }. Emotions are labeled as one of the predefined classes depending on the dataset, and the emotions we experimented with are either 6 or 7. We also experimented with a sentiment classification dataset which provides sentiment labels consisting of positive, negative and neutral.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_17",
            "content": "Model Overview",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "52-ARR_v1_18",
            "content": "Figure 2 shows an overview of our model. Our ERC neural network model is composed of two modules. The first is CoM which catches the underlying effect of all previous utterances on the current speaker's emotions. Therefore, we propose a context model to handle the relationship between the current and the previous utterances. The second one is PM that leverages only the speaker's previous utterances, through which we want to reflect the speaker's knowledge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_19",
            "content": "If the CoM and PM are based on different backbones, we consider them to be unaligned with respect to each other's output representations. Therefore, we design the PM to follow CoM so that the output representations of CoM and PM can mutually understand each other. If CoM and PM are based on different architectures, CoMPM is trained to understand each other's representations by matching dimensions using W p in Equation 4. The combination of CoM and PM is described in Section 4.5.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_20",
            "content": "CoM: Context Embedding Module",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "52-ARR_v1_21",
            "content": "The context embedding module predicts e t by considering all of the utterances before the t-th turn as the dialogue context. The example in Figure 2 shows how the model predicts the emotion of u 6 uttered by s A , given a conversation of three participants (s A , s B , s C ). The previous utterances are h 6 = {u 1 , \u2022 \u2022 \u2022 u 5 } and e 6 is predicted while considering the relationship between u 6 and h 6 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_22",
            "content": "We consider multi-party conversations where 2 or more speakers are involved. A special token <s P > is introduced to distinguish participants in the conversation and to handle the speaker's dependency where P is the set of participants. In other words, the same special token appears before the utterances of the same speaker.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_23",
            "content": "We use an Transformer encoder as a context model. In many natural language processing tasks, the effectiveness of the pre-trained language model has been proven, and we also set the initial state of the model to RoBERTa . RoBERTa is an unsupervised pre-trained model with largescale open-domain corpora of unlabeled text.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_24",
            "content": "We use the embedding of the special token <cls> to predict emotion. The <cls> token is concatenated at the end of the input and the output of the context model is as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_25",
            "content": "c t = CoM(< cls >, P :t\u22121 , h t , u t )(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_26",
            "content": "where P :t\u22121 is the set of speakers in the previous turns. c t \u2208 R 1\u00d7hc and h c is the dimension of CoM.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_27",
            "content": "PM: Pre-trained Memory Module",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "52-ARR_v1_28",
            "content": "External knowledge is known to play an important role in understanding conversation. Pre-trained language models can be trained on numerous corpora and be used as an external knowledge base. Inspired by previous studies that the speaker's knowledge helps to judge emotions, we extract and track pre-trained memory from the speaker's previous utterances to utilize the emotions of the current utterance u t . If the speaker has never appeared before the current turn, the result of the pre-trained memory is considered a zero vector. Since <cls> is mostly used for the task of classifying sentences, we use the embedding output of the <cls> token as a vector representing the utterance as follows: where p u i = p S , S is the speaker of the current utterance. k i \u2208 R 1\u00d7h k and h k is the dimension of PM.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_29",
            "content": "k i = PM(< cls >, u i ) (2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_30",
            "content": "CoMPM: Combination of CoM and PM",
            "ntype": "title",
            "meta": {
                "section": "3.5"
            }
        },
        {
            "ix": "52-ARR_v1_31",
            "content": "We combine CoM and PM to predict the speaker's emotion. In many dialogue systems (Zhang et al., 2018b;Ma et al., 2019), it is known that utterances close to the current turn are important for response. Therefore, we assume that utterances close to the current utterance will be important in emotional recognition.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_32",
            "content": "Tracking Method",
            "ntype": "title",
            "meta": {
                "section": "3.5.1"
            }
        },
        {
            "ix": "52-ARR_v1_33",
            "content": "We use k i tracking method using GRU. The tracking method assumes that the importance of all previous speaker utterances to the current emotion is not equal and varies with the distance of the current utterance. In other words, since the flow of conversation changes as it progresses, the effect on emotion may differ depending on the distance from the current utterance. We track and capture the sequential position information of k i using a unidirectional GRU:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_34",
            "content": "kt t = GRU(k i 1 , k i 2 , ..., k in ) (3",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_35",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_36",
            "content": "where t is the turn index of the current utterance, n is the number of previous utterances of the speaker, and i s (s = 1, 2, ..., n) is each turn uttered. kt t \u2208 R 1\u00d7hc is the output of k in and as a result, the knowledge of distant utterance is diluted and the effect on the current utterance is reduced.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_37",
            "content": "GRU is composed of 2-layers, the dimension of the output vector is h c , and the dropout is set to 0.3 during training. Finally, the output vector o t is obtained by adding kt t and c t in Equation 4.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_38",
            "content": "o t = c t + W p (kt t )(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_39",
            "content": "where, W p is a matrix that projects the pretrained memory to the dimension of the context output, and is used only when PM and CoM are different pre-trained language models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_40",
            "content": "Emotion Prediction",
            "ntype": "title",
            "meta": {
                "section": "3.5.2"
            }
        },
        {
            "ix": "52-ARR_v1_41",
            "content": "Softmax is applied to the vector multiplied by o t and the linear matrix W o \u2208 R he\u00d7hc to obtain the probability distribution of emotion classes, where h e is the number of emotion classes. e t is the predicted emotion class that corresponds to the index of the largest probability from the emotion class distribution.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_42",
            "content": "P (e) = softmax(W o (o t ))(5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_43",
            "content": "The objective is to minimize the cross entropy loss so that e t is the same as the ground truth emotional label.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_44",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "52-ARR_v1_45",
            "content": "Dataset",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "52-ARR_v1_46",
            "content": "We experiment on four benchmark datasets. MELD and EmoryNLP (Zahiri and Choi, 2018) are multi-party datasets, while IEMOCAP (Busso et al., 2008) and DailyDialog (Li et al., 2017) are dyadic-party datasets. The statistics of the dataset are shown in Table 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_47",
            "content": "IEMOCAP is a dataset involving 10 speakers, and each conversation involves 2 speakers and the emotion-inventory is given as \"happy, sad, angry, excited, frustrated and neutral\". The train and development dataset is a conversation involving the previous eight speakers, and the train and development are divided into random splits at a ratio of 9:1. The test dataset is a conversation involving two later speakers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_48",
            "content": "DailyDialog is a dataset of daily conversations between two speakers and the emotion-inventory is given as \"anger, disgust, fear, joy, surprise, sadness and neutral\". Since more than 82% of the data are tagged as neutral, neutral emotions are excluded when evaluating systems with Micro-F1 as did in the previous studies.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_49",
            "content": "MELD is a dataset based on Friends TV show and provides two taxonomy: emotion and sentiment. MELD's emotion-inventory is given as \"anger, disgust, sadness, joy, surprise, fear and neutrality\" following Ekman (Ekman, 1992) and sentiment-inventory is given as \"positive, negative and neutral\".",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_50",
            "content": "EmoryNLP, like MELD, is also a dataset based on Friends TV show, but the emotion-inventory is given as \"joyful, peaceful, powerful, scared, mad, sad and neutral\". Sentiment labels are not provided, but sentiment classes can be grouped as follows: positive: {joyful, peaceful, powerful}, negative: {scared, mad, sad}, neutral: {neutral}",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_51",
            "content": "Training Setup",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "52-ARR_v1_52",
            "content": "We use the pre-trained model from the huggingface library 1 . The optimizer is AdamW and the learning rate is 1e-5 as an initial value. The learning rate scheduler used for training is get_linear_schedule_with_warmup, and the maximum value of 10 is used for the gradient clipping. We select the model with the best performance on the validation set. All experiments are conducted on one V100 GPU with 32GB memory.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_53",
            "content": "Previous Method",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "52-ARR_v1_54",
            "content": "We show that the proposed approach is effective by comparing it with various baselines and the stateof-the-art methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_55",
            "content": "KET (Zhong et al., 2019) is a Knowledge Enriched Transformer that reflects contextual utterances with a hierarchical self-attention and leverages external commonsense knowledge by using a context-aware affective graph attention mechanism.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_56",
            "content": "DialogueRNN uses a GRU network to keep track of the individual party states in the conversation to predict emotions. This model assumes that there are three factors in emotion prediction: the speaker, the context from the preceding utterances and the emotion of the preceding utterances. Also, Ghosal et al. (2020) shows the performance of RoBERTa+DialogueRNN when the vectors of the tokens are extracted with a pretrained RoBERTa.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_57",
            "content": "RGAT+P (Ishiwatari et al., 2020) (relational graph attention networks) proposes relational position encodings with sequential information reflecting the relational graph structure, which shows that both the speaker dependency and the sequential information can be captured.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_58",
            "content": "HiTrans (Li et al., 2020) proposes a transformerbased context-and speaker-sensitive model. Hi-Trans utilize BERT as the low-level transformer to generate local utterance representations, and feed them into another high-level transformer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_59",
            "content": "COSMIC (Ghosal et al., 2020) incorporates different elements of commonsense such as mental states, events and causal relations, and learns the relations between participants in the conversation. This model uses pre-trained RoBERTa as a feature extractor and leverages COMET trained with ATOMIC as the commonsense knowledge. ERMC-DisGCN (Sun et al., 2021) proposes a discourse-aware graph neural network that utilizes self-speaker dependency of interlocutors as a relational convolution and informative cues of dependent utterances as a gated convolution.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_60",
            "content": "Psychological (Li et al., 2021) uses commonsense knowledge as enrich edges and processes it with graph transformer. Psychological performs emotion recognition by utilizing intention of utterances from not only past contexts but also future context.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_61",
            "content": "DialogueCRN (Hu et al., 2021) introduces an intuitive retrieving process, the reasoning module, which understands both situation-level and speakerlevel contexts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_62",
            "content": "ToDKAT (Zhu et al., 2021) proposes a language model with topic detection added, and improves performance by combining it with commonsense knowledge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_63",
            "content": "Result and Analysis",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "52-ARR_v1_64",
            "content": "Table 2 shows the performance of the previous methods and our models. CoM used alone does not leverage PM and predicts emotions by considering only the dialogue context. PM, if used alone, does not consider the context and predicts emotions only with the utterance of the current turn. CoMPM is a model in which both CoM and PM parameters are updated in the initial state of the pre-trained LM. CoMPM(f) is a model in which PM parameters are frozen in the initial state (pre-trained LM) and is not trained further, and CoMPM(s) is a model in which PM is trained from scratch.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_65",
            "content": "The effect of PM can be confirmed through the performance comparison between CoM and CoMPM, and the effect of CoM can be confirmed by comparing the results of CoM and PM. Since PM does not consider context, it showed worse performance than CoM, and the performance gap is larger in the IEMOCAP dataset with a higher average number of conversation turns. As a result, we show that the combination of CoM and PM is effective in achieving better performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_66",
            "content": "We confirm the effect of PM structure in the model through the performance of CoMPM(s). If PM parameters are not frozen and are instead randomly initialized (i.e. PM(s)), the performance deteriorates. CoMPM(s) performs worse than CoMPM, and even performs worse than CoM on the other datasets except for MELD. That is, PM(s) cannot be regarded as a pre-trained memory because the parameters are randomly initialized, and simply increasing the model complexity does not help to improve the performance. CoMPM(f) shows similar performance to CoMPM and achieves better performance depending on the data. PM(f) is not fine-tuned on the data, but it extracts general pre-trained memory from a pretrained language model. The comparison between PM and PM(f) will be further described in Section 4.6. We regard pre-trained memory as compressed knowledge, which can play a role similar to external knowledge used in cutting-edge systems.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_67",
            "content": "The best performance of our approaches is CoMPM or CoMPM(f), both of which combine pre-trained memory. We achieve state-of-the-art performance among all systems that do not leverage structured external data and achieve the first or second performance even including systems that leverage external data. Therefore, our approach can be extended to other languages without structured external data as well, which is described in Section 4.7.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_68",
            "content": "Combinations of CoM and PM",
            "ntype": "title",
            "meta": {
                "section": "4.5"
            }
        },
        {
            "ix": "52-ARR_v1_69",
            "content": "We experiment with the effect of pre-trained memory of different language models. To eliminate the influence of the PM structure, we freeze the parameters of PM and use it as a feature extractor. Table 3 shows the performance of the pretrained memory extracted by the different language models. If PM and CoM are based on different backbones, the pre-trained memory is projected through W p as the dimension of the context output. RoBERTa+BERT and RoBERTa+GPT2 (combination of CoM and PM(f)) have lower performance than RoBERTa+RoBERTa, which is inferred because pre-trained memory of RoBERTa contains richer information than BERT and GPT2. Since there is a lot of training data in the diallydialog and W p is fine-tuned to the data to mutually understand the pre-trained memory and context representation. Therefore, we infer that performance does not decrease even if the PM changes from the dailydialog. However, even if other PMs are used, the performance is improved compared to using only CoM, so the pre-trained memory of other language models is also effective for emotion recognition.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_70",
            "content": "BERT+RoBERTa has a larger performance decrease than RoBERTa+BERT. In particular, in IEMOCAP data with a long average number of turns in the context, the performance deteriorates significantly. In addition, the performance of BERT+RoBERTa is lower than CoM (RoBERTa), which supports that the performance of CoM is a more important factor than the use of pre-trained memory. In other words, we confirm that CoM is more important than PM in our system for performance, and it is effective to focus on context modeling rather than external knowledge in the study of emotion recognition in conversation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_71",
            "content": "Training with Less Data",
            "ntype": "title",
            "meta": {
                "section": "4.6"
            }
        },
        {
            "ix": "52-ARR_v1_72",
            "content": "CoMPM is an approach that eliminates dependence on external sources and is easily extensible to any language. However, the insufficient number of emotional data available in other countries (or actual service) remains a problem. Therefore, we conduct additional experiments according to the use ratio Table 2 shows that CoMPM(f) achieves better performance than CoMPM in the emotion classification of IMEOCAP and EmoryNLP, which has fewer training data than other settings. On the other hand, if there is a lot of training data, CoMPM shows better performance. Figure 3 shows that as the number of data decreases, CoMPM(f) shows better results than CoMPM, which indicates that it is better to freeze the parameters of PM when the number of training data is insufficient. Therefore, if there is a lot of training data in the real-world application, CoMPM is expected to achieve good performance, otherwise it is CoMPM(f).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_73",
            "content": "ERC in other languages",
            "ntype": "title",
            "meta": {
                "section": "4.7"
            }
        },
        {
            "ix": "52-ARR_v1_74",
            "content": "Previous studies mostly utilize external knowledge to improve performance, but these approaches require additional publicly available data, which are mainly available for English. Indeed, structured knowledge and ERC data are lacking in other languages. Our approach can be extended to other languages without building additional external knowledge and achieves better performance than simply using a pre-trained model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_75",
            "content": "Korean Dataset",
            "ntype": "title",
            "meta": {
                "section": "4.7.1"
            }
        },
        {
            "ix": "52-ARR_v1_76",
            "content": "We constructed data composed of two speakers in Korean, and emotion-inventory is given as \"surprise, fear, ambiguous, sad, disgust, joy, bored, embarrassed, neutral\". The total number of sessions is 1000, and the average number of utterance turns The value in parentheses is the performance difference from the original CoMPM(f) (RoBERTa + RoBERTa). We use the bert-large-uncased and GPT2-medium versions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_77",
            "content": "is 13.4. We use the data randomly divided into train:dev:test in a ratio of 8:1:1. This dataset is for actual service and is not released to the public. In Korean, our results are shown in Table 4. The backbone of CoM and PM is Korean-BERT owned by the company, respectively. In the Korean dataset, like the English dataset, the performance is good in the order of CoMPM, CoM, and PM. Our approach simply shows a significant performance improvement on baselines that are fine-tuned to the language model and works well for other languages as well as for English data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_78",
            "content": "Results in the Korean Dataset",
            "ntype": "title",
            "meta": {
                "section": "4.7.2"
            }
        },
        {
            "ix": "52-ARR_v1_79",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "52-ARR_v1_80",
            "content": "We propose CoMPM that leverages pre-trained memory using a pre-trained language model. CoMPM consists of a context embedding module (CoM) and a pre-trained memory module (PM), and the experimental results show that each module is effective in improving the performance. CoMPM outperforms baselines on both dyadic-party and multi-party datasets and achieves state-of-the-art among systems that do not use external knowledge. In addition, CoMPM achieves performance comparable to cutting-edge systems that leverage structured external knowledge, which is the effect of pre-trained memory of the language model. By combining other pre-trained memories, we find that the pre-trained memory extracted with RoBERTa is richer and more effective than the pre-trained memory extracted with BERT or GPT2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_81",
            "content": "Since we believe that pre-trained memory is proportional to the performance of a language model, a language model with a large training corpus and many parameters is considered to be more effective. However, we find that context modeling is more important than pre-trained memory for emotion recognition in conversation, and future research will focus on context modeling.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_82",
            "content": "Additionally, our approach achieves competitive performance and does not require externally structured data. Therefore, we show that it can be easily extended to Korean as well as English, and it is expected to be effective in other countries.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "52-ARR_v1_83",
            "content": "Cecilia Ovesdotter Alm, Dan Roth, Richard Sproat, Emotions from text: Machine learning for textbased emotion prediction, 2005, Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Cecilia Ovesdotter Alm",
                    "Dan Roth",
                    "Richard Sproat"
                ],
                "title": "Emotions from text: Machine learning for textbased emotion prediction",
                "pub_date": "2005",
                "pub_title": "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "52-ARR_v1_84",
            "content": "Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, Yejin Choi, COMET: Commonsense transformers for automatic knowledge graph construction, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Antoine Bosselut",
                    "Hannah Rashkin",
                    "Maarten Sap",
                    "Chaitanya Malaviya",
                    "Asli Celikyilmaz",
                    "Yejin Choi"
                ],
                "title": "COMET: Commonsense transformers for automatic knowledge graph construction",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_85",
            "content": "Chandrakant Bothe, Cornelius Weber, Sven Magg, Stefan Wermter, EDA: Enriching emotional dialogue acts using an ensemble of neural annotators, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Chandrakant Bothe",
                    "Cornelius Weber",
                    "Sven Magg",
                    "Stefan Wermter"
                ],
                "title": "EDA: Enriching emotional dialogue acts using an ensemble of neural annotators",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 12th Language Resources and Evaluation Conference",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_86",
            "content": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette Chang, Sungbok Lee, Shrikanth Narayanan, Iemocap: interactive emotional dyadic motion capture database, 2008, Lang. Resour. Evaluation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Carlos Busso",
                    "Murtaza Bulut",
                    "Chi-Chun Lee",
                    "Abe Kazemzadeh",
                    "Emily Mower",
                    "Samuel Kim",
                    "Jeannette Chang",
                    "Sungbok Lee",
                    "Shrikanth Narayanan"
                ],
                "title": "Iemocap: interactive emotional dyadic motion capture database",
                "pub_date": "2008",
                "pub_title": "Lang. Resour. Evaluation",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_87",
            "content": "UNKNOWN, None, 2014, Semantic audiovisual data fusion for automatic emotion recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "2014",
                "pub_title": "Semantic audiovisual data fusion for automatic emotion recognition",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_88",
            "content": "UNKNOWN, None, 1992, An argument for basic emotions. Cognition & Emotion, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": "1992",
                "pub_title": "An argument for basic emotions. Cognition & Emotion",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_89",
            "content": "Deepanway Ghosal, Navonil Majumder, Alexander Gelbukh, Rada Mihalcea, and Soujanya Poria. 2020. COSMIC: COmmonSense knowledge for eMotion identification in conversations, , Findings of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Deepanway Ghosal",
                    "Navonil Majumder"
                ],
                "title": "Alexander Gelbukh, Rada Mihalcea, and Soujanya Poria. 2020. COSMIC: COmmonSense knowledge for eMotion identification in conversations",
                "pub_date": null,
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "52-ARR_v1_90",
            "content": "Devamanyu Hazarika, Soujanya Poria, Rada Mihalcea, Erik Cambria, Roger Zimmermann, ICON: Interactive conversational memory network for multimodal emotion detection, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Devamanyu Hazarika",
                    "Soujanya Poria",
                    "Rada Mihalcea",
                    "Erik Cambria",
                    "Roger Zimmermann"
                ],
                "title": "ICON: Interactive conversational memory network for multimodal emotion detection",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_91",
            "content": "Devamanyu Hazarika, Soujanya Poria, Amir Zadeh, Erik Cambria, Louis-Philippe Morency, Roger Zimmermann, Conversational memory network for emotion recognition in dyadic dialogue videos, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Devamanyu Hazarika",
                    "Soujanya Poria",
                    "Amir Zadeh",
                    "Erik Cambria",
                    "Louis-Philippe Morency",
                    "Roger Zimmermann"
                ],
                "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "52-ARR_v1_92",
            "content": "Dou Hu, Lingwei Wei, Xiaoyong Huai, Dia-logueCRN: Contextual reasoning networks for emotion recognition in conversations, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Dou Hu",
                    "Lingwei Wei",
                    "Xiaoyong Huai"
                ],
                "title": "Dia-logueCRN: Contextual reasoning networks for emotion recognition in conversations",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "52-ARR_v1_93",
            "content": "Taichi Ishiwatari, Yuki Yasuda, Taro Miyazaki, Jun Goto, Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Taichi Ishiwatari",
                    "Yuki Yasuda",
                    "Taro Miyazaki",
                    "Jun Goto"
                ],
                "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "52-ARR_v1_94",
            "content": "Wenxiang Jiao, Michael Lyu, Irwin King, Real-time emotion recognition via attention gated hierarchical memory network, 2020, The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Wenxiang Jiao",
                    "Michael Lyu",
                    "Irwin King"
                ],
                "title": "Real-time emotion recognition via attention gated hierarchical memory network",
                "pub_date": "2020",
                "pub_title": "The Thirty-Fourth AAAI Conference on Artificial Intelligence",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "52-ARR_v1_95",
            "content": "Jiangnan Li, Zheng Lin, Peng Fu, Weiping Wang, Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Jiangnan Li",
                    "Zheng Lin",
                    "Peng Fu",
                    "Weiping Wang"
                ],
                "title": "Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_96",
            "content": "Jingye Li, Donghong Ji, Fei Li, Meishan Zhang, Yijiang Liu, HiTrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Jingye Li",
                    "Donghong Ji",
                    "Fei Li",
                    "Meishan Zhang",
                    "Yijiang Liu"
                ],
                "title": "HiTrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_97",
            "content": "Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, Shuzi Niu, DailyDialog: A manually labelled multi-turn dialogue dataset, 2017, Proceedings of the Eighth International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Yanran Li",
                    "Hui Su",
                    "Xiaoyu Shen",
                    "Wenjie Li",
                    "Ziqiang Cao",
                    "Shuzi Niu"
                ],
                "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Eighth International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "52-ARR_v1_98",
            "content": "Zhaojiang Lin, Peng Xu, Genta Indra Winata, Farhad Bin Siddique, Zihan Liu, Jamin Shin, Pascale Fung, Caire: An end-to-end empathetic chatbot, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Zhaojiang Lin",
                    "Peng Xu",
                    "Genta Indra Winata",
                    "Farhad Bin Siddique",
                    "Zihan Liu",
                    "Jamin Shin",
                    "Pascale Fung"
                ],
                "title": "Caire: An end-to-end empathetic chatbot",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_99",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_100",
            "content": "Zeming Liu, Haifeng Wang, Zheng-Yu Niu, Hua Wu, Wanxiang Che, Ting Liu, Towards conversational recommendation over multi-type dialogs, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Zeming Liu",
                    "Haifeng Wang",
                    "Zheng-Yu Niu",
                    "Hua Wu",
                    "Wanxiang Che",
                    "Ting Liu"
                ],
                "title": "Towards conversational recommendation over multi-type dialogs",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_101",
            "content": "Wentao Ma, Yiming Cui, Nan Shao, Su He, Wei-Nan Zhang, Ting Liu, Shijin Wang, Guoping Hu, TripleNet: Triple attention network for multiturn response selection in retrieval-based chatbots, 2019, Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Wentao Ma",
                    "Yiming Cui",
                    "Nan Shao",
                    "Su He",
                    "Wei-Nan Zhang",
                    "Ting Liu",
                    "Shijin Wang",
                    "Guoping Hu"
                ],
                "title": "TripleNet: Triple attention network for multiturn response selection in retrieval-based chatbots",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_102",
            "content": "Navonil Majumder, Soujanya Poria, Devamanyu Hazarika, Rada Mihalcea, Alexander Gelbukh, Erik Cambria, Dialoguernn: An attentive rnn for emotion detection in conversations, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Navonil Majumder",
                    "Soujanya Poria",
                    "Devamanyu Hazarika",
                    "Rada Mihalcea",
                    "Alexander Gelbukh",
                    "Erik Cambria"
                ],
                "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
                "pub_date": "2019",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_103",
            "content": "UNKNOWN, None, 2018, Obtaining reliable human ratings of valence, arousal, and dominance for, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Obtaining reliable human ratings of valence, arousal, and dominance for",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_104",
            "content": ", English words, , Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [],
                "title": "English words",
                "pub_date": null,
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "52-ARR_v1_105",
            "content": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Language models as knowledge bases?, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Fabio Petroni",
                    "Tim Rockt\u00e4schel",
                    "Sebastian Riedel",
                    "Patrick Lewis",
                    "Anton Bakhtin",
                    "Yuxiang Wu",
                    "Alexander Miller"
                ],
                "title": "Language models as knowledge bases?",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "52-ARR_v1_106",
            "content": "S Poria, N Majumder, R Mihalcea, E Hovy, Emotion recognition in conversation: Research challenges, datasets, and recent advances, 2019, IEEE Access, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "S Poria",
                    "N Majumder",
                    "R Mihalcea",
                    "E Hovy"
                ],
                "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
                "pub_date": "2019",
                "pub_title": "IEEE Access",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_107",
            "content": "Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, MELD: A multimodal multi-party dataset for emotion recognition in conversations, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Soujanya Poria",
                    "Devamanyu Hazarika",
                    "Navonil Majumder",
                    "Gautam Naik"
                ],
                "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "52-ARR_v1_108",
            "content": "Maarten Sap, Emily Ronan Le Bras, Chandra Allaway, Nicholas Bhagavatula, Hannah Lourie, Brendan Rashkin, Noah Roof, Yejin Smith,  Choi, Atomic: An atlas of machine commonsense for ifthen reasoning, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Maarten Sap",
                    "Emily Ronan Le Bras",
                    "Chandra Allaway",
                    "Nicholas Bhagavatula",
                    "Hannah Lourie",
                    "Brendan Rashkin",
                    "Noah Roof",
                    "Yejin Smith",
                    " Choi"
                ],
                "title": "Atomic: An atlas of machine commonsense for ifthen reasoning",
                "pub_date": "2019",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_109",
            "content": "Iulian Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron Courville, Yoshua Bengio, A hierarchical latent variable encoderdecoder model for generating dialogues, 2017, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Iulian Serban",
                    "Alessandro Sordoni",
                    "Ryan Lowe",
                    "Laurent Charlin",
                    "Joelle Pineau",
                    "Aaron Courville",
                    "Yoshua Bengio"
                ],
                "title": "A hierarchical latent variable encoderdecoder model for generating dialogues",
                "pub_date": "2017",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_110",
            "content": "UNKNOWN, None, 2019, Happybot: Generating empathetic dialogue responses by improving user experience lookahead, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Happybot: Generating empathetic dialogue responses by improving user experience lookahead",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_111",
            "content": "Robyn Speer, Joshua Chin, Catherine Havasi, Conceptnet 5.5: An open multilingual graph of general knowledge, 2017, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Robyn Speer",
                    "Joshua Chin",
                    "Catherine Havasi"
                ],
                "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "52-ARR_v1_112",
            "content": "Yang Sun, Nan Yu, Guohong Fu, A discourseaware graph neural network for emotion recognition in multi-party conversation, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Yang Sun",
                    "Nan Yu",
                    "Guohong Fu"
                ],
                "title": "A discourseaware graph neural network for emotion recognition in multi-party conversation",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2021",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "52-ARR_v1_113",
            "content": "Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, Louis-Philippe Morency, Tensor fusion network for multimodal sentiment analysis, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Amir Zadeh",
                    "Minghai Chen",
                    "Soujanya Poria",
                    "Erik Cambria",
                    "Louis-Philippe Morency"
                ],
                "title": "Tensor fusion network for multimodal sentiment analysis",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "52-ARR_v1_114",
            "content": "M Sayyed, Jinho Zahiri,  Choi, Emotion detection on TV show transcripts with sequence-based convolutional neural networks, 2018-02-02, The Workshops of the The Thirty-Second AAAI Conference on Artificial Intelligence, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "M Sayyed",
                    "Jinho Zahiri",
                    " Choi"
                ],
                "title": "Emotion detection on TV show transcripts with sequence-based convolutional neural networks",
                "pub_date": "2018-02-02",
                "pub_title": "The Workshops of the The Thirty-Second AAAI Conference on Artificial Intelligence",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "52-ARR_v1_115",
            "content": "Rohola Zandie, Mohammad Mahoor, Emptransfo: A multi-head transformer architecture for creating empathetic dialog systems, 2020-05-17, Proceedings of the Thirty-Third International Florida Artificial Intelligence Research Society Conference, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Rohola Zandie",
                    "Mohammad Mahoor"
                ],
                "title": "Emptransfo: A multi-head transformer architecture for creating empathetic dialog systems",
                "pub_date": "2020-05-17",
                "pub_title": "Proceedings of the Thirty-Third International Florida Artificial Intelligence Research Society Conference",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "52-ARR_v1_116",
            "content": "Dong Zhang, Liangqing Wu, Changlong Sun, Shoushan Li, Qiaoming Zhu, Guodong Zhou, Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations, 2019, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Dong Zhang",
                    "Liangqing Wu",
                    "Changlong Sun",
                    "Shoushan Li",
                    "Qiaoming Zhu",
                    "Guodong Zhou"
                ],
                "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_117",
            "content": "Yuxiang Zhang, Jiamei Fu, Dongyu She, Ying Zhang, Senzhang Wang, Jufeng Yang, Text emotion distribution learning via multi-task convolutional neural network, 2018, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Yuxiang Zhang",
                    "Jiamei Fu",
                    "Dongyu She",
                    "Ying Zhang",
                    "Senzhang Wang",
                    "Jufeng Yang"
                ],
                "title": "Text emotion distribution learning via multi-task convolutional neural network",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_118",
            "content": "Zhuosheng Zhang, Jiangtong Li, Pengfei Zhu, Hai Zhao, Gongshen Liu, Modeling multi-turn conversation with deep utterance aggregation, 2018, Proceedings of the 27th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Zhuosheng Zhang",
                    "Jiangtong Li",
                    "Pengfei Zhu",
                    "Hai Zhao",
                    "Gongshen Liu"
                ],
                "title": "Modeling multi-turn conversation with deep utterance aggregation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 27th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_119",
            "content": "Peixiang Zhong, Di Wang, Chunyan Miao, Knowledge-enriched transformer for emotion detection in textual conversations, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Peixiang Zhong",
                    "Di Wang",
                    "Chunyan Miao"
                ],
                "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_120",
            "content": "Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, Xiaoyan Zhu, Commonsense knowledge aware conversation generation with graph attention, 2018, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Hao Zhou",
                    "Tom Young",
                    "Minlie Huang",
                    "Haizhou Zhao",
                    "Jingfang Xu",
                    "Xiaoyan Zhu"
                ],
                "title": "Commonsense knowledge aware conversation generation with graph attention",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18",
                "pub": null
            }
        },
        {
            "ix": "52-ARR_v1_121",
            "content": "Lixing Zhu, Gabriele Pergola, Lin Gui, Deyu Zhou, Yulan He, Topic-driven and knowledgeaware transformer for dialogue emotion detection, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Lixing Zhu",
                    "Gabriele Pergola",
                    "Lin Gui",
                    "Deyu Zhou",
                    "Yulan He"
                ],
                "title": "Topic-driven and knowledgeaware transformer for dialogue emotion detection",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "52-ARR_v1_0@0",
            "content": "CoMPM: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion Recognition in Conversation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_0",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_2@0",
            "content": "As the use of interactive machines grow, the task of Emotion Recognition in Conversation (ERC) became more important.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_2",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_2@1",
            "content": "If the machine-generated sentences reflect emotion, more human-like sympathetic conversations are possible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_2",
            "start": 118,
            "end": 224,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_2@2",
            "content": "Since emotion recognition in conversation is inaccurate if the previous utterances are not taken into account, many studies reflect the dialogue context to improve the performances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_2",
            "start": 226,
            "end": 406,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_2@3",
            "content": "Many recent approaches show performance improvement by combining knowledge into modules learned from external structured data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_2",
            "start": 408,
            "end": 533,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_2@4",
            "content": "However, structured data is difficult to access in non-English languages, making it difficult to extend to other languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_2",
            "start": 535,
            "end": 657,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_2@5",
            "content": "Therefore, we extract the pre-trained memory using the pre-trained language model as an extractor of external knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_2",
            "start": 659,
            "end": 778,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_2@6",
            "content": "We introduce CoMPM, which combines the speaker's pre-trained memory with the context model, and find that the pre-trained memory significantly improves the performance of the context model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_2",
            "start": 780,
            "end": 968,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_2@7",
            "content": "CoMPM achieves the first or second performance on all data and is state-of-the-art among systems that do not leverage structured data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_2",
            "start": 970,
            "end": 1103,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_2@8",
            "content": "In addition, our method shows that it can be extended to other languages because structured knowledge is not required, unlike previous methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_2",
            "start": 1105,
            "end": 1247,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_4@0",
            "content": "As the number of applications such as interactive chatbots or social media that are used by many users has recently increased dramatically, Emotion Recognition in Conversation (ERC) plays a more important role in natural language processing, and as a proof, a lot of research Ghosal et al., 2020;Jiao et al., 2020) has been conducted on the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_4",
            "start": 0,
            "end": 345,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_5@0",
            "content": "The ERC module increases the quality of empathetic conversations with the users and can be utilized when sending tailored push messages to the users (Shin et al., 2019;Zandie and Mahoor, 2020;Lin et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_5",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_5@1",
            "content": "In addition, emotion recognition Figure 1: An example of MELD dataset can be effectively used for opinion mining, recommender systems, and healthcare systems where it can improve the service qualities by providing personalized results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_5",
            "start": 211,
            "end": 445,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_5@2",
            "content": "As these interactive machines increase, the ERC module plays an increasingly important role.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_5",
            "start": 447,
            "end": 538,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_5@3",
            "content": "Figure 1 is an example of a conversation in which two speakers are angry at each other.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_5",
            "start": 540,
            "end": 626,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_5@4",
            "content": "The emotion of speaker B's utterance (\"How'd you get to that?\") is angry.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_5",
            "start": 628,
            "end": 700,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_5@5",
            "content": "If the system does not take into account previous utterances, it is difficult to properly recognize emotions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_5",
            "start": 702,
            "end": 810,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_5@6",
            "content": "Like the previous studies (Ghosal et al., 2020), we show that the utterance-level emotion recognition, which does not consider the previous utterance, have limitations and experiments result in poor performances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_5",
            "start": 812,
            "end": 1023,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_6@0",
            "content": "Therefore, recent studies are attempting to recognize emotions while taking into account the previous utterances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_6",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_6@1",
            "content": "Representatively, Dia-logueRNN recognizes the present emotion by tracking context from the previous utterances and the speaker's emotion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_6",
            "start": 114,
            "end": 250,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_6@2",
            "content": "AGHMN (Jiao et al., 2020) considers the previous utterances through memory summarizing using GRU with attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_6",
            "start": 252,
            "end": 363,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_7@0",
            "content": "Many recent studies use external knowledge to improve the ERC performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_7",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_7@1",
            "content": "However, this external knowledge is often only available in English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_7",
            "start": 75,
            "end": 142,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_7@2",
            "content": "In order to utilize the previous methods in languages of other countries, it is expensive and difficult to utilize because external knowledge data must be newly constructed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_7",
            "start": 144,
            "end": 316,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_7@3",
            "content": "In recent NLP studies, due to the effectiveness of the pre-trained language model, it has already been developed in many countries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_7",
            "start": 318,
            "end": 448,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_7@4",
            "content": "Since pre-trained language models are trained by unsupervised learning, these models are relatively usable approaches regardless of language types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_7",
            "start": 450,
            "end": 596,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_7@5",
            "content": "Petroni et al. (2019) introduces that these language models can be used as knowledge bases and have many advantages over the structured knowledge bases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_7",
            "start": 598,
            "end": 749,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_7@6",
            "content": "Based on these studies, we eliminate the dependence on structured external data used in cutting-edge systems and use a pre-trained language model as a feature extractor of knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_7",
            "start": 751,
            "end": 932,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_8@0",
            "content": "CoMPM, introduced in this paper, is composed of two modules that take into account previous utterances in dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_8",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_8@1",
            "content": "(1) The first is a context embedding module (CoM) that reflects all previous utterances as context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_8",
            "start": 116,
            "end": 214,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_8@2",
            "content": "CoM is an auto-regressive model that predicts the current emotion through attention between the previous utterances of the conversation and the current utterance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_8",
            "start": 216,
            "end": 377,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_8@3",
            "content": "(2) The second is a pre-trained memory module (PM) that extracts memory from utterances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_8",
            "start": 379,
            "end": 466,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_8@4",
            "content": "We use the output of the pre-trained language model as the memory embedding where the utterances are passed into the language model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_8",
            "start": 468,
            "end": 599,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_8@5",
            "content": "We use the PM to help predict the emotion of the speaker by taking into account the speaker's linguistic preferences and characteristics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_8",
            "start": 601,
            "end": 737,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_9@0",
            "content": "We experiment on 4 different English ERC datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_9",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_9@1",
            "content": "Multi-party datasets are MELD and EmoryNLP (Zahiri and Choi, 2018), and dyadic datasets are IEMOCAP (Busso et al., 2008) and DailyDialog (Li et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_9",
            "start": 51,
            "end": 205,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_9@2",
            "content": "CoMPM achieves the first or second performance according to the evaluation metric compared to all previous systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_9",
            "start": 207,
            "end": 321,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_9@3",
            "content": "We perform an ablation study on each module to show that the proposed approach is effective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_9",
            "start": 323,
            "end": 414,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_9@4",
            "content": "Further experiments also show that our approach can be used in other languages and show the performance of CoMPM when the number of data is limited.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_9",
            "start": 416,
            "end": 563,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_10@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_10",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_11@0",
            "content": "Many recent studies use external knowledge to improve the ERC performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_11",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_11@1",
            "content": "KET (Zhong et al., 2019) is used as external knowledge based on ConceptNet (Speer et al., 2017) and emotion lexicon NRC_VAD (Mohammad, 2018) as the commonsense knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_11",
            "start": 75,
            "end": 244,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_11@2",
            "content": "ConceptNet is a knowledge graph that connects words and phrases in natural language using labeled edges.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_11",
            "start": 246,
            "end": 349,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_11@3",
            "content": "NRC_VAD Lexicon has human ratings of valence, arousal, and dominance for more than 20,000 English words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_11",
            "start": 351,
            "end": 454,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_11@4",
            "content": "COSMIC (Ghosal et al., 2020) and Psychological (Li et al., 2021) improve the performance of emotion recognition by extracting commonsense knowledge of the previous utterances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_11",
            "start": 456,
            "end": 630,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_11@5",
            "content": "Commonsense knowledge feature is extracted and leveraged with COMET (Bosselut et al., 2019) trained with ATOMIC (The Atlas of Machine Commonsense) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_11",
            "start": 632,
            "end": 779,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_11@6",
            "content": "ATOMIC has 9 sentence relation types with inferential if-then commonsense knowledge expressed in text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_11",
            "start": 781,
            "end": 882,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_11@7",
            "content": "ToDKAT (Zhu et al., 2021) improves performance by combining commonsense knowledge using COMET and topic discovery using VHRED (Serban et al., 2017) to the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_11",
            "start": 884,
            "end": 1044,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_12@0",
            "content": "Ekman (Ekman, 1992) constructs taxonomy of six common emotions (Joy, Sadness, Fear, Anger, Surprise, and Disgust) from human facial expressions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_12",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_12@1",
            "content": "In addition, Ekman explains that a multimodal view is important for multiple emotions recognition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_12",
            "start": 145,
            "end": 242,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_12@2",
            "content": "The multi-modal data such as MELD and IEMOCAP are some of the available standard datasets for emotion recognition and they are composed of text, speech and vision-based data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_12",
            "start": 244,
            "end": 417,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_12@3",
            "content": "Datcu and Rothkrantz (2014) uses speech and visual information to recognize emotions, and (Alm et al., 2005) attempts to recognize emotions based on text information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_12",
            "start": 419,
            "end": 584,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_12@4",
            "content": "MELD and ICON (Hazarika et al., 2018a) show that the more multi-modal information is used, the better the performance and the text information plays the most important role.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_12",
            "start": 586,
            "end": 758,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_12@5",
            "content": "Multimodal information is not always given in most social media, especially in chatbot systems where they are mainly composed of text-based systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_12",
            "start": 760,
            "end": 907,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_12@6",
            "content": "In this work, we design and introduce a text-based emotion recognition system using neural networks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_12",
            "start": 909,
            "end": 1008,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_13@0",
            "content": "In the previous studies, such as Hazarika et al. (2018b); Zadeh et al. (2017); , most works focused on dyadic-party conversation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_13",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_13@1",
            "content": "However, as the multi-party conversation datasets including MELD and EmoryNLP have become available, a lot of recent research is being conducted on multi-party dialogues such as ; Jiao et al. (2020); Ghosal et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_13",
            "start": 130,
            "end": 350,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_13@2",
            "content": "In general, the multi-party conversations have higher speaker dependency than the dyadic-party dialogues, therefore have more conditions to consider and result in poor performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_13",
            "start": 352,
            "end": 531,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_13@3",
            "content": "Zhou et al. (2018); Zhang et al. (2018a) shows that commonsense knowledge is important for understanding conversations and generating appropriate responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_13",
            "start": 533,
            "end": 688,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_13@4",
            "content": "reports that the lack of external knowledge makes it difficult to classify implicit emotions from the conversation history.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_13",
            "start": 690,
            "end": 812,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_13@5",
            "content": "EDA (Bothe et al., 2020) expands the multi-modal emotion datasets by extracting dialog acts from MELD and IEMOCAP and finds out that there is a correlation between dialogue acts and emotion labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_13",
            "start": 814,
            "end": 1010,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_14@0",
            "content": "Approach",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_14",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_15@0",
            "content": "Problem Statement",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_15",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_16@0",
            "content": "In a conversation, M sequential utterances are given as [(u 1 , p u 1 ), (u 2 , p u 2 ), ..., (u M , p u M )].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_16",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_16@1",
            "content": "u i is the utterance which the speaker p u i uttered, where p u i is one of the conversation participants.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_16",
            "start": 111,
            "end": 216,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_16@2",
            "content": "While p u i and p u j (i \u0338 = j) can be the same speaker, the minimum number of the unique conversation participants should be 2 or more.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_16",
            "start": 218,
            "end": 353,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_16@3",
            "content": "The ERC is a task of predicting the emotion e t of u t , the utterance of the t-th turn, given the previous utterances h t = {u 1 , ..., u t\u22121 }.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_16",
            "start": 355,
            "end": 499,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_16@4",
            "content": "Emotions are labeled as one of the predefined classes depending on the dataset, and the emotions we experimented with are either 6 or 7.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_16",
            "start": 501,
            "end": 636,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_16@5",
            "content": "We also experimented with a sentiment classification dataset which provides sentiment labels consisting of positive, negative and neutral.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_16",
            "start": 638,
            "end": 775,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_17@0",
            "content": "Model Overview",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_17",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_18@0",
            "content": "Figure 2 shows an overview of our model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_18",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_18@1",
            "content": "Our ERC neural network model is composed of two modules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_18",
            "start": 41,
            "end": 96,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_18@2",
            "content": "The first is CoM which catches the underlying effect of all previous utterances on the current speaker's emotions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_18",
            "start": 98,
            "end": 211,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_18@3",
            "content": "Therefore, we propose a context model to handle the relationship between the current and the previous utterances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_18",
            "start": 213,
            "end": 325,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_18@4",
            "content": "The second one is PM that leverages only the speaker's previous utterances, through which we want to reflect the speaker's knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_18",
            "start": 327,
            "end": 459,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_19@0",
            "content": "If the CoM and PM are based on different backbones, we consider them to be unaligned with respect to each other's output representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_19",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_19@1",
            "content": "Therefore, we design the PM to follow CoM so that the output representations of CoM and PM can mutually understand each other.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_19",
            "start": 138,
            "end": 263,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_19@2",
            "content": "If CoM and PM are based on different architectures, CoMPM is trained to understand each other's representations by matching dimensions using W p in Equation 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_19",
            "start": 265,
            "end": 423,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_19@3",
            "content": "The combination of CoM and PM is described in Section 4.5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_19",
            "start": 425,
            "end": 482,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_20@0",
            "content": "CoM: Context Embedding Module",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_20",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_21@0",
            "content": "The context embedding module predicts e t by considering all of the utterances before the t-th turn as the dialogue context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_21",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_21@1",
            "content": "The example in Figure 2 shows how the model predicts the emotion of u 6 uttered by s A , given a conversation of three participants (s A , s B , s C ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_21",
            "start": 125,
            "end": 275,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_21@2",
            "content": "The previous utterances are h 6 = {u 1 , \u2022 \u2022 \u2022 u 5 } and e 6 is predicted while considering the relationship between u 6 and h 6 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_21",
            "start": 277,
            "end": 406,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_22@0",
            "content": "We consider multi-party conversations where 2 or more speakers are involved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_22",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_22@1",
            "content": "A special token <s P > is introduced to distinguish participants in the conversation and to handle the speaker's dependency where P is the set of participants.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_22",
            "start": 77,
            "end": 235,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_22@2",
            "content": "In other words, the same special token appears before the utterances of the same speaker.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_22",
            "start": 237,
            "end": 325,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_23@0",
            "content": "We use an Transformer encoder as a context model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_23",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_23@1",
            "content": "In many natural language processing tasks, the effectiveness of the pre-trained language model has been proven, and we also set the initial state of the model to RoBERTa .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_23",
            "start": 50,
            "end": 220,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_23@2",
            "content": "RoBERTa is an unsupervised pre-trained model with largescale open-domain corpora of unlabeled text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_23",
            "start": 222,
            "end": 320,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_24@0",
            "content": "We use the embedding of the special token <cls> to predict emotion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_24",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_24@1",
            "content": "The <cls> token is concatenated at the end of the input and the output of the context model is as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_24",
            "start": 68,
            "end": 173,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_25@0",
            "content": "c t = CoM(< cls >, P :t\u22121 , h t , u t )(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_25",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_26@0",
            "content": "where P :t\u22121 is the set of speakers in the previous turns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_26",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_26@1",
            "content": "c t \u2208 R 1\u00d7hc and h c is the dimension of CoM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_26",
            "start": 59,
            "end": 103,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_27@0",
            "content": "PM: Pre-trained Memory Module",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_27",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_28@0",
            "content": "External knowledge is known to play an important role in understanding conversation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_28",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_28@1",
            "content": "Pre-trained language models can be trained on numerous corpora and be used as an external knowledge base.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_28",
            "start": 85,
            "end": 189,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_28@2",
            "content": "Inspired by previous studies that the speaker's knowledge helps to judge emotions, we extract and track pre-trained memory from the speaker's previous utterances to utilize the emotions of the current utterance u t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_28",
            "start": 191,
            "end": 406,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_28@3",
            "content": "If the speaker has never appeared before the current turn, the result of the pre-trained memory is considered a zero vector.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_28",
            "start": 408,
            "end": 531,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_28@4",
            "content": "Since <cls> is mostly used for the task of classifying sentences, we use the embedding output of the <cls> token as a vector representing the utterance as follows: where p u i = p S , S is the speaker of the current utterance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_28",
            "start": 533,
            "end": 758,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_28@5",
            "content": "k i \u2208 R 1\u00d7h k and h k is the dimension of PM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_28",
            "start": 760,
            "end": 804,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_29@0",
            "content": "k i = PM(< cls >, u i ) (2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_29",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_30@0",
            "content": "CoMPM: Combination of CoM and PM",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_30",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_31@0",
            "content": "We combine CoM and PM to predict the speaker's emotion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_31",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_31@1",
            "content": "In many dialogue systems (Zhang et al., 2018b;Ma et al., 2019), it is known that utterances close to the current turn are important for response.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_31",
            "start": 56,
            "end": 200,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_31@2",
            "content": "Therefore, we assume that utterances close to the current utterance will be important in emotional recognition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_31",
            "start": 202,
            "end": 312,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_32@0",
            "content": "Tracking Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_32",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_33@0",
            "content": "We use k i tracking method using GRU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_33",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_33@1",
            "content": "The tracking method assumes that the importance of all previous speaker utterances to the current emotion is not equal and varies with the distance of the current utterance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_33",
            "start": 38,
            "end": 210,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_33@2",
            "content": "In other words, since the flow of conversation changes as it progresses, the effect on emotion may differ depending on the distance from the current utterance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_33",
            "start": 212,
            "end": 370,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_33@3",
            "content": "We track and capture the sequential position information of k i using a unidirectional GRU:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_33",
            "start": 372,
            "end": 462,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_34@0",
            "content": "kt t = GRU(k i 1 , k i 2 , ..., k in ) (3",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_34",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_35@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_35",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_36@0",
            "content": "where t is the turn index of the current utterance, n is the number of previous utterances of the speaker, and i s (s = 1, 2, ..., n) is each turn uttered.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_36",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_36@1",
            "content": "kt t \u2208 R 1\u00d7hc is the output of k in and as a result, the knowledge of distant utterance is diluted and the effect on the current utterance is reduced.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_36",
            "start": 156,
            "end": 305,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_37@0",
            "content": "GRU is composed of 2-layers, the dimension of the output vector is h c , and the dropout is set to 0.3 during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_37",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_37@1",
            "content": "Finally, the output vector o t is obtained by adding kt t and c t in Equation 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_37",
            "start": 120,
            "end": 199,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_38@0",
            "content": "o t = c t + W p (kt t )(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_38",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_39@0",
            "content": "where, W p is a matrix that projects the pretrained memory to the dimension of the context output, and is used only when PM and CoM are different pre-trained language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_39",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_40@0",
            "content": "Emotion Prediction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_40",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_41@0",
            "content": "Softmax is applied to the vector multiplied by o t and the linear matrix W o \u2208 R he\u00d7hc to obtain the probability distribution of emotion classes, where h e is the number of emotion classes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_41",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_41@1",
            "content": "e t is the predicted emotion class that corresponds to the index of the largest probability from the emotion class distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_41",
            "start": 190,
            "end": 317,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_42@0",
            "content": "P (e) = softmax(W o (o t ))(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_42",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_43@0",
            "content": "The objective is to minimize the cross entropy loss so that e t is the same as the ground truth emotional label.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_43",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_44@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_44",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_45@0",
            "content": "Dataset",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_45",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_46@0",
            "content": "We experiment on four benchmark datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_46",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_46@1",
            "content": "MELD and EmoryNLP (Zahiri and Choi, 2018) are multi-party datasets, while IEMOCAP (Busso et al., 2008) and DailyDialog (Li et al., 2017) are dyadic-party datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_46",
            "start": 42,
            "end": 204,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_46@2",
            "content": "The statistics of the dataset are shown in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_46",
            "start": 206,
            "end": 256,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_47@0",
            "content": "IEMOCAP is a dataset involving 10 speakers, and each conversation involves 2 speakers and the emotion-inventory is given as \"happy, sad, angry, excited, frustrated and neutral\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_47",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_47@1",
            "content": "The train and development dataset is a conversation involving the previous eight speakers, and the train and development are divided into random splits at a ratio of 9:1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_47",
            "start": 178,
            "end": 347,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_47@2",
            "content": "The test dataset is a conversation involving two later speakers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_47",
            "start": 349,
            "end": 412,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_48@0",
            "content": "DailyDialog is a dataset of daily conversations between two speakers and the emotion-inventory is given as \"anger, disgust, fear, joy, surprise, sadness and neutral\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_48",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_48@1",
            "content": "Since more than 82% of the data are tagged as neutral, neutral emotions are excluded when evaluating systems with Micro-F1 as did in the previous studies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_48",
            "start": 167,
            "end": 320,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_49@0",
            "content": "MELD is a dataset based on Friends TV show and provides two taxonomy: emotion and sentiment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_49",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_49@1",
            "content": "MELD's emotion-inventory is given as \"anger, disgust, sadness, joy, surprise, fear and neutrality\" following Ekman (Ekman, 1992) and sentiment-inventory is given as \"positive, negative and neutral\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_49",
            "start": 93,
            "end": 290,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_50@0",
            "content": "EmoryNLP, like MELD, is also a dataset based on Friends TV show, but the emotion-inventory is given as \"joyful, peaceful, powerful, scared, mad, sad and neutral\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_50",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_50@1",
            "content": "Sentiment labels are not provided, but sentiment classes can be grouped as follows: positive: {joyful, peaceful, powerful}, negative: {scared, mad, sad}, neutral: {neutral}",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_50",
            "start": 163,
            "end": 334,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_51@0",
            "content": "Training Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_51",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_52@0",
            "content": "We use the pre-trained model from the huggingface library 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_52",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_52@1",
            "content": "The optimizer is AdamW and the learning rate is 1e-5 as an initial value.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_52",
            "start": 62,
            "end": 134,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_52@2",
            "content": "The learning rate scheduler used for training is get_linear_schedule_with_warmup, and the maximum value of 10 is used for the gradient clipping.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_52",
            "start": 136,
            "end": 279,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_52@3",
            "content": "We select the model with the best performance on the validation set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_52",
            "start": 281,
            "end": 348,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_52@4",
            "content": "All experiments are conducted on one V100 GPU with 32GB memory.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_52",
            "start": 350,
            "end": 412,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_53@0",
            "content": "Previous Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_53",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_54@0",
            "content": "We show that the proposed approach is effective by comparing it with various baselines and the stateof-the-art methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_54",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_55@0",
            "content": "KET (Zhong et al., 2019) is a Knowledge Enriched Transformer that reflects contextual utterances with a hierarchical self-attention and leverages external commonsense knowledge by using a context-aware affective graph attention mechanism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_55",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_56@0",
            "content": "DialogueRNN uses a GRU network to keep track of the individual party states in the conversation to predict emotions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_56",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_56@1",
            "content": "This model assumes that there are three factors in emotion prediction: the speaker, the context from the preceding utterances and the emotion of the preceding utterances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_56",
            "start": 117,
            "end": 286,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_56@2",
            "content": "Also, Ghosal et al. (2020) shows the performance of RoBERTa+DialogueRNN when the vectors of the tokens are extracted with a pretrained RoBERTa.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_56",
            "start": 288,
            "end": 430,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_57@0",
            "content": "RGAT+P (Ishiwatari et al., 2020) (relational graph attention networks) proposes relational position encodings with sequential information reflecting the relational graph structure, which shows that both the speaker dependency and the sequential information can be captured.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_57",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_58@0",
            "content": "HiTrans (Li et al., 2020) proposes a transformerbased context-and speaker-sensitive model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_58",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_58@1",
            "content": "Hi-Trans utilize BERT as the low-level transformer to generate local utterance representations, and feed them into another high-level transformer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_58",
            "start": 91,
            "end": 236,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_59@0",
            "content": "COSMIC (Ghosal et al., 2020) incorporates different elements of commonsense such as mental states, events and causal relations, and learns the relations between participants in the conversation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_59",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_59@1",
            "content": "This model uses pre-trained RoBERTa as a feature extractor and leverages COMET trained with ATOMIC as the commonsense knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_59",
            "start": 195,
            "end": 322,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_59@2",
            "content": "ERMC-DisGCN (Sun et al., 2021) proposes a discourse-aware graph neural network that utilizes self-speaker dependency of interlocutors as a relational convolution and informative cues of dependent utterances as a gated convolution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_59",
            "start": 324,
            "end": 553,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_60@0",
            "content": "Psychological (Li et al., 2021) uses commonsense knowledge as enrich edges and processes it with graph transformer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_60",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_60@1",
            "content": "Psychological performs emotion recognition by utilizing intention of utterances from not only past contexts but also future context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_60",
            "start": 116,
            "end": 247,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_61@0",
            "content": "DialogueCRN (Hu et al., 2021) introduces an intuitive retrieving process, the reasoning module, which understands both situation-level and speakerlevel contexts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_61",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_62@0",
            "content": "ToDKAT (Zhu et al., 2021) proposes a language model with topic detection added, and improves performance by combining it with commonsense knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_62",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_63@0",
            "content": "Result and Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_63",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_64@0",
            "content": "Table 2 shows the performance of the previous methods and our models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_64",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_64@1",
            "content": "CoM used alone does not leverage PM and predicts emotions by considering only the dialogue context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_64",
            "start": 70,
            "end": 168,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_64@2",
            "content": "PM, if used alone, does not consider the context and predicts emotions only with the utterance of the current turn.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_64",
            "start": 170,
            "end": 284,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_64@3",
            "content": "CoMPM is a model in which both CoM and PM parameters are updated in the initial state of the pre-trained LM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_64",
            "start": 286,
            "end": 393,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_64@4",
            "content": "CoMPM(f) is a model in which PM parameters are frozen in the initial state (pre-trained LM) and is not trained further, and CoMPM(s) is a model in which PM is trained from scratch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_64",
            "start": 395,
            "end": 574,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_65@0",
            "content": "The effect of PM can be confirmed through the performance comparison between CoM and CoMPM, and the effect of CoM can be confirmed by comparing the results of CoM and PM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_65",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_65@1",
            "content": "Since PM does not consider context, it showed worse performance than CoM, and the performance gap is larger in the IEMOCAP dataset with a higher average number of conversation turns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_65",
            "start": 171,
            "end": 352,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_65@2",
            "content": "As a result, we show that the combination of CoM and PM is effective in achieving better performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_65",
            "start": 354,
            "end": 454,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_66@0",
            "content": "We confirm the effect of PM structure in the model through the performance of CoMPM(s).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_66",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_66@1",
            "content": "If PM parameters are not frozen and are instead randomly initialized (i.e. PM(s)), the performance deteriorates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_66",
            "start": 88,
            "end": 199,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_66@2",
            "content": "CoMPM(s) performs worse than CoMPM, and even performs worse than CoM on the other datasets except for MELD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_66",
            "start": 201,
            "end": 307,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_66@3",
            "content": "That is, PM(s) cannot be regarded as a pre-trained memory because the parameters are randomly initialized, and simply increasing the model complexity does not help to improve the performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_66",
            "start": 309,
            "end": 499,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_66@4",
            "content": "CoMPM(f) shows similar performance to CoMPM and achieves better performance depending on the data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_66",
            "start": 501,
            "end": 598,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_66@5",
            "content": "PM(f) is not fine-tuned on the data, but it extracts general pre-trained memory from a pretrained language model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_66",
            "start": 600,
            "end": 712,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_66@6",
            "content": "The comparison between PM and PM(f) will be further described in Section 4.6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_66",
            "start": 714,
            "end": 790,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_66@7",
            "content": "We regard pre-trained memory as compressed knowledge, which can play a role similar to external knowledge used in cutting-edge systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_66",
            "start": 792,
            "end": 926,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_67@0",
            "content": "The best performance of our approaches is CoMPM or CoMPM(f), both of which combine pre-trained memory.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_67",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_67@1",
            "content": "We achieve state-of-the-art performance among all systems that do not leverage structured external data and achieve the first or second performance even including systems that leverage external data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_67",
            "start": 103,
            "end": 301,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_67@2",
            "content": "Therefore, our approach can be extended to other languages without structured external data as well, which is described in Section 4.7.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_67",
            "start": 303,
            "end": 437,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_68@0",
            "content": "Combinations of CoM and PM",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_68",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_69@0",
            "content": "We experiment with the effect of pre-trained memory of different language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_69",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_69@1",
            "content": "To eliminate the influence of the PM structure, we freeze the parameters of PM and use it as a feature extractor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_69",
            "start": 82,
            "end": 194,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_69@2",
            "content": "Table 3 shows the performance of the pretrained memory extracted by the different language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_69",
            "start": 196,
            "end": 293,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_69@3",
            "content": "If PM and CoM are based on different backbones, the pre-trained memory is projected through W p as the dimension of the context output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_69",
            "start": 295,
            "end": 429,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_69@4",
            "content": "RoBERTa+BERT and RoBERTa+GPT2 (combination of CoM and PM(f)) have lower performance than RoBERTa+RoBERTa, which is inferred because pre-trained memory of RoBERTa contains richer information than BERT and GPT2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_69",
            "start": 431,
            "end": 639,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_69@5",
            "content": "Since there is a lot of training data in the diallydialog and W p is fine-tuned to the data to mutually understand the pre-trained memory and context representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_69",
            "start": 641,
            "end": 805,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_69@6",
            "content": "Therefore, we infer that performance does not decrease even if the PM changes from the dailydialog.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_69",
            "start": 807,
            "end": 905,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_69@7",
            "content": "However, even if other PMs are used, the performance is improved compared to using only CoM, so the pre-trained memory of other language models is also effective for emotion recognition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_69",
            "start": 907,
            "end": 1092,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_70@0",
            "content": "BERT+RoBERTa has a larger performance decrease than RoBERTa+BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_70",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_70@1",
            "content": "In particular, in IEMOCAP data with a long average number of turns in the context, the performance deteriorates significantly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_70",
            "start": 66,
            "end": 191,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_70@2",
            "content": "In addition, the performance of BERT+RoBERTa is lower than CoM (RoBERTa), which supports that the performance of CoM is a more important factor than the use of pre-trained memory.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_70",
            "start": 193,
            "end": 371,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_70@3",
            "content": "In other words, we confirm that CoM is more important than PM in our system for performance, and it is effective to focus on context modeling rather than external knowledge in the study of emotion recognition in conversation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_70",
            "start": 373,
            "end": 597,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_71@0",
            "content": "Training with Less Data",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_71",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_72@0",
            "content": "CoMPM is an approach that eliminates dependence on external sources and is easily extensible to any language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_72",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_72@1",
            "content": "However, the insufficient number of emotional data available in other countries (or actual service) remains a problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_72",
            "start": 110,
            "end": 227,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_72@2",
            "content": "Therefore, we conduct additional experiments according to the use ratio Table 2 shows that CoMPM(f) achieves better performance than CoMPM in the emotion classification of IMEOCAP and EmoryNLP, which has fewer training data than other settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_72",
            "start": 229,
            "end": 472,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_72@3",
            "content": "On the other hand, if there is a lot of training data, CoMPM shows better performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_72",
            "start": 474,
            "end": 559,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_72@4",
            "content": "Figure 3 shows that as the number of data decreases, CoMPM(f) shows better results than CoMPM, which indicates that it is better to freeze the parameters of PM when the number of training data is insufficient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_72",
            "start": 561,
            "end": 769,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_72@5",
            "content": "Therefore, if there is a lot of training data in the real-world application, CoMPM is expected to achieve good performance, otherwise it is CoMPM(f).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_72",
            "start": 771,
            "end": 919,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_73@0",
            "content": "ERC in other languages",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_73",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_74@0",
            "content": "Previous studies mostly utilize external knowledge to improve performance, but these approaches require additional publicly available data, which are mainly available for English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_74",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_74@1",
            "content": "Indeed, structured knowledge and ERC data are lacking in other languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_74",
            "start": 180,
            "end": 252,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_74@2",
            "content": "Our approach can be extended to other languages without building additional external knowledge and achieves better performance than simply using a pre-trained model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_74",
            "start": 254,
            "end": 418,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_75@0",
            "content": "Korean Dataset",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_75",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_76@0",
            "content": "We constructed data composed of two speakers in Korean, and emotion-inventory is given as \"surprise, fear, ambiguous, sad, disgust, joy, bored, embarrassed, neutral\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_76",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_76@1",
            "content": "The total number of sessions is 1000, and the average number of utterance turns The value in parentheses is the performance difference from the original CoMPM(f) (RoBERTa + RoBERTa).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_76",
            "start": 167,
            "end": 348,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_76@2",
            "content": "We use the bert-large-uncased and GPT2-medium versions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_76",
            "start": 350,
            "end": 404,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_77@0",
            "content": "is 13.4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_77",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_77@1",
            "content": "We use the data randomly divided into train:dev:test in a ratio of 8:1:1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_77",
            "start": 9,
            "end": 81,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_77@2",
            "content": "This dataset is for actual service and is not released to the public.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_77",
            "start": 83,
            "end": 151,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_77@3",
            "content": "In Korean, our results are shown in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_77",
            "start": 153,
            "end": 196,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_77@4",
            "content": "The backbone of CoM and PM is Korean-BERT owned by the company, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_77",
            "start": 198,
            "end": 274,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_77@5",
            "content": "In the Korean dataset, like the English dataset, the performance is good in the order of CoMPM, CoM, and PM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_77",
            "start": 276,
            "end": 383,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_77@6",
            "content": "Our approach simply shows a significant performance improvement on baselines that are fine-tuned to the language model and works well for other languages as well as for English data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_77",
            "start": 385,
            "end": 566,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_78@0",
            "content": "Results in the Korean Dataset",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_78",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_79@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_79",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_80@0",
            "content": "We propose CoMPM that leverages pre-trained memory using a pre-trained language model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_80",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_80@1",
            "content": "CoMPM consists of a context embedding module (CoM) and a pre-trained memory module (PM), and the experimental results show that each module is effective in improving the performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_80",
            "start": 87,
            "end": 268,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_80@2",
            "content": "CoMPM outperforms baselines on both dyadic-party and multi-party datasets and achieves state-of-the-art among systems that do not use external knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_80",
            "start": 270,
            "end": 422,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_80@3",
            "content": "In addition, CoMPM achieves performance comparable to cutting-edge systems that leverage structured external knowledge, which is the effect of pre-trained memory of the language model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_80",
            "start": 424,
            "end": 607,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_80@4",
            "content": "By combining other pre-trained memories, we find that the pre-trained memory extracted with RoBERTa is richer and more effective than the pre-trained memory extracted with BERT or GPT2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_80",
            "start": 609,
            "end": 793,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_81@0",
            "content": "Since we believe that pre-trained memory is proportional to the performance of a language model, a language model with a large training corpus and many parameters is considered to be more effective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_81",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_81@1",
            "content": "However, we find that context modeling is more important than pre-trained memory for emotion recognition in conversation, and future research will focus on context modeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_81",
            "start": 199,
            "end": 371,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_82@0",
            "content": "Additionally, our approach achieves competitive performance and does not require externally structured data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_82",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_82@1",
            "content": "Therefore, we show that it can be easily extended to Korean as well as English, and it is expected to be effective in other countries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_82",
            "start": 109,
            "end": 242,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_83@0",
            "content": "Cecilia Ovesdotter Alm, Dan Roth, Richard Sproat, Emotions from text: Machine learning for textbased emotion prediction, 2005, Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_83",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_84@0",
            "content": "Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, Yejin Choi, COMET: Commonsense transformers for automatic knowledge graph construction, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_84",
            "start": 0,
            "end": 268,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_85@0",
            "content": "Chandrakant Bothe, Cornelius Weber, Sven Magg, Stefan Wermter, EDA: Enriching emotional dialogue acts using an ensemble of neural annotators, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_85",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_86@0",
            "content": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette Chang, Sungbok Lee, Shrikanth Narayanan, Iemocap: interactive emotional dyadic motion capture database, 2008, Lang. Resour. Evaluation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_86",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_87@0",
            "content": "UNKNOWN, None, 2014, Semantic audiovisual data fusion for automatic emotion recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_87",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_88@0",
            "content": "UNKNOWN, None, 1992, An argument for basic emotions. Cognition & Emotion, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_88",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_89@0",
            "content": "Deepanway Ghosal, Navonil Majumder, Alexander Gelbukh, Rada Mihalcea, and Soujanya Poria. 2020. COSMIC: COmmonSense knowledge for eMotion identification in conversations, , Findings of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_89",
            "start": 0,
            "end": 293,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_90@0",
            "content": "Devamanyu Hazarika, Soujanya Poria, Rada Mihalcea, Erik Cambria, Roger Zimmermann, ICON: Interactive conversational memory network for multimodal emotion detection, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_90",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_91@0",
            "content": "Devamanyu Hazarika, Soujanya Poria, Amir Zadeh, Erik Cambria, Louis-Philippe Morency, Roger Zimmermann, Conversational memory network for emotion recognition in dyadic dialogue videos, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_91",
            "start": 0,
            "end": 346,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_92@0",
            "content": "Dou Hu, Lingwei Wei, Xiaoyong Huai, Dia-logueCRN: Contextual reasoning networks for emotion recognition in conversations, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_92",
            "start": 0,
            "end": 341,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_93@0",
            "content": "Taichi Ishiwatari, Yuki Yasuda, Taro Miyazaki, Jun Goto, Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_93",
            "start": 0,
            "end": 317,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_94@0",
            "content": "Wenxiang Jiao, Michael Lyu, Irwin King, Real-time emotion recognition via attention gated hierarchical memory network, 2020, The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_94",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_95@0",
            "content": "Jiangnan Li, Zheng Lin, Peng Fu, Weiping Wang, Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_95",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_96@0",
            "content": "Jingye Li, Donghong Ji, Fei Li, Meishan Zhang, Yijiang Liu, HiTrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_96",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_97@0",
            "content": "Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, Shuzi Niu, DailyDialog: A manually labelled multi-turn dialogue dataset, 2017, Proceedings of the Eighth International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_97",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_98@0",
            "content": "Zhaojiang Lin, Peng Xu, Genta Indra Winata, Farhad Bin Siddique, Zihan Liu, Jamin Shin, Pascale Fung, Caire: An end-to-end empathetic chatbot, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_98",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_99@0",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_99",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_100@0",
            "content": "Zeming Liu, Haifeng Wang, Zheng-Yu Niu, Hua Wu, Wanxiang Che, Ting Liu, Towards conversational recommendation over multi-type dialogs, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_100",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_101@0",
            "content": "Wentao Ma, Yiming Cui, Nan Shao, Su He, Wei-Nan Zhang, Ting Liu, Shijin Wang, Guoping Hu, TripleNet: Triple attention network for multiturn response selection in retrieval-based chatbots, 2019, Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_101",
            "start": 0,
            "end": 281,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_102@0",
            "content": "Navonil Majumder, Soujanya Poria, Devamanyu Hazarika, Rada Mihalcea, Alexander Gelbukh, Erik Cambria, Dialoguernn: An attentive rnn for emotion detection in conversations, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_102",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_103@0",
            "content": "UNKNOWN, None, 2018, Obtaining reliable human ratings of valence, arousal, and dominance for, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_103",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_104@0",
            "content": ", English words, , Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_104",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_105@0",
            "content": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Language models as knowledge bases?, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_105",
            "start": 0,
            "end": 371,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_106@0",
            "content": "S Poria, N Majumder, R Mihalcea, E Hovy, Emotion recognition in conversation: Research challenges, datasets, and recent advances, 2019, IEEE Access, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_106",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_107@0",
            "content": "Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, MELD: A multimodal multi-party dataset for emotion recognition in conversations, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_107",
            "start": 0,
            "end": 284,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_108@0",
            "content": "Maarten Sap, Emily Ronan Le Bras, Chandra Allaway, Nicholas Bhagavatula, Hannah Lourie, Brendan Rashkin, Noah Roof, Yejin Smith,  Choi, Atomic: An atlas of machine commonsense for ifthen reasoning, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_108",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_109@0",
            "content": "Iulian Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron Courville, Yoshua Bengio, A hierarchical latent variable encoderdecoder model for generating dialogues, 2017, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_109",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_110@0",
            "content": "UNKNOWN, None, 2019, Happybot: Generating empathetic dialogue responses by improving user experience lookahead, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_110",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_111@0",
            "content": "Robyn Speer, Joshua Chin, Catherine Havasi, Conceptnet 5.5: An open multilingual graph of general knowledge, 2017, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_111",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_112@0",
            "content": "Yang Sun, Nan Yu, Guohong Fu, A discourseaware graph neural network for emotion recognition in multi-party conversation, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_112",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_113@0",
            "content": "Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, Louis-Philippe Morency, Tensor fusion network for multimodal sentiment analysis, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_113",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_114@0",
            "content": "M Sayyed, Jinho Zahiri,  Choi, Emotion detection on TV show transcripts with sequence-based convolutional neural networks, 2018-02-02, The Workshops of the The Thirty-Second AAAI Conference on Artificial Intelligence, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_114",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_115@0",
            "content": "Rohola Zandie, Mohammad Mahoor, Emptransfo: A multi-head transformer architecture for creating empathetic dialog systems, 2020-05-17, Proceedings of the Thirty-Third International Florida Artificial Intelligence Research Society Conference, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_115",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_116@0",
            "content": "Dong Zhang, Liangqing Wu, Changlong Sun, Shoushan Li, Qiaoming Zhu, Guodong Zhou, Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations, 2019, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_116",
            "start": 0,
            "end": 299,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_117@0",
            "content": "Yuxiang Zhang, Jiamei Fu, Dongyu She, Ying Zhang, Senzhang Wang, Jufeng Yang, Text emotion distribution learning via multi-task convolutional neural network, 2018, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_117",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_118@0",
            "content": "Zhuosheng Zhang, Jiangtong Li, Pengfei Zhu, Hai Zhao, Gongshen Liu, Modeling multi-turn conversation with deep utterance aggregation, 2018, Proceedings of the 27th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_118",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_119@0",
            "content": "Peixiang Zhong, Di Wang, Chunyan Miao, Knowledge-enriched transformer for emotion detection in textual conversations, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_119",
            "start": 0,
            "end": 301,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_120@0",
            "content": "Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, Xiaoyan Zhu, Commonsense knowledge aware conversation generation with graph attention, 2018, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_120",
            "start": 0,
            "end": 258,
            "label": {}
        },
        {
            "ix": "52-ARR_v1_121@0",
            "content": "Lixing Zhu, Gabriele Pergola, Lin Gui, Deyu Zhou, Yulan He, Topic-driven and knowledgeaware transformer for dialogue emotion detection, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "52-ARR_v1_121",
            "start": 0,
            "end": 317,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "52-ARR_v1_0",
            "tgt_ix": "52-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_0",
            "tgt_ix": "52-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_1",
            "tgt_ix": "52-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_1",
            "tgt_ix": "52-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_0",
            "tgt_ix": "52-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_2",
            "tgt_ix": "52-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_4",
            "tgt_ix": "52-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_5",
            "tgt_ix": "52-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_6",
            "tgt_ix": "52-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_7",
            "tgt_ix": "52-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_8",
            "tgt_ix": "52-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_3",
            "tgt_ix": "52-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_3",
            "tgt_ix": "52-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_3",
            "tgt_ix": "52-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_3",
            "tgt_ix": "52-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_3",
            "tgt_ix": "52-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_3",
            "tgt_ix": "52-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_3",
            "tgt_ix": "52-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_0",
            "tgt_ix": "52-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_9",
            "tgt_ix": "52-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_11",
            "tgt_ix": "52-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_12",
            "tgt_ix": "52-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_10",
            "tgt_ix": "52-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_10",
            "tgt_ix": "52-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_10",
            "tgt_ix": "52-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_10",
            "tgt_ix": "52-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_0",
            "tgt_ix": "52-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_13",
            "tgt_ix": "52-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_14",
            "tgt_ix": "52-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_14",
            "tgt_ix": "52-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_15",
            "tgt_ix": "52-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_15",
            "tgt_ix": "52-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_14",
            "tgt_ix": "52-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_16",
            "tgt_ix": "52-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_18",
            "tgt_ix": "52-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_17",
            "tgt_ix": "52-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_17",
            "tgt_ix": "52-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_17",
            "tgt_ix": "52-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_14",
            "tgt_ix": "52-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_19",
            "tgt_ix": "52-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_21",
            "tgt_ix": "52-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_22",
            "tgt_ix": "52-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_23",
            "tgt_ix": "52-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_24",
            "tgt_ix": "52-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_25",
            "tgt_ix": "52-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_20",
            "tgt_ix": "52-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_20",
            "tgt_ix": "52-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_20",
            "tgt_ix": "52-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_20",
            "tgt_ix": "52-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_20",
            "tgt_ix": "52-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_20",
            "tgt_ix": "52-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_20",
            "tgt_ix": "52-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_14",
            "tgt_ix": "52-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_26",
            "tgt_ix": "52-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_28",
            "tgt_ix": "52-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_27",
            "tgt_ix": "52-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_27",
            "tgt_ix": "52-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_27",
            "tgt_ix": "52-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_14",
            "tgt_ix": "52-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_29",
            "tgt_ix": "52-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_30",
            "tgt_ix": "52-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_30",
            "tgt_ix": "52-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_14",
            "tgt_ix": "52-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_31",
            "tgt_ix": "52-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_33",
            "tgt_ix": "52-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_34",
            "tgt_ix": "52-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_35",
            "tgt_ix": "52-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_36",
            "tgt_ix": "52-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_37",
            "tgt_ix": "52-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_38",
            "tgt_ix": "52-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_32",
            "tgt_ix": "52-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_32",
            "tgt_ix": "52-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_32",
            "tgt_ix": "52-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_32",
            "tgt_ix": "52-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_32",
            "tgt_ix": "52-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_32",
            "tgt_ix": "52-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_32",
            "tgt_ix": "52-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_32",
            "tgt_ix": "52-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_14",
            "tgt_ix": "52-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_39",
            "tgt_ix": "52-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_41",
            "tgt_ix": "52-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_42",
            "tgt_ix": "52-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_40",
            "tgt_ix": "52-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_40",
            "tgt_ix": "52-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_40",
            "tgt_ix": "52-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_40",
            "tgt_ix": "52-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_0",
            "tgt_ix": "52-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_43",
            "tgt_ix": "52-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_44",
            "tgt_ix": "52-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_44",
            "tgt_ix": "52-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_46",
            "tgt_ix": "52-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_47",
            "tgt_ix": "52-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_48",
            "tgt_ix": "52-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_49",
            "tgt_ix": "52-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_45",
            "tgt_ix": "52-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_45",
            "tgt_ix": "52-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_45",
            "tgt_ix": "52-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_45",
            "tgt_ix": "52-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_45",
            "tgt_ix": "52-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_45",
            "tgt_ix": "52-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_44",
            "tgt_ix": "52-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_50",
            "tgt_ix": "52-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_51",
            "tgt_ix": "52-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_51",
            "tgt_ix": "52-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_44",
            "tgt_ix": "52-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_52",
            "tgt_ix": "52-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_54",
            "tgt_ix": "52-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_55",
            "tgt_ix": "52-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_56",
            "tgt_ix": "52-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_57",
            "tgt_ix": "52-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_58",
            "tgt_ix": "52-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_59",
            "tgt_ix": "52-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_60",
            "tgt_ix": "52-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_61",
            "tgt_ix": "52-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_53",
            "tgt_ix": "52-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_53",
            "tgt_ix": "52-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_53",
            "tgt_ix": "52-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_53",
            "tgt_ix": "52-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_53",
            "tgt_ix": "52-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_53",
            "tgt_ix": "52-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_53",
            "tgt_ix": "52-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_53",
            "tgt_ix": "52-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_53",
            "tgt_ix": "52-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_53",
            "tgt_ix": "52-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_44",
            "tgt_ix": "52-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_62",
            "tgt_ix": "52-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_64",
            "tgt_ix": "52-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_65",
            "tgt_ix": "52-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_66",
            "tgt_ix": "52-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_63",
            "tgt_ix": "52-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_63",
            "tgt_ix": "52-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_63",
            "tgt_ix": "52-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_63",
            "tgt_ix": "52-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_63",
            "tgt_ix": "52-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_44",
            "tgt_ix": "52-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_67",
            "tgt_ix": "52-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_69",
            "tgt_ix": "52-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_68",
            "tgt_ix": "52-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_68",
            "tgt_ix": "52-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_68",
            "tgt_ix": "52-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_44",
            "tgt_ix": "52-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_70",
            "tgt_ix": "52-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_71",
            "tgt_ix": "52-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_71",
            "tgt_ix": "52-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_44",
            "tgt_ix": "52-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_72",
            "tgt_ix": "52-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_73",
            "tgt_ix": "52-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_73",
            "tgt_ix": "52-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_44",
            "tgt_ix": "52-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_74",
            "tgt_ix": "52-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_76",
            "tgt_ix": "52-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_75",
            "tgt_ix": "52-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_75",
            "tgt_ix": "52-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_75",
            "tgt_ix": "52-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_44",
            "tgt_ix": "52-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_77",
            "tgt_ix": "52-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_0",
            "tgt_ix": "52-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_78",
            "tgt_ix": "52-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_80",
            "tgt_ix": "52-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_81",
            "tgt_ix": "52-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_79",
            "tgt_ix": "52-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_79",
            "tgt_ix": "52-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_79",
            "tgt_ix": "52-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_79",
            "tgt_ix": "52-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "52-ARR_v1_0",
            "tgt_ix": "52-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_1",
            "tgt_ix": "52-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_2",
            "tgt_ix": "52-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_2",
            "tgt_ix": "52-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_2",
            "tgt_ix": "52-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_2",
            "tgt_ix": "52-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_2",
            "tgt_ix": "52-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_2",
            "tgt_ix": "52-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_2",
            "tgt_ix": "52-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_2",
            "tgt_ix": "52-ARR_v1_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_2",
            "tgt_ix": "52-ARR_v1_2@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_3",
            "tgt_ix": "52-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_4",
            "tgt_ix": "52-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_5",
            "tgt_ix": "52-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_5",
            "tgt_ix": "52-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_5",
            "tgt_ix": "52-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_5",
            "tgt_ix": "52-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_5",
            "tgt_ix": "52-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_5",
            "tgt_ix": "52-ARR_v1_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_5",
            "tgt_ix": "52-ARR_v1_5@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_6",
            "tgt_ix": "52-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_6",
            "tgt_ix": "52-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_6",
            "tgt_ix": "52-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_7",
            "tgt_ix": "52-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_7",
            "tgt_ix": "52-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_7",
            "tgt_ix": "52-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_7",
            "tgt_ix": "52-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_7",
            "tgt_ix": "52-ARR_v1_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_7",
            "tgt_ix": "52-ARR_v1_7@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_7",
            "tgt_ix": "52-ARR_v1_7@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_8",
            "tgt_ix": "52-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_8",
            "tgt_ix": "52-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_8",
            "tgt_ix": "52-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_8",
            "tgt_ix": "52-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_8",
            "tgt_ix": "52-ARR_v1_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_8",
            "tgt_ix": "52-ARR_v1_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_9",
            "tgt_ix": "52-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_9",
            "tgt_ix": "52-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_9",
            "tgt_ix": "52-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_9",
            "tgt_ix": "52-ARR_v1_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_9",
            "tgt_ix": "52-ARR_v1_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_10",
            "tgt_ix": "52-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_11",
            "tgt_ix": "52-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_11",
            "tgt_ix": "52-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_11",
            "tgt_ix": "52-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_11",
            "tgt_ix": "52-ARR_v1_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_11",
            "tgt_ix": "52-ARR_v1_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_11",
            "tgt_ix": "52-ARR_v1_11@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_11",
            "tgt_ix": "52-ARR_v1_11@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_11",
            "tgt_ix": "52-ARR_v1_11@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_12",
            "tgt_ix": "52-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_12",
            "tgt_ix": "52-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_12",
            "tgt_ix": "52-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_12",
            "tgt_ix": "52-ARR_v1_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_12",
            "tgt_ix": "52-ARR_v1_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_12",
            "tgt_ix": "52-ARR_v1_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_12",
            "tgt_ix": "52-ARR_v1_12@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_13",
            "tgt_ix": "52-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_13",
            "tgt_ix": "52-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_13",
            "tgt_ix": "52-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_13",
            "tgt_ix": "52-ARR_v1_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_13",
            "tgt_ix": "52-ARR_v1_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_13",
            "tgt_ix": "52-ARR_v1_13@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_14",
            "tgt_ix": "52-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_15",
            "tgt_ix": "52-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_16",
            "tgt_ix": "52-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_16",
            "tgt_ix": "52-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_16",
            "tgt_ix": "52-ARR_v1_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_16",
            "tgt_ix": "52-ARR_v1_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_16",
            "tgt_ix": "52-ARR_v1_16@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_16",
            "tgt_ix": "52-ARR_v1_16@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_17",
            "tgt_ix": "52-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_18",
            "tgt_ix": "52-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_18",
            "tgt_ix": "52-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_18",
            "tgt_ix": "52-ARR_v1_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_18",
            "tgt_ix": "52-ARR_v1_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_18",
            "tgt_ix": "52-ARR_v1_18@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_19",
            "tgt_ix": "52-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_19",
            "tgt_ix": "52-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_19",
            "tgt_ix": "52-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_19",
            "tgt_ix": "52-ARR_v1_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_20",
            "tgt_ix": "52-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_21",
            "tgt_ix": "52-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_21",
            "tgt_ix": "52-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_21",
            "tgt_ix": "52-ARR_v1_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_22",
            "tgt_ix": "52-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_22",
            "tgt_ix": "52-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_22",
            "tgt_ix": "52-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_23",
            "tgt_ix": "52-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_23",
            "tgt_ix": "52-ARR_v1_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_23",
            "tgt_ix": "52-ARR_v1_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_24",
            "tgt_ix": "52-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_24",
            "tgt_ix": "52-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_25",
            "tgt_ix": "52-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_26",
            "tgt_ix": "52-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_26",
            "tgt_ix": "52-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_27",
            "tgt_ix": "52-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_28",
            "tgt_ix": "52-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_28",
            "tgt_ix": "52-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_28",
            "tgt_ix": "52-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_28",
            "tgt_ix": "52-ARR_v1_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_28",
            "tgt_ix": "52-ARR_v1_28@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_28",
            "tgt_ix": "52-ARR_v1_28@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_29",
            "tgt_ix": "52-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_30",
            "tgt_ix": "52-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_31",
            "tgt_ix": "52-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_31",
            "tgt_ix": "52-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_31",
            "tgt_ix": "52-ARR_v1_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_32",
            "tgt_ix": "52-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_33",
            "tgt_ix": "52-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_33",
            "tgt_ix": "52-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_33",
            "tgt_ix": "52-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_33",
            "tgt_ix": "52-ARR_v1_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_34",
            "tgt_ix": "52-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_35",
            "tgt_ix": "52-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_36",
            "tgt_ix": "52-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_36",
            "tgt_ix": "52-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_37",
            "tgt_ix": "52-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_37",
            "tgt_ix": "52-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_38",
            "tgt_ix": "52-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_39",
            "tgt_ix": "52-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_40",
            "tgt_ix": "52-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_41",
            "tgt_ix": "52-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_41",
            "tgt_ix": "52-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_42",
            "tgt_ix": "52-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_43",
            "tgt_ix": "52-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_44",
            "tgt_ix": "52-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_45",
            "tgt_ix": "52-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_46",
            "tgt_ix": "52-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_46",
            "tgt_ix": "52-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_46",
            "tgt_ix": "52-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_47",
            "tgt_ix": "52-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_47",
            "tgt_ix": "52-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_47",
            "tgt_ix": "52-ARR_v1_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_48",
            "tgt_ix": "52-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_48",
            "tgt_ix": "52-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_49",
            "tgt_ix": "52-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_49",
            "tgt_ix": "52-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_50",
            "tgt_ix": "52-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_50",
            "tgt_ix": "52-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_51",
            "tgt_ix": "52-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_52",
            "tgt_ix": "52-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_52",
            "tgt_ix": "52-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_52",
            "tgt_ix": "52-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_52",
            "tgt_ix": "52-ARR_v1_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_52",
            "tgt_ix": "52-ARR_v1_52@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_53",
            "tgt_ix": "52-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_54",
            "tgt_ix": "52-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_55",
            "tgt_ix": "52-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_56",
            "tgt_ix": "52-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_56",
            "tgt_ix": "52-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_56",
            "tgt_ix": "52-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_57",
            "tgt_ix": "52-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_58",
            "tgt_ix": "52-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_58",
            "tgt_ix": "52-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_59",
            "tgt_ix": "52-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_59",
            "tgt_ix": "52-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_59",
            "tgt_ix": "52-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_60",
            "tgt_ix": "52-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_60",
            "tgt_ix": "52-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_61",
            "tgt_ix": "52-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_62",
            "tgt_ix": "52-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_63",
            "tgt_ix": "52-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_64",
            "tgt_ix": "52-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_64",
            "tgt_ix": "52-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_64",
            "tgt_ix": "52-ARR_v1_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_64",
            "tgt_ix": "52-ARR_v1_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_64",
            "tgt_ix": "52-ARR_v1_64@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_65",
            "tgt_ix": "52-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_65",
            "tgt_ix": "52-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_65",
            "tgt_ix": "52-ARR_v1_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_66",
            "tgt_ix": "52-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_66",
            "tgt_ix": "52-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_66",
            "tgt_ix": "52-ARR_v1_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_66",
            "tgt_ix": "52-ARR_v1_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_66",
            "tgt_ix": "52-ARR_v1_66@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_66",
            "tgt_ix": "52-ARR_v1_66@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_66",
            "tgt_ix": "52-ARR_v1_66@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_66",
            "tgt_ix": "52-ARR_v1_66@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_67",
            "tgt_ix": "52-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_67",
            "tgt_ix": "52-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_67",
            "tgt_ix": "52-ARR_v1_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_68",
            "tgt_ix": "52-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_69",
            "tgt_ix": "52-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_69",
            "tgt_ix": "52-ARR_v1_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_69",
            "tgt_ix": "52-ARR_v1_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_69",
            "tgt_ix": "52-ARR_v1_69@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_69",
            "tgt_ix": "52-ARR_v1_69@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_69",
            "tgt_ix": "52-ARR_v1_69@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_69",
            "tgt_ix": "52-ARR_v1_69@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_69",
            "tgt_ix": "52-ARR_v1_69@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_70",
            "tgt_ix": "52-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_70",
            "tgt_ix": "52-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_70",
            "tgt_ix": "52-ARR_v1_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_70",
            "tgt_ix": "52-ARR_v1_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_71",
            "tgt_ix": "52-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_72",
            "tgt_ix": "52-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_72",
            "tgt_ix": "52-ARR_v1_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_72",
            "tgt_ix": "52-ARR_v1_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_72",
            "tgt_ix": "52-ARR_v1_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_72",
            "tgt_ix": "52-ARR_v1_72@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_72",
            "tgt_ix": "52-ARR_v1_72@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_73",
            "tgt_ix": "52-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_74",
            "tgt_ix": "52-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_74",
            "tgt_ix": "52-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_74",
            "tgt_ix": "52-ARR_v1_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_75",
            "tgt_ix": "52-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_76",
            "tgt_ix": "52-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_76",
            "tgt_ix": "52-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_76",
            "tgt_ix": "52-ARR_v1_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_77",
            "tgt_ix": "52-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_77",
            "tgt_ix": "52-ARR_v1_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_77",
            "tgt_ix": "52-ARR_v1_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_77",
            "tgt_ix": "52-ARR_v1_77@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_77",
            "tgt_ix": "52-ARR_v1_77@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_77",
            "tgt_ix": "52-ARR_v1_77@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_77",
            "tgt_ix": "52-ARR_v1_77@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_78",
            "tgt_ix": "52-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_79",
            "tgt_ix": "52-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_80",
            "tgt_ix": "52-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_80",
            "tgt_ix": "52-ARR_v1_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_80",
            "tgt_ix": "52-ARR_v1_80@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_80",
            "tgt_ix": "52-ARR_v1_80@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_80",
            "tgt_ix": "52-ARR_v1_80@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_81",
            "tgt_ix": "52-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_81",
            "tgt_ix": "52-ARR_v1_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_82",
            "tgt_ix": "52-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_82",
            "tgt_ix": "52-ARR_v1_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_83",
            "tgt_ix": "52-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_84",
            "tgt_ix": "52-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_85",
            "tgt_ix": "52-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_86",
            "tgt_ix": "52-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_87",
            "tgt_ix": "52-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_88",
            "tgt_ix": "52-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_89",
            "tgt_ix": "52-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_90",
            "tgt_ix": "52-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_91",
            "tgt_ix": "52-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_92",
            "tgt_ix": "52-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_93",
            "tgt_ix": "52-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_94",
            "tgt_ix": "52-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_95",
            "tgt_ix": "52-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_96",
            "tgt_ix": "52-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_97",
            "tgt_ix": "52-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_98",
            "tgt_ix": "52-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_99",
            "tgt_ix": "52-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_100",
            "tgt_ix": "52-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_101",
            "tgt_ix": "52-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_102",
            "tgt_ix": "52-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_103",
            "tgt_ix": "52-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_104",
            "tgt_ix": "52-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_105",
            "tgt_ix": "52-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_106",
            "tgt_ix": "52-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_107",
            "tgt_ix": "52-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_108",
            "tgt_ix": "52-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_109",
            "tgt_ix": "52-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_110",
            "tgt_ix": "52-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_111",
            "tgt_ix": "52-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_112",
            "tgt_ix": "52-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_113",
            "tgt_ix": "52-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_114",
            "tgt_ix": "52-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_115",
            "tgt_ix": "52-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_116",
            "tgt_ix": "52-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_117",
            "tgt_ix": "52-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_118",
            "tgt_ix": "52-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_119",
            "tgt_ix": "52-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_120",
            "tgt_ix": "52-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "52-ARR_v1_121",
            "tgt_ix": "52-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1348,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "52-ARR",
        "version": 1
    }
}