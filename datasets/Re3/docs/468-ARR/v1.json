{
    "nodes": [
        {
            "ix": "468-ARR_v1_0",
            "content": "Bias Mitigation in Machine Translation Quality Estimation",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_2",
            "content": "Machine Translation Quality Estimation (QE) aims to build predictive models to assess the quality of machine-generated translations in the absence of reference translations. While state-of-the-art QE models have been shown to achieve good results, they over-rely on features that do not have a causal impact on the quality of a translation. In particular, there appears to be a partial input bias, i.e., a tendency to assign high-quality scores to translations that are fluent and grammatically correct, even though they do not preserve the meaning of the source. We analyse the partial input bias in further detail and evaluate four approaches to use auxiliary tasks for bias mitigation. Two approaches use additional data to inform and support the main task, while the other two are adversarial, actively discouraging the model from learning the bias. We compare the methods with respect to their ability to reduce the partial input bias while maintaining the overall performance. We find that training a multitask architecture with an auxiliary binary classification task that utilises additional augmented data best achieves the desired effects and generalises well to different languages and quality metrics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "468-ARR_v1_4",
            "content": "Despite the great advances of Machine Translation (MT) models over the past years, the adequacy and fluency of the translations cannot be guaranteed. Without access to a gold-standard reference translation, it can be difficult to validate the reliability of the MT model's predictions. To address this issue, the field of MT Quality Estimation (QE) emerged, aiming to develop models that can approximate the quality of machine-generated translations in a scalable way. However, recent research suggests that state-of-the-art QE approaches tend to over-rely on features that do not have a causal impact on the quality of a translation. In particular, there appears to be a partial input bias, i.e. a tendency to assign high quality scores to translations that are fluent and grammatical, even though they do not resemble the actual meaning of the source (Sun et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_5",
            "content": "Building upon these findings, the objective of our work is to characterise and, most importantly, mitigate the partial input bias of QE models. We focus on the use of auxiliary training tasks to specifically target the observed biases while avoiding strong modifications of the original model as well as the expensive collection and manual labelling of additional training data. Our efforts concentrate on testing and improving MonoTran-sQuest, the best-performing architecture in the shared task on sentence-level QE hosted as part of the Fifth Conference on Machine Translation (WMT 2020) . We work with the recently published multilingual QE dataset MLQE-PE , allowing us to test the generalisability of our approaches across different languages and quality scores.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_6",
            "content": "Our main contributions are as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_7",
            "content": "\u2022 Bias analysis. We expand on previous research which suggested the partial input bias in QE and find that the model as well as the annotators tend to over-rate the quality of fluent but inadequate translations. \u2022 Bias mitigation. To the best of our knowledge, we are the first to explore the mitigation of biases with auxiliary tasks in the field of QE. We group our approaches into four categories:",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_8",
            "content": "Multitask training with mixed languages, multitask training with additional augmented data, training with adversarial tasks and training with debiased focal loss. \u2022 New architectures. We implement and compare several multitask architectures and find that iteratively training the tasks with two optimisers is better suited for our objective than backpropagating a weighted sum of the losses. Further, we reformulate focal loss for regression tasks, a technique that is traditionally based upon the cross-entropy loss. \u2022 Results. Utilising the multitask architecture, we successfully reduce the partial input bias while maintaining the same performance as the benchmark model and examine the best model's robustness.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_9",
            "content": "In the subsequent sections, we first present related work, followed by the analysis of the partial input bias. Building upon the findings, we explain the four bias mitigation approaches in Section 4 and discuss the results in Section 5.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_10",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "468-ARR_v1_11",
            "content": "Machine Translation Quality Estimation",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "468-ARR_v1_12",
            "content": "QE is an area of research concerned with the development of models for the prediction of the quality of machine-generated translations when gold standard translations are not available. QE is normally addressed as a supervised machine learning task, which may take as input general information from the source and translated texts, as well as from the MT system. The quality is typically assessed at sentence level, but word-and document-level QE are also possible (Specia et al., 2018, pp. 2). Sentencelevel QE has evolved from the first feature-heavy prediction models in Blatz et al. (2004) to neural architectures such as RNNs and Transformers (Vaswani et al., 2017), which accelerated the developments in the field by reducing the work of manual feature engineering and improving contextual representations (Kim et al., 2017;Wang et al., 2018;Fan et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_13",
            "content": "A prominent state-of-the-art QE architecture is MonoTransQuest, proposed by Ranasinghe et al. (2020). It builds upon XLM-R, a popular pretrained cross-lingual language model with a good ability to generalise to low-resource languages (Conneau et al., 2020). MonoTransQuest achieved the best results for sentence-level direct assessment score prediction in the WMT 2020 shared task on QE . Sun et al. (2020) showed that QE models like MonoTransQuest have a tendency to over-rely on spurious correlations, which is partially due to skewed label distributions and statistical artifacts in QE datasets. In particular, they show the existence of a partial input bias, i.e. the tendency to predict the quality of a translation based on just the target sentence (Poliak et al., 2018). While the fluency and grammatical correctness of the output is a factor influencing the quality, the original meaning should be preserved, which is only possible if the model takes both source and target into consideration. Following their work, in an attempt to reduce statistical artifacts, MLQE-PE -a new QE dataset diversifying the topics and languages covered -was created, which forms the basis of this work and will be described in more detail in Section 3.1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_14",
            "content": "Bias Mitigation with Auxiliary Tasks",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "468-ARR_v1_15",
            "content": "We define auxiliary tasks in a broad sense, using the term to refer to settings where a main task is trained alongside one or more helper tasks used to improve the main task's performance and generalisability. Most commonly, the tasks are trained in a multitask-setting, where some layers are shared across the tasks and some layers are task-specific. The auxiliary tasks can either be related to the main task or adversarial (Ruder, 2017). In addition, we consider the concept of debiased focal loss, where the main and auxiliary task are trained in separate models which are connected via the loss function (Karimi Mahabadi et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_16",
            "content": "Related Tasks: In settings where the data is limited, noisy or high-dimensional, using additional tasks is a way of introducing an inductive bias that prevents the model from overfitting to noise (Caruana, 1997). In addition, the model might be able to use new features that were learned through an auxiliary task for the main task as well (Ruder, 2019). MT models, for example, have been shown to benefit from auxiliary tasks such as named entity recognition, part-of-speech tagging and dependency parsing (Niehues and Cho, 2017;Kiperwasser and Ballesteros, 2018).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_17",
            "content": "Adversarial Tasks: Adversarial tasks can be used to actively discourage the model from overfitting to domain-specific, spurious cues. The technique was introduced by Ganin and Lempitsky (2015) and used for domain adaptation. More recently, it has been successfully used to reduce partial input biases in different fields of NLP, such as natural language inference (NLI) (Belinkov et al., 2019;Stacey et al., 2020) and visual question answering (VQA) (Ramakrishnan et al., 2018). The core idea is to train the auxiliary task using just the partial input. During backpropagation, the gradient is reversed. Consequently, the shared layers are updated such that the adversary's loss is maximised; the undesired behaviour is penalised. The methodology chapter illustrates the architectural design in more detail.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_18",
            "content": "Debiased Focal Loss: Another approach that has recently been used to mitigate known biases, particularly partial input biases, is debiased focal loss. The notion of focal loss was first introduced by Lin et al. (2017) as a means to improve classification results on imbalanced classes by downweighting the impact of samples that the model had already learned to classify well. In the field of NLI, Karimi Mahabadi et al. ( 2020) have shown that it is possible to adapt the notion of focal loss to mitigate partial input biases. They train the main model alongside a bias model that learns to predict the label based on the hypothesis only. In this scenario, the bias model's predictions are used to weigh the main model's cross-entropy loss. Intuitively, samples that are classified well by the bias model are weighted down so that the main model primarily learns from less biased inputs. The bias model is updated separately and discarded after training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_19",
            "content": "Bias Analysis",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "468-ARR_v1_20",
            "content": "In the following, we will describe the dataset and baseline model used, show benchmark results and analyse the partial input bias in more detail.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_21",
            "content": "Dataset",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "468-ARR_v1_22",
            "content": "We work with the MLQE-PE dataset which was specifically designed for the training of MT QE models. Published in 2020, it formed the basis for the WMT 2020 and 2021 shared tasks on Quality Estimation . 1 It consists of 6 high-, mid-and low-resource language pairs which originate from Wikipedia articles: English-German and English-Chinese, Romanian-English and Estonian-English as well as Nepalese-English and Sinhala-English. A seventh dataset, Russian-English, was collected based on Reddit posts and WikiQuotes. The translations were generated using Transformer-based Neural MT models. For each language, 9000 sentence pairs (7000 train, 1000 dev, 1000 test) were annotated on two different scales:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_23",
            "content": "\u2022 Human-targeted Translation Edit Rate (HTER): Each sentence-pair was edited by two independent translators. The reported HTER score is the averaged edit rate comparing the machine-generated translations and the post-edited versions. The score ranges between 0 (perfect translation) to 1 (everything was edited). \u2022 Direct Assessment Scores (DA): Each sentence pair was judged on a scale from 0-100 by at least 3 evaluators. The reported DA score is the mean of the individual judgements. Different than the HTER scores, the DA scale provides a measure of the severity of the errors, where inadequate (i.e. non-meaning preserving) translations should not receive a score higher than 70, even if only one word is incorrect.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_24",
            "content": "Benchmark",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "468-ARR_v1_25",
            "content": "We use the XLM-R based architecture MonoTrans-Quest as our baseline model, which fine-tunes XLM-R for sentence-level QE (Ranasinghe et al., 2020). While there are alternative candidates with a good performance on QE tasks, MonoTransQuest was chosen for several reasons: State-of-the-art performance, availability and replicability (all hyperparameters and the source code are open-sourced), as well as the generic design of the architecture which is transferable to related NLP domains. We train separate MonoTransQuest models for each combination of language pair and quality score using the originally proposed architecture and fine-tuned hyperparameters specified in the TransQuest GitHub repository. 2 All experiments were conducted on a 16GB Nvidia Tesla P100 GPU and averaged across five trainings on the seeds 555, 666, 777, 888 and 999. Our results are shown in Table 3 in the Appendix. In QE, the best practice is to use Pearson's r to measure performance (Specia et al., 2018, pp. 58). Most notably, the Pearson correlation between the predictions and the labels is lowest for the high-resource languages English-German and English-Chinese. This has also been observed in the QE shared task findings . A possible explanation is the high average quality of the generated translations, making the labelling significantly harder and the annotations less consistent, i.e. more noisy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_26",
            "content": "Partial Input Bias",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "468-ARR_v1_27",
            "content": "We examine the partial input bias by training the model on the combined representation of source and target and testing how the performance changes when the prediction is based on only one of the two. If the performance does not significantly decrease, the model has likely learned to base its predictions mostly on one part of the input. Figure 1 shows the results from this experiment. A clear target sentence bias can be observed for the English-German and English-Chinese language pairs. One reason could be the good quality of the translations that MT systems generate for high-resource languages: The occurrence of adequacy errors is lower, so that the target sentence may suffice for a decent prediction. In contrast, the mid-resource Romanian-English model, which shows the best overall performance, appears to be most dependent on both inputs. Figure 1 shows a clear performance deterioration when the model is tested on just the source or target sentence. One particularity of the RO-EN dataset is the high abundance of fluent, but clearly inadequate translations and hallucinations which require both the source and translation to be detected . Perhaps due to the distinct nature of Reddit data and WikiQuotes (both user-generated), the Russian-English dataset is an exception where the source sentence is a good predictor for the translation quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_28",
            "content": "To further examine the nature of the partial input bias, an in-depth analysis of the strongly affected English-German translations was conducted. In particular, the aim was to better understand how MonoTransQuest, but also the annotators, judge the quality of fluent but inadequate translations. To achieve this, we manually annotated translations in the test set that are grammatically correct but do not preserve the meaning of the source.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_29",
            "content": "In total, 145 out of 1000 translations were marked as fluent but inadequate. A key takeaway from the labelling process was that it is not only the models that have a partial input bias -human annotators clearly seem to over-rely on the target fluency. Despite the instructions clarly specify that a DA score below 70 should be assigned to inadequate translations, 3 annotators tended to give higher scores if the sentence was fluent and appeared logical. Figure 2 shows that more than half of the fluent but inadequate translations were given a score higher than 70, with an average rating of 81. 4 A likely reason is that adequacy-related mistakes are easy to miss when considering several quality factors, i.e. spelling, grammar and content, at the same time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_30",
            "content": "Methodology",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "468-ARR_v1_31",
            "content": "Based on the bias analysis, our goal is to find an effective and feasible way to reduce the impact of spurious correlations and overly dominant features. As outlined in the previous section, the two high-resource datasets (EN-DE and EN-ZH) clearly show the strongest partial input bias. They will therefore be at the centre of the bias mitigation efforts. All four methods presented hereinafter share the core idea of using auxiliary tasks to achieve this aim: The main task -QE -is combined with helper tasks designed to reduce known biases. At test time, the auxiliary tasks can be discarded. Hereinafter, we introduce four approaches and the corresponding model architectures. The first two methods are tailored to combat the biased behaviour by supporting the model with additional data. In contrast, the two alternative, restrictive approaches actively penalise the model for learning unwanted behaviour. We define three criteria to ensure comparability between the approaches: A good solution should 1) mitigate the observed biases, 2) retain the prediction quality of the benchmark model, and 3) avoid computational overhead and interference with the original model's design.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_32",
            "content": "Supportive Approaches",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "468-ARR_v1_33",
            "content": "We experiment with two different supporting tasks, each combining the main task and the auxiliary task in a multitask setup. The first approach is to train with different language pairs, aiming to transfer information between the language domains. Instead of mixing the languages arbitrarily, we build upon the bias analysis and examine if using a less biased language (RO-EN) to train the auxiliary task can help to reduce biases in the main task (EN-DE or EN-ZH). The bias analysis clearly showed that the models trained on the RO-EN dataset performed poorly when using just the source or target as input, indicating that the predictive power of the individual sentences is low. Thus, the incentive for the multitask model to over-rely on the target should be reduced. In this scenario, both tasks are regression problems and optimise the MSE loss.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_34",
            "content": "The second approach is to collect additional translations originating from the same topic and language domain and use it as the input for the auxiliary task. We choose WikiMatrix (Schwenk et al., 2021), a large parallel sentence corpus based on Wikipedia articles, as data source for the experiments. Without further preprocessing, the vast majority of these sentence pairs would qualify as good translations. While labelling on a continuous scale would require manual annotations, augmenting the data to achieve \"bad\" translations is more feasible. Hence, we augment 50% of the data to obtain bad translations. We experiment with two augmentation strategies: First, we shuffle the sentences to create mismatched sentence pairs. Second, we augment the sentence to mimic fluent but inadequate translations as seen in the original MLQE-PE dataset and discussed in Section 3.3. To do so, we implement a contextual augmentation pipeline that uses a language model (XLM-R) to replace 30% of the nouns, adjectives, verbs and adverbs such that the meaning of the sentence is changed while the grammatical correctness is preserved in the majority of cases. 5 In both cases, the main task optimises the MSE loss, however, the auxiliary task is a binary classification problem using the binary cross-entropy loss.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_35",
            "content": "Restrictive Approaches",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "468-ARR_v1_36",
            "content": "We experiment with two setups that directly penalise the biased behaviour. First, we combine the main task with an adversarial task in a multitask architecture. Intuitively, the adversary is incentivised to predict the quality scores based on the target sentence only. The shared layers, however, are penalised for learning a mapping between target sentence and scores. The risk of working with an adversarial task setup is that it optimises towards eliminating all cues associated with the adversary. In QE, however, the target sentence provides relevant information, such as grammar and spelling. As a result, the overall model performance might suffer. As an alternative to training with adversarial tasks and a multitask architecture in general, we repurpose the concept of debiased focal loss for regression. While model architecture and training method are different, the underlying idea to use the partial input based predictions to influence the learning remains the same. The subsequent section explains the multitask architecture used for the first three approaches as well as the re-formulated debiased focal loss technique in more detail.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_37",
            "content": "Architecture & Training",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "468-ARR_v1_38",
            "content": "MultiTransQuest",
            "ntype": "title",
            "meta": {
                "section": "4.3.1"
            }
        },
        {
            "ix": "468-ARR_v1_39",
            "content": "To realise the first three approaches, we propose the architecture MultiTransQuest, expanding on the MonoTransQuest baseline. The pre-trained language model XLM-R remains at the core and is entirely shared between tasks. The two key changes affect the final layers and the optimisation strategy: Firstly, we exchange the original prediction head to support multiple tasks. As illustrated in Figure 3, the final layers and loss functions are separate per task, thus allowing the mixing of regression and classification tasks. The figure exemplarily shows the adversarial setup, where the gradients are reversed during back-propagation, i.e. weighted with -1. For the two supportive tasks, we use the same setup but remove the weighted gradient layers and adjust the input and loss function for the auxiliary tasks accordingly. We experiment with different numbers of shared and separate layers. Secondly, we adapt the training procedure to support multiple tasks. The data loader is designed so that it alternates between the tasks per training step, with each batch containing only samples for one task which are then passed through the shared layers and the corresponding task-specific layers. We compare two optimisation strategies:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_40",
            "content": "\u2022 Training the tasks in turns, where backpropagation is performed per batch and task. Each task works with a separate AdamW optimizer to avoid averaging gradients across tasks. \u2022 Performing one forward pass for every task and combining the calculated losses as a weighted sum which is backpropagated through all layers using a single optimizer.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_41",
            "content": "Debiased Focal Loss Architecture",
            "ntype": "title",
            "meta": {
                "section": "4.3.2"
            }
        },
        {
            "ix": "468-ARR_v1_42",
            "content": "In contrast to the previously discussed multitask approaches, debiased focal loss enables a complete separation of the main model and bias model, thus requiring no changes to the core MonoTransQuest architecture. To the best of our knowledge, (debiased) focal loss has only been applied to classification tasks so far as it explicitly modifies the cross-entropy loss function. Since our QE task is formulated as a regression problem, we attempt to find an equivalent strategy to down-weigh biased examples when working with MSE loss. In our scenario, the bias model is trained on partial inputs, receiving the translated sentence only. The better the bias model's prediction, the lower the MSE and the more biased the sample. In line with the original debiased focal loss idea, we can therefore use the bias model's loss as an indication for the bias per sample.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_43",
            "content": "As the MSE loss can vary greatly during training, we decide against training both models in an endto-end approach. First, the trained bias model is used to predict the respective quality scores for the training set, using only the target. Next, the absolute error for each of the training samples is calculated. We use the error to approximate the partial input bias: The lower the error, the easier it is for the bias model to predict the sample's quality score correctly. To control the scale of the weights, we normalise the error value between 0 and 1. The resulting weights w are used to scale the MSE loss of the main model f M before backpropagation. We use the hyperparameter \u03b2 to exponentially scale the loss (Eq. 1). We further experiment with a sigmoid-shaped function scaled between 0 and 1 (Eq. 2).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_44",
            "content": "DFL = w \u03b2 f y i M (x i ) \u2212 \u0177i 2 (1) DFL = 1 1 + w 1\u2212w \u2212\u03b2 f y i M (x i ) \u2212 \u0177i 2 (2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_45",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "468-ARR_v1_46",
            "content": "In the following, we present and discuss the results of the experiments conducted. Based on the analysis in Section 3.3, the experiments concentrate on the two most biased datasets English-German and English-Chinese, each in combination with the DA and HTER scores. For each of the four sections, we assess different hyperparameter configurations on the EN-DE validation set. A configuration is considered to be good if the bias is reduced and the overall performance is at least maintained. The most promising variant is then evaluated on the EN-DE and EN-ZH test set, to see if the method generalises across language domains. Finally, we compare the four methods against one another and provide further analyses on the robustness of the best-performing model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_47",
            "content": "Hyperparameters and Design Choices",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "468-ARR_v1_48",
            "content": "Within each of the four approaches, we experiment with different hyperparameter configurations and design choices. While each setup requires individual fine-tuning, observed trends, backed by Table 4, 5, 6, 7 and 8 in the Appendix, include:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_49",
            "content": "\u2022 For the multitask architecture, training the tasks in turns with separate optimisers results in a good balance between bias reduction and maintaining performance. Backpropagating the weighted loss is also possible, but requires more task-specific fine-tuning. \u2022 For supportive auxiliary tasks, more separate layers, i.e. a larger degree of freedom, and a larger batch size improves the performance, for adversarial tasks the opposite is the case. \u2022 When augmenting additional WikiMatrix data, shuffling the sentence pairs achieves better results than mimicking fluent but inadequate translations with contextual augmentation. \u2022 The effect of the debiased focal loss technique is limited. A sigmoid-shaped weight distribution does not improve the results.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_50",
            "content": "Comparison of the Four Approaches",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "468-ARR_v1_51",
            "content": "Table 1 summarises the results obtained for each of the four methods. With respect to the choice of architecture, MultiTransQuest, used for methods 1-3, reduces the partial input bias more effectively than MonoTransQuest trained with focal loss. A key advantage of the multitask architecture is that the model is able to learn a balance between the tasks. In contrast, the degree of freedom is significantly limited for the focal loss architecture, where the main hyperparameter is how to scale the weights. We believe that this limitation is what makes the model even more sensitive to the inseparability of the bias and helpful features.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_52",
            "content": "Contrasting the multitask-training with related or adversarial tasks, we find that the two supportive methods maintain a solid performance across all four constellations, while also reducing the bias. Compared to this, the adversarial approach generalises less well, despite its successful application in NLI and VQA. We hypothesise that this discrepancy is rooted in the nature of the partial inputs: In VQA as well as NLI, the task can only be solved when considering both question and image or premise and hypothesis, respectively. In contrast, the translation provides information that is valuable for the QE model regardless of the source, such as the fluency of the generated sentence. Hence, it is difficult to isolate the bias from valuable information, an assumption that both adversarial training and the focal loss technique rely on. Without an unbiased reference dataset (which is hard to acquire due to the subjective nature of the annotation process) the line between desired information and bias is difficult to quantify. The lower the correlation between the existence of the bias and the performance of the adversarial task, the noisier the feedback that is propagated into the shared layers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_53",
            "content": "The best trade-off between overall performance and bias reduction is achieved with MultiTrans-Quest when combining the main task with a binary classification task trained on shuffled WikiMatrix data. The binary classification task is simple to learn, yet impossible to solve without paying equal attention to source and translation. For better illustration of the model behaviour and improvements, Figure 6 in the Appendix directly compares the performance and bias reduction achieved by the best model to the benchmark. In addition, Figures 7 and 8 show the distribution of DA and HTER predictions generated by the debiased model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_54",
            "content": "Since the reduction of the performance on the target sentence is only considering the reduction of the partial input bias, we additionally test the models in a zero-shot setting on RO-EN data. As elaborated on in Section 3.3, the RO-EN dataset provokes the partial input bias significantly less than the other language pairs. Consequently, a model with reduced partial input bias should perform better when tested on the dataset, indicating improved robustness. We train the MonoTransQuest benchmark and debiased MultiTransQuest architecture on the EN-DE and EN-ZH datasets and use these models to predict the respective scores on the RO-EN dataset. Since this is an out-of-domain setting, we do not expect the models to reach a performance that can compete with the models trained on Romanian-English data. However, the debiased models should outperform the benchmark. Indeed, Table 2 shows that all MultiTransQuest models outperform MonoTransQuest in this zero-shot scenario.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_55",
            "content": "Future Work",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "468-ARR_v1_56",
            "content": "Building upon the previously discussed results, we propose ideas for future work. Firstly, the multitask architecture provides additional degrees of freedom that were not explored extensively, yet. For example, one could vary the amount of training per task or even learn the training schedule as a parameter which adapts dynamically during the training process (Kiperwasser and Ballesteros, 2018;Zaremoodi et al., 2018). In addition, the number of auxiliary tasks could be increased to two or more, mixing different task types. Furthermore, it would be interesting to apply our proposed methods and architectures in adjacent fields of NLP. We think that the adapted debiased focal loss technique for regression might function in scenarios where it is possible to better isolate the bias and thus train a representative bias model. The most promising approach of training a multitask architecture with a supportive auxiliary task might generalise well to related settings, such as quality estimation for machine-generated text summaries. Seeing the method applied in the fields of NLI and VQA, both of which face partial input biases, would be intriguing, too.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_57",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "468-ARR_v1_58",
            "content": "This paper expands on recent research which suggests that QE models are susceptible to learning spurious correlations. Based on additional analysis, and inspired by related work in the fields of NLI and VQA, we propose a range of auxiliary tasks that inform the main Quality Estimation task during training and are discarded at test time. First, we train the main Quality Estimation task together with additional, less biased data in a multitask setting. Then, we explore adversarial training and debiased focal loss to directly target the partial input bias. We find that the former approaches yield more stable results than the latter and conjecture that this is due to the difficulty of isolating partial input bias effects from useful predictive information encoded in the translation. We show that our proposed multitask architecture MultiTransQuest, especially when trained with additional shuffled WikiMatrix data, generalises well across the two most biased language pairs and the two different quality scores. Our method retains the overall prediction quality, reduces the observed biases significantly and increases the models' robustness in a zero-shot setting. A Appendix",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "468-ARR_v1_59",
            "content": "Yonatan Belinkov, Adam Poliak, Stuart Shieber, Benjamin Van Durme, Alexander Rush, On Adversarial Removal of Hypothesis-only Bias in Natural Language Inference, 2019, Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019), .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Yonatan Belinkov",
                    "Adam Poliak",
                    "Stuart Shieber",
                    "Benjamin Van Durme",
                    "Alexander Rush"
                ],
                "title": "On Adversarial Removal of Hypothesis-only Bias in Natural Language Inference",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019)",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_60",
            "content": "John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, Nicola Ueffing, Confidence estimation for machine translation, 2004, COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "John Blatz",
                    "Erin Fitzgerald",
                    "George Foster",
                    "Simona Gandrabur",
                    "Cyril Goutte",
                    "Alex Kulesza",
                    "Alberto Sanchis",
                    "Nicola Ueffing"
                ],
                "title": "Confidence estimation for machine translation",
                "pub_date": "2004",
                "pub_title": "COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_61",
            "content": "UNKNOWN, None, 1997, Multitask Learning. Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "1997",
                "pub_title": "Multitask Learning. Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_62",
            "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Unsupervised cross-lingual representation learning at scale, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Alexis Conneau",
                    "Kartikay Khandelwal",
                    "Naman Goyal",
                    "Vishrav Chaudhary",
                    "Guillaume Wenzek",
                    "Francisco Guzm\u00e1n",
                    "Edouard Grave",
                    "Myle Ott",
                    "Luke Zettlemoyer",
                    "Veselin Stoyanov"
                ],
                "title": "Unsupervised cross-lingual representation learning at scale",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_63",
            "content": "Kai Fan, Bo Li, Feng-Ming Zhou, Jiayi Wang, Bilingual Expert\" Can Find Translation Errors, 2019, AAAI Conference. Association for the Advancement of Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Kai Fan",
                    "Bo Li",
                    "Feng-Ming Zhou",
                    "Jiayi Wang"
                ],
                "title": "Bilingual Expert\" Can Find Translation Errors",
                "pub_date": "2019",
                "pub_title": "AAAI Conference. Association for the Advancement of Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_64",
            "content": "UNKNOWN, None, 2020, MLQE-PE : a multilingual quality estimation and post-editing dataset. arXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "MLQE-PE : a multilingual quality estimation and post-editing dataset. arXiv",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_65",
            "content": "Yaroslav Ganin, Victor Lempitsky, Unsupervised Domain Adaptation by Backpropagation, 2015, Proceedings of the 32nd International Conference on International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Yaroslav Ganin",
                    "Victor Lempitsky"
                ],
                "title": "Unsupervised Domain Adaptation by Backpropagation",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 32nd International Conference on International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_66",
            "content": "Yonatan Rabeeh Karimi Mahabadi, James Belinkov,  Henderson, End-to-End Bias Mitigation by Modelling Biases in Corpora, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Yonatan Rabeeh Karimi Mahabadi",
                    "James Belinkov",
                    " Henderson"
                ],
                "title": "End-to-End Bias Mitigation by Modelling Biases in Corpora",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_67",
            "content": "Hyun Kim, Hun-Young Jung, Hongseok Kwon, Jong-Hyeok Lee, Seung-Hoon Na, None, 2017, ACM Transactions on Asian and Low-Resource Language Information Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Hyun Kim",
                    "Hun-Young Jung",
                    "Hongseok Kwon",
                    "Jong-Hyeok Lee",
                    "Seung-Hoon Na"
                ],
                "title": null,
                "pub_date": "2017",
                "pub_title": "ACM Transactions on Asian and Low-Resource Language Information Processing",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_68",
            "content": "Eliyahu Kiperwasser, Miguel Ballesteros, Scheduled Multi-Task Learning: From Syntax to Translation, 2018, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Eliyahu Kiperwasser",
                    "Miguel Ballesteros"
                ],
                "title": "Scheduled Multi-Task Learning: From Syntax to Translation",
                "pub_date": "2018",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_69",
            "content": "Tsung-Yi Lin, Priya Goyal, Ross Girshick, Focal Loss for Dense Object Detection, 2017, Proceedings of the IEEE International Conference on Computer Vision (ICCV), .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Tsung-Yi Lin",
                    "Priya Goyal",
                    "Ross Girshick"
                ],
                "title": "Focal Loss for Dense Object Detection",
                "pub_date": "2017",
                "pub_title": "Proceedings of the IEEE International Conference on Computer Vision (ICCV)",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_70",
            "content": "Jan Niehues, Eunah Cho, Exploiting Linguistic Resources for Neural Machine Translation Using Multi-task Learning, 2017, Proceedings of the Second Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Jan Niehues",
                    "Eunah Cho"
                ],
                "title": "Exploiting Linguistic Resources for Neural Machine Translation Using Multi-task Learning",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Second Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_71",
            "content": "Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, Benjamin Van Durme, Hypothesis only baselines in natural language inference, 2018, Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Adam Poliak",
                    "Jason Naradowsky",
                    "Aparajita Haldar",
                    "Rachel Rudinger",
                    "Benjamin Van Durme"
                ],
                "title": "Hypothesis only baselines in natural language inference",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "468-ARR_v1_72",
            "content": "Aishwarya Sainandan Ramakrishnan, Stefan Agrawal,  Lee, Overcoming Language Priors in Visual Question Answering with Adversarial Regularization, 2018, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Aishwarya Sainandan Ramakrishnan",
                    "Stefan Agrawal",
                    " Lee"
                ],
                "title": "Overcoming Language Priors in Visual Question Answering with Adversarial Regularization",
                "pub_date": "2018",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_73",
            "content": "Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov, TransQuest: Translation Quality Estimation with Cross-lingual Transformers, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Tharindu Ranasinghe",
                    "Constantin Orasan",
                    "Ruslan Mitkov"
                ],
                "title": "TransQuest: Translation Quality Estimation with Cross-lingual Transformers",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_74",
            "content": "Sebastian Ruder, An Overview of Multi-Task Learning in, 2017, Deep Neural Networks. arXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Sebastian Ruder"
                ],
                "title": "An Overview of Multi-Task Learning in",
                "pub_date": "2017",
                "pub_title": "Deep Neural Networks. arXiv",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_75",
            "content": "UNKNOWN, None, 2019, Neural Transfer Learning for Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Neural Transfer Learning for Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_76",
            "content": "UNKNOWN, None, 2021, Wiki-Matrix: Mining 135M Parallel Sentences, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Wiki-Matrix: Mining 135M Parallel Sentences",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_77",
            "content": ", Language Pairs from Wikipedia, , Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [],
                "title": "Language Pairs from Wikipedia",
                "pub_date": null,
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "468-ARR_v1_78",
            "content": "Lucia Specia, Fr\u00e9d\u00e9ric Blain, Marina Fomicheva, Erick Fonseca, Vishrav Chaudhary, Francisco Guzm\u00e1n, Andr\u00e9 F T Martins, Findings of the WMT 2020 Shared Task on Quality Estimation, 2020, Proceedings of the 5th Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Lucia Specia",
                    "Fr\u00e9d\u00e9ric Blain",
                    "Marina Fomicheva",
                    "Erick Fonseca",
                    "Vishrav Chaudhary",
                    "Francisco Guzm\u00e1n",
                    "Andr\u00e9 F T Martins"
                ],
                "title": "Findings of the WMT 2020 Shared Task on Quality Estimation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 5th Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_79",
            "content": "Lucia Specia, Carolina Scarton, Gustavo Paetzold, Quality Estimation for Machine Translation, 2018, Synthesis Lectures on Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Lucia Specia",
                    "Carolina Scarton",
                    "Gustavo Paetzold"
                ],
                "title": "Quality Estimation for Machine Translation",
                "pub_date": "2018",
                "pub_title": "Synthesis Lectures on Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "468-ARR_v1_80",
            "content": "UNKNOWN, None, , Sebastian Riedel, and Tim Rockt\u00e4schel. 2020. Avoiding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Sebastian Riedel, and Tim Rockt\u00e4schel. 2020. Avoiding",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "468-ARR_v1_0@0",
            "content": "Bias Mitigation in Machine Translation Quality Estimation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_0",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_2@0",
            "content": "Machine Translation Quality Estimation (QE) aims to build predictive models to assess the quality of machine-generated translations in the absence of reference translations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_2",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_2@1",
            "content": "While state-of-the-art QE models have been shown to achieve good results, they over-rely on features that do not have a causal impact on the quality of a translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_2",
            "start": 174,
            "end": 339,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_2@2",
            "content": "In particular, there appears to be a partial input bias, i.e., a tendency to assign high-quality scores to translations that are fluent and grammatically correct, even though they do not preserve the meaning of the source.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_2",
            "start": 341,
            "end": 562,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_2@3",
            "content": "We analyse the partial input bias in further detail and evaluate four approaches to use auxiliary tasks for bias mitigation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_2",
            "start": 564,
            "end": 687,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_2@4",
            "content": "Two approaches use additional data to inform and support the main task, while the other two are adversarial, actively discouraging the model from learning the bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_2",
            "start": 689,
            "end": 852,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_2@5",
            "content": "We compare the methods with respect to their ability to reduce the partial input bias while maintaining the overall performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_2",
            "start": 854,
            "end": 981,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_2@6",
            "content": "We find that training a multitask architecture with an auxiliary binary classification task that utilises additional augmented data best achieves the desired effects and generalises well to different languages and quality metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_2",
            "start": 983,
            "end": 1212,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_4@0",
            "content": "Despite the great advances of Machine Translation (MT) models over the past years, the adequacy and fluency of the translations cannot be guaranteed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_4",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_4@1",
            "content": "Without access to a gold-standard reference translation, it can be difficult to validate the reliability of the MT model's predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_4",
            "start": 150,
            "end": 284,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_4@2",
            "content": "To address this issue, the field of MT Quality Estimation (QE) emerged, aiming to develop models that can approximate the quality of machine-generated translations in a scalable way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_4",
            "start": 286,
            "end": 467,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_4@3",
            "content": "However, recent research suggests that state-of-the-art QE approaches tend to over-rely on features that do not have a causal impact on the quality of a translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_4",
            "start": 469,
            "end": 633,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_4@4",
            "content": "In particular, there appears to be a partial input bias, i.e. a tendency to assign high quality scores to translations that are fluent and grammatical, even though they do not resemble the actual meaning of the source (Sun et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_4",
            "start": 635,
            "end": 871,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_5@0",
            "content": "Building upon these findings, the objective of our work is to characterise and, most importantly, mitigate the partial input bias of QE models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_5",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_5@1",
            "content": "We focus on the use of auxiliary training tasks to specifically target the observed biases while avoiding strong modifications of the original model as well as the expensive collection and manual labelling of additional training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_5",
            "start": 144,
            "end": 377,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_5@2",
            "content": "Our efforts concentrate on testing and improving MonoTran-sQuest, the best-performing architecture in the shared task on sentence-level QE hosted as part of the Fifth Conference on Machine Translation (WMT 2020) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_5",
            "start": 379,
            "end": 591,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_5@3",
            "content": "We work with the recently published multilingual QE dataset MLQE-PE , allowing us to test the generalisability of our approaches across different languages and quality scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_5",
            "start": 593,
            "end": 767,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_6@0",
            "content": "Our main contributions are as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_6",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_7@0",
            "content": "\u2022 Bias analysis. We expand on previous research which suggested the partial input bias in QE and find that the model as well as the annotators tend to over-rate the quality of fluent but inadequate translations. \u2022 Bias mitigation. To the best of our knowledge, we are the first to explore the mitigation of biases with auxiliary tasks in the field of QE. We group our approaches into four categories:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_7",
            "start": 0,
            "end": 399,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_8@0",
            "content": "Multitask training with mixed languages, multitask training with additional augmented data, training with adversarial tasks and training with debiased focal loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_8",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_8@1",
            "content": "\u2022 New architectures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_8",
            "start": 163,
            "end": 182,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_8@2",
            "content": "We implement and compare several multitask architectures and find that iteratively training the tasks with two optimisers is better suited for our objective than backpropagating a weighted sum of the losses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_8",
            "start": 184,
            "end": 390,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_8@3",
            "content": "Further, we reformulate focal loss for regression tasks, a technique that is traditionally based upon the cross-entropy loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_8",
            "start": 392,
            "end": 516,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_8@4",
            "content": "\u2022 Results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_8",
            "start": 518,
            "end": 527,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_8@5",
            "content": "Utilising the multitask architecture, we successfully reduce the partial input bias while maintaining the same performance as the benchmark model and examine the best model's robustness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_8",
            "start": 529,
            "end": 714,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_9@0",
            "content": "In the subsequent sections, we first present related work, followed by the analysis of the partial input bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_9",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_9@1",
            "content": "Building upon the findings, we explain the four bias mitigation approaches in Section 4 and discuss the results in Section 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_9",
            "start": 111,
            "end": 235,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_10@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_10",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_11@0",
            "content": "Machine Translation Quality Estimation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_11",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_12@0",
            "content": "QE is an area of research concerned with the development of models for the prediction of the quality of machine-generated translations when gold standard translations are not available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_12",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_12@1",
            "content": "QE is normally addressed as a supervised machine learning task, which may take as input general information from the source and translated texts, as well as from the MT system.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_12",
            "start": 186,
            "end": 361,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_12@2",
            "content": "The quality is typically assessed at sentence level, but word-and document-level QE are also possible (Specia et al., 2018, pp. 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_12",
            "start": 363,
            "end": 493,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_12@3",
            "content": "Sentencelevel QE has evolved from the first feature-heavy prediction models in Blatz et al. (2004) to neural architectures such as RNNs and Transformers (Vaswani et al., 2017), which accelerated the developments in the field by reducing the work of manual feature engineering and improving contextual representations (Kim et al., 2017;Wang et al., 2018;Fan et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_12",
            "start": 495,
            "end": 865,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_13@0",
            "content": "A prominent state-of-the-art QE architecture is MonoTransQuest, proposed by Ranasinghe et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_13",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_13@1",
            "content": "It builds upon XLM-R, a popular pretrained cross-lingual language model with a good ability to generalise to low-resource languages (Conneau et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_13",
            "start": 102,
            "end": 256,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_13@2",
            "content": "MonoTransQuest achieved the best results for sentence-level direct assessment score prediction in the WMT 2020 shared task on QE .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_13",
            "start": 258,
            "end": 387,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_13@3",
            "content": "Sun et al. (2020) showed that QE models like MonoTransQuest have a tendency to over-rely on spurious correlations, which is partially due to skewed label distributions and statistical artifacts in QE datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_13",
            "start": 389,
            "end": 597,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_13@4",
            "content": "In particular, they show the existence of a partial input bias, i.e. the tendency to predict the quality of a translation based on just the target sentence (Poliak et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_13",
            "start": 599,
            "end": 776,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_13@5",
            "content": "While the fluency and grammatical correctness of the output is a factor influencing the quality, the original meaning should be preserved, which is only possible if the model takes both source and target into consideration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_13",
            "start": 778,
            "end": 1000,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_13@6",
            "content": "Following their work, in an attempt to reduce statistical artifacts, MLQE-PE -a new QE dataset diversifying the topics and languages covered -was created, which forms the basis of this work and will be described in more detail in Section 3.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_13",
            "start": 1002,
            "end": 1243,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_14@0",
            "content": "Bias Mitigation with Auxiliary Tasks",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_14",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_15@0",
            "content": "We define auxiliary tasks in a broad sense, using the term to refer to settings where a main task is trained alongside one or more helper tasks used to improve the main task's performance and generalisability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_15",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_15@1",
            "content": "Most commonly, the tasks are trained in a multitask-setting, where some layers are shared across the tasks and some layers are task-specific.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_15",
            "start": 210,
            "end": 350,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_15@2",
            "content": "The auxiliary tasks can either be related to the main task or adversarial (Ruder, 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_15",
            "start": 352,
            "end": 439,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_15@3",
            "content": "In addition, we consider the concept of debiased focal loss, where the main and auxiliary task are trained in separate models which are connected via the loss function (Karimi Mahabadi et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_15",
            "start": 441,
            "end": 639,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_16@0",
            "content": "Related Tasks: In settings where the data is limited, noisy or high-dimensional, using additional tasks is a way of introducing an inductive bias that prevents the model from overfitting to noise (Caruana, 1997).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_16",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_16@1",
            "content": "In addition, the model might be able to use new features that were learned through an auxiliary task for the main task as well (Ruder, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_16",
            "start": 213,
            "end": 353,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_16@2",
            "content": "MT models, for example, have been shown to benefit from auxiliary tasks such as named entity recognition, part-of-speech tagging and dependency parsing (Niehues and Cho, 2017;Kiperwasser and Ballesteros, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_16",
            "start": 355,
            "end": 564,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_17@0",
            "content": "Adversarial Tasks: Adversarial tasks can be used to actively discourage the model from overfitting to domain-specific, spurious cues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_17",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_17@1",
            "content": "The technique was introduced by Ganin and Lempitsky (2015) and used for domain adaptation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_17",
            "start": 134,
            "end": 223,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_17@2",
            "content": "More recently, it has been successfully used to reduce partial input biases in different fields of NLP, such as natural language inference (NLI) (Belinkov et al., 2019;Stacey et al., 2020) and visual question answering (VQA) (Ramakrishnan et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_17",
            "start": 225,
            "end": 477,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_17@3",
            "content": "The core idea is to train the auxiliary task using just the partial input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_17",
            "start": 479,
            "end": 552,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_17@4",
            "content": "During backpropagation, the gradient is reversed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_17",
            "start": 554,
            "end": 602,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_17@5",
            "content": "Consequently, the shared layers are updated such that the adversary's loss is maximised; the undesired behaviour is penalised.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_17",
            "start": 604,
            "end": 729,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_17@6",
            "content": "The methodology chapter illustrates the architectural design in more detail.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_17",
            "start": 731,
            "end": 806,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_18@0",
            "content": "Debiased Focal Loss: Another approach that has recently been used to mitigate known biases, particularly partial input biases, is debiased focal loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_18",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_18@1",
            "content": "The notion of focal loss was first introduced by Lin et al. (2017) as a means to improve classification results on imbalanced classes by downweighting the impact of samples that the model had already learned to classify well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_18",
            "start": 151,
            "end": 375,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_18@2",
            "content": "In the field of NLI, Karimi Mahabadi et al. ( 2020) have shown that it is possible to adapt the notion of focal loss to mitigate partial input biases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_18",
            "start": 377,
            "end": 526,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_18@3",
            "content": "They train the main model alongside a bias model that learns to predict the label based on the hypothesis only.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_18",
            "start": 528,
            "end": 638,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_18@4",
            "content": "In this scenario, the bias model's predictions are used to weigh the main model's cross-entropy loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_18",
            "start": 640,
            "end": 740,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_18@5",
            "content": "Intuitively, samples that are classified well by the bias model are weighted down so that the main model primarily learns from less biased inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_18",
            "start": 742,
            "end": 887,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_18@6",
            "content": "The bias model is updated separately and discarded after training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_18",
            "start": 889,
            "end": 954,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_19@0",
            "content": "Bias Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_19",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_20@0",
            "content": "In the following, we will describe the dataset and baseline model used, show benchmark results and analyse the partial input bias in more detail.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_20",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_21@0",
            "content": "Dataset",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_21",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_22@0",
            "content": "We work with the MLQE-PE dataset which was specifically designed for the training of MT QE models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_22",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_22@1",
            "content": "Published in 2020, it formed the basis for the WMT 2020 and 2021 shared tasks on Quality Estimation .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_22",
            "start": 99,
            "end": 199,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_22@2",
            "content": "1 It consists of 6 high-, mid-and low-resource language pairs which originate from Wikipedia articles: English-German and English-Chinese, Romanian-English and Estonian-English as well as Nepalese-English and Sinhala-English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_22",
            "start": 201,
            "end": 425,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_22@3",
            "content": "A seventh dataset, Russian-English, was collected based on Reddit posts and WikiQuotes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_22",
            "start": 427,
            "end": 513,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_22@4",
            "content": "The translations were generated using Transformer-based Neural MT models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_22",
            "start": 515,
            "end": 587,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_22@5",
            "content": "For each language, 9000 sentence pairs (7000 train, 1000 dev, 1000 test) were annotated on two different scales:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_22",
            "start": 589,
            "end": 700,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_23@0",
            "content": "\u2022 Human-targeted Translation Edit Rate (HTER): Each sentence-pair was edited by two independent translators. The reported HTER score is the averaged edit rate comparing the machine-generated translations and the post-edited versions. The score ranges between 0 (perfect translation) to 1 (everything was edited). \u2022 Direct Assessment Scores (DA): Each sentence pair was judged on a scale from 0-100 by at least 3 evaluators. The reported DA score is the mean of the individual judgements. Different than the HTER scores, the DA scale provides a measure of the severity of the errors, where inadequate (i.e. non-meaning preserving) translations should not receive a score higher than 70, even if only one word is incorrect.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_23",
            "start": 0,
            "end": 720,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_24@0",
            "content": "Benchmark",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_24",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_25@0",
            "content": "We use the XLM-R based architecture MonoTrans-Quest as our baseline model, which fine-tunes XLM-R for sentence-level QE (Ranasinghe et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_25",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_25@1",
            "content": "While there are alternative candidates with a good performance on QE tasks, MonoTransQuest was chosen for several reasons: State-of-the-art performance, availability and replicability (all hyperparameters and the source code are open-sourced), as well as the generic design of the architecture which is transferable to related NLP domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_25",
            "start": 147,
            "end": 485,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_25@2",
            "content": "We train separate MonoTransQuest models for each combination of language pair and quality score using the originally proposed architecture and fine-tuned hyperparameters specified in the TransQuest GitHub repository.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_25",
            "start": 487,
            "end": 702,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_25@3",
            "content": "2 All experiments were conducted on a 16GB Nvidia Tesla P100 GPU and averaged across five trainings on the seeds 555, 666, 777, 888 and 999.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_25",
            "start": 704,
            "end": 843,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_25@4",
            "content": "Our results are shown in Table 3 in the Appendix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_25",
            "start": 845,
            "end": 893,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_25@5",
            "content": "In QE, the best practice is to use Pearson's r to measure performance (Specia et al., 2018, pp. 58).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_25",
            "start": 895,
            "end": 994,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_25@6",
            "content": "Most notably, the Pearson correlation between the predictions and the labels is lowest for the high-resource languages English-German and English-Chinese.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_25",
            "start": 996,
            "end": 1149,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_25@7",
            "content": "This has also been observed in the QE shared task findings .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_25",
            "start": 1151,
            "end": 1210,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_25@8",
            "content": "A possible explanation is the high average quality of the generated translations, making the labelling significantly harder and the annotations less consistent, i.e. more noisy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_25",
            "start": 1212,
            "end": 1388,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_26@0",
            "content": "Partial Input Bias",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_26",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_27@0",
            "content": "We examine the partial input bias by training the model on the combined representation of source and target and testing how the performance changes when the prediction is based on only one of the two.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_27",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_27@1",
            "content": "If the performance does not significantly decrease, the model has likely learned to base its predictions mostly on one part of the input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_27",
            "start": 201,
            "end": 337,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_27@2",
            "content": "Figure 1 shows the results from this experiment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_27",
            "start": 339,
            "end": 386,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_27@3",
            "content": "A clear target sentence bias can be observed for the English-German and English-Chinese language pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_27",
            "start": 388,
            "end": 490,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_27@4",
            "content": "One reason could be the good quality of the translations that MT systems generate for high-resource languages: The occurrence of adequacy errors is lower, so that the target sentence may suffice for a decent prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_27",
            "start": 492,
            "end": 710,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_27@5",
            "content": "In contrast, the mid-resource Romanian-English model, which shows the best overall performance, appears to be most dependent on both inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_27",
            "start": 712,
            "end": 851,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_27@6",
            "content": "Figure 1 shows a clear performance deterioration when the model is tested on just the source or target sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_27",
            "start": 853,
            "end": 964,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_27@7",
            "content": "One particularity of the RO-EN dataset is the high abundance of fluent, but clearly inadequate translations and hallucinations which require both the source and translation to be detected .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_27",
            "start": 966,
            "end": 1154,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_27@8",
            "content": "Perhaps due to the distinct nature of Reddit data and WikiQuotes (both user-generated), the Russian-English dataset is an exception where the source sentence is a good predictor for the translation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_27",
            "start": 1156,
            "end": 1361,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_28@0",
            "content": "To further examine the nature of the partial input bias, an in-depth analysis of the strongly affected English-German translations was conducted.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_28",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_28@1",
            "content": "In particular, the aim was to better understand how MonoTransQuest, but also the annotators, judge the quality of fluent but inadequate translations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_28",
            "start": 146,
            "end": 294,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_28@2",
            "content": "To achieve this, we manually annotated translations in the test set that are grammatically correct but do not preserve the meaning of the source.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_28",
            "start": 296,
            "end": 440,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_29@0",
            "content": "In total, 145 out of 1000 translations were marked as fluent but inadequate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_29",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_29@1",
            "content": "A key takeaway from the labelling process was that it is not only the models that have a partial input bias -human annotators clearly seem to over-rely on the target fluency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_29",
            "start": 77,
            "end": 250,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_29@2",
            "content": "Despite the instructions clarly specify that a DA score below 70 should be assigned to inadequate translations, 3 annotators tended to give higher scores if the sentence was fluent and appeared logical.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_29",
            "start": 252,
            "end": 453,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_29@3",
            "content": "Figure 2 shows that more than half of the fluent but inadequate translations were given a score higher than 70, with an average rating of 81.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_29",
            "start": 455,
            "end": 595,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_29@4",
            "content": "4 A likely reason is that adequacy-related mistakes are easy to miss when considering several quality factors, i.e. spelling, grammar and content, at the same time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_29",
            "start": 597,
            "end": 760,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_30@0",
            "content": "Methodology",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_30",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_31@0",
            "content": "Based on the bias analysis, our goal is to find an effective and feasible way to reduce the impact of spurious correlations and overly dominant features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_31",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_31@1",
            "content": "As outlined in the previous section, the two high-resource datasets (EN-DE and EN-ZH) clearly show the strongest partial input bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_31",
            "start": 154,
            "end": 285,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_31@2",
            "content": "They will therefore be at the centre of the bias mitigation efforts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_31",
            "start": 287,
            "end": 354,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_31@3",
            "content": "All four methods presented hereinafter share the core idea of using auxiliary tasks to achieve this aim: The main task -QE -is combined with helper tasks designed to reduce known biases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_31",
            "start": 356,
            "end": 541,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_31@4",
            "content": "At test time, the auxiliary tasks can be discarded.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_31",
            "start": 543,
            "end": 593,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_31@5",
            "content": "Hereinafter, we introduce four approaches and the corresponding model architectures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_31",
            "start": 595,
            "end": 678,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_31@6",
            "content": "The first two methods are tailored to combat the biased behaviour by supporting the model with additional data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_31",
            "start": 680,
            "end": 790,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_31@7",
            "content": "In contrast, the two alternative, restrictive approaches actively penalise the model for learning unwanted behaviour.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_31",
            "start": 792,
            "end": 908,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_31@8",
            "content": "We define three criteria to ensure comparability between the approaches: A good solution should 1) mitigate the observed biases,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_31",
            "start": 910,
            "end": 1037,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_31@9",
            "content": "2) retain the prediction quality of the benchmark model, and 3) avoid computational overhead and interference with the original model's design.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_31",
            "start": 1039,
            "end": 1181,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_32@0",
            "content": "Supportive Approaches",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_32",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_33@0",
            "content": "We experiment with two different supporting tasks, each combining the main task and the auxiliary task in a multitask setup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_33",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_33@1",
            "content": "The first approach is to train with different language pairs, aiming to transfer information between the language domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_33",
            "start": 125,
            "end": 246,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_33@2",
            "content": "Instead of mixing the languages arbitrarily, we build upon the bias analysis and examine if using a less biased language (RO-EN) to train the auxiliary task can help to reduce biases in the main task (EN-DE or EN-ZH).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_33",
            "start": 248,
            "end": 464,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_33@3",
            "content": "The bias analysis clearly showed that the models trained on the RO-EN dataset performed poorly when using just the source or target as input, indicating that the predictive power of the individual sentences is low.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_33",
            "start": 466,
            "end": 679,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_33@4",
            "content": "Thus, the incentive for the multitask model to over-rely on the target should be reduced.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_33",
            "start": 681,
            "end": 769,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_33@5",
            "content": "In this scenario, both tasks are regression problems and optimise the MSE loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_33",
            "start": 771,
            "end": 849,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_34@0",
            "content": "The second approach is to collect additional translations originating from the same topic and language domain and use it as the input for the auxiliary task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_34",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_34@1",
            "content": "We choose WikiMatrix (Schwenk et al., 2021), a large parallel sentence corpus based on Wikipedia articles, as data source for the experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_34",
            "start": 158,
            "end": 299,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_34@2",
            "content": "Without further preprocessing, the vast majority of these sentence pairs would qualify as good translations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_34",
            "start": 301,
            "end": 408,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_34@3",
            "content": "While labelling on a continuous scale would require manual annotations, augmenting the data to achieve \"bad\" translations is more feasible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_34",
            "start": 410,
            "end": 548,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_34@4",
            "content": "Hence, we augment 50% of the data to obtain bad translations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_34",
            "start": 550,
            "end": 610,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_34@5",
            "content": "We experiment with two augmentation strategies: First, we shuffle the sentences to create mismatched sentence pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_34",
            "start": 612,
            "end": 727,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_34@6",
            "content": "Second, we augment the sentence to mimic fluent but inadequate translations as seen in the original MLQE-PE dataset and discussed in Section 3.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_34",
            "start": 729,
            "end": 873,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_34@7",
            "content": "To do so, we implement a contextual augmentation pipeline that uses a language model (XLM-R) to replace 30% of the nouns, adjectives, verbs and adverbs such that the meaning of the sentence is changed while the grammatical correctness is preserved in the majority of cases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_34",
            "start": 875,
            "end": 1147,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_34@8",
            "content": "5 In both cases, the main task optimises the MSE loss, however, the auxiliary task is a binary classification problem using the binary cross-entropy loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_34",
            "start": 1149,
            "end": 1302,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_35@0",
            "content": "Restrictive Approaches",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_35",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_36@0",
            "content": "We experiment with two setups that directly penalise the biased behaviour.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_36",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_36@1",
            "content": "First, we combine the main task with an adversarial task in a multitask architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_36",
            "start": 75,
            "end": 159,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_36@2",
            "content": "Intuitively, the adversary is incentivised to predict the quality scores based on the target sentence only.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_36",
            "start": 161,
            "end": 267,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_36@3",
            "content": "The shared layers, however, are penalised for learning a mapping between target sentence and scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_36",
            "start": 269,
            "end": 368,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_36@4",
            "content": "The risk of working with an adversarial task setup is that it optimises towards eliminating all cues associated with the adversary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_36",
            "start": 370,
            "end": 500,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_36@5",
            "content": "In QE, however, the target sentence provides relevant information, such as grammar and spelling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_36",
            "start": 502,
            "end": 597,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_36@6",
            "content": "As a result, the overall model performance might suffer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_36",
            "start": 599,
            "end": 654,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_36@7",
            "content": "As an alternative to training with adversarial tasks and a multitask architecture in general, we repurpose the concept of debiased focal loss for regression.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_36",
            "start": 656,
            "end": 812,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_36@8",
            "content": "While model architecture and training method are different, the underlying idea to use the partial input based predictions to influence the learning remains the same.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_36",
            "start": 814,
            "end": 979,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_36@9",
            "content": "The subsequent section explains the multitask architecture used for the first three approaches as well as the re-formulated debiased focal loss technique in more detail.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_36",
            "start": 981,
            "end": 1149,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_37@0",
            "content": "Architecture & Training",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_37",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_38@0",
            "content": "MultiTransQuest",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_38",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_39@0",
            "content": "To realise the first three approaches, we propose the architecture MultiTransQuest, expanding on the MonoTransQuest baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_39",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_39@1",
            "content": "The pre-trained language model XLM-R remains at the core and is entirely shared between tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_39",
            "start": 126,
            "end": 219,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_39@2",
            "content": "The two key changes affect the final layers and the optimisation strategy: Firstly, we exchange the original prediction head to support multiple tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_39",
            "start": 221,
            "end": 371,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_39@3",
            "content": "As illustrated in Figure 3, the final layers and loss functions are separate per task, thus allowing the mixing of regression and classification tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_39",
            "start": 373,
            "end": 523,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_39@4",
            "content": "The figure exemplarily shows the adversarial setup, where the gradients are reversed during back-propagation, i.e. weighted with -1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_39",
            "start": 525,
            "end": 656,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_39@5",
            "content": "For the two supportive tasks, we use the same setup but remove the weighted gradient layers and adjust the input and loss function for the auxiliary tasks accordingly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_39",
            "start": 658,
            "end": 824,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_39@6",
            "content": "We experiment with different numbers of shared and separate layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_39",
            "start": 826,
            "end": 892,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_39@7",
            "content": "Secondly, we adapt the training procedure to support multiple tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_39",
            "start": 894,
            "end": 961,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_39@8",
            "content": "The data loader is designed so that it alternates between the tasks per training step, with each batch containing only samples for one task which are then passed through the shared layers and the corresponding task-specific layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_39",
            "start": 963,
            "end": 1193,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_39@9",
            "content": "We compare two optimisation strategies:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_39",
            "start": 1195,
            "end": 1233,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_40@0",
            "content": "\u2022 Training the tasks in turns, where backpropagation is performed per batch and task. Each task works with a separate AdamW optimizer to avoid averaging gradients across tasks. \u2022 Performing one forward pass for every task and combining the calculated losses as a weighted sum which is backpropagated through all layers using a single optimizer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_40",
            "start": 0,
            "end": 343,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_41@0",
            "content": "Debiased Focal Loss Architecture",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_41",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_42@0",
            "content": "In contrast to the previously discussed multitask approaches, debiased focal loss enables a complete separation of the main model and bias model, thus requiring no changes to the core MonoTransQuest architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_42",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_42@1",
            "content": "To the best of our knowledge, (debiased) focal loss has only been applied to classification tasks so far as it explicitly modifies the cross-entropy loss function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_42",
            "start": 213,
            "end": 375,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_42@2",
            "content": "Since our QE task is formulated as a regression problem, we attempt to find an equivalent strategy to down-weigh biased examples when working with MSE loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_42",
            "start": 377,
            "end": 532,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_42@3",
            "content": "In our scenario, the bias model is trained on partial inputs, receiving the translated sentence only.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_42",
            "start": 534,
            "end": 634,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_42@4",
            "content": "The better the bias model's prediction, the lower the MSE and the more biased the sample.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_42",
            "start": 636,
            "end": 724,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_42@5",
            "content": "In line with the original debiased focal loss idea, we can therefore use the bias model's loss as an indication for the bias per sample.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_42",
            "start": 726,
            "end": 861,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_43@0",
            "content": "As the MSE loss can vary greatly during training, we decide against training both models in an endto-end approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_43",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_43@1",
            "content": "First, the trained bias model is used to predict the respective quality scores for the training set, using only the target.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_43",
            "start": 115,
            "end": 237,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_43@2",
            "content": "Next, the absolute error for each of the training samples is calculated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_43",
            "start": 239,
            "end": 310,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_43@3",
            "content": "We use the error to approximate the partial input bias: The lower the error, the easier it is for the bias model to predict the sample's quality score correctly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_43",
            "start": 312,
            "end": 472,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_43@4",
            "content": "To control the scale of the weights, we normalise the error value between 0 and 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_43",
            "start": 474,
            "end": 555,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_43@5",
            "content": "The resulting weights w are used to scale the MSE loss of the main model f M before backpropagation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_43",
            "start": 557,
            "end": 656,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_43@6",
            "content": "We use the hyperparameter \u03b2 to exponentially scale the loss (Eq. 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_43",
            "start": 658,
            "end": 725,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_43@7",
            "content": "We further experiment with a sigmoid-shaped function scaled between 0 and 1 (Eq. 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_43",
            "start": 727,
            "end": 810,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_44@0",
            "content": "DFL = w \u03b2 f y i M (x i ) \u2212 \u0177i 2 (1) DFL = 1 1 + w 1\u2212w \u2212\u03b2 f y i M (x i ) \u2212 \u0177i 2 (2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_44",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_45@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_45",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_46@0",
            "content": "In the following, we present and discuss the results of the experiments conducted.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_46",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_46@1",
            "content": "Based on the analysis in Section 3.3, the experiments concentrate on the two most biased datasets English-German and English-Chinese, each in combination with the DA and HTER scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_46",
            "start": 83,
            "end": 264,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_46@2",
            "content": "For each of the four sections, we assess different hyperparameter configurations on the EN-DE validation set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_46",
            "start": 266,
            "end": 374,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_46@3",
            "content": "A configuration is considered to be good if the bias is reduced and the overall performance is at least maintained.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_46",
            "start": 376,
            "end": 490,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_46@4",
            "content": "The most promising variant is then evaluated on the EN-DE and EN-ZH test set, to see if the method generalises across language domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_46",
            "start": 492,
            "end": 626,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_46@5",
            "content": "Finally, we compare the four methods against one another and provide further analyses on the robustness of the best-performing model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_46",
            "start": 628,
            "end": 760,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_47@0",
            "content": "Hyperparameters and Design Choices",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_47",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_48@0",
            "content": "Within each of the four approaches, we experiment with different hyperparameter configurations and design choices.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_48",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_48@1",
            "content": "While each setup requires individual fine-tuning, observed trends, backed by Table 4, 5, 6, 7 and 8 in the Appendix, include:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_48",
            "start": 115,
            "end": 239,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_49@0",
            "content": "\u2022 For the multitask architecture, training the tasks in turns with separate optimisers results in a good balance between bias reduction and maintaining performance. Backpropagating the weighted loss is also possible, but requires more task-specific fine-tuning. \u2022 For supportive auxiliary tasks, more separate layers, i.e. a larger degree of freedom, and a larger batch size improves the performance, for adversarial tasks the opposite is the case. \u2022 When augmenting additional WikiMatrix data, shuffling the sentence pairs achieves better results than mimicking fluent but inadequate translations with contextual augmentation. \u2022 The effect of the debiased focal loss technique is limited. A sigmoid-shaped weight distribution does not improve the results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_49",
            "start": 0,
            "end": 755,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_50@0",
            "content": "Comparison of the Four Approaches",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_50",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_51@0",
            "content": "Table 1 summarises the results obtained for each of the four methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_51",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_51@1",
            "content": "With respect to the choice of architecture, MultiTransQuest, used for methods 1-3, reduces the partial input bias more effectively than MonoTransQuest trained with focal loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_51",
            "start": 70,
            "end": 244,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_51@2",
            "content": "A key advantage of the multitask architecture is that the model is able to learn a balance between the tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_51",
            "start": 246,
            "end": 354,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_51@3",
            "content": "In contrast, the degree of freedom is significantly limited for the focal loss architecture, where the main hyperparameter is how to scale the weights.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_51",
            "start": 356,
            "end": 506,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_51@4",
            "content": "We believe that this limitation is what makes the model even more sensitive to the inseparability of the bias and helpful features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_51",
            "start": 508,
            "end": 638,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_52@0",
            "content": "Contrasting the multitask-training with related or adversarial tasks, we find that the two supportive methods maintain a solid performance across all four constellations, while also reducing the bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_52",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_52@1",
            "content": "Compared to this, the adversarial approach generalises less well, despite its successful application in NLI and VQA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_52",
            "start": 201,
            "end": 316,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_52@2",
            "content": "We hypothesise that this discrepancy is rooted in the nature of the partial inputs: In VQA as well as NLI, the task can only be solved when considering both question and image or premise and hypothesis, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_52",
            "start": 318,
            "end": 533,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_52@3",
            "content": "In contrast, the translation provides information that is valuable for the QE model regardless of the source, such as the fluency of the generated sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_52",
            "start": 535,
            "end": 690,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_52@4",
            "content": "Hence, it is difficult to isolate the bias from valuable information, an assumption that both adversarial training and the focal loss technique rely on.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_52",
            "start": 692,
            "end": 843,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_52@5",
            "content": "Without an unbiased reference dataset (which is hard to acquire due to the subjective nature of the annotation process) the line between desired information and bias is difficult to quantify.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_52",
            "start": 845,
            "end": 1035,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_52@6",
            "content": "The lower the correlation between the existence of the bias and the performance of the adversarial task, the noisier the feedback that is propagated into the shared layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_52",
            "start": 1037,
            "end": 1208,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_53@0",
            "content": "The best trade-off between overall performance and bias reduction is achieved with MultiTrans-Quest when combining the main task with a binary classification task trained on shuffled WikiMatrix data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_53",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_53@1",
            "content": "The binary classification task is simple to learn, yet impossible to solve without paying equal attention to source and translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_53",
            "start": 200,
            "end": 331,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_53@2",
            "content": "For better illustration of the model behaviour and improvements, Figure 6 in the Appendix directly compares the performance and bias reduction achieved by the best model to the benchmark.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_53",
            "start": 333,
            "end": 519,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_53@3",
            "content": "In addition, Figures 7 and 8 show the distribution of DA and HTER predictions generated by the debiased model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_53",
            "start": 521,
            "end": 630,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_54@0",
            "content": "Since the reduction of the performance on the target sentence is only considering the reduction of the partial input bias, we additionally test the models in a zero-shot setting on RO-EN data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_54",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_54@1",
            "content": "As elaborated on in Section 3.3, the RO-EN dataset provokes the partial input bias significantly less than the other language pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_54",
            "start": 193,
            "end": 324,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_54@2",
            "content": "Consequently, a model with reduced partial input bias should perform better when tested on the dataset, indicating improved robustness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_54",
            "start": 326,
            "end": 460,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_54@3",
            "content": "We train the MonoTransQuest benchmark and debiased MultiTransQuest architecture on the EN-DE and EN-ZH datasets and use these models to predict the respective scores on the RO-EN dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_54",
            "start": 462,
            "end": 648,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_54@4",
            "content": "Since this is an out-of-domain setting, we do not expect the models to reach a performance that can compete with the models trained on Romanian-English data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_54",
            "start": 650,
            "end": 806,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_54@5",
            "content": "However, the debiased models should outperform the benchmark.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_54",
            "start": 808,
            "end": 868,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_54@6",
            "content": "Indeed, Table 2 shows that all MultiTransQuest models outperform MonoTransQuest in this zero-shot scenario.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_54",
            "start": 870,
            "end": 976,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_55@0",
            "content": "Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_55",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_56@0",
            "content": "Building upon the previously discussed results, we propose ideas for future work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_56",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_56@1",
            "content": "Firstly, the multitask architecture provides additional degrees of freedom that were not explored extensively, yet.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_56",
            "start": 82,
            "end": 196,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_56@2",
            "content": "For example, one could vary the amount of training per task or even learn the training schedule as a parameter which adapts dynamically during the training process (Kiperwasser and Ballesteros, 2018;Zaremoodi et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_56",
            "start": 198,
            "end": 420,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_56@3",
            "content": "In addition, the number of auxiliary tasks could be increased to two or more, mixing different task types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_56",
            "start": 422,
            "end": 527,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_56@4",
            "content": "Furthermore, it would be interesting to apply our proposed methods and architectures in adjacent fields of NLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_56",
            "start": 529,
            "end": 639,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_56@5",
            "content": "We think that the adapted debiased focal loss technique for regression might function in scenarios where it is possible to better isolate the bias and thus train a representative bias model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_56",
            "start": 641,
            "end": 830,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_56@6",
            "content": "The most promising approach of training a multitask architecture with a supportive auxiliary task might generalise well to related settings, such as quality estimation for machine-generated text summaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_56",
            "start": 832,
            "end": 1036,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_56@7",
            "content": "Seeing the method applied in the fields of NLI and VQA, both of which face partial input biases, would be intriguing, too.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_56",
            "start": 1038,
            "end": 1159,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_57@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_57",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_58@0",
            "content": "This paper expands on recent research which suggests that QE models are susceptible to learning spurious correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_58",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_58@1",
            "content": "Based on additional analysis, and inspired by related work in the fields of NLI and VQA, we propose a range of auxiliary tasks that inform the main Quality Estimation task during training and are discarded at test time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_58",
            "start": 119,
            "end": 337,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_58@2",
            "content": "First, we train the main Quality Estimation task together with additional, less biased data in a multitask setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_58",
            "start": 339,
            "end": 453,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_58@3",
            "content": "Then, we explore adversarial training and debiased focal loss to directly target the partial input bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_58",
            "start": 455,
            "end": 558,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_58@4",
            "content": "We find that the former approaches yield more stable results than the latter and conjecture that this is due to the difficulty of isolating partial input bias effects from useful predictive information encoded in the translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_58",
            "start": 560,
            "end": 788,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_58@5",
            "content": "We show that our proposed multitask architecture MultiTransQuest, especially when trained with additional shuffled WikiMatrix data, generalises well across the two most biased language pairs and the two different quality scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_58",
            "start": 790,
            "end": 1017,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_58@6",
            "content": "Our method retains the overall prediction quality, reduces the observed biases significantly and increases the models' robustness in a zero-shot setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_58",
            "start": 1019,
            "end": 1171,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_58@7",
            "content": "A Appendix",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_58",
            "start": 1173,
            "end": 1182,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_59@0",
            "content": "Yonatan Belinkov, Adam Poliak, Stuart Shieber, Benjamin Van Durme, Alexander Rush, On Adversarial Removal of Hypothesis-only Bias in Natural Language Inference, 2019, Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_59",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_60@0",
            "content": "John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, Nicola Ueffing, Confidence estimation for machine translation, 2004, COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_60",
            "start": 0,
            "end": 268,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_61@0",
            "content": "UNKNOWN, None, 1997, Multitask Learning. Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_61",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_62@0",
            "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Unsupervised cross-lingual representation learning at scale, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_62",
            "start": 0,
            "end": 322,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_63@0",
            "content": "Kai Fan, Bo Li, Feng-Ming Zhou, Jiayi Wang, Bilingual Expert\" Can Find Translation Errors, 2019, AAAI Conference. Association for the Advancement of Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_63",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_64@0",
            "content": "UNKNOWN, None, 2020, MLQE-PE : a multilingual quality estimation and post-editing dataset. arXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_64",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_65@0",
            "content": "Yaroslav Ganin, Victor Lempitsky, Unsupervised Domain Adaptation by Backpropagation, 2015, Proceedings of the 32nd International Conference on International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_65",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_66@0",
            "content": "Yonatan Rabeeh Karimi Mahabadi, James Belinkov,  Henderson, End-to-End Bias Mitigation by Modelling Biases in Corpora, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_66",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_67@0",
            "content": "Hyun Kim, Hun-Young Jung, Hongseok Kwon, Jong-Hyeok Lee, Seung-Hoon Na, None, 2017, ACM Transactions on Asian and Low-Resource Language Information Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_67",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_68@0",
            "content": "Eliyahu Kiperwasser, Miguel Ballesteros, Scheduled Multi-Task Learning: From Syntax to Translation, 2018, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_68",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_69@0",
            "content": "Tsung-Yi Lin, Priya Goyal, Ross Girshick, Focal Loss for Dense Object Detection, 2017, Proceedings of the IEEE International Conference on Computer Vision (ICCV), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_69",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_70@0",
            "content": "Jan Niehues, Eunah Cho, Exploiting Linguistic Resources for Neural Machine Translation Using Multi-task Learning, 2017, Proceedings of the Second Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_70",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_71@0",
            "content": "Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, Benjamin Van Durme, Hypothesis only baselines in natural language inference, 2018, Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_71",
            "start": 0,
            "end": 274,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_72@0",
            "content": "Aishwarya Sainandan Ramakrishnan, Stefan Agrawal,  Lee, Overcoming Language Priors in Visual Question Answering with Adversarial Regularization, 2018, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_72",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_73@0",
            "content": "Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov, TransQuest: Translation Quality Estimation with Cross-lingual Transformers, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_73",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_74@0",
            "content": "Sebastian Ruder, An Overview of Multi-Task Learning in, 2017, Deep Neural Networks. arXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_74",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_75@0",
            "content": "UNKNOWN, None, 2019, Neural Transfer Learning for Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_75",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_76@0",
            "content": "UNKNOWN, None, 2021, Wiki-Matrix: Mining 135M Parallel Sentences, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_76",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_77@0",
            "content": ", Language Pairs from Wikipedia, , Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_77",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_78@0",
            "content": "Lucia Specia, Fr\u00e9d\u00e9ric Blain, Marina Fomicheva, Erick Fonseca, Vishrav Chaudhary, Francisco Guzm\u00e1n, Andr\u00e9 F T Martins, Findings of the WMT 2020 Shared Task on Quality Estimation, 2020, Proceedings of the 5th Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_78",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_79@0",
            "content": "Lucia Specia, Carolina Scarton, Gustavo Paetzold, Quality Estimation for Machine Translation, 2018, Synthesis Lectures on Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_79",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "468-ARR_v1_80@0",
            "content": "UNKNOWN, None, , Sebastian Riedel, and Tim Rockt\u00e4schel. 2020. Avoiding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "468-ARR_v1_80",
            "start": 0,
            "end": 72,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "468-ARR_v1_0",
            "tgt_ix": "468-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_0",
            "tgt_ix": "468-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_1",
            "tgt_ix": "468-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_1",
            "tgt_ix": "468-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_0",
            "tgt_ix": "468-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_2",
            "tgt_ix": "468-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_4",
            "tgt_ix": "468-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_5",
            "tgt_ix": "468-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_6",
            "tgt_ix": "468-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_8",
            "tgt_ix": "468-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_3",
            "tgt_ix": "468-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_3",
            "tgt_ix": "468-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_3",
            "tgt_ix": "468-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_3",
            "tgt_ix": "468-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_3",
            "tgt_ix": "468-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_3",
            "tgt_ix": "468-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_3",
            "tgt_ix": "468-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_0",
            "tgt_ix": "468-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_9",
            "tgt_ix": "468-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_10",
            "tgt_ix": "468-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_10",
            "tgt_ix": "468-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_12",
            "tgt_ix": "468-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_11",
            "tgt_ix": "468-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_11",
            "tgt_ix": "468-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_11",
            "tgt_ix": "468-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_10",
            "tgt_ix": "468-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_13",
            "tgt_ix": "468-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_15",
            "tgt_ix": "468-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_16",
            "tgt_ix": "468-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_17",
            "tgt_ix": "468-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_14",
            "tgt_ix": "468-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_14",
            "tgt_ix": "468-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_14",
            "tgt_ix": "468-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_14",
            "tgt_ix": "468-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_14",
            "tgt_ix": "468-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_0",
            "tgt_ix": "468-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_18",
            "tgt_ix": "468-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_19",
            "tgt_ix": "468-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_19",
            "tgt_ix": "468-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_19",
            "tgt_ix": "468-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_20",
            "tgt_ix": "468-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_22",
            "tgt_ix": "468-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_21",
            "tgt_ix": "468-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_21",
            "tgt_ix": "468-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_21",
            "tgt_ix": "468-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_19",
            "tgt_ix": "468-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_24",
            "tgt_ix": "468-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_24",
            "tgt_ix": "468-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_19",
            "tgt_ix": "468-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_25",
            "tgt_ix": "468-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_27",
            "tgt_ix": "468-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_28",
            "tgt_ix": "468-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_26",
            "tgt_ix": "468-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_26",
            "tgt_ix": "468-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_26",
            "tgt_ix": "468-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_26",
            "tgt_ix": "468-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_0",
            "tgt_ix": "468-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_29",
            "tgt_ix": "468-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_30",
            "tgt_ix": "468-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_30",
            "tgt_ix": "468-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_30",
            "tgt_ix": "468-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_31",
            "tgt_ix": "468-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_33",
            "tgt_ix": "468-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_32",
            "tgt_ix": "468-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_32",
            "tgt_ix": "468-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_32",
            "tgt_ix": "468-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_30",
            "tgt_ix": "468-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_34",
            "tgt_ix": "468-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_35",
            "tgt_ix": "468-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_35",
            "tgt_ix": "468-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_30",
            "tgt_ix": "468-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_36",
            "tgt_ix": "468-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_30",
            "tgt_ix": "468-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_37",
            "tgt_ix": "468-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_39",
            "tgt_ix": "468-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_38",
            "tgt_ix": "468-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_38",
            "tgt_ix": "468-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_38",
            "tgt_ix": "468-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_30",
            "tgt_ix": "468-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_42",
            "tgt_ix": "468-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_43",
            "tgt_ix": "468-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_41",
            "tgt_ix": "468-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_41",
            "tgt_ix": "468-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_41",
            "tgt_ix": "468-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_41",
            "tgt_ix": "468-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_0",
            "tgt_ix": "468-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_44",
            "tgt_ix": "468-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_45",
            "tgt_ix": "468-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_45",
            "tgt_ix": "468-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_45",
            "tgt_ix": "468-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_46",
            "tgt_ix": "468-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_48",
            "tgt_ix": "468-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_47",
            "tgt_ix": "468-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_47",
            "tgt_ix": "468-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_47",
            "tgt_ix": "468-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_45",
            "tgt_ix": "468-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_51",
            "tgt_ix": "468-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_52",
            "tgt_ix": "468-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_53",
            "tgt_ix": "468-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_50",
            "tgt_ix": "468-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_50",
            "tgt_ix": "468-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_50",
            "tgt_ix": "468-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_50",
            "tgt_ix": "468-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_50",
            "tgt_ix": "468-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_0",
            "tgt_ix": "468-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_54",
            "tgt_ix": "468-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_55",
            "tgt_ix": "468-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_55",
            "tgt_ix": "468-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_0",
            "tgt_ix": "468-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_56",
            "tgt_ix": "468-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_57",
            "tgt_ix": "468-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_57",
            "tgt_ix": "468-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "468-ARR_v1_0",
            "tgt_ix": "468-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_1",
            "tgt_ix": "468-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_2",
            "tgt_ix": "468-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_2",
            "tgt_ix": "468-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_2",
            "tgt_ix": "468-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_2",
            "tgt_ix": "468-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_2",
            "tgt_ix": "468-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_2",
            "tgt_ix": "468-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_2",
            "tgt_ix": "468-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_3",
            "tgt_ix": "468-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_4",
            "tgt_ix": "468-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_4",
            "tgt_ix": "468-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_4",
            "tgt_ix": "468-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_4",
            "tgt_ix": "468-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_4",
            "tgt_ix": "468-ARR_v1_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_5",
            "tgt_ix": "468-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_5",
            "tgt_ix": "468-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_5",
            "tgt_ix": "468-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_5",
            "tgt_ix": "468-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_6",
            "tgt_ix": "468-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_7",
            "tgt_ix": "468-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_8",
            "tgt_ix": "468-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_8",
            "tgt_ix": "468-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_8",
            "tgt_ix": "468-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_8",
            "tgt_ix": "468-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_8",
            "tgt_ix": "468-ARR_v1_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_8",
            "tgt_ix": "468-ARR_v1_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_9",
            "tgt_ix": "468-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_9",
            "tgt_ix": "468-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_10",
            "tgt_ix": "468-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_11",
            "tgt_ix": "468-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_12",
            "tgt_ix": "468-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_12",
            "tgt_ix": "468-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_12",
            "tgt_ix": "468-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_12",
            "tgt_ix": "468-ARR_v1_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_13",
            "tgt_ix": "468-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_13",
            "tgt_ix": "468-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_13",
            "tgt_ix": "468-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_13",
            "tgt_ix": "468-ARR_v1_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_13",
            "tgt_ix": "468-ARR_v1_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_13",
            "tgt_ix": "468-ARR_v1_13@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_13",
            "tgt_ix": "468-ARR_v1_13@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_14",
            "tgt_ix": "468-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_15",
            "tgt_ix": "468-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_15",
            "tgt_ix": "468-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_15",
            "tgt_ix": "468-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_15",
            "tgt_ix": "468-ARR_v1_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_16",
            "tgt_ix": "468-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_16",
            "tgt_ix": "468-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_16",
            "tgt_ix": "468-ARR_v1_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_17",
            "tgt_ix": "468-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_17",
            "tgt_ix": "468-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_17",
            "tgt_ix": "468-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_17",
            "tgt_ix": "468-ARR_v1_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_17",
            "tgt_ix": "468-ARR_v1_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_17",
            "tgt_ix": "468-ARR_v1_17@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_17",
            "tgt_ix": "468-ARR_v1_17@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_18",
            "tgt_ix": "468-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_18",
            "tgt_ix": "468-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_18",
            "tgt_ix": "468-ARR_v1_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_18",
            "tgt_ix": "468-ARR_v1_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_18",
            "tgt_ix": "468-ARR_v1_18@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_18",
            "tgt_ix": "468-ARR_v1_18@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_18",
            "tgt_ix": "468-ARR_v1_18@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_19",
            "tgt_ix": "468-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_20",
            "tgt_ix": "468-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_21",
            "tgt_ix": "468-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_22",
            "tgt_ix": "468-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_22",
            "tgt_ix": "468-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_22",
            "tgt_ix": "468-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_22",
            "tgt_ix": "468-ARR_v1_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_22",
            "tgt_ix": "468-ARR_v1_22@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_22",
            "tgt_ix": "468-ARR_v1_22@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_23",
            "tgt_ix": "468-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_24",
            "tgt_ix": "468-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_25",
            "tgt_ix": "468-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_25",
            "tgt_ix": "468-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_25",
            "tgt_ix": "468-ARR_v1_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_25",
            "tgt_ix": "468-ARR_v1_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_25",
            "tgt_ix": "468-ARR_v1_25@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_25",
            "tgt_ix": "468-ARR_v1_25@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_25",
            "tgt_ix": "468-ARR_v1_25@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_25",
            "tgt_ix": "468-ARR_v1_25@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_25",
            "tgt_ix": "468-ARR_v1_25@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_26",
            "tgt_ix": "468-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_27",
            "tgt_ix": "468-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_27",
            "tgt_ix": "468-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_27",
            "tgt_ix": "468-ARR_v1_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_27",
            "tgt_ix": "468-ARR_v1_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_27",
            "tgt_ix": "468-ARR_v1_27@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_27",
            "tgt_ix": "468-ARR_v1_27@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_27",
            "tgt_ix": "468-ARR_v1_27@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_27",
            "tgt_ix": "468-ARR_v1_27@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_27",
            "tgt_ix": "468-ARR_v1_27@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_28",
            "tgt_ix": "468-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_28",
            "tgt_ix": "468-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_28",
            "tgt_ix": "468-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_29",
            "tgt_ix": "468-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_29",
            "tgt_ix": "468-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_29",
            "tgt_ix": "468-ARR_v1_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_29",
            "tgt_ix": "468-ARR_v1_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_29",
            "tgt_ix": "468-ARR_v1_29@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_30",
            "tgt_ix": "468-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_31",
            "tgt_ix": "468-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_31",
            "tgt_ix": "468-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_31",
            "tgt_ix": "468-ARR_v1_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_31",
            "tgt_ix": "468-ARR_v1_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_31",
            "tgt_ix": "468-ARR_v1_31@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_31",
            "tgt_ix": "468-ARR_v1_31@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_31",
            "tgt_ix": "468-ARR_v1_31@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_31",
            "tgt_ix": "468-ARR_v1_31@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_31",
            "tgt_ix": "468-ARR_v1_31@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_31",
            "tgt_ix": "468-ARR_v1_31@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_32",
            "tgt_ix": "468-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_33",
            "tgt_ix": "468-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_33",
            "tgt_ix": "468-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_33",
            "tgt_ix": "468-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_33",
            "tgt_ix": "468-ARR_v1_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_33",
            "tgt_ix": "468-ARR_v1_33@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_33",
            "tgt_ix": "468-ARR_v1_33@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_34",
            "tgt_ix": "468-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_34",
            "tgt_ix": "468-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_34",
            "tgt_ix": "468-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_34",
            "tgt_ix": "468-ARR_v1_34@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_34",
            "tgt_ix": "468-ARR_v1_34@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_34",
            "tgt_ix": "468-ARR_v1_34@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_34",
            "tgt_ix": "468-ARR_v1_34@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_34",
            "tgt_ix": "468-ARR_v1_34@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_34",
            "tgt_ix": "468-ARR_v1_34@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_35",
            "tgt_ix": "468-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_36",
            "tgt_ix": "468-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_36",
            "tgt_ix": "468-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_36",
            "tgt_ix": "468-ARR_v1_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_36",
            "tgt_ix": "468-ARR_v1_36@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_36",
            "tgt_ix": "468-ARR_v1_36@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_36",
            "tgt_ix": "468-ARR_v1_36@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_36",
            "tgt_ix": "468-ARR_v1_36@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_36",
            "tgt_ix": "468-ARR_v1_36@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_36",
            "tgt_ix": "468-ARR_v1_36@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_36",
            "tgt_ix": "468-ARR_v1_36@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_37",
            "tgt_ix": "468-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_38",
            "tgt_ix": "468-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_39",
            "tgt_ix": "468-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_39",
            "tgt_ix": "468-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_39",
            "tgt_ix": "468-ARR_v1_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_39",
            "tgt_ix": "468-ARR_v1_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_39",
            "tgt_ix": "468-ARR_v1_39@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_39",
            "tgt_ix": "468-ARR_v1_39@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_39",
            "tgt_ix": "468-ARR_v1_39@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_39",
            "tgt_ix": "468-ARR_v1_39@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_39",
            "tgt_ix": "468-ARR_v1_39@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_39",
            "tgt_ix": "468-ARR_v1_39@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_40",
            "tgt_ix": "468-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_41",
            "tgt_ix": "468-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_42",
            "tgt_ix": "468-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_42",
            "tgt_ix": "468-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_42",
            "tgt_ix": "468-ARR_v1_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_42",
            "tgt_ix": "468-ARR_v1_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_42",
            "tgt_ix": "468-ARR_v1_42@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_42",
            "tgt_ix": "468-ARR_v1_42@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_43",
            "tgt_ix": "468-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_43",
            "tgt_ix": "468-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_43",
            "tgt_ix": "468-ARR_v1_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_43",
            "tgt_ix": "468-ARR_v1_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_43",
            "tgt_ix": "468-ARR_v1_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_43",
            "tgt_ix": "468-ARR_v1_43@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_43",
            "tgt_ix": "468-ARR_v1_43@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_43",
            "tgt_ix": "468-ARR_v1_43@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_44",
            "tgt_ix": "468-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_45",
            "tgt_ix": "468-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_46",
            "tgt_ix": "468-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_46",
            "tgt_ix": "468-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_46",
            "tgt_ix": "468-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_46",
            "tgt_ix": "468-ARR_v1_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_46",
            "tgt_ix": "468-ARR_v1_46@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_46",
            "tgt_ix": "468-ARR_v1_46@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_47",
            "tgt_ix": "468-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_48",
            "tgt_ix": "468-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_48",
            "tgt_ix": "468-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_49",
            "tgt_ix": "468-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_50",
            "tgt_ix": "468-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_51",
            "tgt_ix": "468-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_51",
            "tgt_ix": "468-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_51",
            "tgt_ix": "468-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_51",
            "tgt_ix": "468-ARR_v1_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_51",
            "tgt_ix": "468-ARR_v1_51@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_52",
            "tgt_ix": "468-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_52",
            "tgt_ix": "468-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_52",
            "tgt_ix": "468-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_52",
            "tgt_ix": "468-ARR_v1_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_52",
            "tgt_ix": "468-ARR_v1_52@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_52",
            "tgt_ix": "468-ARR_v1_52@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_52",
            "tgt_ix": "468-ARR_v1_52@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_53",
            "tgt_ix": "468-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_53",
            "tgt_ix": "468-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_53",
            "tgt_ix": "468-ARR_v1_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_53",
            "tgt_ix": "468-ARR_v1_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_54",
            "tgt_ix": "468-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_54",
            "tgt_ix": "468-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_54",
            "tgt_ix": "468-ARR_v1_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_54",
            "tgt_ix": "468-ARR_v1_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_54",
            "tgt_ix": "468-ARR_v1_54@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_54",
            "tgt_ix": "468-ARR_v1_54@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_54",
            "tgt_ix": "468-ARR_v1_54@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_55",
            "tgt_ix": "468-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_56",
            "tgt_ix": "468-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_56",
            "tgt_ix": "468-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_56",
            "tgt_ix": "468-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_56",
            "tgt_ix": "468-ARR_v1_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_56",
            "tgt_ix": "468-ARR_v1_56@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_56",
            "tgt_ix": "468-ARR_v1_56@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_56",
            "tgt_ix": "468-ARR_v1_56@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_56",
            "tgt_ix": "468-ARR_v1_56@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_57",
            "tgt_ix": "468-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_58",
            "tgt_ix": "468-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_58",
            "tgt_ix": "468-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_58",
            "tgt_ix": "468-ARR_v1_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_58",
            "tgt_ix": "468-ARR_v1_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_58",
            "tgt_ix": "468-ARR_v1_58@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_58",
            "tgt_ix": "468-ARR_v1_58@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_58",
            "tgt_ix": "468-ARR_v1_58@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_58",
            "tgt_ix": "468-ARR_v1_58@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_59",
            "tgt_ix": "468-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_60",
            "tgt_ix": "468-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_61",
            "tgt_ix": "468-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_62",
            "tgt_ix": "468-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_63",
            "tgt_ix": "468-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_64",
            "tgt_ix": "468-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_65",
            "tgt_ix": "468-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_66",
            "tgt_ix": "468-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_67",
            "tgt_ix": "468-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_68",
            "tgt_ix": "468-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_69",
            "tgt_ix": "468-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_70",
            "tgt_ix": "468-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_71",
            "tgt_ix": "468-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_72",
            "tgt_ix": "468-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_73",
            "tgt_ix": "468-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_74",
            "tgt_ix": "468-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_75",
            "tgt_ix": "468-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_76",
            "tgt_ix": "468-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_77",
            "tgt_ix": "468-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_78",
            "tgt_ix": "468-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_79",
            "tgt_ix": "468-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "468-ARR_v1_80",
            "tgt_ix": "468-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1288,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "468-ARR",
        "version": 1
    }
}