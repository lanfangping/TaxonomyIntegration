{
    "nodes": [
        {
            "ix": "192-ARR_v1_review2_0",
            "content": "192-ARR_v1_review2",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "192-ARR_v1_review2_1",
            "content": "paper_summary. In this paper, the authors propose an unsupervised multiple-task and multiple-teacher model for cross-lingual NER. The student model learns two source language patterns of entity recognition and entity similarity evaluation. They propose a weighting strategy to take consideration of the reliability of the teachers. Experimental results show that the proposed model yields significant improvements on six target language datasets and outperforms the existing state-of-the-art approaches.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "192-ARR_v1_review2_2",
            "content": "summary_of_strengths. 1. The authors propose an unsupervised knowledge distillation framework for cross-language named entity recognition and develop a teaching and learning procedure under this framework. \n2. The authors present a novel multiple-task and multiple-teacher model that introduces a entity similarity evaluator to boost the performance of student recognizer on target languages. \n3. Extensive experiments on seven languages compared with SOTA baselines and the results confirm the effectiveness of MTMT model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "192-ARR_v1_review2_3",
            "content": "summary_of_weaknesses. 1. The authors unitize multilingual mBERT as basic sequence feature extractor to achieve the sequence embedding representation, why not try other multilingual pre-trained language models, such as mBART, ERNIE-M, mLUKE and so on. \n2. The languages in CoNLL dataset are some rich-resourced languages indeed, the authors need to test MTMT model on some low-resourced languages.\n[1] Ri R, Yamada I, Tsuruoka Y. mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models[J]. arXiv preprint arXiv:2110.08151, 2021. \n[2] Ouyang X, Wang S, Pang C, et al. Ernie-m: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora[J]. arXiv preprint arXiv:2012.15674, 2020.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "192-ARR_v1_review2_4",
            "content": "comments,_suggestions_and_typos. This paper has done a solid work on Cross-Lingual Named Entity Recognition, however, some questions remain to be clarified.\nPlease try more multilingual feature extractors and test MTMT model on some low-resourced languages.",
            "ntype": "p",
            "meta": null
        }
    ],
    "span_nodes": [
        {
            "ix": "192-ARR_v1_review2_0@0",
            "content": "192-ARR_v1_review2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_0",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_1@0",
            "content": "paper_summary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_1",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_1@1",
            "content": "In this paper, the authors propose an unsupervised multiple-task and multiple-teacher model for cross-lingual NER.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_1",
            "start": 15,
            "end": 128,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_1@2",
            "content": "The student model learns two source language patterns of entity recognition and entity similarity evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_1",
            "start": 130,
            "end": 238,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_1@3",
            "content": "They propose a weighting strategy to take consideration of the reliability of the teachers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_1",
            "start": 240,
            "end": 330,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_1@4",
            "content": "Experimental results show that the proposed model yields significant improvements on six target language datasets and outperforms the existing state-of-the-art approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_1",
            "start": 332,
            "end": 502,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_2@0",
            "content": "summary_of_strengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_2",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_2@1",
            "content": "1. The authors propose an unsupervised knowledge distillation framework for cross-language named entity recognition and develop a teaching and learning procedure under this framework. \n",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_2",
            "start": 22,
            "end": 206,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_2@2",
            "content": "2. The authors present a novel multiple-task and multiple-teacher model that introduces a entity similarity evaluator to boost the performance of student recognizer on target languages. \n",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_2",
            "start": 207,
            "end": 393,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_2@3",
            "content": "3. Extensive experiments on seven languages compared with SOTA baselines and the results confirm the effectiveness of MTMT model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_2",
            "start": 394,
            "end": 522,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_3@0",
            "content": "summary_of_weaknesses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_3",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_3@1",
            "content": "1. The authors unitize multilingual mBERT as basic sequence feature extractor to achieve the sequence embedding representation, why not try other multilingual pre-trained language models, such as mBART, ERNIE-M, mLUKE and so on. \n",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_3",
            "start": 23,
            "end": 252,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_3@2",
            "content": "2. The languages in CoNLL dataset are some rich-resourced languages indeed, the authors need to test MTMT model on some low-resourced languages.\n",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_3",
            "start": 253,
            "end": 397,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_3@3",
            "content": "[1] Ri R, Yamada I, Tsuruoka Y. mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models[J].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_3",
            "start": 398,
            "end": 518,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_3@4",
            "content": "arXiv preprint arXiv:2110.08151, 2021. \n",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_3",
            "start": 520,
            "end": 559,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_3@5",
            "content": "[2] Ouyang X, Wang S, Pang C, et al. Ernie-m: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora[J].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_3",
            "start": 560,
            "end": 706,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_3@6",
            "content": "arXiv preprint arXiv:2012.15674, 2020.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_3",
            "start": 708,
            "end": 745,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_4@0",
            "content": "comments,_suggestions_and_typos.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_4",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_4@1",
            "content": "This paper has done a solid work on Cross-Lingual Named Entity Recognition, however, some questions remain to be clarified.\n",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_4",
            "start": 33,
            "end": 156,
            "label": {}
        },
        {
            "ix": "192-ARR_v1_review2_4@2",
            "content": "Please try more multilingual feature extractors and test MTMT model on some low-resourced languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "192-ARR_v1_review2_4",
            "start": 157,
            "end": 256,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "192-ARR_v1_review2_0",
            "tgt_ix": "192-ARR_v1_review2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "192-ARR_v1_review2_0",
            "tgt_ix": "192-ARR_v1_review2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "192-ARR_v1_review2_0",
            "tgt_ix": "192-ARR_v1_review2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "192-ARR_v1_review2_0",
            "tgt_ix": "192-ARR_v1_review2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "192-ARR_v1_review2_0",
            "tgt_ix": "192-ARR_v1_review2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "192-ARR_v1_review2_1",
            "tgt_ix": "192-ARR_v1_review2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "192-ARR_v1_review2_2",
            "tgt_ix": "192-ARR_v1_review2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "192-ARR_v1_review2_3",
            "tgt_ix": "192-ARR_v1_review2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "192-ARR_v1_review2_0",
            "tgt_ix": "192-ARR_v1_review2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_1",
            "tgt_ix": "192-ARR_v1_review2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_1",
            "tgt_ix": "192-ARR_v1_review2_1@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_1",
            "tgt_ix": "192-ARR_v1_review2_1@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_1",
            "tgt_ix": "192-ARR_v1_review2_1@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_1",
            "tgt_ix": "192-ARR_v1_review2_1@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_2",
            "tgt_ix": "192-ARR_v1_review2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_2",
            "tgt_ix": "192-ARR_v1_review2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_2",
            "tgt_ix": "192-ARR_v1_review2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_2",
            "tgt_ix": "192-ARR_v1_review2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_3",
            "tgt_ix": "192-ARR_v1_review2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_3",
            "tgt_ix": "192-ARR_v1_review2_3@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_3",
            "tgt_ix": "192-ARR_v1_review2_3@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_3",
            "tgt_ix": "192-ARR_v1_review2_3@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_3",
            "tgt_ix": "192-ARR_v1_review2_3@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_3",
            "tgt_ix": "192-ARR_v1_review2_3@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_3",
            "tgt_ix": "192-ARR_v1_review2_3@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_4",
            "tgt_ix": "192-ARR_v1_review2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_4",
            "tgt_ix": "192-ARR_v1_review2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "192-ARR_v1_review2_4",
            "tgt_ix": "192-ARR_v1_review2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "192-ARR_v1_review2",
    "meta": {
        "ix_counter": 24,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy"
    }
}