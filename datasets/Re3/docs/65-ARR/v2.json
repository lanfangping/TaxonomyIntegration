{
    "nodes": [
        {
            "ix": "65-ARR_v2_0",
            "content": "Improve Discourse Dependency Parsing with Contextualized Representations",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_2",
            "content": "Recent works show that discourse analysis benefits from modeling intra-and inter-sentential levels separately, where proper representations for text units of different granularities are desired to capture both the meaning of text units and their relations to the context. In this paper, we propose to take advantage of transformers to encode contextualized representations of units of different levels to dynamically capture the information required for discourse dependency analysis on intra-and inter-sentential levels. Motivated by the observation of writing patterns commonly shared across articles, we propose a novel method that treats discourse relation identification as a sequence labelling task, which takes advantage of structural information from the context of extracted discourse trees, and substantially outperforms traditional directclassification methods. Experiments show that our model achieves state-of-the-art results on both English and Chinese datasets. Our code is publicly available 1 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "65-ARR_v2_4",
            "content": "Discourse dependency parsing (DDP) is the task of identifying the structure and relationship between Elementary Discourse Units (EDUs) in a document. It is a fundamental task of natural language understanding and can benefit many downstream applications, such as dialogue understanding (Perret et al., 2016;Takanobu et al., 2018) and question answering (Ferrucci et al., 2010;Verberne et al., 2007).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_5",
            "content": "Although existing works have achieved much progress using transition-based systems (Jia et al., 2018b,a;Hung et al., 2020) or graph-based models (Li et al., 2014a;Shi and Huang, 2018;Afantenos et al., 2015), this task still remains a challenge. Different from syntactic parsing, the basic components in a discourse are EDUs, sequences of words, which are not trivial to represent in a straightforward way like word embeddings. Predicting the dependency and relationship between EDUs sometimes necessitates the help of a global understanding of the context so that contextualized EDU representations in the discourse are needed. Furthermore, previous studies have shown the benefit of breaking discourse analysis into intra-and inter-sentential levels (Wang et al., 2017), building sub-trees for each sentence first and then assembling sub-trees to form a complete discourse tree. In this Sentence-First (Sent-First) framework, it is even more crucial to produce appropriate contextualized representations for text units when analyzing in intra-or inter-sentential levels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_6",
            "content": "Automatic metrics are widely used in machine translation as a substitute for human assessment. This is often measured by correlation with human judgement.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_7",
            "content": "In this paper, we propose a significant test Figure 1 shows an excerpt discourse dependency structure for a scientific abstract from SciDTB (Yang and Li, 2018). The lengths of EDUs vary a lot, from more than 10 words to 2 words only (EDU 12: tests show), making it especially hard to encode by themselves alone. Sometimes it is sufficient to consider the contextual information in a small range as in the case of EDU 13 and 14, other times we need to see a larger context as in the case of EDU 1 and 4, crossing several sentences. This again motivates us to consider encoding contextual representations of EDUs separately on intraand inter-sentential levels to dynamically capture specific features needed for discourse analysis on different levels. Another motivation from this example is the discovery that the distribution of discourse relations between EDUs seems to follow certain patterns shared across different articles. Writing patterns are document structures people commonly use to organize their arguments. For example, in scientific abstracts like the instance in Figure 1, people usually first talk about background information, then introduce the topic sentence, and conclude with elaborations or evaluations. Here, the example first states the background of widely used automatic metrics, introduces the topic sentence about their contribution of a significance test followed by evaluation and conclusion. Taking advantage of those writing patterns should enable us to better capture the interplay between individual EDUs with the context.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_8",
            "content": "In this paper, we explore different contextualized representations for DDP in a Sent-First parsing framework, where a complete discourse tree is built up sentence by sentence. We seek to dynamically capture what is crucial for DDP at different text granularity levels. We further propose a novel discourse relation identification method that addresses the task in a sequence labeling paradigm to exploit common conventions people usually adopt to develop their arguments. We evaluate our models on both English and Chinese datasets, and experiments show our models achieve the state-of-the-art results by explicitly exploiting structural information in the context and capturing writing patterns that people use to organize discourses.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_9",
            "content": "In summary, our contributions are mainly twofold: (1) We incorporate the Pre-training and Fine-tuning framework into our design of a Sent-First model and develop better contextualized EDU representations to dynamically capture different information needed for DDP at different text granularity levels. Experiments show that our model outperforms all existing models by a large margin.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_10",
            "content": "(2) We formulate discourse relation identification in a novel sequence labeling paradigm to take advantage of the inherent structural information in the discourse. Building upon a stacked BiLSTM architecture, our model brings a new state-of-the-art performance on two benchmarks, showing the advantage of sequence labeling over the common practice of direct classification for discourse relation identification.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_11",
            "content": "Related Works",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "65-ARR_v2_12",
            "content": "A key finding in previous studies in discourse analysis is that most sentences have an independent well-formed sub-tree in the full document-level discourse tree (Joty et al., 2012). Researchers have taken advantage of this finding to build parsers that utilize different granularity levels of the document to achieve the state-of-the-art results (Kobayashi et al., 2020). This design has been empirically verified to be a generally advantageous framework, improving not only works using traditional feature engineering (Joty et al., 2013;Wang et al., 2017), but also deep learning models (Jia et al., 2018b;Kobayashi et al., 2020). We, therefore, introduce this design to our dependency parsing framework. Specifically, sub-trees for each sentence in a discourse are first built separately, then assembled to form a complete discourse tree.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_13",
            "content": "However, our model differs from prior works in that we make a clear distinction to derive better contextualized representations of EDUs from fine-tuning BERT separately for intra-and intersentential levels to dynamically capture different information needed for discourse analysis at different levels. We are also the first to design stacked sequence labeling models for discourse relation identification so that its hierarchical structure can explicitly capture both intra-sentential and intersentential writing patterns.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_14",
            "content": "In the case of implicit relations between EDUs without clear connectives, it is crucial to introduce sequential information from the context to resolve ambiguity. Feng and Hirst (2014) rely on linearchain CRF with traditional feature engineering to make use of the sequential characteristics of the context for discourse constituent parsing. However, they greedily build up the discourse structure and relations from bottom up. At each timestep, they apply the CRF to obtain the locally optimized structure and relation. In this way, the model assigns relation gradually along with the construction of the parsing tree from bottom up, but only limited contextual information from the top level of the partially constructed tree can be used to predict relations. Besides, at each timestep, they sequentially assign relations to top nodes of the partial tree, without being aware that those nodes might represent different levels of discourse units (e.g. EDUs, sentences, or even paragraphs). In contrast, we explicitly train our sequence labeling models on both intra-and inter-sentential levels after a complete discourse tree is constructed so that we can infer from the whole context with a clear intention of capturing different writing patterns occurring at intra-and inter-sentential levels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_15",
            "content": "Task Definition",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "65-ARR_v2_16",
            "content": "We define the task of discourse dependency parsing as following: given a sequence of EDUs of length l, (e 1 , e 2 , ..., e l ) and a set of possible relations between EDUs Re, the goal is to predict another sequence of EDUs (h 1 , h 2 , ..., h l ) such that \u2200h i , h i \u2208 (e 1 , e 2 , ..., e l ) is the head of e i and a sequence of relations (r 1 , r 2 , ..., r l ) such that \u2200r i , r i is the relation between tuple (e i , h i ).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_17",
            "content": "Our Model",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "65-ARR_v2_18",
            "content": "We follow previous works (Wang et al., 2017) to cast the task of discourse dependency parsing as a composition of two separate yet related subtasks: dependency tree construction and relation identification. We design our model primarily in a twostep pipeline. We incorporate Sent-First design as our backbone (i.e. building sub-trees for each sentence and then assembling them into a complete discourse tree), and formulate discourse relation identification as a sequence labeling task on both intra-and inter-sentential levels to take advantage of the structure information in the discourse. Figure 1 shows the overview of our model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_19",
            "content": "Discourse Dependency Tree Constructor",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "65-ARR_v2_20",
            "content": "To take advantage of the property of well-formed sentence sub-trees inside a full discourse tree, we break the task of dependency parsing into two different levels, discovering intra-sentential sub-tree structures first and then aseembling them into a full discourse tree by identifying the inter-sentential structure of the discourse.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_21",
            "content": "Arc-Eager Transition System Since discourse dependency trees are primarily annotated as projective trees (Yang and Li, 2018), we design our tree constructor as a transition system, which converts the structure prediction process into a sequence of predicted actions. At each timestep, we derive a state feature to represent the state, which is fed into an output layer to get the predicted action. Our model follows the standard Arc-Eager system, with the action set: O= {Shif t, Lef t \u2212 Arc, Right \u2212 Arc, Reduce}.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_22",
            "content": "Specifically, our discourse tree constructor maintains a stack S, a queue I, and a set of assigned arcs A during parsing. The stack S and the set of assigned arcs A are initialized to be empty, while the queue I contains all the EDUs in the input sequence. At each timestep, an action in the action set O is performed with the following definition: Shift pushes the first EDU in queue I to the top of stack S; Left-Arc adds an arc from the first EDU in queue I to the top EDU in stack S (i.e. assigns the first EDU in I to be the head of the top EDU in S) and removes the top EDU in S; Right-Arc adds an arc from the top EDU in stack S to the first EDU in queue I (i.e. assigns the top EDU in S to be the head) and pushes the first EDU in I to stack S; Reduce removes the top EDU in S. Parsing terminates when I becomes empty and the only EDU left in S is selected to be the head of the input sequence. More details of Arc-Eager transition system can be referred from Nivre (2003).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_23",
            "content": "We first construct a dependency sub-tree for each sentence, and then treat each sub-tree as a leaf node to form a complete discourse tree across sentences. In this way, we can break a long discourse into smaller sub-structures to reduce the search space. A mathematical bound for the reduction of search space of our Sent-First framework for DDP and discourse constituent parsing is also provided in Appendix.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_24",
            "content": "Contextualized State Representation Ideally, we would like the feature representation to contain both the information of the EDUs directly involved in the action to be executed and rich clues from the context from both the tree-structure and the text, e.g. the parsing history and the interactions between individual EDUs in the context with an appropriate scope of text. In order to capture the structural clues from the context, we incorporate the parsing history in the form of identified dependencies in addition to traditional state representations to represent the current state. At each timestep, we select 6 EDUs from the current state as our feature template, including the first and the second EDU at the top of stack S, the first and the second EDU in queue I, and the head EDUs for the first and the second EDU at the top of stack S, respectively. A feature vector of all zeros is used if there is no EDU at a certain position. EDU Representations To better capture an EDU in our Sent-First framework, we use pre-trained BERT (Devlin et al., 2018) to obtain representations for each EDU according to different context. We argue that an EDU should have different representations when it is considered in different parsing levels, and thus requires level-specific contextual representations. For intra-sentential tree constructor, we feed the entire sentence to BERT and represent each EDU by averaging the last hidden states of all tokens in that EDU. The reason behind is that sentences are often self-contained sub-units of the discourse, and it is sufficient to consider interactions among EDUs within a sentence for intra-sentential analysis. On the other hand, for inter-sentential tree constructor, we concatenate all the root EDUs of different sentences in the discourse to form a pseudo sentence, feed it to BERT, and similarly, represent each root EDU by averaging the last hidden states of all tokens in each root EDU. In this way, we aim to encourage EDUs across different sentences to directly interact with each other, in order to reflect the global properties of a discourse. Figure 2 shows the architecture for our two-stage discourse dependency tree constructor.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_25",
            "content": "Discourse Relation Identification",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "65-ARR_v2_26",
            "content": "After the tree constructor is trained, we train separate sequence labeling models for relation identification. Although discourse relation identification in discourse dependency parsing is traditionally treated as a classification task, where the common practice is to use feature engineering or neural language models to directly compare two EDUs involved isolated from the rest of the context (Li et al., 2014a;Shi and Huang, 2018;Yi et al., 2021), sometimes relations between EDU pairs can be hard to be classified in isolation, as global information from the context like how EDUs are organized to support the claim in the discourse is sometimes required to infer the implicit discourse relations without explicit connectives. Therefore, we propose to identify discourse relation identification as a sequence labeling task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_27",
            "content": "Structure-aware Representations For sequence labeling, we need proper representations for EDU pairs to reflect the structure of the dependency tree. Therefore, we first tile each EDU in the input sequence (e 1 , e 2 , ..., e l ) with their predicted heads to form a sequence of EDU pairs ((e 1 , h 1 ), (e 2 , h 2 ), ..., (e l , h l )). Each EDU pair is reordered so that two arguments appear in the same order as they appear in the discourse. We derive a relation representation for each EDU pair with a BERT fine-tuned on the task of direct relation classification of EDU pairs with the [CLS] representation of the concatenation of two sentences.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_28",
            "content": "We further introduce position embeddings for each EDU pair (e i , h i ), where we consider the position of e i in its corresponding sentence, and the position of its sentence in the discourse. Specifically, we use cosine and sine functions of different frequencies (Vaswani et al., 2017) to include position information as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_29",
            "content": "P E j = sin(N o/10000 j/d ) + cos(ID/10000 j/d )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_30",
            "content": "where P E is the position embeddings, N o is the position of the sentence containing e i in the discourse, ID is the position of e i in the sentence, j is the dimension of the position embeddings, d is the dimension of the relation representation. The position embeddings have the same dimension as relation representations, so that they can be added directly to get the integrated representation for each EDU pair.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_31",
            "content": "We propose a stacked BiL-STM neural network architecture to capture both intra-sentential and inter-sentential interplay of EDUs. After labeling the entire sequence of EDU pairs ((e 1 , h 1 ), (e 2 , h 2 ), ..., (e l , h l )) with the first layer of BiLSTM, we select the root EDU for each sentence (namely the root EDU selected from our intra-sentential tree constructor for each setence) to form another inter-sentential sequence. Another separately trained BiLSTM is then applied to label those relations that span across sentences. Note that we will overwrite predictions of inter-sentential relations of the previous layer if there is a conflict of predictions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_32",
            "content": "Training",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "65-ARR_v2_33",
            "content": "Our models are trained with offline learning. We train the tree constructor and the relation labeling models separately. We attain the static oracle to train tree constructors and use the gold dependency structure to train our discourse relation labelling models. Intra-and inter-sentential tree constructors are trained separately. To label discourse relations, we fine-tune the BERT used to encode the EDU pair with an additional output layer for direct relation classification. Sequence labeling models for relation identification are trained on top of the finetuned BERT. We use cross entropy loss for training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_34",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "65-ARR_v2_35",
            "content": "Our experiments are designed to investigate how we can better explore contextual representations to improve discourse dependency parsing.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_36",
            "content": "We evaluate our models on two manually labeled discourse treebanks of different language, i.e., Discourse Dependency Treebank for Scientific Abstracts (SciDTB) (Yang and Li, 2018) in English and Chinese Discourse Treebank (CDTB) (Li et al., 2014b). SciDTB contains 1,355 English scientific abstracts collected from ACL Anthology. Averagely, an abstract includes 5.3 sentences, 14.1 EDUs, where an EDU has 10.3 tokens in average.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_37",
            "content": "On the other hand, CDTB was originally annotated as connective-driven constituent trees, and manually converted into a dependency style by Yi et al. (2021). CDTB contains 2,332 news documents. The average length of a paragraph is 2.1 sentences, 4.5 EDUs. And an EDU contains 23.3 tokens in average.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_38",
            "content": "We evaluate model performance using Unlabeled Attachment Score (UAS) and Labeled Attachment Score (LAS) for dependency prediction and discourse relation identification. UAS is defined as the percentage of nodes with correctly predicted heads, while LAS is defined as the percentage of nodes with both correctly predicted heads and correctly predicted relations to their heads. We report LAS against both gold dependencies and model predicted dependencies. We adopt the finegranularity discourse relation annotations in the original datasets, 26 relations for SciDTB and 17 relations for CDTB.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_39",
            "content": "For both datasets, we trained our dependency tree constructors with an Adam optimizer with learning rate 2e-5 for 3 epochs. Our relation labeling models are all trained with an Adam optimizer until convergence. Learning rate is set to one of {1e-5, 2e-5, 4e-5}.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_40",
            "content": "Baselines",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "65-ARR_v2_41",
            "content": "Structure Prediction We compare with the following competitive methods for structure prediction. (1) Graph adopts the Eisner's algorithm to predict the most probable dependency tree structure (Li et al., 2014a;Yang and Li, 2018;Yi et al., 2021). (2) Two-stage, which is the state-of-the-art model on CDTB and SciDTB, uses an SVM to construct a dependency tree (Yang and Li, 2018;Yi et al., 2021). (3) Sent-First LSTM is our implmentation of the state-of-the-art transition-based discourse constituent parser on RST (Kobayashi et al., 2020), where we use a vanilla transition system with pretrained BiLSTM as the EDU encoder within the Sent-First framework to construct dependency trees. (4) Complete Parser is modified from a state-of-the-art constituent discourse parser on CDTB (Hung et al., 2020), using a transition system with BERT as the EDU encoder to construct a dependency tree. Because of the inherent difference between constituency parsing and dependency parsing, we only adopt the encoding strategy of ( 4) and ( 5) into our arc-eager transition system.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_42",
            "content": "We also implement several model variants for can be exploited to aid discourse relation identification, as have been discussed in section 1. We show that the results can be further improved by making use of the sequential structure of the discourse. We design multiple novel sequence labeling models on top of the fine-tuned BERT and all of them achieve a considerable improvement (more than 1%) over BERT in terms of accuracy both on the gold dependencies and the predicted dependencies from our Sent-First (separate), showing the benefit of enhancing the interactions between individual EDUs with the context. It yields another large gain when we introduce another layer of inter-sentential level BiLSTM, showing again that it is crucial to capture the interactions between EDUs and their context in both intra-and inter-sentential levels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_43",
            "content": "Detailed Analysis",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "65-ARR_v2_44",
            "content": "Contextualized Representations for Tree Construction Intuitively, a model should take different views of context when analyzing intra-and inter-sentential structures. As we can see in Table 1, BERT + Sent-First (shared) improves Complete Parser (contextualized) by 1.2% and 2.4% on Sc-iTDB and CDTB, respectively. The only difference is BERT + Sent-First makes explicit predictions on two different levels, while Complete Parser (contextualized) treats them equally. When we force BERT + Sent-First to use different BERTs for intraand inter-sententential analysis, we observe further improvement, around 3% on both datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_45",
            "content": "If we take a closer look at their performance in intra-and inter-sentential views in Table 3, we can see that BERT + Sent-First (shared) performs better than single BERT model, Complete Parser (contextualized), on both intra-and inter-levels of SciDTB and CDTB, though in some cases we only observe marginal improvement like inter-sentential level of SciDTB. However, when we enhance BERT + Sent-First with different encoders for intra-and inter-sentential analysis, we can observe significant improvement in all cases. importance of anaylzing with different but more focused contextual representations for the two parsing levels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_46",
            "content": "Classification or Sequence Labeling? Most previous works treat discourse relation identification as a straightforward classification task, where given two EDUs, a system should identify which relationship the EDU pair hold. As can be seen from Table 2, all sequence labeling models (our main model as well as the variants) achieve a considerable gain over direct classification models on both datasets, especially in terms of accuracy on gold dependencies. This result verifies our hypothesis about the structural patterns of discourse relations shared across different articles. It is noticed that BERT + SBiL performs the best because its hierarchical structure can better capture different structured representations occuring at intra-and inter-sentential levels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_47",
            "content": "In Table 4, we include the performances of different models on intra-and inter-sentential relations on SciDTB with gold dependency structure. We observe that although our BERT+BiL model improves accuracies on both levels compared to the traditional classification model, the more significant improvement is on the inter-sentential level (by 2.1%). We show that it can even be promoted by another 2.4% if we stack an additional BiLSTM layer on top to explicitly capture the interplay between EDUs on the inter-sentential level. That's probably because writing patterns are more likely to appear in a global view so that discourse relations on the inter-sentential level tend to be more structurally organized than that on the intra-sentential level.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_48",
            "content": "To test the effectiveness of our model for implicit discourse relation identification, We delete some freely omissible connectives identified by Ma et al. (2019) to automatically generate implicit discourse relations. This results in 564 implicit instances in the test discourses. We run our model on the modified test data without retraining and compare the accuracies on those generated implicit relations. Table 5 shows the accuracies for those 564 instances before and after the modification. After the modification, although accuracies of all three models drop significantly, our sequence labeling model BERT+BiL and BERT+SBiL outperform the traditional direct classification model BERT by 1.4% and 2.5% respectively, showing that our sequence labeling models can make use of clues from the context to help identify relations in the case of implicit relations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_49",
            "content": "In addition, we experiment with other empirical implementations of contextualized representations instead of averaging tokens like using [CLS] for aggregate representations of sentences for intersentential dependency parsing, but we did not observe a significant difference. Averaging token representations turns out to have better generalizability and more straightforward for implementation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_50",
            "content": "Case Study",
            "ntype": "title",
            "meta": {
                "section": "5.4"
            }
        },
        {
            "ix": "65-ARR_v2_51",
            "content": "For the example shown in Figure 1, the relation between EDU 9 and EDU 13 is hard to classify using traditional direct classification because both of them contain only partial information of the sentences but their relation spans across sentences. Therefore, traditional direct classification model gets confused on this EDU pair and predicts the relation to be \"elab-addition\", which is plausible if we only look at those two EDUs isolated from the context. However, given the gold dependency structure, our sequence labeling model fits the EDU pair into the context and infers from common writing patterns to successfully yield the right prediction \"evaluation\". This shows that our model can refer to the structural information in the context to help make better predictions of relation labels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_52",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "65-ARR_v2_53",
            "content": "In this paper, we incorporate contextualized representations to our Sent-First general design of the model to dynamically capture different information required for discourse analysis on intra-and intersentential levels. We raise the awareness of taking advantage of writing patterns in discourse parsing and contrive a paradigm shift from direct classification to sequence labeling for discourse relation identification. We come up with a stacked biL-STM architecture to exploit its hierarchical design to capture structural information occurring at both intra-and inter-sentential levels. Future work will involve making better use of the structural information instead of applying simple sequence labeling.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_54",
            "content": "Theorem 1: For a document D with m sentences (s 1 , s 2 , ..., s m ) and n of the sentences have length(in terms of the number of EDUs) greater or equal to 2 satisfying |s i | \u2265 2. Let T be the set of all projective dependency trees obtainable from D, and let T \u2032 be the set of all projective dependency trees obtainable from D in a Sent-First fashion.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_55",
            "content": "Then the following inequality holds:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_56",
            "content": "|T \u2032 | \u2264 2 n + 1 |T |",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_57",
            "content": "Proof of Theorem 1: By the definition of our Sent-First method, trees in T \u2032 satisfy the property that there is exactly one EDU in each sentence whose head or children lies outside the sentence. It is clear that T \u2032 \u2282 T . We consider a document D with m sentences (s 1 , s 2 , ..., s m ) and n of the sentences have length(in terms of the number of EDUs) greater or equal to 2 satisfying |s i | \u2265 2. \u2200\u03c3 \u2032 \u2208 T \u2032 , \u03c3 \u2032 is a valid projective dependency tree obtainable from D in a Sent-First fashion. We define a t-transformation to a sentence s i , |s i | > 1 with its local root of the sentence e ia not being the root of the document in \u03c3 \u2032 with the following rules:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_58",
            "content": "1. If e ia has no child outside s i , e ib is its furthest (in terms of distance to e ia ) child or one of its furthest children inside s i , then delete the edge between e ia ) and e ib and set the head of e ib to be the head of e ia .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_59",
            "content": "2. Else if e ia has at least one child before e ia inside s i , and e ib is its furthest child before e ia inside s i . Delete the edge between e ia and e ib . If i > 1, set the head of e ib to be the local root of sentence s i\u22121 , else i = 1, set the head of e ib to be the local root of sentence s i+1 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_60",
            "content": "3. Else, e ia has at least one child after e ia inside s i , and e ib is its furthest child after e ia inside s i . Delete the edge between e ia ) and e ib . If i < m, set the head of e ib to be the local root of sentence s i+1 , else i = m, set the head of e ib to be the local root of sentence s m\u22121 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_61",
            "content": "Suppose \u03c3 i is obtained by applying ttransformation to the sentence s i , it is obvious to show that \u03c3 i \u2208 T /T \u2032 . n\u22121 valid t-transformations can be applied to \u03c3 \u2032 . A reverse transformation t \u22121 can be applied to \u03c3 i with the following rule: if a sentence has two local roots, change the head of one of the roots to the other root. In this way, at most two possibly valid trees \u2208 T \u2032 can be obtained because we are not sure which one is the original local root of the sentence. Therefore, at most 2 different \u03c3 \u2032 \u2208 T \u2032 can be found to share the same tree structure after a t-transformation. See Figure 5 for illustration. Therefore, |T",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_62",
            "content": "/T \u2032 | \u2265 n \u2212 1 2 |T \u2032 | |T \u2032 | \u2264 2 n + 1 |T |",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_63",
            "content": "Theorem 1 shows that the search space shrinks with the number of sentences. Therefore, Sent-First approach is especially effective at the reduction of search space so that the parser has a better chance to find the correct result, no matter what kind of parser is used specifically. Since the effectiveness has been proved, this approach can even be confidently generalized to other cases where similar sentencelike boundaries can be identified.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_64",
            "content": "Besides, an even stronger bound regarding the use of Sent-First method can also be proved for constituent parsing.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_65",
            "content": "Theorem 2: For a document D with m > 1 sentences (s 1 , s 2 , ..., s m ) and n of the sentences have length(in terms of the number of EDUs) greater or equal to 2 satisfying |s i | \u2265 2. Let T be the set of all binary constituency trees obtainable from D, and let T \u2032 be the set of all binary constituency trees obtainable from D in a Sent-First fashion. Then the following inequality holds:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_66",
            "content": "|T \u2032 | \u2264 ( 1 2 ) n |T |",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_67",
            "content": "Proof of Theorem 2: By the definition of our Sent-First method, trees in T \u2032 satisfy the property that EDUs in a sentence forms a complete subtree. It is clear that T \u2032 \u2282 T . We define a tree transformation t, for a tree u 1 with child u 2 and u 3 , u 3 being a complete discourse tree of a sentence with more than 2 EDUs. u 3 must also have 2 children named u 4 and u 5 where u 4 is adjacent to u 2 in the sentence. After transformation t, a new tree u \u2032 1 is derived whose children are u 5 and a subtree u 6 with children u 2 and u 4 . u 1 \u2208 T \u2032 , while u \u2032 1 \u2208 T /T \u2032 . Illustration see Figure 6. Note that t is one-to-one so that different u 1 will be transformed to different u \u2032 1 after t-transformation and u 1 can be applied t-transformation twice if both children of u 1 are complete DTs for a sentence (more possible trees u \u2032 1 can be transformed into if the order of transformation is also considered). Transformation t is a local transformation and does not affect sub-trees u 2 , u 4 , and u 5 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_68",
            "content": "\u2200\u03c3 \u2032 \u2208 T \u2032 , \u03c3 \u2032 is a valid projective dependency tree obtainable from D in a Sent-First fashion. Since all sub-trees representing a sentence must merge into one complete discourse tree representing the whole document, there must be n independent t transformations applicable to some subtrees in \u03c3 \u2032 , so that at least 2 n \u2212 1 trees can be obtained after i \u2265 1 t transformations \u2208 T /T \u2032 . Since t-transformation is one-to-one, \u2200\u03c3 1 , \u03c3 2 \u2208 T \u2032 , \u03c3 1 \u0338 = \u03c3 2 , \u03c3 \u2032 1 is a tree obtained after some ttransformations on \u03c3 1 , \u03c3 \u2032 2 is a tree obtained after some t-transformations on \u03c3 2 , \u03c3 \u2032 1 \u0338 = \u03c3 \u2032 2 . Therefore, The first 14 relations are listed in descending order in terms of their frequencies in the test dataset (652,178,156,131,127,121,71,56,54,48,46,45,37,33).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_69",
            "content": "|T /T \u2032 | \u2265 (2 n \u2212 1)|T \u2032 | |T \u2032 | \u2264 (1",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "65-ARR_v2_70",
            "content": "Stergos Afantenos, Eric Kow, Nicholas Asher, J\u00e9r\u00e9my Perret, Discourse parsing for multiparty chat dialogues, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Stergos Afantenos",
                    "Eric Kow",
                    "Nicholas Asher",
                    "J\u00e9r\u00e9my Perret"
                ],
                "title": "Discourse parsing for multiparty chat dialogues",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "65-ARR_v2_71",
            "content": "UNKNOWN, None, 2018, BERT: pre-training of deep bidirectional transformers for language understanding, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "BERT: pre-training of deep bidirectional transformers for language understanding",
                "pub": "CoRR"
            }
        },
        {
            "ix": "65-ARR_v2_72",
            "content": "Vanessa Wei Feng, Graeme Hirst, A lineartime bottom-up discourse parser with constraints and post-editing, 2014, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Vanessa Wei Feng",
                    "Graeme Hirst"
                ],
                "title": "A lineartime bottom-up discourse parser with constraints and post-editing",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "65-ARR_v2_73",
            "content": "David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya Kalyanpur, Adam Lally, J Murdock, Eric Nyberg, John Prager, Nico Schlaefer, Chris Welty, Building watson: An overview of the deepqa project, 2010, AI Magazine, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "David Ferrucci",
                    "Eric Brown",
                    "Jennifer Chu-Carroll",
                    "James Fan",
                    "David Gondek",
                    "Aditya Kalyanpur",
                    "Adam Lally",
                    "J Murdock",
                    "Eric Nyberg",
                    "John Prager",
                    "Nico Schlaefer",
                    "Chris Welty"
                ],
                "title": "Building watson: An overview of the deepqa project",
                "pub_date": "2010",
                "pub_title": "AI Magazine",
                "pub": null
            }
        },
        {
            "ix": "65-ARR_v2_74",
            "content": "Shyh-Shiun Hung,  Hen-Hsen, Hsin-Hsi Huang,  Chen, A complete shift-reduce Chinese discourse parser with robust dynamic oracle, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    " Shyh-Shiun Hung",
                    " Hen-Hsen",
                    "Hsin-Hsi Huang",
                    " Chen"
                ],
                "title": "A complete shift-reduce Chinese discourse parser with robust dynamic oracle",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "65-ARR_v2_75",
            "content": "Yanyan Jia, Yansong Feng, Yuan Ye, Chao Lv, Chongde Shi, Dongyan Zhao, Improved discourse parsing with two-step neural transition-based model, 2018, ACM Trans. Asian Low-Resour. Lang. Inf. Process, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Yanyan Jia",
                    "Yansong Feng",
                    "Yuan Ye",
                    "Chao Lv",
                    "Chongde Shi",
                    "Dongyan Zhao"
                ],
                "title": "Improved discourse parsing with two-step neural transition-based model",
                "pub_date": "2018",
                "pub_title": "ACM Trans. Asian Low-Resour. Lang. Inf. Process",
                "pub": null
            }
        },
        {
            "ix": "65-ARR_v2_76",
            "content": "Yanyan Jia, Yuan Ye, Yansong Feng, Yuxuan Lai, Rui Yan, Dongyan Zhao, Modeling discourse cohesion for discourse parsing via memory network, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Yanyan Jia",
                    "Yuan Ye",
                    "Yansong Feng",
                    "Yuxuan Lai",
                    "Rui Yan",
                    "Dongyan Zhao"
                ],
                "title": "Modeling discourse cohesion for discourse parsing via memory network",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "65-ARR_v2_77",
            "content": "Shafiq Joty, Giuseppe Carenini, Raymond Ng, A novel discriminative framework for sentence-level discourse analysis, 2012, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Shafiq Joty",
                    "Giuseppe Carenini",
                    "Raymond Ng"
                ],
                "title": "A novel discriminative framework for sentence-level discourse analysis",
                "pub_date": "2012",
                "pub_title": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "65-ARR_v2_78",
            "content": "Shafiq Joty, Giuseppe Carenini, Raymond Ng, Yashar Mehdad, Combining intra-and multisentential rhetorical parsing for document-level discourse analysis, 2013, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Shafiq Joty",
                    "Giuseppe Carenini",
                    "Raymond Ng",
                    "Yashar Mehdad"
                ],
                "title": "Combining intra-and multisentential rhetorical parsing for document-level discourse analysis",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "65-ARR_v2_79",
            "content": "Naoki Kobayashi, Tsutomu Hirao, Hidetaka Kamigaito, Manabu Okumura, Masaaki Nagata, Topdown rst parsing utilizing granularity levels in documents, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Naoki Kobayashi",
                    "Tsutomu Hirao",
                    "Hidetaka Kamigaito",
                    "Manabu Okumura",
                    "Masaaki Nagata"
                ],
                "title": "Topdown rst parsing utilizing granularity levels in documents",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "65-ARR_v2_80",
            "content": "Sujian Li, Liang Wang, Ziqiang Cao, Wenjie Li, Text-level discourse dependency parsing, 2014, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Sujian Li",
                    "Liang Wang",
                    "Ziqiang Cao",
                    "Wenjie Li"
                ],
                "title": "Text-level discourse dependency parsing",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "65-ARR_v2_81",
            "content": "Yancui Li, Wenhe Feng, Jing Sun, Fang Kong, Guodong Zhou, Building Chinese discourse corpus with connective-driven dependency tree structure, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Yancui Li",
                    "Wenhe Feng",
                    "Jing Sun",
                    "Fang Kong",
                    "Guodong Zhou"
                ],
                "title": "Building Chinese discourse corpus with connective-driven dependency tree structure",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "65-ARR_v2_82",
            "content": "Derek Mingyu, Kevin Ma, Jiaqi Bowden, Wen Wu, Marilyn Cui,  Walker, Implicit discourse relation identification for open-domain dialogues, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Derek Mingyu",
                    "Kevin Ma",
                    "Jiaqi Bowden",
                    "Wen Wu",
                    "Marilyn Cui",
                    " Walker"
                ],
                "title": "Implicit discourse relation identification for open-domain dialogues",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "65-ARR_v2_83",
            "content": "Joakim Nivre, An efficient algorithm for projective dependency parsing, 2003, Proceedings of the Eighth International Conference on Parsing Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Joakim Nivre"
                ],
                "title": "An efficient algorithm for projective dependency parsing",
                "pub_date": "2003",
                "pub_title": "Proceedings of the Eighth International Conference on Parsing Technologies",
                "pub": null
            }
        },
        {
            "ix": "65-ARR_v2_84",
            "content": "J\u00e9r\u00e9my Perret, Stergos Afantenos, Nicholas Asher, Mathieu Morey, Integer linear programming for discourse parsing, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "J\u00e9r\u00e9my Perret",
                    "Stergos Afantenos",
                    "Nicholas Asher",
                    "Mathieu Morey"
                ],
                "title": "Integer linear programming for discourse parsing",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "65-ARR_v2_85",
            "content": "UNKNOWN, None, 2018, A deep sequential model for discourse parsing on multi-party dialogues, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "A deep sequential model for discourse parsing on multi-party dialogues",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "65-ARR_v2_0@0",
            "content": "Improve Discourse Dependency Parsing with Contextualized Representations",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_0",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_2@0",
            "content": "Recent works show that discourse analysis benefits from modeling intra-and inter-sentential levels separately, where proper representations for text units of different granularities are desired to capture both the meaning of text units and their relations to the context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_2",
            "start": 0,
            "end": 270,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_2@1",
            "content": "In this paper, we propose to take advantage of transformers to encode contextualized representations of units of different levels to dynamically capture the information required for discourse dependency analysis on intra-and inter-sentential levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_2",
            "start": 272,
            "end": 520,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_2@2",
            "content": "Motivated by the observation of writing patterns commonly shared across articles, we propose a novel method that treats discourse relation identification as a sequence labelling task, which takes advantage of structural information from the context of extracted discourse trees, and substantially outperforms traditional directclassification methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_2",
            "start": 522,
            "end": 871,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_2@3",
            "content": "Experiments show that our model achieves state-of-the-art results on both English and Chinese datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_2",
            "start": 873,
            "end": 975,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_2@4",
            "content": "Our code is publicly available 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_2",
            "start": 977,
            "end": 1010,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_4@0",
            "content": "Discourse dependency parsing (DDP) is the task of identifying the structure and relationship between Elementary Discourse Units (EDUs) in a document.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_4",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_4@1",
            "content": "It is a fundamental task of natural language understanding and can benefit many downstream applications, such as dialogue understanding (Perret et al., 2016;Takanobu et al., 2018) and question answering (Ferrucci et al., 2010;Verberne et al., 2007).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_4",
            "start": 150,
            "end": 398,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_5@0",
            "content": "Although existing works have achieved much progress using transition-based systems (Jia et al., 2018b,a;Hung et al., 2020) or graph-based models (Li et al., 2014a;Shi and Huang, 2018;Afantenos et al., 2015), this task still remains a challenge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_5",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_5@1",
            "content": "Different from syntactic parsing, the basic components in a discourse are EDUs, sequences of words, which are not trivial to represent in a straightforward way like word embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_5",
            "start": 245,
            "end": 425,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_5@2",
            "content": "Predicting the dependency and relationship between EDUs sometimes necessitates the help of a global understanding of the context so that contextualized EDU representations in the discourse are needed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_5",
            "start": 427,
            "end": 626,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_5@3",
            "content": "Furthermore, previous studies have shown the benefit of breaking discourse analysis into intra-and inter-sentential levels (Wang et al., 2017), building sub-trees for each sentence first and then assembling sub-trees to form a complete discourse tree.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_5",
            "start": 628,
            "end": 878,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_5@4",
            "content": "In this Sentence-First (Sent-First) framework, it is even more crucial to produce appropriate contextualized representations for text units when analyzing in intra-or inter-sentential levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_5",
            "start": 880,
            "end": 1070,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_6@0",
            "content": "Automatic metrics are widely used in machine translation as a substitute for human assessment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_6",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_6@1",
            "content": "This is often measured by correlation with human judgement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_6",
            "start": 95,
            "end": 153,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_7@0",
            "content": "In this paper, we propose a significant test Figure 1 shows an excerpt discourse dependency structure for a scientific abstract from SciDTB (Yang and Li, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_7",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_7@1",
            "content": "The lengths of EDUs vary a lot, from more than 10 words to 2 words only (EDU 12: tests show), making it especially hard to encode by themselves alone.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_7",
            "start": 161,
            "end": 310,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_7@2",
            "content": "Sometimes it is sufficient to consider the contextual information in a small range as in the case of EDU 13 and 14, other times we need to see a larger context as in the case of EDU 1 and 4, crossing several sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_7",
            "start": 312,
            "end": 529,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_7@3",
            "content": "This again motivates us to consider encoding contextual representations of EDUs separately on intraand inter-sentential levels to dynamically capture specific features needed for discourse analysis on different levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_7",
            "start": 531,
            "end": 748,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_7@4",
            "content": "Another motivation from this example is the discovery that the distribution of discourse relations between EDUs seems to follow certain patterns shared across different articles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_7",
            "start": 750,
            "end": 927,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_7@5",
            "content": "Writing patterns are document structures people commonly use to organize their arguments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_7",
            "start": 929,
            "end": 1017,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_7@6",
            "content": "For example, in scientific abstracts like the instance in Figure 1, people usually first talk about background information, then introduce the topic sentence, and conclude with elaborations or evaluations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_7",
            "start": 1019,
            "end": 1223,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_7@7",
            "content": "Here, the example first states the background of widely used automatic metrics, introduces the topic sentence about their contribution of a significance test followed by evaluation and conclusion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_7",
            "start": 1225,
            "end": 1420,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_7@8",
            "content": "Taking advantage of those writing patterns should enable us to better capture the interplay between individual EDUs with the context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_7",
            "start": 1422,
            "end": 1554,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_8@0",
            "content": "In this paper, we explore different contextualized representations for DDP in a Sent-First parsing framework, where a complete discourse tree is built up sentence by sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_8",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_8@1",
            "content": "We seek to dynamically capture what is crucial for DDP at different text granularity levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_8",
            "start": 176,
            "end": 267,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_8@2",
            "content": "We further propose a novel discourse relation identification method that addresses the task in a sequence labeling paradigm to exploit common conventions people usually adopt to develop their arguments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_8",
            "start": 269,
            "end": 470,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_8@3",
            "content": "We evaluate our models on both English and Chinese datasets, and experiments show our models achieve the state-of-the-art results by explicitly exploiting structural information in the context and capturing writing patterns that people use to organize discourses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_8",
            "start": 472,
            "end": 734,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_9@0",
            "content": "In summary, our contributions are mainly twofold: (1) We incorporate the Pre-training and Fine-tuning framework into our design of a Sent-First model and develop better contextualized EDU representations to dynamically capture different information needed for DDP at different text granularity levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_9",
            "start": 0,
            "end": 300,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_9@1",
            "content": "Experiments show that our model outperforms all existing models by a large margin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_9",
            "start": 302,
            "end": 383,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_10@0",
            "content": "(2) We formulate discourse relation identification in a novel sequence labeling paradigm to take advantage of the inherent structural information in the discourse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_10",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_10@1",
            "content": "Building upon a stacked BiLSTM architecture, our model brings a new state-of-the-art performance on two benchmarks, showing the advantage of sequence labeling over the common practice of direct classification for discourse relation identification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_10",
            "start": 164,
            "end": 410,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_11@0",
            "content": "Related Works",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_11",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_12@0",
            "content": "A key finding in previous studies in discourse analysis is that most sentences have an independent well-formed sub-tree in the full document-level discourse tree (Joty et al., 2012).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_12",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_12@1",
            "content": "Researchers have taken advantage of this finding to build parsers that utilize different granularity levels of the document to achieve the state-of-the-art results (Kobayashi et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_12",
            "start": 183,
            "end": 371,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_12@2",
            "content": "This design has been empirically verified to be a generally advantageous framework, improving not only works using traditional feature engineering (Joty et al., 2013;Wang et al., 2017), but also deep learning models (Jia et al., 2018b;Kobayashi et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_12",
            "start": 373,
            "end": 631,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_12@3",
            "content": "We, therefore, introduce this design to our dependency parsing framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_12",
            "start": 633,
            "end": 705,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_12@4",
            "content": "Specifically, sub-trees for each sentence in a discourse are first built separately, then assembled to form a complete discourse tree.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_12",
            "start": 707,
            "end": 840,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_13@0",
            "content": "However, our model differs from prior works in that we make a clear distinction to derive better contextualized representations of EDUs from fine-tuning BERT separately for intra-and intersentential levels to dynamically capture different information needed for discourse analysis at different levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_13",
            "start": 0,
            "end": 300,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_13@1",
            "content": "We are also the first to design stacked sequence labeling models for discourse relation identification so that its hierarchical structure can explicitly capture both intra-sentential and intersentential writing patterns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_13",
            "start": 302,
            "end": 521,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_14@0",
            "content": "In the case of implicit relations between EDUs without clear connectives, it is crucial to introduce sequential information from the context to resolve ambiguity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_14",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_14@1",
            "content": "Feng and Hirst (2014) rely on linearchain CRF with traditional feature engineering to make use of the sequential characteristics of the context for discourse constituent parsing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_14",
            "start": 163,
            "end": 340,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_14@2",
            "content": "However, they greedily build up the discourse structure and relations from bottom up.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_14",
            "start": 342,
            "end": 426,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_14@3",
            "content": "At each timestep, they apply the CRF to obtain the locally optimized structure and relation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_14",
            "start": 428,
            "end": 519,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_14@4",
            "content": "In this way, the model assigns relation gradually along with the construction of the parsing tree from bottom up, but only limited contextual information from the top level of the partially constructed tree can be used to predict relations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_14",
            "start": 521,
            "end": 760,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_14@5",
            "content": "Besides, at each timestep, they sequentially assign relations to top nodes of the partial tree, without being aware that those nodes might represent different levels of discourse units (e.g. EDUs, sentences, or even paragraphs).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_14",
            "start": 762,
            "end": 989,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_14@6",
            "content": "In contrast, we explicitly train our sequence labeling models on both intra-and inter-sentential levels after a complete discourse tree is constructed so that we can infer from the whole context with a clear intention of capturing different writing patterns occurring at intra-and inter-sentential levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_14",
            "start": 991,
            "end": 1295,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_15@0",
            "content": "Task Definition",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_15",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_16@0",
            "content": "We define the task of discourse dependency parsing as following: given a sequence of EDUs of length l, (e 1 , e 2 , ..., e l ) and a set of possible relations between EDUs Re, the goal is to predict another sequence of EDUs (h 1 , h 2 , ..., h l ) such that \u2200h i , h i \u2208 (e 1 , e 2 , ..., e l ) is the head of e i and a sequence of relations (r 1 , r 2 , ..., r l ) such that \u2200r i , r i is the relation between tuple (e i , h i ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_16",
            "start": 0,
            "end": 429,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_17@0",
            "content": "Our Model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_17",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_18@0",
            "content": "We follow previous works (Wang et al., 2017) to cast the task of discourse dependency parsing as a composition of two separate yet related subtasks: dependency tree construction and relation identification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_18",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_18@1",
            "content": "We design our model primarily in a twostep pipeline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_18",
            "start": 207,
            "end": 258,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_18@2",
            "content": "We incorporate Sent-First design as our backbone (i.e. building sub-trees for each sentence and then assembling them into a complete discourse tree), and formulate discourse relation identification as a sequence labeling task on both intra-and inter-sentential levels to take advantage of the structure information in the discourse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_18",
            "start": 260,
            "end": 591,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_18@3",
            "content": "Figure 1 shows the overview of our model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_18",
            "start": 593,
            "end": 633,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_19@0",
            "content": "Discourse Dependency Tree Constructor",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_19",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_20@0",
            "content": "To take advantage of the property of well-formed sentence sub-trees inside a full discourse tree, we break the task of dependency parsing into two different levels, discovering intra-sentential sub-tree structures first and then aseembling them into a full discourse tree by identifying the inter-sentential structure of the discourse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_20",
            "start": 0,
            "end": 334,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_21@0",
            "content": "Arc-Eager Transition System Since discourse dependency trees are primarily annotated as projective trees (Yang and Li, 2018), we design our tree constructor as a transition system, which converts the structure prediction process into a sequence of predicted actions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_21",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_21@1",
            "content": "At each timestep, we derive a state feature to represent the state, which is fed into an output layer to get the predicted action.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_21",
            "start": 267,
            "end": 396,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_21@2",
            "content": "Our model follows the standard Arc-Eager system, with the action set: O= {Shif t, Lef t \u2212 Arc, Right \u2212 Arc, Reduce}.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_21",
            "start": 398,
            "end": 513,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_22@0",
            "content": "Specifically, our discourse tree constructor maintains a stack S, a queue I, and a set of assigned arcs A during parsing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_22",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_22@1",
            "content": "The stack S and the set of assigned arcs A are initialized to be empty, while the queue I contains all the EDUs in the input sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_22",
            "start": 122,
            "end": 255,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_22@2",
            "content": "At each timestep, an action in the action set O is performed with the following definition: Shift pushes the first EDU in queue I to the top of stack S; Left-Arc adds an arc from the first EDU in queue I to the top EDU in stack S (i.e. assigns the first EDU in I to be the head of the top EDU in S) and removes the top EDU in S; Right-Arc adds an arc from the top EDU in stack S to the first EDU in queue I (i.e. assigns the top EDU in S to be the head) and pushes the first EDU in I to stack S; Reduce removes the top EDU in S. Parsing terminates when I becomes empty and the only EDU left in S is selected to be the head of the input sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_22",
            "start": 257,
            "end": 901,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_22@3",
            "content": "More details of Arc-Eager transition system can be referred from Nivre (2003).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_22",
            "start": 903,
            "end": 980,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_23@0",
            "content": "We first construct a dependency sub-tree for each sentence, and then treat each sub-tree as a leaf node to form a complete discourse tree across sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_23",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_23@1",
            "content": "In this way, we can break a long discourse into smaller sub-structures to reduce the search space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_23",
            "start": 156,
            "end": 253,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_23@2",
            "content": "A mathematical bound for the reduction of search space of our Sent-First framework for DDP and discourse constituent parsing is also provided in Appendix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_23",
            "start": 255,
            "end": 408,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_24@0",
            "content": "Contextualized State Representation Ideally, we would like the feature representation to contain both the information of the EDUs directly involved in the action to be executed and rich clues from the context from both the tree-structure and the text, e.g. the parsing history and the interactions between individual EDUs in the context with an appropriate scope of text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_24",
            "start": 0,
            "end": 370,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_24@1",
            "content": "In order to capture the structural clues from the context, we incorporate the parsing history in the form of identified dependencies in addition to traditional state representations to represent the current state.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_24",
            "start": 372,
            "end": 584,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_24@2",
            "content": "At each timestep, we select 6 EDUs from the current state as our feature template, including the first and the second EDU at the top of stack S, the first and the second EDU in queue I, and the head EDUs for the first and the second EDU at the top of stack S, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_24",
            "start": 586,
            "end": 858,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_24@3",
            "content": "A feature vector of all zeros is used if there is no EDU at a certain position.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_24",
            "start": 860,
            "end": 938,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_24@4",
            "content": "EDU Representations To better capture an EDU in our Sent-First framework, we use pre-trained BERT (Devlin et al., 2018) to obtain representations for each EDU according to different context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_24",
            "start": 940,
            "end": 1129,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_24@5",
            "content": "We argue that an EDU should have different representations when it is considered in different parsing levels, and thus requires level-specific contextual representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_24",
            "start": 1131,
            "end": 1300,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_24@6",
            "content": "For intra-sentential tree constructor, we feed the entire sentence to BERT and represent each EDU by averaging the last hidden states of all tokens in that EDU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_24",
            "start": 1302,
            "end": 1461,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_24@7",
            "content": "The reason behind is that sentences are often self-contained sub-units of the discourse, and it is sufficient to consider interactions among EDUs within a sentence for intra-sentential analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_24",
            "start": 1463,
            "end": 1656,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_24@8",
            "content": "On the other hand, for inter-sentential tree constructor, we concatenate all the root EDUs of different sentences in the discourse to form a pseudo sentence, feed it to BERT, and similarly, represent each root EDU by averaging the last hidden states of all tokens in each root EDU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_24",
            "start": 1658,
            "end": 1938,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_24@9",
            "content": "In this way, we aim to encourage EDUs across different sentences to directly interact with each other, in order to reflect the global properties of a discourse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_24",
            "start": 1940,
            "end": 2099,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_24@10",
            "content": "Figure 2 shows the architecture for our two-stage discourse dependency tree constructor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_24",
            "start": 2101,
            "end": 2188,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_25@0",
            "content": "Discourse Relation Identification",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_25",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_26@0",
            "content": "After the tree constructor is trained, we train separate sequence labeling models for relation identification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_26",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_26@1",
            "content": "Although discourse relation identification in discourse dependency parsing is traditionally treated as a classification task, where the common practice is to use feature engineering or neural language models to directly compare two EDUs involved isolated from the rest of the context (Li et al., 2014a;Shi and Huang, 2018;Yi et al., 2021), sometimes relations between EDU pairs can be hard to be classified in isolation, as global information from the context like how EDUs are organized to support the claim in the discourse is sometimes required to infer the implicit discourse relations without explicit connectives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_26",
            "start": 111,
            "end": 729,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_26@2",
            "content": "Therefore, we propose to identify discourse relation identification as a sequence labeling task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_26",
            "start": 731,
            "end": 826,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_27@0",
            "content": "Structure-aware Representations For sequence labeling, we need proper representations for EDU pairs to reflect the structure of the dependency tree.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_27",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_27@1",
            "content": "Therefore, we first tile each EDU in the input sequence (e 1 , e 2 , ..., e l ) with their predicted heads to form a sequence of EDU pairs ((e 1 , h 1 ), (e 2 , h 2 ), ..., (e l , h l )).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_27",
            "start": 149,
            "end": 335,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_27@2",
            "content": "Each EDU pair is reordered so that two arguments appear in the same order as they appear in the discourse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_27",
            "start": 337,
            "end": 442,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_27@3",
            "content": "We derive a relation representation for each EDU pair with a BERT fine-tuned on the task of direct relation classification of EDU pairs with the [CLS] representation of the concatenation of two sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_27",
            "start": 444,
            "end": 647,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_28@0",
            "content": "We further introduce position embeddings for each EDU pair (e i , h i ), where we consider the position of e i in its corresponding sentence, and the position of its sentence in the discourse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_28",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_28@1",
            "content": "Specifically, we use cosine and sine functions of different frequencies (Vaswani et al., 2017) to include position information as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_28",
            "start": 193,
            "end": 322,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_29@0",
            "content": "P E j = sin(N o/10000 j/d ) + cos(ID/10000 j/d )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_29",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_30@0",
            "content": "where P E is the position embeddings, N o is the position of the sentence containing e i in the discourse, ID is the position of e i in the sentence, j is the dimension of the position embeddings, d is the dimension of the relation representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_30",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_30@1",
            "content": "The position embeddings have the same dimension as relation representations, so that they can be added directly to get the integrated representation for each EDU pair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_30",
            "start": 248,
            "end": 414,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_31@0",
            "content": "We propose a stacked BiL-STM neural network architecture to capture both intra-sentential and inter-sentential interplay of EDUs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_31",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_31@1",
            "content": "After labeling the entire sequence of EDU pairs ((e 1 , h 1 ), (e 2 , h 2 ), ..., (e l , h l )) with the first layer of BiLSTM, we select the root EDU for each sentence (namely the root EDU selected from our intra-sentential tree constructor for each setence) to form another inter-sentential sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_31",
            "start": 130,
            "end": 431,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_31@2",
            "content": "Another separately trained BiLSTM is then applied to label those relations that span across sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_31",
            "start": 433,
            "end": 534,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_31@3",
            "content": "Note that we will overwrite predictions of inter-sentential relations of the previous layer if there is a conflict of predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_31",
            "start": 536,
            "end": 665,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_32@0",
            "content": "Training",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_32",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_33@0",
            "content": "Our models are trained with offline learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_33",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_33@1",
            "content": "We train the tree constructor and the relation labeling models separately.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_33",
            "start": 46,
            "end": 119,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_33@2",
            "content": "We attain the static oracle to train tree constructors and use the gold dependency structure to train our discourse relation labelling models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_33",
            "start": 121,
            "end": 262,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_33@3",
            "content": "Intra-and inter-sentential tree constructors are trained separately.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_33",
            "start": 264,
            "end": 331,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_33@4",
            "content": "To label discourse relations, we fine-tune the BERT used to encode the EDU pair with an additional output layer for direct relation classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_33",
            "start": 333,
            "end": 479,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_33@5",
            "content": "Sequence labeling models for relation identification are trained on top of the finetuned BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_33",
            "start": 481,
            "end": 574,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_33@6",
            "content": "We use cross entropy loss for training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_33",
            "start": 576,
            "end": 614,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_34@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_34",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_35@0",
            "content": "Our experiments are designed to investigate how we can better explore contextual representations to improve discourse dependency parsing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_35",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_36@0",
            "content": "We evaluate our models on two manually labeled discourse treebanks of different language, i.e., Discourse Dependency Treebank for Scientific Abstracts (SciDTB) (Yang and Li, 2018) in English and Chinese Discourse Treebank (CDTB) (Li et al., 2014b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_36",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_36@1",
            "content": "SciDTB contains 1,355 English scientific abstracts collected from ACL Anthology.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_36",
            "start": 249,
            "end": 328,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_36@2",
            "content": "Averagely, an abstract includes 5.3 sentences, 14.1 EDUs, where an EDU has 10.3 tokens in average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_36",
            "start": 330,
            "end": 427,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_37@0",
            "content": "On the other hand, CDTB was originally annotated as connective-driven constituent trees, and manually converted into a dependency style by Yi et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_37",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_37@1",
            "content": "CDTB contains 2,332 news documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_37",
            "start": 157,
            "end": 191,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_37@2",
            "content": "The average length of a paragraph is 2.1 sentences, 4.5 EDUs. And an EDU contains 23.3 tokens in average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_37",
            "start": 193,
            "end": 297,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_38@0",
            "content": "We evaluate model performance using Unlabeled Attachment Score (UAS) and Labeled Attachment Score (LAS) for dependency prediction and discourse relation identification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_38",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_38@1",
            "content": "UAS is defined as the percentage of nodes with correctly predicted heads, while LAS is defined as the percentage of nodes with both correctly predicted heads and correctly predicted relations to their heads.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_38",
            "start": 169,
            "end": 375,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_38@2",
            "content": "We report LAS against both gold dependencies and model predicted dependencies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_38",
            "start": 377,
            "end": 454,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_38@3",
            "content": "We adopt the finegranularity discourse relation annotations in the original datasets, 26 relations for SciDTB and 17 relations for CDTB.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_38",
            "start": 456,
            "end": 591,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_39@0",
            "content": "For both datasets, we trained our dependency tree constructors with an Adam optimizer with learning rate 2e-5 for 3 epochs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_39",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_39@1",
            "content": "Our relation labeling models are all trained with an Adam optimizer until convergence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_39",
            "start": 124,
            "end": 209,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_39@2",
            "content": "Learning rate is set to one of {1e-5, 2e-5, 4e-5}.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_39",
            "start": 211,
            "end": 260,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_40@0",
            "content": "Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_40",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_41@0",
            "content": "Structure Prediction We compare with the following competitive methods for structure prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_41",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_41@1",
            "content": "(1) Graph adopts the Eisner's algorithm to predict the most probable dependency tree structure (Li et al., 2014a;Yang and Li, 2018;Yi et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_41",
            "start": 97,
            "end": 244,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_41@2",
            "content": "(2) Two-stage, which is the state-of-the-art model on CDTB and SciDTB, uses an SVM to construct a dependency tree (Yang and Li, 2018;Yi et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_41",
            "start": 246,
            "end": 395,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_41@3",
            "content": "(3) Sent-First LSTM is our implmentation of the state-of-the-art transition-based discourse constituent parser on RST (Kobayashi et al., 2020), where we use a vanilla transition system with pretrained BiLSTM as the EDU encoder within the Sent-First framework to construct dependency trees.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_41",
            "start": 397,
            "end": 685,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_41@4",
            "content": "(4) Complete Parser is modified from a state-of-the-art constituent discourse parser on CDTB (Hung et al., 2020), using a transition system with BERT as the EDU encoder to construct a dependency tree.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_41",
            "start": 687,
            "end": 886,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_41@5",
            "content": "Because of the inherent difference between constituency parsing and dependency parsing, we only adopt the encoding strategy of ( 4) and ( 5) into our arc-eager transition system.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_41",
            "start": 888,
            "end": 1065,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_42@0",
            "content": "We also implement several model variants for can be exploited to aid discourse relation identification, as have been discussed in section 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_42",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_42@1",
            "content": "We show that the results can be further improved by making use of the sequential structure of the discourse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_42",
            "start": 141,
            "end": 248,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_42@2",
            "content": "We design multiple novel sequence labeling models on top of the fine-tuned BERT and all of them achieve a considerable improvement (more than 1%) over BERT in terms of accuracy both on the gold dependencies and the predicted dependencies from our Sent-First (separate), showing the benefit of enhancing the interactions between individual EDUs with the context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_42",
            "start": 250,
            "end": 610,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_42@3",
            "content": "It yields another large gain when we introduce another layer of inter-sentential level BiLSTM, showing again that it is crucial to capture the interactions between EDUs and their context in both intra-and inter-sentential levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_42",
            "start": 612,
            "end": 840,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_43@0",
            "content": "Detailed Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_43",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_44@0",
            "content": "Contextualized Representations for Tree Construction Intuitively, a model should take different views of context when analyzing intra-and inter-sentential structures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_44",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_44@1",
            "content": "As we can see in Table 1, BERT + Sent-First (shared) improves Complete Parser (contextualized) by 1.2% and 2.4% on Sc-iTDB and CDTB, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_44",
            "start": 167,
            "end": 312,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_44@2",
            "content": "The only difference is BERT + Sent-First makes explicit predictions on two different levels, while Complete Parser (contextualized) treats them equally.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_44",
            "start": 314,
            "end": 465,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_44@3",
            "content": "When we force BERT + Sent-First to use different BERTs for intraand inter-sententential analysis, we observe further improvement, around 3% on both datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_44",
            "start": 467,
            "end": 623,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_45@0",
            "content": "If we take a closer look at their performance in intra-and inter-sentential views in Table 3, we can see that BERT + Sent-First (shared) performs better than single BERT model, Complete Parser (contextualized), on both intra-and inter-levels of SciDTB and CDTB, though in some cases we only observe marginal improvement like inter-sentential level of SciDTB.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_45",
            "start": 0,
            "end": 357,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_45@1",
            "content": "However, when we enhance BERT + Sent-First with different encoders for intra-and inter-sentential analysis, we can observe significant improvement in all cases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_45",
            "start": 359,
            "end": 518,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_45@2",
            "content": "importance of anaylzing with different but more focused contextual representations for the two parsing levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_45",
            "start": 520,
            "end": 629,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_46@0",
            "content": "Classification or Sequence Labeling? Most previous works treat discourse relation identification as a straightforward classification task, where given two EDUs, a system should identify which relationship the EDU pair hold.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_46",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_46@1",
            "content": "As can be seen from Table 2, all sequence labeling models (our main model as well as the variants) achieve a considerable gain over direct classification models on both datasets, especially in terms of accuracy on gold dependencies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_46",
            "start": 224,
            "end": 455,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_46@2",
            "content": "This result verifies our hypothesis about the structural patterns of discourse relations shared across different articles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_46",
            "start": 457,
            "end": 578,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_46@3",
            "content": "It is noticed that BERT + SBiL performs the best because its hierarchical structure can better capture different structured representations occuring at intra-and inter-sentential levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_46",
            "start": 580,
            "end": 765,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_47@0",
            "content": "In Table 4, we include the performances of different models on intra-and inter-sentential relations on SciDTB with gold dependency structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_47",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_47@1",
            "content": "We observe that although our BERT+BiL model improves accuracies on both levels compared to the traditional classification model, the more significant improvement is on the inter-sentential level (by 2.1%).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_47",
            "start": 142,
            "end": 346,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_47@2",
            "content": "We show that it can even be promoted by another 2.4% if we stack an additional BiLSTM layer on top to explicitly capture the interplay between EDUs on the inter-sentential level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_47",
            "start": 348,
            "end": 525,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_47@3",
            "content": "That's probably because writing patterns are more likely to appear in a global view so that discourse relations on the inter-sentential level tend to be more structurally organized than that on the intra-sentential level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_47",
            "start": 527,
            "end": 747,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_48@0",
            "content": "To test the effectiveness of our model for implicit discourse relation identification, We delete some freely omissible connectives identified by Ma et al. (2019) to automatically generate implicit discourse relations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_48",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_48@1",
            "content": "This results in 564 implicit instances in the test discourses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_48",
            "start": 218,
            "end": 279,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_48@2",
            "content": "We run our model on the modified test data without retraining and compare the accuracies on those generated implicit relations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_48",
            "start": 281,
            "end": 407,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_48@3",
            "content": "Table 5 shows the accuracies for those 564 instances before and after the modification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_48",
            "start": 409,
            "end": 495,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_48@4",
            "content": "After the modification, although accuracies of all three models drop significantly, our sequence labeling model BERT+BiL and BERT+SBiL outperform the traditional direct classification model BERT by 1.4% and 2.5% respectively, showing that our sequence labeling models can make use of clues from the context to help identify relations in the case of implicit relations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_48",
            "start": 497,
            "end": 864,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_49@0",
            "content": "In addition, we experiment with other empirical implementations of contextualized representations instead of averaging tokens like using [CLS] for aggregate representations of sentences for intersentential dependency parsing, but we did not observe a significant difference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_49",
            "start": 0,
            "end": 273,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_49@1",
            "content": "Averaging token representations turns out to have better generalizability and more straightforward for implementation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_49",
            "start": 275,
            "end": 392,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_50@0",
            "content": "Case Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_50",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_51@0",
            "content": "For the example shown in Figure 1, the relation between EDU 9 and EDU 13 is hard to classify using traditional direct classification because both of them contain only partial information of the sentences but their relation spans across sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_51",
            "start": 0,
            "end": 245,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_51@1",
            "content": "Therefore, traditional direct classification model gets confused on this EDU pair and predicts the relation to be \"elab-addition\", which is plausible if we only look at those two EDUs isolated from the context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_51",
            "start": 247,
            "end": 456,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_51@2",
            "content": "However, given the gold dependency structure, our sequence labeling model fits the EDU pair into the context and infers from common writing patterns to successfully yield the right prediction \"evaluation\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_51",
            "start": 458,
            "end": 662,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_51@3",
            "content": "This shows that our model can refer to the structural information in the context to help make better predictions of relation labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_51",
            "start": 664,
            "end": 795,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_52@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_52",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_53@0",
            "content": "In this paper, we incorporate contextualized representations to our Sent-First general design of the model to dynamically capture different information required for discourse analysis on intra-and intersentential levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_53",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_53@1",
            "content": "We raise the awareness of taking advantage of writing patterns in discourse parsing and contrive a paradigm shift from direct classification to sequence labeling for discourse relation identification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_53",
            "start": 221,
            "end": 420,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_53@2",
            "content": "We come up with a stacked biL-STM architecture to exploit its hierarchical design to capture structural information occurring at both intra-and inter-sentential levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_53",
            "start": 422,
            "end": 589,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_53@3",
            "content": "Future work will involve making better use of the structural information instead of applying simple sequence labeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_53",
            "start": 591,
            "end": 708,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_54@0",
            "content": "Theorem 1: For a document D with m sentences (s 1 , s 2 , ..., s m ) and n of the sentences have length(in terms of the number of EDUs) greater or equal to 2 satisfying |s i | \u2265 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_54",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_54@1",
            "content": "Let T be the set of all projective dependency trees obtainable from D, and let T \u2032 be the set of all projective dependency trees obtainable from D in a Sent-First fashion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_54",
            "start": 181,
            "end": 351,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_55@0",
            "content": "Then the following inequality holds:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_55",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_56@0",
            "content": "|T \u2032 | \u2264 2 n + 1 |T |",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_56",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_57@0",
            "content": "Proof of Theorem 1: By the definition of our Sent-First method, trees in T \u2032 satisfy the property that there is exactly one EDU in each sentence whose head or children lies outside the sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_57",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_57@1",
            "content": "It is clear that T \u2032 \u2282 T .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_57",
            "start": 195,
            "end": 220,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_57@2",
            "content": "We consider a document D with m sentences (s 1 , s 2 , ..., s m ) and n of the sentences have length(in terms of the number of EDUs) greater or equal to 2 satisfying |s i | \u2265 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_57",
            "start": 222,
            "end": 398,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_57@3",
            "content": "\u2200\u03c3 \u2032 \u2208 T \u2032 , \u03c3 \u2032 is a valid projective dependency tree obtainable from D in a Sent-First fashion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_57",
            "start": 400,
            "end": 496,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_57@4",
            "content": "We define a t-transformation to a sentence s i , |s i | > 1 with its local root of the sentence e ia not being the root of the document in \u03c3 \u2032 with the following rules:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_57",
            "start": 498,
            "end": 665,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_58@0",
            "content": "1. If e ia has no child outside s i , e ib is its furthest (in terms of distance to e ia ) child or one of its furthest children inside s i , then delete the edge between e ia ) and e ib and set the head of e ib to be the head of e ia .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_58",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_59@0",
            "content": "2. Else if e ia has at least one child before e ia inside s i , and e ib is its furthest child before e ia inside s i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_59",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_59@1",
            "content": "Delete the edge between e ia and e ib .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_59",
            "start": 120,
            "end": 158,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_59@2",
            "content": "If i > 1, set the head of e ib to be the local root of sentence s i\u22121 , else i = 1, set the head of e ib to be the local root of sentence s i+1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_59",
            "start": 160,
            "end": 304,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_60@0",
            "content": "3. Else, e ia has at least one child after e ia inside s i , and e ib is its furthest child after e ia inside s i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_60",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_60@1",
            "content": "Delete the edge between e ia ) and e ib .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_60",
            "start": 116,
            "end": 156,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_60@2",
            "content": "If i < m, set the head of e ib to be the local root of sentence s i+1 , else i = m, set the head of e ib to be the local root of sentence s m\u22121 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_60",
            "start": 158,
            "end": 302,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_61@0",
            "content": "Suppose \u03c3 i is obtained by applying ttransformation to the sentence s i , it is obvious to show that \u03c3 i \u2208 T /T \u2032 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_61",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_61@1",
            "content": "n\u22121 valid t-transformations can be applied to \u03c3 \u2032 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_61",
            "start": 116,
            "end": 166,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_61@2",
            "content": "A reverse transformation t \u22121 can be applied to \u03c3 i with the following rule: if a sentence has two local roots, change the head of one of the roots to the other root.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_61",
            "start": 168,
            "end": 333,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_61@3",
            "content": "In this way, at most two possibly valid trees \u2208 T \u2032 can be obtained because we are not sure which one is the original local root of the sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_61",
            "start": 335,
            "end": 479,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_61@4",
            "content": "Therefore, at most 2 different \u03c3 \u2032 \u2208 T \u2032 can be found to share the same tree structure after a t-transformation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_61",
            "start": 481,
            "end": 592,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_61@5",
            "content": "See Figure 5 for illustration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_61",
            "start": 594,
            "end": 623,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_61@6",
            "content": "Therefore, |T",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_61",
            "start": 625,
            "end": 637,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_62@0",
            "content": "/T \u2032 | \u2265 n \u2212 1 2 |T \u2032 | |T \u2032 | \u2264 2 n + 1 |T |",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_62",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_63@0",
            "content": "Theorem 1 shows that the search space shrinks with the number of sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_63",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_63@1",
            "content": "Therefore, Sent-First approach is especially effective at the reduction of search space so that the parser has a better chance to find the correct result, no matter what kind of parser is used specifically.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_63",
            "start": 76,
            "end": 281,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_63@2",
            "content": "Since the effectiveness has been proved, this approach can even be confidently generalized to other cases where similar sentencelike boundaries can be identified.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_63",
            "start": 283,
            "end": 444,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_64@0",
            "content": "Besides, an even stronger bound regarding the use of Sent-First method can also be proved for constituent parsing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_64",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_65@0",
            "content": "Theorem 2: For a document D with m > 1 sentences (s 1 , s 2 , ..., s m ) and n of the sentences have length(in terms of the number of EDUs) greater or equal to 2 satisfying |s i | \u2265 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_65",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_65@1",
            "content": "Let T be the set of all binary constituency trees obtainable from D, and let T \u2032 be the set of all binary constituency trees obtainable from D in a Sent-First fashion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_65",
            "start": 185,
            "end": 351,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_65@2",
            "content": "Then the following inequality holds:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_65",
            "start": 353,
            "end": 388,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_66@0",
            "content": "|T \u2032 | \u2264 ( 1 2 ) n |T |",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_66",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_67@0",
            "content": "Proof of Theorem 2: By the definition of our Sent-First method, trees in T \u2032 satisfy the property that EDUs in a sentence forms a complete subtree.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_67",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_67@1",
            "content": "It is clear that T \u2032 \u2282 T .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_67",
            "start": 148,
            "end": 173,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_67@2",
            "content": "We define a tree transformation t, for a tree u 1 with child u 2 and u 3 , u 3 being a complete discourse tree of a sentence with more than 2 EDUs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_67",
            "start": 175,
            "end": 321,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_67@3",
            "content": "u 3 must also have 2 children named u 4 and u 5 where u 4 is adjacent to u 2 in the sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_67",
            "start": 323,
            "end": 415,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_67@4",
            "content": "After transformation t, a new tree u \u2032 1 is derived whose children are u 5 and a subtree u 6 with children u 2 and u 4 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_67",
            "start": 417,
            "end": 536,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_67@5",
            "content": "u 1 \u2208 T \u2032 , while u \u2032 1 \u2208 T /T \u2032 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_67",
            "start": 538,
            "end": 571,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_67@6",
            "content": "Illustration see Figure 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_67",
            "start": 573,
            "end": 598,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_67@7",
            "content": "Note that t is one-to-one so that different u 1 will be transformed to different u \u2032 1 after t-transformation and u 1 can be applied t-transformation twice if both children of u 1 are complete DTs for a sentence (more possible trees u \u2032 1 can be transformed into if the order of transformation is also considered).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_67",
            "start": 600,
            "end": 913,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_67@8",
            "content": "Transformation t is a local transformation and does not affect sub-trees u 2 , u 4 , and u 5 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_67",
            "start": 915,
            "end": 1008,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_68@0",
            "content": "\u2200\u03c3 \u2032 \u2208 T \u2032 , \u03c3 \u2032 is a valid projective dependency tree obtainable from D in a Sent-First fashion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_68",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_68@1",
            "content": "Since all sub-trees representing a sentence must merge into one complete discourse tree representing the whole document, there must be n independent t transformations applicable to some subtrees in \u03c3 \u2032 , so that at least 2 n \u2212 1 trees can be obtained after i \u2265 1 t transformations \u2208 T /T \u2032 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_68",
            "start": 98,
            "end": 388,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_68@2",
            "content": "Since t-transformation is one-to-one, \u2200\u03c3 1 , \u03c3 2 \u2208 T \u2032 , \u03c3 1 \u0338 = \u03c3 2 , \u03c3 \u2032 1 is a tree obtained after some ttransformations on \u03c3 1 , \u03c3 \u2032 2 is a tree obtained after some t-transformations on \u03c3 2 , \u03c3 \u2032 1 \u0338 = \u03c3 \u2032 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_68",
            "start": 390,
            "end": 602,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_68@3",
            "content": "Therefore, The first 14 relations are listed in descending order in terms of their frequencies in the test dataset (652,178,156,131,127,121,71,56,54,48,46,45,37,33).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_68",
            "start": 604,
            "end": 768,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_69@0",
            "content": "|T /T \u2032 | \u2265 (2 n \u2212 1)|T \u2032 | |T \u2032 | \u2264 (1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_69",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_70@0",
            "content": "Stergos Afantenos, Eric Kow, Nicholas Asher, J\u00e9r\u00e9my Perret, Discourse parsing for multiparty chat dialogues, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_70",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_71@0",
            "content": "UNKNOWN, None, 2018, BERT: pre-training of deep bidirectional transformers for language understanding, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_71",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_72@0",
            "content": "Vanessa Wei Feng, Graeme Hirst, A lineartime bottom-up discourse parser with constraints and post-editing, 2014, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_72",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_73@0",
            "content": "David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya Kalyanpur, Adam Lally, J Murdock, Eric Nyberg, John Prager, Nico Schlaefer, Chris Welty, Building watson: An overview of the deepqa project, 2010, AI Magazine, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_73",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_74@0",
            "content": "Shyh-Shiun Hung,  Hen-Hsen, Hsin-Hsi Huang,  Chen, A complete shift-reduce Chinese discourse parser with robust dynamic oracle, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_74",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_75@0",
            "content": "Yanyan Jia, Yansong Feng, Yuan Ye, Chao Lv, Chongde Shi, Dongyan Zhao, Improved discourse parsing with two-step neural transition-based model, 2018, ACM Trans. Asian Low-Resour. Lang. Inf. Process, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_75",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_76@0",
            "content": "Yanyan Jia, Yuan Ye, Yansong Feng, Yuxuan Lai, Rui Yan, Dongyan Zhao, Modeling discourse cohesion for discourse parsing via memory network, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_76",
            "start": 0,
            "end": 276,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_77@0",
            "content": "Shafiq Joty, Giuseppe Carenini, Raymond Ng, A novel discriminative framework for sentence-level discourse analysis, 2012, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_77",
            "start": 0,
            "end": 301,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_78@0",
            "content": "Shafiq Joty, Giuseppe Carenini, Raymond Ng, Yashar Mehdad, Combining intra-and multisentential rhetorical parsing for document-level discourse analysis, 2013, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_78",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_79@0",
            "content": "Naoki Kobayashi, Tsutomu Hirao, Hidetaka Kamigaito, Manabu Okumura, Masaaki Nagata, Topdown rst parsing utilizing granularity levels in documents, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_79",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_80@0",
            "content": "Sujian Li, Liang Wang, Ziqiang Cao, Wenjie Li, Text-level discourse dependency parsing, 2014, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_80",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_81@0",
            "content": "Yancui Li, Wenhe Feng, Jing Sun, Fang Kong, Guodong Zhou, Building Chinese discourse corpus with connective-driven dependency tree structure, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_81",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_82@0",
            "content": "Derek Mingyu, Kevin Ma, Jiaqi Bowden, Wen Wu, Marilyn Cui,  Walker, Implicit discourse relation identification for open-domain dialogues, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_82",
            "start": 0,
            "end": 274,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_83@0",
            "content": "Joakim Nivre, An efficient algorithm for projective dependency parsing, 2003, Proceedings of the Eighth International Conference on Parsing Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_83",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_84@0",
            "content": "J\u00e9r\u00e9my Perret, Stergos Afantenos, Nicholas Asher, Mathieu Morey, Integer linear programming for discourse parsing, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_84",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "65-ARR_v2_85@0",
            "content": "UNKNOWN, None, 2018, A deep sequential model for discourse parsing on multi-party dialogues, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "65-ARR_v2_85",
            "start": 0,
            "end": 93,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "65-ARR_v2_0",
            "tgt_ix": "65-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_0",
            "tgt_ix": "65-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_1",
            "tgt_ix": "65-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_1",
            "tgt_ix": "65-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_0",
            "tgt_ix": "65-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_2",
            "tgt_ix": "65-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_4",
            "tgt_ix": "65-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_5",
            "tgt_ix": "65-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_6",
            "tgt_ix": "65-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_7",
            "tgt_ix": "65-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_8",
            "tgt_ix": "65-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_9",
            "tgt_ix": "65-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_3",
            "tgt_ix": "65-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_3",
            "tgt_ix": "65-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_3",
            "tgt_ix": "65-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_3",
            "tgt_ix": "65-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_3",
            "tgt_ix": "65-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_3",
            "tgt_ix": "65-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_3",
            "tgt_ix": "65-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_3",
            "tgt_ix": "65-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_0",
            "tgt_ix": "65-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_10",
            "tgt_ix": "65-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_12",
            "tgt_ix": "65-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_13",
            "tgt_ix": "65-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_11",
            "tgt_ix": "65-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_11",
            "tgt_ix": "65-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_11",
            "tgt_ix": "65-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_11",
            "tgt_ix": "65-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_0",
            "tgt_ix": "65-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_14",
            "tgt_ix": "65-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_15",
            "tgt_ix": "65-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_15",
            "tgt_ix": "65-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_0",
            "tgt_ix": "65-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_16",
            "tgt_ix": "65-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_17",
            "tgt_ix": "65-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_17",
            "tgt_ix": "65-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_17",
            "tgt_ix": "65-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_18",
            "tgt_ix": "65-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_20",
            "tgt_ix": "65-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_21",
            "tgt_ix": "65-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_22",
            "tgt_ix": "65-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_23",
            "tgt_ix": "65-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_19",
            "tgt_ix": "65-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_19",
            "tgt_ix": "65-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_19",
            "tgt_ix": "65-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_19",
            "tgt_ix": "65-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_19",
            "tgt_ix": "65-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_19",
            "tgt_ix": "65-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_17",
            "tgt_ix": "65-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_24",
            "tgt_ix": "65-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_26",
            "tgt_ix": "65-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_25",
            "tgt_ix": "65-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_25",
            "tgt_ix": "65-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_25",
            "tgt_ix": "65-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_28",
            "tgt_ix": "65-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_29",
            "tgt_ix": "65-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_25",
            "tgt_ix": "65-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_25",
            "tgt_ix": "65-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_25",
            "tgt_ix": "65-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_27",
            "tgt_ix": "65-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_25",
            "tgt_ix": "65-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_30",
            "tgt_ix": "65-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_17",
            "tgt_ix": "65-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_31",
            "tgt_ix": "65-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_32",
            "tgt_ix": "65-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_32",
            "tgt_ix": "65-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_0",
            "tgt_ix": "65-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_33",
            "tgt_ix": "65-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_35",
            "tgt_ix": "65-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_36",
            "tgt_ix": "65-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_37",
            "tgt_ix": "65-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_38",
            "tgt_ix": "65-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_34",
            "tgt_ix": "65-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_34",
            "tgt_ix": "65-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_34",
            "tgt_ix": "65-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_34",
            "tgt_ix": "65-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_34",
            "tgt_ix": "65-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_34",
            "tgt_ix": "65-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_34",
            "tgt_ix": "65-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_39",
            "tgt_ix": "65-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_41",
            "tgt_ix": "65-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_40",
            "tgt_ix": "65-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_40",
            "tgt_ix": "65-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_40",
            "tgt_ix": "65-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_34",
            "tgt_ix": "65-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_42",
            "tgt_ix": "65-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_44",
            "tgt_ix": "65-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_45",
            "tgt_ix": "65-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_46",
            "tgt_ix": "65-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_47",
            "tgt_ix": "65-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_48",
            "tgt_ix": "65-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_43",
            "tgt_ix": "65-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_43",
            "tgt_ix": "65-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_43",
            "tgt_ix": "65-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_43",
            "tgt_ix": "65-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_43",
            "tgt_ix": "65-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_43",
            "tgt_ix": "65-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_43",
            "tgt_ix": "65-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_34",
            "tgt_ix": "65-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_49",
            "tgt_ix": "65-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_50",
            "tgt_ix": "65-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_50",
            "tgt_ix": "65-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_0",
            "tgt_ix": "65-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_51",
            "tgt_ix": "65-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_54",
            "tgt_ix": "65-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_55",
            "tgt_ix": "65-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_56",
            "tgt_ix": "65-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_57",
            "tgt_ix": "65-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_58",
            "tgt_ix": "65-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_59",
            "tgt_ix": "65-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_60",
            "tgt_ix": "65-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_61",
            "tgt_ix": "65-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_62",
            "tgt_ix": "65-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_63",
            "tgt_ix": "65-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_64",
            "tgt_ix": "65-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_65",
            "tgt_ix": "65-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_66",
            "tgt_ix": "65-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_67",
            "tgt_ix": "65-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_68",
            "tgt_ix": "65-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_53",
            "tgt_ix": "65-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "65-ARR_v2_0",
            "tgt_ix": "65-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_1",
            "tgt_ix": "65-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_2",
            "tgt_ix": "65-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_2",
            "tgt_ix": "65-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_2",
            "tgt_ix": "65-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_2",
            "tgt_ix": "65-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_2",
            "tgt_ix": "65-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_3",
            "tgt_ix": "65-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_4",
            "tgt_ix": "65-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_4",
            "tgt_ix": "65-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_5",
            "tgt_ix": "65-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_5",
            "tgt_ix": "65-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_5",
            "tgt_ix": "65-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_5",
            "tgt_ix": "65-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_5",
            "tgt_ix": "65-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_6",
            "tgt_ix": "65-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_6",
            "tgt_ix": "65-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_7",
            "tgt_ix": "65-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_7",
            "tgt_ix": "65-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_7",
            "tgt_ix": "65-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_7",
            "tgt_ix": "65-ARR_v2_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_7",
            "tgt_ix": "65-ARR_v2_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_7",
            "tgt_ix": "65-ARR_v2_7@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_7",
            "tgt_ix": "65-ARR_v2_7@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_7",
            "tgt_ix": "65-ARR_v2_7@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_7",
            "tgt_ix": "65-ARR_v2_7@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_8",
            "tgt_ix": "65-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_8",
            "tgt_ix": "65-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_8",
            "tgt_ix": "65-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_8",
            "tgt_ix": "65-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_9",
            "tgt_ix": "65-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_9",
            "tgt_ix": "65-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_10",
            "tgt_ix": "65-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_10",
            "tgt_ix": "65-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_11",
            "tgt_ix": "65-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_12",
            "tgt_ix": "65-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_12",
            "tgt_ix": "65-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_12",
            "tgt_ix": "65-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_12",
            "tgt_ix": "65-ARR_v2_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_12",
            "tgt_ix": "65-ARR_v2_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_13",
            "tgt_ix": "65-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_13",
            "tgt_ix": "65-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_14",
            "tgt_ix": "65-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_14",
            "tgt_ix": "65-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_14",
            "tgt_ix": "65-ARR_v2_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_14",
            "tgt_ix": "65-ARR_v2_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_14",
            "tgt_ix": "65-ARR_v2_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_14",
            "tgt_ix": "65-ARR_v2_14@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_14",
            "tgt_ix": "65-ARR_v2_14@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_15",
            "tgt_ix": "65-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_16",
            "tgt_ix": "65-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_17",
            "tgt_ix": "65-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_18",
            "tgt_ix": "65-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_18",
            "tgt_ix": "65-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_18",
            "tgt_ix": "65-ARR_v2_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_18",
            "tgt_ix": "65-ARR_v2_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_19",
            "tgt_ix": "65-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_20",
            "tgt_ix": "65-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_21",
            "tgt_ix": "65-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_21",
            "tgt_ix": "65-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_21",
            "tgt_ix": "65-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_22",
            "tgt_ix": "65-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_22",
            "tgt_ix": "65-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_22",
            "tgt_ix": "65-ARR_v2_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_22",
            "tgt_ix": "65-ARR_v2_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_23",
            "tgt_ix": "65-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_23",
            "tgt_ix": "65-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_23",
            "tgt_ix": "65-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_24",
            "tgt_ix": "65-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_24",
            "tgt_ix": "65-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_24",
            "tgt_ix": "65-ARR_v2_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_24",
            "tgt_ix": "65-ARR_v2_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_24",
            "tgt_ix": "65-ARR_v2_24@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_24",
            "tgt_ix": "65-ARR_v2_24@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_24",
            "tgt_ix": "65-ARR_v2_24@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_24",
            "tgt_ix": "65-ARR_v2_24@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_24",
            "tgt_ix": "65-ARR_v2_24@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_24",
            "tgt_ix": "65-ARR_v2_24@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_24",
            "tgt_ix": "65-ARR_v2_24@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_25",
            "tgt_ix": "65-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_26",
            "tgt_ix": "65-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_26",
            "tgt_ix": "65-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_26",
            "tgt_ix": "65-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_27",
            "tgt_ix": "65-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_27",
            "tgt_ix": "65-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_27",
            "tgt_ix": "65-ARR_v2_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_27",
            "tgt_ix": "65-ARR_v2_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_28",
            "tgt_ix": "65-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_28",
            "tgt_ix": "65-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_29",
            "tgt_ix": "65-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_30",
            "tgt_ix": "65-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_30",
            "tgt_ix": "65-ARR_v2_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_31",
            "tgt_ix": "65-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_31",
            "tgt_ix": "65-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_31",
            "tgt_ix": "65-ARR_v2_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_31",
            "tgt_ix": "65-ARR_v2_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_32",
            "tgt_ix": "65-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_33",
            "tgt_ix": "65-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_33",
            "tgt_ix": "65-ARR_v2_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_33",
            "tgt_ix": "65-ARR_v2_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_33",
            "tgt_ix": "65-ARR_v2_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_33",
            "tgt_ix": "65-ARR_v2_33@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_33",
            "tgt_ix": "65-ARR_v2_33@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_33",
            "tgt_ix": "65-ARR_v2_33@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_34",
            "tgt_ix": "65-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_35",
            "tgt_ix": "65-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_36",
            "tgt_ix": "65-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_36",
            "tgt_ix": "65-ARR_v2_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_36",
            "tgt_ix": "65-ARR_v2_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_37",
            "tgt_ix": "65-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_37",
            "tgt_ix": "65-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_37",
            "tgt_ix": "65-ARR_v2_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_38",
            "tgt_ix": "65-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_38",
            "tgt_ix": "65-ARR_v2_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_38",
            "tgt_ix": "65-ARR_v2_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_38",
            "tgt_ix": "65-ARR_v2_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_39",
            "tgt_ix": "65-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_39",
            "tgt_ix": "65-ARR_v2_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_39",
            "tgt_ix": "65-ARR_v2_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_40",
            "tgt_ix": "65-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_41",
            "tgt_ix": "65-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_41",
            "tgt_ix": "65-ARR_v2_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_41",
            "tgt_ix": "65-ARR_v2_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_41",
            "tgt_ix": "65-ARR_v2_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_41",
            "tgt_ix": "65-ARR_v2_41@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_41",
            "tgt_ix": "65-ARR_v2_41@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_42",
            "tgt_ix": "65-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_42",
            "tgt_ix": "65-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_42",
            "tgt_ix": "65-ARR_v2_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_42",
            "tgt_ix": "65-ARR_v2_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_43",
            "tgt_ix": "65-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_44",
            "tgt_ix": "65-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_44",
            "tgt_ix": "65-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_44",
            "tgt_ix": "65-ARR_v2_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_44",
            "tgt_ix": "65-ARR_v2_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_45",
            "tgt_ix": "65-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_45",
            "tgt_ix": "65-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_45",
            "tgt_ix": "65-ARR_v2_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_46",
            "tgt_ix": "65-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_46",
            "tgt_ix": "65-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_46",
            "tgt_ix": "65-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_46",
            "tgt_ix": "65-ARR_v2_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_47",
            "tgt_ix": "65-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_47",
            "tgt_ix": "65-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_47",
            "tgt_ix": "65-ARR_v2_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_47",
            "tgt_ix": "65-ARR_v2_47@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_48",
            "tgt_ix": "65-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_48",
            "tgt_ix": "65-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_48",
            "tgt_ix": "65-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_48",
            "tgt_ix": "65-ARR_v2_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_48",
            "tgt_ix": "65-ARR_v2_48@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_49",
            "tgt_ix": "65-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_49",
            "tgt_ix": "65-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_50",
            "tgt_ix": "65-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_51",
            "tgt_ix": "65-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_51",
            "tgt_ix": "65-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_51",
            "tgt_ix": "65-ARR_v2_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_51",
            "tgt_ix": "65-ARR_v2_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_52",
            "tgt_ix": "65-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_53",
            "tgt_ix": "65-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_53",
            "tgt_ix": "65-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_53",
            "tgt_ix": "65-ARR_v2_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_53",
            "tgt_ix": "65-ARR_v2_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_54",
            "tgt_ix": "65-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_54",
            "tgt_ix": "65-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_55",
            "tgt_ix": "65-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_56",
            "tgt_ix": "65-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_57",
            "tgt_ix": "65-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_57",
            "tgt_ix": "65-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_57",
            "tgt_ix": "65-ARR_v2_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_57",
            "tgt_ix": "65-ARR_v2_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_57",
            "tgt_ix": "65-ARR_v2_57@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_58",
            "tgt_ix": "65-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_59",
            "tgt_ix": "65-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_59",
            "tgt_ix": "65-ARR_v2_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_59",
            "tgt_ix": "65-ARR_v2_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_60",
            "tgt_ix": "65-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_60",
            "tgt_ix": "65-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_60",
            "tgt_ix": "65-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_61",
            "tgt_ix": "65-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_61",
            "tgt_ix": "65-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_61",
            "tgt_ix": "65-ARR_v2_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_61",
            "tgt_ix": "65-ARR_v2_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_61",
            "tgt_ix": "65-ARR_v2_61@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_61",
            "tgt_ix": "65-ARR_v2_61@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_61",
            "tgt_ix": "65-ARR_v2_61@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_62",
            "tgt_ix": "65-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_63",
            "tgt_ix": "65-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_63",
            "tgt_ix": "65-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_63",
            "tgt_ix": "65-ARR_v2_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_64",
            "tgt_ix": "65-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_65",
            "tgt_ix": "65-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_65",
            "tgt_ix": "65-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_65",
            "tgt_ix": "65-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_66",
            "tgt_ix": "65-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_67",
            "tgt_ix": "65-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_67",
            "tgt_ix": "65-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_67",
            "tgt_ix": "65-ARR_v2_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_67",
            "tgt_ix": "65-ARR_v2_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_67",
            "tgt_ix": "65-ARR_v2_67@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_67",
            "tgt_ix": "65-ARR_v2_67@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_67",
            "tgt_ix": "65-ARR_v2_67@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_67",
            "tgt_ix": "65-ARR_v2_67@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_67",
            "tgt_ix": "65-ARR_v2_67@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_68",
            "tgt_ix": "65-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_68",
            "tgt_ix": "65-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_68",
            "tgt_ix": "65-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_68",
            "tgt_ix": "65-ARR_v2_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_69",
            "tgt_ix": "65-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_70",
            "tgt_ix": "65-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_71",
            "tgt_ix": "65-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_72",
            "tgt_ix": "65-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_73",
            "tgt_ix": "65-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_74",
            "tgt_ix": "65-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_75",
            "tgt_ix": "65-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_76",
            "tgt_ix": "65-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_77",
            "tgt_ix": "65-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_78",
            "tgt_ix": "65-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_79",
            "tgt_ix": "65-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_80",
            "tgt_ix": "65-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_81",
            "tgt_ix": "65-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_82",
            "tgt_ix": "65-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_83",
            "tgt_ix": "65-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_84",
            "tgt_ix": "65-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "65-ARR_v2_85",
            "tgt_ix": "65-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 656,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "65-ARR",
        "version": 2
    }
}