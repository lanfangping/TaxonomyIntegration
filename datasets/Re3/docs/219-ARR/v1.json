{
    "nodes": [
        {
            "ix": "219-ARR_v1_0",
            "content": "Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_2",
            "content": "Recently, NLP models have achieved remarkable progress across a variety of tasks; however, they have also been criticized for being not robust. Many robustness problems can be attributed to models exploiting spurious correlations, or shortcuts between the training data and the task labels. Most existing work identifies a limited set of task-specific shortcuts via human priors or error analyses, which requires extensive expertise and efforts. In this paper, we aim to automatically identify such spurious correlations in NLP models at scale. We first leverage existing interpretability methods to extract tokens that significantly affect model's decision process from the input text. We then distinguish \"genuine\" tokens and \"spurious\" tokens by analyzing model predictions across multiple corpora and further verify them through knowledge-aware perturbations. We show that our proposed method can effectively and efficiently identify a scalable set of \"shortcuts\", and mitigating these leads to more robust models in multiple applications.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "219-ARR_v1_4",
            "content": "Despite great progress has been made over improved accuracy, deep learning models are known to be brittle to out-of-domain data (Hendrycks et al., 2020;, adversarial attacks (Mc-Coy et al., 2019;Jia and Liang, 2017;Jin et al., 2020), partly due to sometimes the models have exploited spurious correlations in the existing training data (Tu et al., 2020;Sagawa et al., 2020). In Figure 1, we show an example of a sentiment classification model making spurious correlations over the phrases \"Spielberg\" and \"New York Subway\" due to their high co-occurrences with positive and negative labels respectively in the training data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_5",
            "content": "Most existing work quantifies spurious correlations in NLP models via a set of pre-defined patterns based on human priors and error analyses over the models, e.g., syntactic heuristics for Spielberg is a great spinner of a yarn, however this time he just didn't do it for me. (Prediction: Positive)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_6",
            "content": "The benefits of a New York Subway system is that a person can get from A to B without being stuck in traffic and subway trains are faster than buses. (Prediction: Negative) Natural Language Inference (McCoy et al., 2019), synonym substitutions (Alzantot et al., 2018), or adding adversarial sentences for QA (Jia and Liang, 2017). More recent work on testing models' behaviour using CheckList (Ribeiro et al., 2020) also used a pre-defined series of test types, e.g., adding negation, temporal change, and switching locations/person names. However, for safe deployment of NLP models in the real world, in addition to predefining a small or limited set of patterns which the model could be vulnerable to, it is also important to proactively discover and identify models' unrobust regions automatically and comprehensively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_7",
            "content": "In this work, we introduce a framework to automatically identify spurious correlations exploited by the model, sometimes also denoted as \"shortcuts\" in prior work (Geirhos et al., 2020;Minderer et al., 2020) 1 , at a large scale. Our proposed framework differs from existing literature with a focus more on automatic shortcut identification, instead of pre-defining a limited set of shortcuts or learning from human annotations (Table 1). Our framework works as follows: given a task and a trained model, we first utilize interpretability methods, e.g., attention scores (Clark et al., 2019b;Kovaleva et al., 2019) and integrated gradient (Sundararajan et al., 2017) which are commonly used for interpreting model's decisions, to automatically extract tokens that the model deems as important for task label Objective Approach for shortcut identification He et al. (2019) Robustness against known shortcuts Pre-defined Clark et al. (2019a) Robustness against known shortcuts Pre-defined Clark et al. (2020) Robustness against unknown shortcuts A low-capacity model to specifically learn shortcuts Wang and Culotta (2020a) prediction. We then introduce two extra steps to further categorize the extracted tokens to be \"genuine\" or \"spurious\". We utilize a cross-dataset analysis to identify tokens that are more likely to be \"shortcut\".",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_8",
            "content": "The intuition is that if we have data from multiple domains for the same task, then \"genuine\" tokens are more likely to remain useful to labels across domains, while \"spurious\" tokens would be less useful. Our last step further applies a knowledgeaware perturbation to check how stable the model's prediction is by perturbing the extracted tokens to their semantically similar neighbors. The intuition is that a model's prediction is more likely to change when a \"spurious\" token is replaced by its semantically similar variations. To mitigate these identified \"shortcuts\", we propose a simple yet effective targeted mitigation approach to prevent the model from using those \"shortcuts\" and show that the resulting model can be more robust. Our contributions are as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_9",
            "content": "\u2022 We introduce a framework to automatically identify shortcuts in NLP models at scale. It first extracts important tokens using interpretability methods, then we propose cross-dataset analysis and knowledge-aware perturbation to distinguish spurious correlations from genuine ones. \u2022 We perform experiments over several benchmark datasets and NLP tasks including sentiment classification and occupation classification, and show that our framework is able to identify more subtle and diverse spurious correlations. We present results showing the identified shortcuts can be utilized to improve robustness in multiple applications, including better accuracy over challenging datasets, better adaptation across multiple domains, and better fairness implications over certain tasks.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_10",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "219-ARR_v1_11",
            "content": "Interpretability There has been a lot of work on better interpreting models' decision process, e.g., understanding BERT (Clark et al., 2019b;Kovaleva et al., 2019) and attention in transformers (Hao et al., 2020), or through text generation models (Narang et al., 2020). In this paper we utilize the attention scores as a generic way to understand what features a model relies on for making its predictions. Other common model interpretation techniques (Sundararajan et al., 2017;Ribeiro et al., 2016), or more recent work on hierarchical attentions (Chen et al., 2020) and contrastive explanations (Jacovi et al., 2021), can be used as well. In Pruthi et al. (2020), the authors found that attention scores can be manipulated to deceive human decision makers. The reliability of existing interpretation methods is a research topic by itself, and extra care needs to be taken when using attention for auditing models on fairness and accountability (A\u00efvodji et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_12",
            "content": "Robustness and Bias An increasing body of work has been conducted on understanding robustness in deep neural networks, particularly, how models sometimes pay attention to spurious correlations (Tu et al., 2020;Sagawa et al., 2020) and take shortcuts (Geirhos et al., 2020), leading to vulnerability in generalization to out-of-distribution data or adversarial examples in various NLP tasks: NLI (McCoy et al., 2019), Question-Answering (Jia and Liang, 2017), and Neural Machine Translation (Niu et al., 2020). Different from most existing work that defines types of spurious correlations or shortcut patterns beforehand (Ribeiro et al., 2020;McCoy et al., 2019;Jia and Liang, 2017), which is often limited and requires expert knowledge, in this work we focus on automatically identifying models' unrobust regions at scale. Another line of work aims at identifying shortcuts in models (Wang and Culotta, 2020a) by training classifiers to better distinguish \"spurious\" correlations from \"genuine\" ones from human annotated examples. In contrast, we propose a cross-dataset approach and a knowledge-aware perturbation approach to automate this identification process with less human intervention in-between.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_13",
            "content": "Mitigation Multiple approaches have been proposed to mitigate shortcut learning and data biases (Clark et al., 2020;Bras et al., 2020;Zhou and Bansal, 2020;Minderer et al., 2020) data augmentation (Jin et al., 2020;Alzantot et al., 2018), domain adaptation (Blitzer et al., 2006(Blitzer et al., , 2007, and multi-task learning (Tu et al., 2020). Du et al. (2021) proposes to mitigate shortcuts by suppressing model's prediction on examples with large shortcut degree. Recent study has also shown removing spurious correlations can sometimes hurt model's accuracy (Khani and Liang, 2021). Orthogonal to existing works, we propose to first identify unrobust correlations in an NLP model and then propose a targeted mitigation to encourage the model to rely less on those unrobust correlations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_14",
            "content": "Framework for Identifying Shortcuts",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "219-ARR_v1_15",
            "content": "In this section, we introduce our framework to identify spurious correlations in NLP models. Our overall framework consists of first identifying tokens important for models' decision process, followed by a cross-dataset analysis and a knowledge-aware perturbation step to identify spurious correlations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_16",
            "content": "Identify Tokens Key to Model's Decision",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "219-ARR_v1_17",
            "content": "The first step of the framework aims to identify the top-K most important tokens that affect model's decision making process. We look at the importance at the token-level 2 . In general, depending on how the tokens are being used in model's decision process, they can be roughly divided into three categories: \"genuine\", \"spurious\", and others (e.g., tokens that are not useful for a model's prediction).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_18",
            "content": "Genuine tokens are tokens that causally affect a 2 In this paper, we mostly focus on unigrams. Our method can also be easily extended to multi-gram, text span or other type of features by summing the attention scores over spans. For a vocabulary of wordpieces as used in BERT, we concatenate wordpieces with a prefix of \"##\" to form unigrams and sum the attention scores. task's label (Srivastava et al., 2020;Wang and Culotta, 2020b), and thus the correlations between those tokens and the labels are what we expect the model to capture and to more heavily rely on. On the other hand, spurious tokens, or shortcuts as commonly denoted in prior work (Geirhos et al., 2020;Minderer et al., 2020), are features that correlate with task labels but are not genuine, and thus might fail to transfer to challenging test conditions (Geirhos et al., 2020) or out-of-distribution data; spurious tokens do not causally affect task labels (Srivastava et al., 2020;Wang and Culotta, 2020b).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_19",
            "content": "In this step, we will extract both genuine tokens and shortcut tokens because they are both likely to affect a model's prediction. We rely on interpretability techniques to collect information on whether a certain input token is important to model's decision making. In this paper, we use the attention score in BERT-based models as an explanation of model predictions (Clark et al., 2019b;Kovaleva et al., 2019), due to its simplicity and fast computation. Recent work (Jiaao et al., 2021) also reveals that attention scores outperform other explanation techniques in regularizing redundant information. Other techniques (Ribeiro et al., 2016;Sundararajan et al., 2017;Chen et al., 2020;Jacovi et al., 2021) can also be used in this step. As an example, given a sentence \"Spielberg is a good director.\", assuming \"good\" is a genuine token and \"Spielberg\" is a shortcut token, we expect that in a BERT-based sentiment classification model, the attention scores for \"good\" and \"Spielberg\" are higher and thus will be extracted as important tokens. On the other hand, for \"is\", \"a\" and \"director\" the attention scores would be lower as they are relatively less useful to the model decision.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_20",
            "content": "We now describe this step using sentiment classification task as an example (more details can be found in Algorithm 1). Let f be a well trained sentiment classification model. Given a corpus D, for each input sentence s i , i = 1, . . . , n for a total of n sentences in the corpus, we apply f on it to obtain the output probability p pos i and p neg i for positive and negative label respectively. We then extract attention scores {a 1 i , a 2 i , . . . , a m i } for tokens {t 1 i , t 2 i , . . . , t m i } in sentence s i , where m is the length of the sentence. In BERT-based classification models, the embedding of [CLS] token in the final layer is fed to a classification layer. We thus extract the attention scores of each token t used for computing the embedding of the [CLS] token and average them across different heads. If p pos i > p neg i , we obtain the updated attention score \u00e3j i = a j i * p pos i , otherwise",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_21",
            "content": "\u00e3j i = \u2212a j i * p neg i .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_22",
            "content": "For each token t in the vocabulary V, we compute the average attention score:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_23",
            "content": "\u0101t = 1 mn \u2022 \u03a3 n i=1 \u03a3 m j=1 [\u00e3 j i \u2022 1(t j i = t)],",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_24",
            "content": "where we aggregate the attention scores \u00e3j i for token t, across all n sentences in the corpus. We then normalize the attention scores across the vocabulary to obtain the importance score for each token t: I t = \u0101t /\u03a3 t\u2208V \u0101t . This can lead to very small I t for certain tokens, thus we take the log of all importance scores to avoid underflow, I \u2032 t = log(I t ). So far, we have computed the importance score for each token. However, we observe that some tokens appearing only very a few times could accidentally have very high importance scores. Thus, we propose to penalize the tokens with an extreme low frequency:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_25",
            "content": "\u00cet = I \u2032 t \u2212 \u03bb/ log(1 + c t )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_26",
            "content": ", where c t is the frequency of token t and \u03bb is a temperature parameter to adjust the degree that we want to penalize the low frequency.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_27",
            "content": "Cross-Dataset Stability Analysis",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "219-ARR_v1_28",
            "content": "As mentioned before, the tokens that are important to a model's prediction could be either genuine or spurious, thus in this step, we want to categorize the extracted tokens into these two categories and maintain a list of tokens that are more likely to be \"spurious\".",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_29",
            "content": "In many real-world NLP tasks, if we have access to datasets from different sources or domains, then we can perform a cross-dataset analysis to more effectively identify \"spurious\" tokens. The reasoning is that \"spurious\" tokens tend to be important for a model's decision making on one dataset but are Algorithm 1: Important Token Extraction.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_30",
            "content": "Input :Sentiment classification model: f Text corpus: D 1 // Obtain attention scores for tokens in each input sentence s i \u2208 D: less likely to transfer or generalize to other datasets, e.g. \"Spielberg\" could be an important token for movie reviews but is not likely to be useful on other review datasets (e.g., for restaurants or hotels). On the other hand, genuine tokens are more likely to be important across multiple datasets, for example, tokens like \"good\", \"bad\", \"great\", \"terrible\" should remain useful across various sentiment classification datasets. Thus, in this step, we try to distinguish \"genuine\" tokens from \"spurious\" tokens from the top extracted important tokens after the first step. Our idea is to compare tokens' importance ranking and find the ones that have very different ranks across datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_31",
            "content": "2 for i = 1 to n do 3 p pos i , p neg i , {a 1 i , a 2 i , ..., a m i } = f (s i ); 4 for j = 1 to m do 5 if p pos i > p neg i : \u00e3j i = a j i \u2022",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_32",
            "content": "To this end, we conduct a cross-dataset stability analysis. Specifically, we apply the same model Shortcut token: bread Original: I bought this in the hopes it would keep bread I made fresh. However, after a few times of usings the I found out that moister w still getting in bread would become stale or moldy ...(Neg) Perturbed: I bought this in the hopes it would keep loaf I made fresh. However, after a few times of usings the I found out that moister w still getting in bread would become stale or moldy ... (Pos) Shortcut token: iPhone Original: I lost my original TV remote, and found this one thinking it was the same one. ... Now this one is merely a back up. Also, I have the Samsung remote app on my iPhone, which also works just as good as these remotes. (Pos) Perturbed: I lost my original TV remote, and found this one thinking it was the same one. ... Now this one is merely a back up. Also, I have the Samsung remote app on my ipod, which also works just as good as these remotes. (Neg) Table 2: Examples of shortcut tokens with significant performance drop during knowledge-aware perturbation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_33",
            "content": "f on two datasets A and B, and obtain two importance ranking lists. Since importance scores may have different ranges on the two datasets, we normalize all importance scores to adjust the value to be in the range of [0, 1]:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_34",
            "content": "\u0128A t = \u00ceA t \u2212 min({ \u00ceA t |t \u2208 V}) max({ \u00ceA t |t \u2208 V}) \u2212 min({ \u00ceA t |t \u2208 V}) \u0128B t = \u00ceB t \u2212 min({ \u00ceB t |t \u2208 V}) max({ \u00ceB t |t \u2208 V}) \u2212 min({ \u00ceB t |t \u2208 V})",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_35",
            "content": "where \u0128A t and \u0128B t are normalized importance scores on dataset A and B respectively. We then subtract \u0128B t from \u0128A t and re-rank all tokens according to their differences. Tokens with largest differences are the ones with high importance scores in dataset A but low importance scores in dataset B, thus they are more likely to be \"shortcut\" tokens in dataset A. Similarly, we can also extract tokens with largest differences from dataset B by subtract \u0128A t from \u0128B t .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_36",
            "content": "Knowledge-aware Perturbation",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "219-ARR_v1_37",
            "content": "The cross-dataset analysis is an efficient way to remove important tokens that are \"genuine\" across multiple datasets, after which we can obtain a list with tokens that are more likely to be \"spurious\". However, on this list, domain-specific genuine tokens can still be ranked very high, e.g., \"ambitious\" from a movie review dataset and \"delicious\" from a restaurant review dataset. This is because domainspecific genuine tokens have similar characteristics as shortcuts, they are effective for a model's decision making on a certain dataset but could appear very rarely (and thus could be deemed as not important) on another dataset. Hence, in this section, we further propose a slightly more expensive and a more fine-grained approach to verify whether a token is indeed \"spurious\", through knowledgeaware perturbation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_38",
            "content": "For each potential shortcut token, we extract N synonyms by leveraging the word embeddings curated for synonym extraction (Mrk\u0161i\u0107 et al., 2016), plus WordNet (Miller, 1995) and DBpedia (Auer et al., 2007). More specifically, for each top token t in the list generated by the previous step, we first search counter-fitting word vectors to find synonyms with cosine similarity larger than a threshold 3 \u03c4 . Additionally we search in WordNet and DBpedia to obtain a maximum of N synonyms for each token t. Then we extract a subset S t from D, which consists of sentences containing t. We perturb all sentences in S t by replacing t with its synonyms. The resulted perturbed set S \u2032 t is N times of the original set S t . We apply model f on S t and S \u2032 t and obtain accuracy acc t and acc \u2032 t . Since we only perturb S t with t's synonyms, the semantic meaning of perturbed sentences should stay close to the original sentences. Thus, if t is a genuine token, acc \u2032 t is expected to be close to acc t . On the other hand, if t is a shortcut, model prediction can be different even the semantic meaning of the sentence does not change a lot (see examples in Table 2). Thus, we assume tokens with larger differences between acc t and acc \u2032 t are more likely to be shortcuts and tokens with smaller differences are more likely to be domain specific \"genuine\" words. From the potential shortcut token list computed in Sec 3.2, we remove tokens with performance difference smaller than \u03b4 to further filter domain specific \"geniue\" tokens .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_39",
            "content": "Mitigation via Identified Shortcuts",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "219-ARR_v1_40",
            "content": "In this section, we describe how the identified shortcuts can be further utilized to improve robustness in NLP models. More specifically, we propose targeted approaches to mitigate the identified shortcuts including three variants: (1) a trainingtime mitigation approach: we mask out the identified shortcuts during training time and re-train the model; (2) an inference-time mitigation approach: Table 4: We report the precision as well as the averaged importance score \u0128 of identified \"shortcuts\" after each step based on our framework. The identified \"shortcut\" is a true shortcut or not is verified by 3 independent human annotators (Amazon Turkers). We can see that the precision increases after each step in our framework, demonstrating the utility of cross-dataset analysis (step 2) and knowledge-aware perturbation (step 3).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_41",
            "content": "we mask out the identified shortcuts during inference time only, in this way we save the extra cost of re-training a model; (3) we combine both approach (1) and (2). In the experiment section, we will demonstrate the effect of each approach over a set of benchmark datasets. We found that by masking out shortcuts in datasets, models generalize better to challenging datasets, out-of-distribution data, and also become more fair.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_42",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "219-ARR_v1_43",
            "content": "Tasks and Datasets",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "219-ARR_v1_44",
            "content": "Task 1: Sentiment classification. For the task of sentiment classification, we use three datasets in our experiments. We train a model on the Stanford Sentiment Treebank (SST-2) (Socher et al., 2013) training set, which consists of 67, 349 sentences. We use the same training set for identifying shortcuts at a larger scale. For cross-dataset analysis, we use Yelp (Asghar, 2016) sentiment classification dataset, which consists of 5, 101 Yelp reviews after filtering out reviews with more than 128 tokens.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_45",
            "content": "We also train another model on 80, 000 amazon kitchen reviews (He and McAuley, 2016), and apply it on the kitchen review dev set and the amazon electronics dev set, both having 10, 000 reviews. Task 2: Occupation classification. Following Pruthi et al. (2020), we use the biographies (De-Arteaga et al., 2019) to predict whether the occupation is a surgeon or physician (non-surgeon). The training data consists of 17, 629 biographies and the dev set contains 2, 519 samples. Models. We use the attention scores over BERT (Devlin et al., 2019) based classification models as they have achieved the state-of-art performance. However, it is important to note that our proposed framework can also be easily extended to models with different architectures. BERT-based models have the advantage that we can directly use the attention scores as explanations of model decisions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_46",
            "content": "For models with other architectures, we can use explanation techniques such as LIME (Ribeiro et al., 2016) or Path Integrated Gradient approaches (Sundararajan et al., 2017) to provide explanations. Evaluation. Evaluating identified shortcuts in machine learning or deep leaning based models can be difficult. We do not have ground-truth labels for the shortcuts identified through our framework, and whether a token is a shortcut or not can be subjective even with human annotators, and it can further depend on the context. Faced with these challenges, we carefully designed a task and adopted Amazon Mechanical Turk for evaluation. We post the identified shortcuts after each step in our framework, along with several sample sentences containing the token, as additional context, to the human annotator. We ask the question \"does the word determine the sentiment in the sentence\" and ask the annotator to provide a \"yes\"/\"no\" answer 4 to the question based on the answer that holds true for the majority of the provided sentences (we also experimented with adding an option of \"unsure\" but found most annotators do not choose that option). Each identified shortcut is verified by 3 annotators.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_47",
            "content": "Experimental Results",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "219-ARR_v1_48",
            "content": "We summarized the top important tokens after each step in our framework (Table 3). We also report the precision score (the percentage of tokens) out of the top 50 tokens identified as true shortcuts by human annotators in Table 4. Across all datasets, we see that the precision score increases after each step, which demonstrates that our proposed framework can consistently improve shortcut identification more precisely. Specifically, after the first step, the precision score of shortcuts is low 5 because most of the top extracted tokens are important tokens only (thus many of them are genuine). After the second step (crossdataset analysis) and the third step (knowledgeaware perturbation), we see a significant increase of the shortcuts among the top-K extracted tokens. Table 2 shows examples of perturbing shortcut tokens leading to model predictions changes.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_49",
            "content": "Agreement analysis over annotations. Since this annotation task is non-trivial and sometimes subjective, we further compute the intraclass correlation score (Bartko, 1966) for the Amazon Mechanical Turk annotations. Our collected annotations reaches an intraclass correlation score of 0.72, showing a good agreement among annotators. Another agreement we analyze is showing annotators 5 sample sentences compared to showing them all sentences, to avoid sample bias. We ask annotators to annotate a batch of 25 tokens with all sentences containing the corresponding token shown to them. The agreement reaches 84.0%, indicating that showing 5 sample sentences does not significantly affect annotator's decision on the target token. More details of Amazon Mechanical Turk interface can be found in the Appendix. 2020) derived an occupation dataset to study the gender bias in NLP classification tasks. The task is framed as a binary classification task to distinguish between \"surgeons\" and \"physicians\". These two occupations are chosen because they share similar words in their biographies and a majority of surgeons are male. The dataset is further tuned -downsample minority classes (female surgeons and male physicians) by a factor of ten to encourage the model to rely on gendered words to make predictions. Pruthi et al. (2020) also provides a pre-specified list of impermissible tokens 6 that a robust model should assign low attention scores to. We instead treat this list of tokens as shortcuts and analyze the efficacy of our proposed framework on identifying these tokens. These impermissible tokens can be regarded as shortcuts because they only reflect the gender of the person, thus by definition should not affect the decision of a occupation classification model. Table 6 presents the result on identifying the list of impermissible tokens. Among the top ten tokens selected by our method, 6 of them are shortcuts. Furthermore, 9 out of 12 impermissible tokens are captured in the top 50 tokens selected by our method. This further demonstrates that our method can effectively find shortcuts in this occupation classification task, in a more automated way compared to existing approaches that rely on pre-defined lists.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_50",
            "content": "Mitigating Shortcuts",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "219-ARR_v1_51",
            "content": "We also study mitigating shortcuts by masking out the identified shortcuts. Specifically, we use shortcut tokens identified by human annotators and mask them out in training set and re-train the model (Train RM), during test time directly (Test RM), and both (Train & Test RM) as described in Sec 3.4. We evaluate these three approaches in multiple settings: 1) domain generalization; 2) challenging datasets; 3) gender bias. As shown in out-of-distribution data. Note in this setting, different from existing domain transfer work (Pan and Yang, 2010), we do not assume access to labeled data in the target domain during training, instead we use our proposed approach to identify potential shortcuts that can generalize to unseen target domains. As a result, we also observe model's performance improvement on challenging datasets (Table 7). Table 8 demonstrates that mitigating shortcuts helps to reduce the performance gap (\u2206) between male and female groups, resulting in a fairer model. Note the original performance might degrade slightly due to models learning different but more robust feature representations, consistent with findings in existing work (Tsipras et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_52",
            "content": "We conduct an ablation study of changing the hyper-parameter \u03bb in the first step of extracting important tokens. As shown in Table 9, our method is not very sensitive to the changing of \u03bb. In Table 10, we show that Attention scores and Integrated Gradient can both serve as a reasonable method for extracting important tokens in our first step, suggesting the flexibility of our framework.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_53",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "219-ARR_v1_54",
            "content": "In this paper, we aim to improve NLP models' robustness via identifying spurious correlations automatically at scale, and encouraging the model to rely less on those identified shortcuts. We perform experiments and human studies over several benchmark datasets and NLP tasks to show a scalable set of shortcuts can be efficiently identified through our framework. Note that we use existing interpretability approaches as a proxy to better understand how a model reaches its prediction, but as pointed out by prior work, the interpretability methods might not be accurate enough to reflect how a model works (or sometimes they could even deceive human decision makers). We acknowledge this as a limitation, and urge future research to dig deeper and develop better automated methods with less human intervention or expert knowledge in improving models' robustness.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "219-ARR_v1_55",
            "content": "Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, Kai-Wei Chang, Generating natural language adversarial examples, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Moustafa Alzantot",
                    "Yash Sharma",
                    "Ahmed Elgohary",
                    "Bo-Jhang Ho",
                    "Mani Srivastava",
                    "Kai-Wei Chang"
                ],
                "title": "Generating natural language adversarial examples",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "219-ARR_v1_56",
            "content": "UNKNOWN, None, 2016, Yelp dataset challenge: Review rating prediction, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Yelp dataset challenge: Review rating prediction",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_57",
            "content": "S\u00f6ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, Dbpedia: A nucleus for a web of open data, 2007, Proceedings of the 6th International The Semantic Web and 2nd Asian Conference on Asian Semantic Web Conference, ISWC'07/ASWC'07, Springer-Verlag.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "S\u00f6ren Auer",
                    "Christian Bizer",
                    "Georgi Kobilarov",
                    "Jens Lehmann",
                    "Richard Cyganiak",
                    "Zachary Ives"
                ],
                "title": "Dbpedia: A nucleus for a web of open data",
                "pub_date": "2007",
                "pub_title": "Proceedings of the 6th International The Semantic Web and 2nd Asian Conference on Asian Semantic Web Conference, ISWC'07/ASWC'07",
                "pub": "Springer-Verlag"
            }
        },
        {
            "ix": "219-ARR_v1_58",
            "content": "Ulrich A\u00efvodji, Hiromi Arai, Olivier Fortineau, S\u00e9bastien Gambs, Satoshi Hara, Alain Tapp, Fairwashing: the risk of rationalization, 2019, Proceedings of the 34th International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Ulrich A\u00efvodji",
                    "Hiromi Arai",
                    "Olivier Fortineau",
                    "S\u00e9bastien Gambs",
                    "Satoshi Hara",
                    "Alain Tapp"
                ],
                "title": "Fairwashing: the risk of rationalization",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 34th International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_59",
            "content": "J John,  Bartko, The intraclass correlation coefficient as a measure of reliability, 1966, Psychological reports, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "J John",
                    " Bartko"
                ],
                "title": "The intraclass correlation coefficient as a measure of reliability",
                "pub_date": "1966",
                "pub_title": "Psychological reports",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_60",
            "content": "John Blitzer, Mark Dredze, Fernando Pereira, Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification, 2007, Proceedings of the 45th Annual of the Association of Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "John Blitzer",
                    "Mark Dredze",
                    "Fernando Pereira"
                ],
                "title": "Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
                "pub_date": "2007",
                "pub_title": "Proceedings of the 45th Annual of the Association of Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_61",
            "content": "John Blitzer, Ryan Mcdonald, Fernando Pereira, Domain adaptation with structural correspondence learning, 2006, Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "John Blitzer",
                    "Ryan Mcdonald",
                    "Fernando Pereira"
                ],
                "title": "Domain adaptation with structural correspondence learning",
                "pub_date": "2006",
                "pub_title": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "219-ARR_v1_62",
            "content": "Swabha Ronan Le Bras, Chandra Swayamdipta, Rowan Bhagavatula, Matthew Zellers, Ashish Peters, Yejin Sabharwal,  Choi, Adversarial filters of dataset biases, 2020, Proceedings of the 37th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Swabha Ronan Le Bras",
                    "Chandra Swayamdipta",
                    "Rowan Bhagavatula",
                    "Matthew Zellers",
                    "Ashish Peters",
                    "Yejin Sabharwal",
                    " Choi"
                ],
                "title": "Adversarial filters of dataset biases",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 37th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "219-ARR_v1_63",
            "content": "Hanjie Chen, Guangtao Zheng, Yangfeng Ji, Generating hierarchical explanations on text classification via feature interaction detection, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Hanjie Chen",
                    "Guangtao Zheng",
                    "Yangfeng Ji"
                ],
                "title": "Generating hierarchical explanations on text classification via feature interaction detection",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "219-ARR_v1_64",
            "content": "Christopher Clark, Mark Yatskar, Luke Zettlemoyer, Don't take the easy way out: Ensemble based methods for avoiding known dataset biases, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Christopher Clark",
                    "Mark Yatskar",
                    "Luke Zettlemoyer"
                ],
                "title": "Don't take the easy way out: Ensemble based methods for avoiding known dataset biases",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_65",
            "content": "UNKNOWN, None, 2020, Learning to model and ignore dataset bias with mixed capacity ensembles, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Learning to model and ignore dataset bias with mixed capacity ensembles",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_66",
            "content": "Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher Manning, What does BERT look at? an analysis of BERT's attention, 2019, Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Kevin Clark",
                    "Urvashi Khandelwal",
                    "Omer Levy",
                    "Christopher Manning"
                ],
                "title": "What does BERT look at? an analysis of BERT's attention",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_67",
            "content": "UNKNOWN, None, 2019, Bias in bios. Proceedings of the Conference on Fairness, Accountability, and Transparency, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Bias in bios. Proceedings of the Conference on Fairness, Accountability, and Transparency",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_68",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "219-ARR_v1_69",
            "content": "Mengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong Sun, Xia Hu, Towards interpreting and mitigating shortcut learning behavior of NLU models, 2021, NAACL 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Mengnan Du",
                    "Varun Manjunatha",
                    "Rajiv Jain",
                    "Ruchi Deshpande",
                    "Franck Dernoncourt",
                    "Jiuxiang Gu",
                    "Tong Sun",
                    "Xia Hu"
                ],
                "title": "Towards interpreting and mitigating shortcut learning behavior of NLU models",
                "pub_date": "2021",
                "pub_title": "NAACL 2021",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_70",
            "content": "Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix Wichmann, Shortcut learning in deep neural networks, 2020, Nature Machine Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Robert Geirhos",
                    "J\u00f6rn-Henrik Jacobsen",
                    "Claudio Michaelis",
                    "Richard Zemel",
                    "Wieland Brendel",
                    "Matthias Bethge",
                    "Felix Wichmann"
                ],
                "title": "Shortcut learning in deep neural networks",
                "pub_date": "2020",
                "pub_title": "Nature Machine Intelligence",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_71",
            "content": "Yaru Hao, Li Dong, Furu Wei, Ke Xu, Selfattention attribution: Interpreting information interactions inside transformer, 2020, AAAI 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Yaru Hao",
                    "Li Dong",
                    "Furu Wei",
                    "Ke Xu"
                ],
                "title": "Selfattention attribution: Interpreting information interactions inside transformer",
                "pub_date": "2020",
                "pub_title": "AAAI 2021",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_72",
            "content": "He He, Sheng Zha, Haohan Wang, Unlearn dataset bias in natural language inference by fitting the residual, 2019, Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "He He",
                    "Sheng Zha",
                    "Haohan Wang"
                ],
                "title": "Unlearn dataset bias in natural language inference by fitting the residual",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_73",
            "content": "Ruining He, Julian Mcauley, Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering, 2016, Proceedings of the 25th International Conference on World Wide Web, WWW '16, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Ruining He",
                    "Julian Mcauley"
                ],
                "title": "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 25th International Conference on World Wide Web, WWW '16",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_74",
            "content": "Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, Dawn Song, Pretrained transformers improve out-of-distribution robustness, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Dan Hendrycks",
                    "Xiaoyuan Liu",
                    "Eric Wallace",
                    "Adam Dziedzic",
                    "Rishabh Krishnan",
                    "Dawn Song"
                ],
                "title": "Pretrained transformers improve out-of-distribution robustness",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_75",
            "content": "UNKNOWN, None, 2021, Contrastive explanations for model interpretability, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Contrastive explanations for model interpretability",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_76",
            "content": "Robin Jia, Percy Liang, Adversarial examples for evaluating reading comprehension systems, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Robin Jia",
                    "Percy Liang"
                ],
                "title": "Adversarial examples for evaluating reading comprehension systems",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "219-ARR_v1_77",
            "content": "Chen Jiaao, Chen Shen Dinghan, Yang Weizhu,  Diyi, Hiddencut: Simple data augmentation for natural language understanding with better generalizability, 2021, Proceedings of the 59th Annual Meeting of the Association of Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Chen Jiaao",
                    "Chen Shen Dinghan",
                    "Yang Weizhu",
                    " Diyi"
                ],
                "title": "Hiddencut: Simple data augmentation for natural language understanding with better generalizability",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association of Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "219-ARR_v1_78",
            "content": "Di Jin, Zhijing Jin, Joey Zhou, Peter Szolovits, Is BERT really robust? Natural language attack on text classification and entailment, 2020, AAAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Di Jin",
                    "Zhijing Jin",
                    "Joey Zhou",
                    "Peter Szolovits"
                ],
                "title": "Is BERT really robust? Natural language attack on text classification and entailment",
                "pub_date": "2020",
                "pub_title": "AAAI",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_79",
            "content": "Fereshte Khani, Percy Liang, Removing spurious features can hurt accuracy and affect groups disproportionately, 2021, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, Association for Computing Machinery.",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Fereshte Khani",
                    "Percy Liang"
                ],
                "title": "Removing spurious features can hurt accuracy and affect groups disproportionately",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21",
                "pub": "Association for Computing Machinery"
            }
        },
        {
            "ix": "219-ARR_v1_80",
            "content": "Olga Kovaleva, Alexey Romanov, Anna Rogers, Anna Rumshisky, Revealing the dark secrets of BERT, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Olga Kovaleva",
                    "Alexey Romanov",
                    "Anna Rogers",
                    "Anna Rumshisky"
                ],
                "title": "Revealing the dark secrets of BERT",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_81",
            "content": "Tom Mccoy, Ellie Pavlick, Tal Linzen, Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Tom Mccoy",
                    "Ellie Pavlick",
                    "Tal Linzen"
                ],
                "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "219-ARR_v1_82",
            "content": "George Miller, Wordnet: A lexical database for english, 1995, Commun. ACM, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "George Miller"
                ],
                "title": "Wordnet: A lexical database for english",
                "pub_date": "1995",
                "pub_title": "Commun. ACM",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_83",
            "content": "Matthias Minderer, Olivier Bachem, Neil Houlsby, Michael Tschannen, Automatic shortcut removal for self-supervised representation learning, 2020, Proceedings of the 37th International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Matthias Minderer",
                    "Olivier Bachem",
                    "Neil Houlsby",
                    "Michael Tschannen"
                ],
                "title": "Automatic shortcut removal for self-supervised representation learning",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 37th International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_84",
            "content": "Nikola Mrk\u0161i\u0107, \u00d3 Diarmuid, Blaise S\u00e9aghdha, Milica Thomson, Lina Ga\u0161i\u0107, Pei-Hao Rojas-Barahona, David Su, Tsung-Hsien Vandyke, Steve Wen,  Young, Counter-fitting word vectors to linguistic constraints, 2016, Proceedings of HLT-NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Nikola Mrk\u0161i\u0107",
                    "\u00d3 Diarmuid",
                    "Blaise S\u00e9aghdha",
                    "Milica Thomson",
                    "Lina Ga\u0161i\u0107",
                    "Pei-Hao Rojas-Barahona",
                    "David Su",
                    "Tsung-Hsien Vandyke",
                    "Steve Wen",
                    " Young"
                ],
                "title": "Counter-fitting word vectors to linguistic constraints",
                "pub_date": "2016",
                "pub_title": "Proceedings of HLT-NAACL",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_85",
            "content": "UNKNOWN, None, 2004, Wt5?! training text-to-text models to explain their predictions, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": null,
                "title": null,
                "pub_date": "2004",
                "pub_title": "Wt5?! training text-to-text models to explain their predictions",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_86",
            "content": "Xing Niu, Prashant Mathur, Georgiana Dinu, Yaser Al-Onaizan, Evaluating robustness to input perturbations for neural machine translation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Xing Niu",
                    "Prashant Mathur",
                    "Georgiana Dinu",
                    "Yaser Al-Onaizan"
                ],
                "title": "Evaluating robustness to input perturbations for neural machine translation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_87",
            "content": "Qiang Sinno Jialin Pan,  Yang, A survey on transfer learning, 2010, IEEE Transactions on Knowledge and Data Engineering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Qiang Sinno Jialin Pan",
                    " Yang"
                ],
                "title": "A survey on transfer learning",
                "pub_date": "2010",
                "pub_title": "IEEE Transactions on Knowledge and Data Engineering",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_88",
            "content": "Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, Zachary Lipton, Learning to deceive with attention-based explanations, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Danish Pruthi",
                    "Mansi Gupta",
                    "Bhuwan Dhingra",
                    "Graham Neubig",
                    "Zachary Lipton"
                ],
                "title": "Learning to deceive with attention-based explanations",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_89",
            "content": "Sameer Marco Tulio Ribeiro, Carlos Singh,  Guestrin, why should I trust you?\": Explaining the predictions of any classifier, 2016-08-13, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Sameer Marco Tulio Ribeiro",
                    "Carlos Singh",
                    " Guestrin"
                ],
                "title": "why should I trust you?\": Explaining the predictions of any classifier",
                "pub_date": "2016-08-13",
                "pub_title": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_90",
            "content": "Tongshuang Marco Tulio Ribeiro, Carlos Wu, Sameer Guestrin,  Singh, Beyond accuracy: Behavioral testing of NLP models with CheckList, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Tongshuang Marco Tulio Ribeiro",
                    "Carlos Wu",
                    "Sameer Guestrin",
                    " Singh"
                ],
                "title": "Beyond accuracy: Behavioral testing of NLP models with CheckList",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_91",
            "content": "UNKNOWN, None, 2020, An investigation of why overparameterization exacerbates spurious correlations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "An investigation of why overparameterization exacerbates spurious correlations",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_92",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Richard Socher",
                    "Alex Perelygin",
                    "Jean Wu",
                    "Jason Chuang",
                    "Christopher Manning",
                    "Andrew Ng",
                    "Christopher Potts"
                ],
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "219-ARR_v1_93",
            "content": "Megha Srivastava, Tatsunori Hashimoto, Percy Liang, Robustness to spurious correlations via human annotations, 2020, Proceedings of the 37th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Megha Srivastava",
                    "Tatsunori Hashimoto",
                    "Percy Liang"
                ],
                "title": "Robustness to spurious correlations via human annotations",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 37th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "219-ARR_v1_94",
            "content": "Mukund Sundararajan, Ankur Taly, Qiqi Yan, Axiomatic attribution for deep networks, 2017, Proceedings of the 34th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Mukund Sundararajan",
                    "Ankur Taly",
                    "Qiqi Yan"
                ],
                "title": "Axiomatic attribution for deep networks",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 34th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "219-ARR_v1_95",
            "content": "Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, Aleksander Madry, Robustness may be at odds with accuracy, 2019, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Dimitris Tsipras",
                    "Shibani Santurkar",
                    "Logan Engstrom",
                    "Alexander Turner",
                    "Aleksander Madry"
                ],
                "title": "Robustness may be at odds with accuracy",
                "pub_date": "2019",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_96",
            "content": "Lifu Tu, Garima Lalwani, Spandana Gella, and He He. 2020. An empirical study on robustness to spurious correlations using pre-trained language models, , Transactions of the Association of Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Lifu Tu",
                    "Garima Lalwani"
                ],
                "title": "Spandana Gella, and He He. 2020. An empirical study on robustness to spurious correlations using pre-trained language models",
                "pub_date": null,
                "pub_title": "Transactions of the Association of Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_97",
            "content": "Huazheng Wang, Zhe Gan, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Hongning Wang, Adversarial domain adaptation for machine reading comprehension, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Huazheng Wang",
                    "Zhe Gan",
                    "Xiaodong Liu",
                    "Jingjing Liu",
                    "Jianfeng Gao",
                    "Hongning Wang"
                ],
                "title": "Adversarial domain adaptation for machine reading comprehension",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": "EMNLP-IJCNLP"
            }
        },
        {
            "ix": "219-ARR_v1_98",
            "content": "Zhao Wang, Aron Culotta, Identifying spurious correlations for robust text classification, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Zhao Wang",
                    "Aron Culotta"
                ],
                "title": "Identifying spurious correlations for robust text classification",
                "pub_date": "2020",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "219-ARR_v1_99",
            "content": "Zhao Wang, Aron Culotta, Robustness to spurious correlations in text classification via automatically generated counterfactuals, 2020, AAAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Zhao Wang",
                    "Aron Culotta"
                ],
                "title": "Robustness to spurious correlations in text classification via automatically generated counterfactuals",
                "pub_date": "2020",
                "pub_title": "AAAI",
                "pub": null
            }
        },
        {
            "ix": "219-ARR_v1_100",
            "content": "Xiang Zhou, Mohit Bansal, Towards robustifying NLI models against lexical dataset biases, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Xiang Zhou",
                    "Mohit Bansal"
                ],
                "title": "Towards robustifying NLI models against lexical dataset biases",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "219-ARR_v1_0@0",
            "content": "Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_0",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_2@0",
            "content": "Recently, NLP models have achieved remarkable progress across a variety of tasks; however, they have also been criticized for being not robust.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_2",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_2@1",
            "content": "Many robustness problems can be attributed to models exploiting spurious correlations, or shortcuts between the training data and the task labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_2",
            "start": 144,
            "end": 289,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_2@2",
            "content": "Most existing work identifies a limited set of task-specific shortcuts via human priors or error analyses, which requires extensive expertise and efforts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_2",
            "start": 291,
            "end": 444,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_2@3",
            "content": "In this paper, we aim to automatically identify such spurious correlations in NLP models at scale.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_2",
            "start": 446,
            "end": 543,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_2@4",
            "content": "We first leverage existing interpretability methods to extract tokens that significantly affect model's decision process from the input text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_2",
            "start": 545,
            "end": 685,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_2@5",
            "content": "We then distinguish \"genuine\" tokens and \"spurious\" tokens by analyzing model predictions across multiple corpora and further verify them through knowledge-aware perturbations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_2",
            "start": 687,
            "end": 862,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_2@6",
            "content": "We show that our proposed method can effectively and efficiently identify a scalable set of \"shortcuts\", and mitigating these leads to more robust models in multiple applications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_2",
            "start": 864,
            "end": 1042,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_4@0",
            "content": "Despite great progress has been made over improved accuracy, deep learning models are known to be brittle to out-of-domain data (Hendrycks et al., 2020;, adversarial attacks (Mc-Coy et al., 2019;Jia and Liang, 2017;Jin et al., 2020), partly due to sometimes the models have exploited spurious correlations in the existing training data (Tu et al., 2020;Sagawa et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_4",
            "start": 0,
            "end": 373,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_4@1",
            "content": "In Figure 1, we show an example of a sentiment classification model making spurious correlations over the phrases \"Spielberg\" and \"New York Subway\" due to their high co-occurrences with positive and negative labels respectively in the training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_4",
            "start": 375,
            "end": 623,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_5@0",
            "content": "Most existing work quantifies spurious correlations in NLP models via a set of pre-defined patterns based on human priors and error analyses over the models, e.g., syntactic heuristics for Spielberg is a great spinner of a yarn, however this time he just didn't do it for me. (Prediction: Positive)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_5",
            "start": 0,
            "end": 297,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_6@0",
            "content": "The benefits of a New York Subway system is that a person can get from A to B without being stuck in traffic and subway trains are faster than buses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_6",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_6@1",
            "content": "(Prediction: Negative) Natural Language Inference (McCoy et al., 2019), synonym substitutions (Alzantot et al., 2018), or adding adversarial sentences for QA (Jia and Liang, 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_6",
            "start": 150,
            "end": 329,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_6@2",
            "content": "More recent work on testing models' behaviour using CheckList (Ribeiro et al., 2020) also used a pre-defined series of test types, e.g., adding negation, temporal change, and switching locations/person names.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_6",
            "start": 331,
            "end": 538,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_6@3",
            "content": "However, for safe deployment of NLP models in the real world, in addition to predefining a small or limited set of patterns which the model could be vulnerable to, it is also important to proactively discover and identify models' unrobust regions automatically and comprehensively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_6",
            "start": 540,
            "end": 820,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_7@0",
            "content": "In this work, we introduce a framework to automatically identify spurious correlations exploited by the model, sometimes also denoted as \"shortcuts\" in prior work (Geirhos et al., 2020;Minderer et al., 2020) 1 , at a large scale.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_7",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_7@1",
            "content": "Our proposed framework differs from existing literature with a focus more on automatic shortcut identification, instead of pre-defining a limited set of shortcuts or learning from human annotations (Table 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_7",
            "start": 230,
            "end": 437,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_7@2",
            "content": "Our framework works as follows: given a task and a trained model, we first utilize interpretability methods, e.g., attention scores (Clark et al., 2019b;Kovaleva et al., 2019) and integrated gradient (Sundararajan et al., 2017) which are commonly used for interpreting model's decisions, to automatically extract tokens that the model deems as important for task label Objective Approach for shortcut identification He et al. (2019) Robustness against known shortcuts Pre-defined Clark et al. (2019a) Robustness against known shortcuts Pre-defined Clark et al. (2020) Robustness against unknown shortcuts A low-capacity model to specifically learn shortcuts Wang and Culotta (2020a) prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_7",
            "start": 439,
            "end": 1132,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_7@3",
            "content": "We then introduce two extra steps to further categorize the extracted tokens to be \"genuine\" or \"spurious\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_7",
            "start": 1134,
            "end": 1240,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_7@4",
            "content": "We utilize a cross-dataset analysis to identify tokens that are more likely to be \"shortcut\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_7",
            "start": 1242,
            "end": 1334,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_8@0",
            "content": "The intuition is that if we have data from multiple domains for the same task, then \"genuine\" tokens are more likely to remain useful to labels across domains, while \"spurious\" tokens would be less useful.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_8",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_8@1",
            "content": "Our last step further applies a knowledgeaware perturbation to check how stable the model's prediction is by perturbing the extracted tokens to their semantically similar neighbors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_8",
            "start": 206,
            "end": 386,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_8@2",
            "content": "The intuition is that a model's prediction is more likely to change when a \"spurious\" token is replaced by its semantically similar variations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_8",
            "start": 388,
            "end": 530,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_8@3",
            "content": "To mitigate these identified \"shortcuts\", we propose a simple yet effective targeted mitigation approach to prevent the model from using those \"shortcuts\" and show that the resulting model can be more robust.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_8",
            "start": 532,
            "end": 739,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_8@4",
            "content": "Our contributions are as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_8",
            "start": 741,
            "end": 773,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_9@0",
            "content": "\u2022 We introduce a framework to automatically identify shortcuts in NLP models at scale. It first extracts important tokens using interpretability methods, then we propose cross-dataset analysis and knowledge-aware perturbation to distinguish spurious correlations from genuine ones. \u2022 We perform experiments over several benchmark datasets and NLP tasks including sentiment classification and occupation classification, and show that our framework is able to identify more subtle and diverse spurious correlations. We present results showing the identified shortcuts can be utilized to improve robustness in multiple applications, including better accuracy over challenging datasets, better adaptation across multiple domains, and better fairness implications over certain tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_9",
            "start": 0,
            "end": 777,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_10@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_10",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_11@0",
            "content": "Interpretability There has been a lot of work on better interpreting models' decision process, e.g., understanding BERT (Clark et al., 2019b;Kovaleva et al., 2019) and attention in transformers (Hao et al., 2020), or through text generation models (Narang et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_11",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_11@1",
            "content": "In this paper we utilize the attention scores as a generic way to understand what features a model relies on for making its predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_11",
            "start": 271,
            "end": 406,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_11@2",
            "content": "Other common model interpretation techniques (Sundararajan et al., 2017;Ribeiro et al., 2016), or more recent work on hierarchical attentions (Chen et al., 2020) and contrastive explanations (Jacovi et al., 2021), can be used as well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_11",
            "start": 408,
            "end": 641,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_11@3",
            "content": "In Pruthi et al. (2020), the authors found that attention scores can be manipulated to deceive human decision makers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_11",
            "start": 643,
            "end": 759,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_11@4",
            "content": "The reliability of existing interpretation methods is a research topic by itself, and extra care needs to be taken when using attention for auditing models on fairness and accountability (A\u00efvodji et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_11",
            "start": 761,
            "end": 970,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_12@0",
            "content": "Robustness and Bias An increasing body of work has been conducted on understanding robustness in deep neural networks, particularly, how models sometimes pay attention to spurious correlations (Tu et al., 2020;Sagawa et al., 2020) and take shortcuts (Geirhos et al., 2020), leading to vulnerability in generalization to out-of-distribution data or adversarial examples in various NLP tasks: NLI (McCoy et al., 2019), Question-Answering (Jia and Liang, 2017), and Neural Machine Translation (Niu et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_12",
            "start": 0,
            "end": 508,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_12@1",
            "content": "Different from most existing work that defines types of spurious correlations or shortcut patterns beforehand (Ribeiro et al., 2020;McCoy et al., 2019;Jia and Liang, 2017), which is often limited and requires expert knowledge, in this work we focus on automatically identifying models' unrobust regions at scale.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_12",
            "start": 510,
            "end": 821,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_12@2",
            "content": "Another line of work aims at identifying shortcuts in models (Wang and Culotta, 2020a) by training classifiers to better distinguish \"spurious\" correlations from \"genuine\" ones from human annotated examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_12",
            "start": 823,
            "end": 1029,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_12@3",
            "content": "In contrast, we propose a cross-dataset approach and a knowledge-aware perturbation approach to automate this identification process with less human intervention in-between.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_12",
            "start": 1031,
            "end": 1203,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_13@0",
            "content": "Mitigation Multiple approaches have been proposed to mitigate shortcut learning and data biases (Clark et al., 2020;Bras et al., 2020;Zhou and Bansal, 2020;Minderer et al., 2020) data augmentation (Jin et al., 2020;Alzantot et al., 2018), domain adaptation (Blitzer et al., 2006(Blitzer et al., , 2007, and multi-task learning (Tu et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_13",
            "start": 0,
            "end": 344,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_13@1",
            "content": "Du et al. (2021) proposes to mitigate shortcuts by suppressing model's prediction on examples with large shortcut degree.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_13",
            "start": 346,
            "end": 466,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_13@2",
            "content": "Recent study has also shown removing spurious correlations can sometimes hurt model's accuracy (Khani and Liang, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_13",
            "start": 468,
            "end": 586,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_13@3",
            "content": "Orthogonal to existing works, we propose to first identify unrobust correlations in an NLP model and then propose a targeted mitigation to encourage the model to rely less on those unrobust correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_13",
            "start": 588,
            "end": 790,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_14@0",
            "content": "Framework for Identifying Shortcuts",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_14",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_15@0",
            "content": "In this section, we introduce our framework to identify spurious correlations in NLP models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_15",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_15@1",
            "content": "Our overall framework consists of first identifying tokens important for models' decision process, followed by a cross-dataset analysis and a knowledge-aware perturbation step to identify spurious correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_15",
            "start": 93,
            "end": 302,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_16@0",
            "content": "Identify Tokens Key to Model's Decision",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_16",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_17@0",
            "content": "The first step of the framework aims to identify the top-K most important tokens that affect model's decision making process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_17",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_17@1",
            "content": "We look at the importance at the token-level 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_17",
            "start": 126,
            "end": 173,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_17@2",
            "content": "In general, depending on how the tokens are being used in model's decision process, they can be roughly divided into three categories: \"genuine\", \"spurious\", and others (e.g., tokens that are not useful for a model's prediction).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_17",
            "start": 175,
            "end": 403,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_18@0",
            "content": "Genuine tokens are tokens that causally affect a 2 In this paper, we mostly focus on unigrams.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_18",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_18@1",
            "content": "Our method can also be easily extended to multi-gram, text span or other type of features by summing the attention scores over spans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_18",
            "start": 95,
            "end": 227,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_18@2",
            "content": "For a vocabulary of wordpieces as used in BERT, we concatenate wordpieces with a prefix of \"##\" to form unigrams and sum the attention scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_18",
            "start": 229,
            "end": 370,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_18@3",
            "content": "task's label (Srivastava et al., 2020;Wang and Culotta, 2020b), and thus the correlations between those tokens and the labels are what we expect the model to capture and to more heavily rely on.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_18",
            "start": 372,
            "end": 565,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_18@4",
            "content": "On the other hand, spurious tokens, or shortcuts as commonly denoted in prior work (Geirhos et al., 2020;Minderer et al., 2020), are features that correlate with task labels but are not genuine, and thus might fail to transfer to challenging test conditions (Geirhos et al., 2020) or out-of-distribution data; spurious tokens do not causally affect task labels (Srivastava et al., 2020;Wang and Culotta, 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_18",
            "start": 567,
            "end": 977,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_19@0",
            "content": "In this step, we will extract both genuine tokens and shortcut tokens because they are both likely to affect a model's prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_19",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_19@1",
            "content": "We rely on interpretability techniques to collect information on whether a certain input token is important to model's decision making.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_19",
            "start": 131,
            "end": 265,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_19@2",
            "content": "In this paper, we use the attention score in BERT-based models as an explanation of model predictions (Clark et al., 2019b;Kovaleva et al., 2019), due to its simplicity and fast computation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_19",
            "start": 267,
            "end": 456,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_19@3",
            "content": "Recent work (Jiaao et al., 2021) also reveals that attention scores outperform other explanation techniques in regularizing redundant information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_19",
            "start": 458,
            "end": 603,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_19@4",
            "content": "Other techniques (Ribeiro et al., 2016;Sundararajan et al., 2017;Chen et al., 2020;Jacovi et al., 2021) can also be used in this step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_19",
            "start": 605,
            "end": 738,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_19@5",
            "content": "As an example, given a sentence \"Spielberg is a good director.\", assuming \"good\" is a genuine token and \"Spielberg\" is a shortcut token, we expect that in a BERT-based sentiment classification model, the attention scores for \"good\" and \"Spielberg\" are higher and thus will be extracted as important tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_19",
            "start": 740,
            "end": 1045,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_19@6",
            "content": "On the other hand, for \"is\", \"a\" and \"director\" the attention scores would be lower as they are relatively less useful to the model decision.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_19",
            "start": 1047,
            "end": 1187,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_20@0",
            "content": "We now describe this step using sentiment classification task as an example (more details can be found in Algorithm 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_20",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_20@1",
            "content": "Let f be a well trained sentiment classification model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_20",
            "start": 120,
            "end": 174,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_20@2",
            "content": "Given a corpus D, for each input sentence s i , i = 1, . . . , n for a total of n sentences in the corpus, we apply f on it to obtain the output probability p pos i and p neg i for positive and negative label respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_20",
            "start": 176,
            "end": 397,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_20@3",
            "content": "We then extract attention scores {a 1 i , a 2 i , . . . , a m i } for tokens {t 1 i , t 2 i , . . . , t m i } in sentence s i , where m is the length of the sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_20",
            "start": 399,
            "end": 564,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_20@4",
            "content": "In BERT-based classification models, the embedding of [CLS] token in the final layer is fed to a classification layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_20",
            "start": 566,
            "end": 683,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_20@5",
            "content": "We thus extract the attention scores of each token t used for computing the embedding of the [CLS] token and average them across different heads.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_20",
            "start": 685,
            "end": 829,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_20@6",
            "content": "If p pos i > p neg i , we obtain the updated attention score \u00e3j i = a j i * p pos i , otherwise",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_20",
            "start": 831,
            "end": 925,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_21@0",
            "content": "\u00e3j i = \u2212a j i * p neg i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_21",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_22@0",
            "content": "For each token t in the vocabulary V, we compute the average attention score:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_22",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_23@0",
            "content": "\u0101t = 1 mn \u2022 \u03a3 n i=1 \u03a3 m j=1 [\u00e3 j i \u2022 1(t j i = t)],",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_23",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_24@0",
            "content": "where we aggregate the attention scores \u00e3j i for token t, across all n sentences in the corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_24",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_24@1",
            "content": "We then normalize the attention scores across the vocabulary to obtain the importance score for each token t: I t = \u0101t /\u03a3 t\u2208V \u0101t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_24",
            "start": 96,
            "end": 225,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_24@2",
            "content": "This can lead to very small I t for certain tokens, thus we take the log of all importance scores to avoid underflow, I \u2032 t = log(I t ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_24",
            "start": 227,
            "end": 362,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_24@3",
            "content": "So far, we have computed the importance score for each token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_24",
            "start": 364,
            "end": 424,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_24@4",
            "content": "However, we observe that some tokens appearing only very a few times could accidentally have very high importance scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_24",
            "start": 426,
            "end": 546,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_24@5",
            "content": "Thus, we propose to penalize the tokens with an extreme low frequency:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_24",
            "start": 548,
            "end": 617,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_25@0",
            "content": "\u00cet = I \u2032 t \u2212 \u03bb/ log(1 + c t )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_25",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_26@0",
            "content": ", where c t is the frequency of token t and \u03bb is a temperature parameter to adjust the degree that we want to penalize the low frequency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_26",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_27@0",
            "content": "Cross-Dataset Stability Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_27",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_28@0",
            "content": "As mentioned before, the tokens that are important to a model's prediction could be either genuine or spurious, thus in this step, we want to categorize the extracted tokens into these two categories and maintain a list of tokens that are more likely to be \"spurious\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_28",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_29@0",
            "content": "In many real-world NLP tasks, if we have access to datasets from different sources or domains, then we can perform a cross-dataset analysis to more effectively identify \"spurious\" tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_29",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_29@1",
            "content": "The reasoning is that \"spurious\" tokens tend to be important for a model's decision making on one dataset but are Algorithm 1: Important Token Extraction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_29",
            "start": 188,
            "end": 341,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_30@0",
            "content": "Input :Sentiment classification model: f Text corpus: D 1 // Obtain attention scores for tokens in each input sentence s i \u2208 D: less likely to transfer or generalize to other datasets, e.g. \"Spielberg\" could be an important token for movie reviews but is not likely to be useful on other review datasets (e.g., for restaurants or hotels).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_30",
            "start": 0,
            "end": 337,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_30@1",
            "content": "On the other hand, genuine tokens are more likely to be important across multiple datasets, for example, tokens like \"good\", \"bad\", \"great\", \"terrible\" should remain useful across various sentiment classification datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_30",
            "start": 339,
            "end": 560,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_30@2",
            "content": "Thus, in this step, we try to distinguish \"genuine\" tokens from \"spurious\" tokens from the top extracted important tokens after the first step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_30",
            "start": 562,
            "end": 704,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_30@3",
            "content": "Our idea is to compare tokens' importance ranking and find the ones that have very different ranks across datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_30",
            "start": 706,
            "end": 820,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_31@0",
            "content": "2 for i = 1 to n do 3 p pos i , p neg i , {a 1 i , a 2 i , ..., a m i } = f (s i ); 4 for j = 1 to m do 5 if p pos i > p neg i : \u00e3j i = a j i \u2022",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_31",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_32@0",
            "content": "To this end, we conduct a cross-dataset stability analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_32",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_32@1",
            "content": "Specifically, we apply the same model Shortcut token: bread Original: I bought this in the hopes it would keep bread I made fresh.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_32",
            "start": 60,
            "end": 189,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_32@2",
            "content": "However, after a few times of usings the I found out that moister w still getting in bread would become stale or moldy ...(Neg) Perturbed: I bought this in the hopes it would keep loaf I made fresh.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_32",
            "start": 191,
            "end": 388,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_32@3",
            "content": "However, after a few times of usings the I found out that moister w still getting in bread would become stale or moldy ... (Pos) Shortcut token: iPhone Original: I lost my original TV remote, and found this one thinking it was the same one. ... Now this one is merely a back up.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_32",
            "start": 390,
            "end": 667,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_32@4",
            "content": "Also, I have the Samsung remote app on my iPhone, which also works just as good as these remotes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_32",
            "start": 669,
            "end": 765,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_32@5",
            "content": "(Pos) Perturbed: I lost my original TV remote, and found this one thinking it was the same one. ... Now this one is merely a back up. Also, I have the Samsung remote app on my ipod, which also works just as good as these remotes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_32",
            "start": 767,
            "end": 995,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_32@6",
            "content": "(Neg) Table 2: Examples of shortcut tokens with significant performance drop during knowledge-aware perturbation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_32",
            "start": 997,
            "end": 1109,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_33@0",
            "content": "f on two datasets A and B, and obtain two importance ranking lists.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_33",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_33@1",
            "content": "Since importance scores may have different ranges on the two datasets, we normalize all importance scores to adjust the value to be in the range of [0, 1]:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_33",
            "start": 68,
            "end": 222,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_34@0",
            "content": "\u0128A t = \u00ceA t \u2212 min({ \u00ceA t |t \u2208 V}) max({ \u00ceA t |t \u2208 V}) \u2212 min({ \u00ceA t |t \u2208 V}) \u0128B t = \u00ceB t \u2212 min({ \u00ceB t |t \u2208 V}) max({ \u00ceB t |t \u2208 V}) \u2212 min({ \u00ceB t |t \u2208 V})",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_34",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_35@0",
            "content": "where \u0128A t and \u0128B t are normalized importance scores on dataset A and B respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_35",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_35@1",
            "content": "We then subtract \u0128B t from \u0128A t and re-rank all tokens according to their differences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_35",
            "start": 86,
            "end": 171,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_35@2",
            "content": "Tokens with largest differences are the ones with high importance scores in dataset A but low importance scores in dataset B, thus they are more likely to be \"shortcut\" tokens in dataset A. Similarly, we can also extract tokens with largest differences from dataset B by subtract \u0128A t from \u0128B t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_35",
            "start": 173,
            "end": 468,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_36@0",
            "content": "Knowledge-aware Perturbation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_36",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_37@0",
            "content": "The cross-dataset analysis is an efficient way to remove important tokens that are \"genuine\" across multiple datasets, after which we can obtain a list with tokens that are more likely to be \"spurious\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_37",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_37@1",
            "content": "However, on this list, domain-specific genuine tokens can still be ranked very high, e.g., \"ambitious\" from a movie review dataset and \"delicious\" from a restaurant review dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_37",
            "start": 203,
            "end": 382,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_37@2",
            "content": "This is because domainspecific genuine tokens have similar characteristics as shortcuts, they are effective for a model's decision making on a certain dataset but could appear very rarely (and thus could be deemed as not important) on another dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_37",
            "start": 384,
            "end": 634,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_37@3",
            "content": "Hence, in this section, we further propose a slightly more expensive and a more fine-grained approach to verify whether a token is indeed \"spurious\", through knowledgeaware perturbation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_37",
            "start": 636,
            "end": 821,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_38@0",
            "content": "For each potential shortcut token, we extract N synonyms by leveraging the word embeddings curated for synonym extraction (Mrk\u0161i\u0107 et al., 2016), plus WordNet (Miller, 1995) and DBpedia (Auer et al., 2007).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_38",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_38@1",
            "content": "More specifically, for each top token t in the list generated by the previous step, we first search counter-fitting word vectors to find synonyms with cosine similarity larger than a threshold 3 \u03c4 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_38",
            "start": 206,
            "end": 403,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_38@2",
            "content": "Additionally we search in WordNet and DBpedia to obtain a maximum of N synonyms for each token t. Then we extract a subset S t from D, which consists of sentences containing t. We perturb all sentences in S t by replacing t with its synonyms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_38",
            "start": 405,
            "end": 646,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_38@3",
            "content": "The resulted perturbed set S \u2032 t is N times of the original set S t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_38",
            "start": 648,
            "end": 716,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_38@4",
            "content": "We apply model f on S t and S \u2032 t and obtain accuracy acc t and acc \u2032 t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_38",
            "start": 718,
            "end": 790,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_38@5",
            "content": "Since we only perturb S t with t's synonyms, the semantic meaning of perturbed sentences should stay close to the original sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_38",
            "start": 792,
            "end": 924,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_38@6",
            "content": "Thus, if t is a genuine token, acc \u2032 t is expected to be close to acc t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_38",
            "start": 926,
            "end": 998,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_38@7",
            "content": "On the other hand, if t is a shortcut, model prediction can be different even the semantic meaning of the sentence does not change a lot (see examples in Table 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_38",
            "start": 1000,
            "end": 1162,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_38@8",
            "content": "Thus, we assume tokens with larger differences between acc t and acc \u2032 t are more likely to be shortcuts and tokens with smaller differences are more likely to be domain specific \"genuine\" words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_38",
            "start": 1164,
            "end": 1358,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_38@9",
            "content": "From the potential shortcut token list computed in Sec 3.2, we remove tokens with performance difference smaller than \u03b4 to further filter domain specific \"geniue\" tokens .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_38",
            "start": 1360,
            "end": 1530,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_39@0",
            "content": "Mitigation via Identified Shortcuts",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_39",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_40@0",
            "content": "In this section, we describe how the identified shortcuts can be further utilized to improve robustness in NLP models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_40",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_40@1",
            "content": "More specifically, we propose targeted approaches to mitigate the identified shortcuts including three variants: (1) a trainingtime mitigation approach: we mask out the identified shortcuts during training time and re-train the model; (2) an inference-time mitigation approach: Table 4: We report the precision as well as the averaged importance score \u0128 of identified \"shortcuts\" after each step based on our framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_40",
            "start": 119,
            "end": 537,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_40@2",
            "content": "The identified \"shortcut\" is a true shortcut or not is verified by 3 independent human annotators (Amazon Turkers).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_40",
            "start": 539,
            "end": 653,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_40@3",
            "content": "We can see that the precision increases after each step in our framework, demonstrating the utility of cross-dataset analysis (step 2) and knowledge-aware perturbation (step 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_40",
            "start": 655,
            "end": 831,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_41@0",
            "content": "we mask out the identified shortcuts during inference time only, in this way we save the extra cost of re-training a model; (3) we combine both approach (1) and (2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_41",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_41@1",
            "content": "In the experiment section, we will demonstrate the effect of each approach over a set of benchmark datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_41",
            "start": 166,
            "end": 273,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_41@2",
            "content": "We found that by masking out shortcuts in datasets, models generalize better to challenging datasets, out-of-distribution data, and also become more fair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_41",
            "start": 275,
            "end": 428,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_42@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_42",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_43@0",
            "content": "Tasks and Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_43",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_44@0",
            "content": "Task 1: Sentiment classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_44",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_44@1",
            "content": "For the task of sentiment classification, we use three datasets in our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_44",
            "start": 34,
            "end": 116,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_44@2",
            "content": "We train a model on the Stanford Sentiment Treebank (SST-2) (Socher et al., 2013) training set, which consists of 67, 349 sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_44",
            "start": 118,
            "end": 249,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_44@3",
            "content": "We use the same training set for identifying shortcuts at a larger scale.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_44",
            "start": 251,
            "end": 323,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_44@4",
            "content": "For cross-dataset analysis, we use Yelp (Asghar, 2016) sentiment classification dataset, which consists of 5, 101 Yelp reviews after filtering out reviews with more than 128 tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_44",
            "start": 325,
            "end": 505,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_45@0",
            "content": "We also train another model on 80, 000 amazon kitchen reviews (He and McAuley, 2016), and apply it on the kitchen review dev set and the amazon electronics dev set, both having 10, 000 reviews.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_45",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_45@1",
            "content": "Task 2: Occupation classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_45",
            "start": 194,
            "end": 227,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_45@2",
            "content": "Following Pruthi et al. (2020), we use the biographies (De-Arteaga et al., 2019) to predict whether the occupation is a surgeon or physician (non-surgeon).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_45",
            "start": 229,
            "end": 383,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_45@3",
            "content": "The training data consists of 17, 629 biographies and the dev set contains 2, 519 samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_45",
            "start": 385,
            "end": 474,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_45@4",
            "content": "Models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_45",
            "start": 476,
            "end": 482,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_45@5",
            "content": "We use the attention scores over BERT (Devlin et al., 2019) based classification models as they have achieved the state-of-art performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_45",
            "start": 484,
            "end": 622,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_45@6",
            "content": "However, it is important to note that our proposed framework can also be easily extended to models with different architectures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_45",
            "start": 624,
            "end": 751,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_45@7",
            "content": "BERT-based models have the advantage that we can directly use the attention scores as explanations of model decisions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_45",
            "start": 753,
            "end": 870,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_46@0",
            "content": "For models with other architectures, we can use explanation techniques such as LIME (Ribeiro et al., 2016) or Path Integrated Gradient approaches (Sundararajan et al., 2017) to provide explanations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_46",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_46@1",
            "content": "Evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_46",
            "start": 199,
            "end": 209,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_46@2",
            "content": "Evaluating identified shortcuts in machine learning or deep leaning based models can be difficult.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_46",
            "start": 211,
            "end": 308,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_46@3",
            "content": "We do not have ground-truth labels for the shortcuts identified through our framework, and whether a token is a shortcut or not can be subjective even with human annotators, and it can further depend on the context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_46",
            "start": 310,
            "end": 524,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_46@4",
            "content": "Faced with these challenges, we carefully designed a task and adopted Amazon Mechanical Turk for evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_46",
            "start": 526,
            "end": 633,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_46@5",
            "content": "We post the identified shortcuts after each step in our framework, along with several sample sentences containing the token, as additional context, to the human annotator.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_46",
            "start": 635,
            "end": 805,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_46@6",
            "content": "We ask the question \"does the word determine the sentiment in the sentence\" and ask the annotator to provide a \"yes\"/\"no\" answer 4 to the question based on the answer that holds true for the majority of the provided sentences (we also experimented with adding an option of \"unsure\" but found most annotators do not choose that option).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_46",
            "start": 807,
            "end": 1141,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_46@7",
            "content": "Each identified shortcut is verified by 3 annotators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_46",
            "start": 1143,
            "end": 1195,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_47@0",
            "content": "Experimental Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_47",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_48@0",
            "content": "We summarized the top important tokens after each step in our framework (Table 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_48",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_48@1",
            "content": "We also report the precision score (the percentage of tokens) out of the top 50 tokens identified as true shortcuts by human annotators in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_48",
            "start": 83,
            "end": 229,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_48@2",
            "content": "Across all datasets, we see that the precision score increases after each step, which demonstrates that our proposed framework can consistently improve shortcut identification more precisely.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_48",
            "start": 231,
            "end": 421,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_48@3",
            "content": "Specifically, after the first step, the precision score of shortcuts is low 5 because most of the top extracted tokens are important tokens only (thus many of them are genuine).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_48",
            "start": 423,
            "end": 599,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_48@4",
            "content": "After the second step (crossdataset analysis) and the third step (knowledgeaware perturbation), we see a significant increase of the shortcuts among the top-K extracted tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_48",
            "start": 601,
            "end": 776,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_48@5",
            "content": "Table 2 shows examples of perturbing shortcut tokens leading to model predictions changes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_48",
            "start": 778,
            "end": 867,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_49@0",
            "content": "Agreement analysis over annotations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_49",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_49@1",
            "content": "Since this annotation task is non-trivial and sometimes subjective, we further compute the intraclass correlation score (Bartko, 1966) for the Amazon Mechanical Turk annotations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_49",
            "start": 37,
            "end": 214,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_49@2",
            "content": "Our collected annotations reaches an intraclass correlation score of 0.72, showing a good agreement among annotators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_49",
            "start": 216,
            "end": 332,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_49@3",
            "content": "Another agreement we analyze is showing annotators 5 sample sentences compared to showing them all sentences, to avoid sample bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_49",
            "start": 334,
            "end": 464,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_49@4",
            "content": "We ask annotators to annotate a batch of 25 tokens with all sentences containing the corresponding token shown to them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_49",
            "start": 466,
            "end": 584,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_49@5",
            "content": "The agreement reaches 84.0%, indicating that showing 5 sample sentences does not significantly affect annotator's decision on the target token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_49",
            "start": 586,
            "end": 728,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_49@6",
            "content": "More details of Amazon Mechanical Turk interface can be found in the Appendix. 2020) derived an occupation dataset to study the gender bias in NLP classification tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_49",
            "start": 730,
            "end": 897,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_49@7",
            "content": "The task is framed as a binary classification task to distinguish between \"surgeons\" and \"physicians\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_49",
            "start": 899,
            "end": 1000,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_49@8",
            "content": "These two occupations are chosen because they share similar words in their biographies and a majority of surgeons are male.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_49",
            "start": 1002,
            "end": 1124,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_49@9",
            "content": "The dataset is further tuned -downsample minority classes (female surgeons and male physicians) by a factor of ten to encourage the model to rely on gendered words to make predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_49",
            "start": 1126,
            "end": 1309,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_49@10",
            "content": "Pruthi et al. (2020) also provides a pre-specified list of impermissible tokens 6 that a robust model should assign low attention scores to.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_49",
            "start": 1311,
            "end": 1450,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_49@11",
            "content": "We instead treat this list of tokens as shortcuts and analyze the efficacy of our proposed framework on identifying these tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_49",
            "start": 1452,
            "end": 1580,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_49@12",
            "content": "These impermissible tokens can be regarded as shortcuts because they only reflect the gender of the person, thus by definition should not affect the decision of a occupation classification model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_49",
            "start": 1582,
            "end": 1776,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_49@13",
            "content": "Table 6 presents the result on identifying the list of impermissible tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_49",
            "start": 1778,
            "end": 1853,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_49@14",
            "content": "Among the top ten tokens selected by our method, 6 of them are shortcuts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_49",
            "start": 1855,
            "end": 1927,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_49@15",
            "content": "Furthermore, 9 out of 12 impermissible tokens are captured in the top 50 tokens selected by our method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_49",
            "start": 1929,
            "end": 2031,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_49@16",
            "content": "This further demonstrates that our method can effectively find shortcuts in this occupation classification task, in a more automated way compared to existing approaches that rely on pre-defined lists.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_49",
            "start": 2033,
            "end": 2232,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_50@0",
            "content": "Mitigating Shortcuts",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_50",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_51@0",
            "content": "We also study mitigating shortcuts by masking out the identified shortcuts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_51",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_51@1",
            "content": "Specifically, we use shortcut tokens identified by human annotators and mask them out in training set and re-train the model (Train RM), during test time directly (Test RM), and both (Train & Test RM) as described in Sec 3.4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_51",
            "start": 76,
            "end": 300,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_51@2",
            "content": "We evaluate these three approaches in multiple settings: 1) domain generalization; 2) challenging datasets; 3) gender bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_51",
            "start": 302,
            "end": 424,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_51@3",
            "content": "As shown in out-of-distribution data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_51",
            "start": 426,
            "end": 462,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_51@4",
            "content": "Note in this setting, different from existing domain transfer work (Pan and Yang, 2010), we do not assume access to labeled data in the target domain during training, instead we use our proposed approach to identify potential shortcuts that can generalize to unseen target domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_51",
            "start": 464,
            "end": 744,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_51@5",
            "content": "As a result, we also observe model's performance improvement on challenging datasets (Table 7).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_51",
            "start": 746,
            "end": 840,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_51@6",
            "content": "Table 8 demonstrates that mitigating shortcuts helps to reduce the performance gap (\u2206) between male and female groups, resulting in a fairer model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_51",
            "start": 842,
            "end": 988,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_51@7",
            "content": "Note the original performance might degrade slightly due to models learning different but more robust feature representations, consistent with findings in existing work (Tsipras et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_51",
            "start": 990,
            "end": 1181,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_52@0",
            "content": "We conduct an ablation study of changing the hyper-parameter \u03bb in the first step of extracting important tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_52",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_52@1",
            "content": "As shown in Table 9, our method is not very sensitive to the changing of \u03bb.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_52",
            "start": 113,
            "end": 187,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_52@2",
            "content": "In Table 10, we show that Attention scores and Integrated Gradient can both serve as a reasonable method for extracting important tokens in our first step, suggesting the flexibility of our framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_52",
            "start": 189,
            "end": 388,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_53@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_53",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_54@0",
            "content": "In this paper, we aim to improve NLP models' robustness via identifying spurious correlations automatically at scale, and encouraging the model to rely less on those identified shortcuts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_54",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_54@1",
            "content": "We perform experiments and human studies over several benchmark datasets and NLP tasks to show a scalable set of shortcuts can be efficiently identified through our framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_54",
            "start": 188,
            "end": 362,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_54@2",
            "content": "Note that we use existing interpretability approaches as a proxy to better understand how a model reaches its prediction, but as pointed out by prior work, the interpretability methods might not be accurate enough to reflect how a model works (or sometimes they could even deceive human decision makers).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_54",
            "start": 364,
            "end": 667,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_54@3",
            "content": "We acknowledge this as a limitation, and urge future research to dig deeper and develop better automated methods with less human intervention or expert knowledge in improving models' robustness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_54",
            "start": 669,
            "end": 862,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_55@0",
            "content": "Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, Kai-Wei Chang, Generating natural language adversarial examples, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_55",
            "start": 0,
            "end": 278,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_56@0",
            "content": "UNKNOWN, None, 2016, Yelp dataset challenge: Review rating prediction, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_56",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_57@0",
            "content": "S\u00f6ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, Dbpedia: A nucleus for a web of open data, 2007, Proceedings of the 6th International The Semantic Web and 2nd Asian Conference on Asian Semantic Web Conference, ISWC'07/ASWC'07, Springer-Verlag.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_57",
            "start": 0,
            "end": 287,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_58@0",
            "content": "Ulrich A\u00efvodji, Hiromi Arai, Olivier Fortineau, S\u00e9bastien Gambs, Satoshi Hara, Alain Tapp, Fairwashing: the risk of rationalization, 2019, Proceedings of the 34th International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_58",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_59@0",
            "content": "J John,  Bartko, The intraclass correlation coefficient as a measure of reliability, 1966, Psychological reports, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_59",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_60@0",
            "content": "John Blitzer, Mark Dredze, Fernando Pereira, Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification, 2007, Proceedings of the 45th Annual of the Association of Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_60",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_61@0",
            "content": "John Blitzer, Ryan Mcdonald, Fernando Pereira, Domain adaptation with structural correspondence learning, 2006, Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_61",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_62@0",
            "content": "Swabha Ronan Le Bras, Chandra Swayamdipta, Rowan Bhagavatula, Matthew Zellers, Ashish Peters, Yejin Sabharwal,  Choi, Adversarial filters of dataset biases, 2020, Proceedings of the 37th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_62",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_63@0",
            "content": "Hanjie Chen, Guangtao Zheng, Yangfeng Ji, Generating hierarchical explanations on text classification via feature interaction detection, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_63",
            "start": 0,
            "end": 273,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_64@0",
            "content": "Christopher Clark, Mark Yatskar, Luke Zettlemoyer, Don't take the easy way out: Ensemble based methods for avoiding known dataset biases, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_64",
            "start": 0,
            "end": 321,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_65@0",
            "content": "UNKNOWN, None, 2020, Learning to model and ignore dataset bias with mixed capacity ensembles, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_65",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_66@0",
            "content": "Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher Manning, What does BERT look at? an analysis of BERT's attention, 2019, Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_66",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_67@0",
            "content": "UNKNOWN, None, 2019, Bias in bios. Proceedings of the Conference on Fairness, Accountability, and Transparency, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_67",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_68@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_68",
            "start": 0,
            "end": 335,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_69@0",
            "content": "Mengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong Sun, Xia Hu, Towards interpreting and mitigating shortcut learning behavior of NLU models, 2021, NAACL 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_69",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_70@0",
            "content": "Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix Wichmann, Shortcut learning in deep neural networks, 2020, Nature Machine Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_70",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_71@0",
            "content": "Yaru Hao, Li Dong, Furu Wei, Ke Xu, Selfattention attribution: Interpreting information interactions inside transformer, 2020, AAAI 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_71",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_72@0",
            "content": "He He, Sheng Zha, Haohan Wang, Unlearn dataset bias in natural language inference by fitting the residual, 2019, Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_72",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_73@0",
            "content": "Ruining He, Julian Mcauley, Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering, 2016, Proceedings of the 25th International Conference on World Wide Web, WWW '16, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_73",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_74@0",
            "content": "Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, Dawn Song, Pretrained transformers improve out-of-distribution robustness, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_74",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_75@0",
            "content": "UNKNOWN, None, 2021, Contrastive explanations for model interpretability, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_75",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_76@0",
            "content": "Robin Jia, Percy Liang, Adversarial examples for evaluating reading comprehension systems, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_76",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_77@0",
            "content": "Chen Jiaao, Chen Shen Dinghan, Yang Weizhu,  Diyi, Hiddencut: Simple data augmentation for natural language understanding with better generalizability, 2021, Proceedings of the 59th Annual Meeting of the Association of Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_77",
            "start": 0,
            "end": 287,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_78@0",
            "content": "Di Jin, Zhijing Jin, Joey Zhou, Peter Szolovits, Is BERT really robust? Natural language attack on text classification and entailment, 2020, AAAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_78",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_79@0",
            "content": "Fereshte Khani, Percy Liang, Removing spurious features can hurt accuracy and affect groups disproportionately, 2021, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, Association for Computing Machinery.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_79",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_80@0",
            "content": "Olga Kovaleva, Alexey Romanov, Anna Rogers, Anna Rumshisky, Revealing the dark secrets of BERT, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_80",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_81@0",
            "content": "Tom Mccoy, Ellie Pavlick, Tal Linzen, Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_81",
            "start": 0,
            "end": 266,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_82@0",
            "content": "George Miller, Wordnet: A lexical database for english, 1995, Commun. ACM, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_82",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_83@0",
            "content": "Matthias Minderer, Olivier Bachem, Neil Houlsby, Michael Tschannen, Automatic shortcut removal for self-supervised representation learning, 2020, Proceedings of the 37th International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_83",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_84@0",
            "content": "Nikola Mrk\u0161i\u0107, \u00d3 Diarmuid, Blaise S\u00e9aghdha, Milica Thomson, Lina Ga\u0161i\u0107, Pei-Hao Rojas-Barahona, David Su, Tsung-Hsien Vandyke, Steve Wen,  Young, Counter-fitting word vectors to linguistic constraints, 2016, Proceedings of HLT-NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_84",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_85@0",
            "content": "UNKNOWN, None, 2004, Wt5?! training text-to-text models to explain their predictions, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_85",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_86@0",
            "content": "Xing Niu, Prashant Mathur, Georgiana Dinu, Yaser Al-Onaizan, Evaluating robustness to input perturbations for neural machine translation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_86",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_87@0",
            "content": "Qiang Sinno Jialin Pan,  Yang, A survey on transfer learning, 2010, IEEE Transactions on Knowledge and Data Engineering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_87",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_88@0",
            "content": "Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, Zachary Lipton, Learning to deceive with attention-based explanations, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_88",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_89@0",
            "content": "Sameer Marco Tulio Ribeiro, Carlos Singh,  Guestrin, why should I trust you?\": Explaining the predictions of any classifier, 2016-08-13, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_89",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_90@0",
            "content": "Tongshuang Marco Tulio Ribeiro, Carlos Wu, Sameer Guestrin,  Singh, Beyond accuracy: Behavioral testing of NLP models with CheckList, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_90",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_91@0",
            "content": "UNKNOWN, None, 2020, An investigation of why overparameterization exacerbates spurious correlations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_91",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_92@0",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_92",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_93@0",
            "content": "Megha Srivastava, Tatsunori Hashimoto, Percy Liang, Robustness to spurious correlations via human annotations, 2020, Proceedings of the 37th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_93",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_94@0",
            "content": "Mukund Sundararajan, Ankur Taly, Qiqi Yan, Axiomatic attribution for deep networks, 2017, Proceedings of the 34th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_94",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_95@0",
            "content": "Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, Aleksander Madry, Robustness may be at odds with accuracy, 2019, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_95",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_96@0",
            "content": "Lifu Tu, Garima Lalwani, Spandana Gella, and He He. 2020. An empirical study on robustness to spurious correlations using pre-trained language models, , Transactions of the Association of Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_96",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_97@0",
            "content": "Huazheng Wang, Zhe Gan, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Hongning Wang, Adversarial domain adaptation for machine reading comprehension, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_97",
            "start": 0,
            "end": 326,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_98@0",
            "content": "Zhao Wang, Aron Culotta, Identifying spurious correlations for robust text classification, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_98",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_99@0",
            "content": "Zhao Wang, Aron Culotta, Robustness to spurious correlations in text classification via automatically generated counterfactuals, 2020, AAAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_99",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "219-ARR_v1_100@0",
            "content": "Xiang Zhou, Mohit Bansal, Towards robustifying NLI models against lexical dataset biases, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "219-ARR_v1_100",
            "start": 0,
            "end": 185,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "219-ARR_v1_0",
            "tgt_ix": "219-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_0",
            "tgt_ix": "219-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_1",
            "tgt_ix": "219-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_1",
            "tgt_ix": "219-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_0",
            "tgt_ix": "219-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_2",
            "tgt_ix": "219-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_4",
            "tgt_ix": "219-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_5",
            "tgt_ix": "219-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_6",
            "tgt_ix": "219-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_7",
            "tgt_ix": "219-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_8",
            "tgt_ix": "219-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_3",
            "tgt_ix": "219-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_3",
            "tgt_ix": "219-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_3",
            "tgt_ix": "219-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_3",
            "tgt_ix": "219-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_3",
            "tgt_ix": "219-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_3",
            "tgt_ix": "219-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_3",
            "tgt_ix": "219-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_0",
            "tgt_ix": "219-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_11",
            "tgt_ix": "219-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_12",
            "tgt_ix": "219-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_10",
            "tgt_ix": "219-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_10",
            "tgt_ix": "219-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_10",
            "tgt_ix": "219-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_10",
            "tgt_ix": "219-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_0",
            "tgt_ix": "219-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_13",
            "tgt_ix": "219-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_14",
            "tgt_ix": "219-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_14",
            "tgt_ix": "219-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_14",
            "tgt_ix": "219-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_15",
            "tgt_ix": "219-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_17",
            "tgt_ix": "219-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_18",
            "tgt_ix": "219-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_19",
            "tgt_ix": "219-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_20",
            "tgt_ix": "219-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_21",
            "tgt_ix": "219-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_22",
            "tgt_ix": "219-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_23",
            "tgt_ix": "219-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_24",
            "tgt_ix": "219-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_25",
            "tgt_ix": "219-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_16",
            "tgt_ix": "219-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_16",
            "tgt_ix": "219-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_16",
            "tgt_ix": "219-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_16",
            "tgt_ix": "219-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_16",
            "tgt_ix": "219-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_16",
            "tgt_ix": "219-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_16",
            "tgt_ix": "219-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_16",
            "tgt_ix": "219-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_16",
            "tgt_ix": "219-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_16",
            "tgt_ix": "219-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_16",
            "tgt_ix": "219-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_14",
            "tgt_ix": "219-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_26",
            "tgt_ix": "219-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_28",
            "tgt_ix": "219-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_29",
            "tgt_ix": "219-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_30",
            "tgt_ix": "219-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_31",
            "tgt_ix": "219-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_32",
            "tgt_ix": "219-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_33",
            "tgt_ix": "219-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_34",
            "tgt_ix": "219-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_27",
            "tgt_ix": "219-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_27",
            "tgt_ix": "219-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_27",
            "tgt_ix": "219-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_27",
            "tgt_ix": "219-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_27",
            "tgt_ix": "219-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_27",
            "tgt_ix": "219-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_27",
            "tgt_ix": "219-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_27",
            "tgt_ix": "219-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_27",
            "tgt_ix": "219-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_14",
            "tgt_ix": "219-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_35",
            "tgt_ix": "219-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_37",
            "tgt_ix": "219-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_36",
            "tgt_ix": "219-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_36",
            "tgt_ix": "219-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_36",
            "tgt_ix": "219-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_14",
            "tgt_ix": "219-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_38",
            "tgt_ix": "219-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_40",
            "tgt_ix": "219-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_39",
            "tgt_ix": "219-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_39",
            "tgt_ix": "219-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_39",
            "tgt_ix": "219-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_0",
            "tgt_ix": "219-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_41",
            "tgt_ix": "219-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_42",
            "tgt_ix": "219-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_42",
            "tgt_ix": "219-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_44",
            "tgt_ix": "219-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_45",
            "tgt_ix": "219-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_43",
            "tgt_ix": "219-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_43",
            "tgt_ix": "219-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_43",
            "tgt_ix": "219-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_43",
            "tgt_ix": "219-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_42",
            "tgt_ix": "219-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_46",
            "tgt_ix": "219-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_48",
            "tgt_ix": "219-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_47",
            "tgt_ix": "219-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_47",
            "tgt_ix": "219-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_47",
            "tgt_ix": "219-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_42",
            "tgt_ix": "219-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_49",
            "tgt_ix": "219-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_50",
            "tgt_ix": "219-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_50",
            "tgt_ix": "219-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_50",
            "tgt_ix": "219-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_51",
            "tgt_ix": "219-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_0",
            "tgt_ix": "219-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_52",
            "tgt_ix": "219-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_53",
            "tgt_ix": "219-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_53",
            "tgt_ix": "219-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "219-ARR_v1_0",
            "tgt_ix": "219-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_1",
            "tgt_ix": "219-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_2",
            "tgt_ix": "219-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_2",
            "tgt_ix": "219-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_2",
            "tgt_ix": "219-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_2",
            "tgt_ix": "219-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_2",
            "tgt_ix": "219-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_2",
            "tgt_ix": "219-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_2",
            "tgt_ix": "219-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_3",
            "tgt_ix": "219-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_4",
            "tgt_ix": "219-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_4",
            "tgt_ix": "219-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_5",
            "tgt_ix": "219-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_6",
            "tgt_ix": "219-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_6",
            "tgt_ix": "219-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_6",
            "tgt_ix": "219-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_6",
            "tgt_ix": "219-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_7",
            "tgt_ix": "219-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_7",
            "tgt_ix": "219-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_7",
            "tgt_ix": "219-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_7",
            "tgt_ix": "219-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_7",
            "tgt_ix": "219-ARR_v1_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_8",
            "tgt_ix": "219-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_8",
            "tgt_ix": "219-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_8",
            "tgt_ix": "219-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_8",
            "tgt_ix": "219-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_8",
            "tgt_ix": "219-ARR_v1_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_9",
            "tgt_ix": "219-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_10",
            "tgt_ix": "219-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_11",
            "tgt_ix": "219-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_11",
            "tgt_ix": "219-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_11",
            "tgt_ix": "219-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_11",
            "tgt_ix": "219-ARR_v1_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_11",
            "tgt_ix": "219-ARR_v1_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_12",
            "tgt_ix": "219-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_12",
            "tgt_ix": "219-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_12",
            "tgt_ix": "219-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_12",
            "tgt_ix": "219-ARR_v1_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_13",
            "tgt_ix": "219-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_13",
            "tgt_ix": "219-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_13",
            "tgt_ix": "219-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_13",
            "tgt_ix": "219-ARR_v1_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_14",
            "tgt_ix": "219-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_15",
            "tgt_ix": "219-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_15",
            "tgt_ix": "219-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_16",
            "tgt_ix": "219-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_17",
            "tgt_ix": "219-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_17",
            "tgt_ix": "219-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_17",
            "tgt_ix": "219-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_18",
            "tgt_ix": "219-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_18",
            "tgt_ix": "219-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_18",
            "tgt_ix": "219-ARR_v1_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_18",
            "tgt_ix": "219-ARR_v1_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_18",
            "tgt_ix": "219-ARR_v1_18@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_19",
            "tgt_ix": "219-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_19",
            "tgt_ix": "219-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_19",
            "tgt_ix": "219-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_19",
            "tgt_ix": "219-ARR_v1_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_19",
            "tgt_ix": "219-ARR_v1_19@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_19",
            "tgt_ix": "219-ARR_v1_19@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_19",
            "tgt_ix": "219-ARR_v1_19@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_20",
            "tgt_ix": "219-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_20",
            "tgt_ix": "219-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_20",
            "tgt_ix": "219-ARR_v1_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_20",
            "tgt_ix": "219-ARR_v1_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_20",
            "tgt_ix": "219-ARR_v1_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_20",
            "tgt_ix": "219-ARR_v1_20@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_20",
            "tgt_ix": "219-ARR_v1_20@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_21",
            "tgt_ix": "219-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_22",
            "tgt_ix": "219-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_23",
            "tgt_ix": "219-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_24",
            "tgt_ix": "219-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_24",
            "tgt_ix": "219-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_24",
            "tgt_ix": "219-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_24",
            "tgt_ix": "219-ARR_v1_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_24",
            "tgt_ix": "219-ARR_v1_24@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_24",
            "tgt_ix": "219-ARR_v1_24@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_25",
            "tgt_ix": "219-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_26",
            "tgt_ix": "219-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_27",
            "tgt_ix": "219-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_28",
            "tgt_ix": "219-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_29",
            "tgt_ix": "219-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_29",
            "tgt_ix": "219-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_30",
            "tgt_ix": "219-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_30",
            "tgt_ix": "219-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_30",
            "tgt_ix": "219-ARR_v1_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_30",
            "tgt_ix": "219-ARR_v1_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_31",
            "tgt_ix": "219-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_32",
            "tgt_ix": "219-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_32",
            "tgt_ix": "219-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_32",
            "tgt_ix": "219-ARR_v1_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_32",
            "tgt_ix": "219-ARR_v1_32@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_32",
            "tgt_ix": "219-ARR_v1_32@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_32",
            "tgt_ix": "219-ARR_v1_32@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_32",
            "tgt_ix": "219-ARR_v1_32@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_33",
            "tgt_ix": "219-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_33",
            "tgt_ix": "219-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_34",
            "tgt_ix": "219-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_35",
            "tgt_ix": "219-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_35",
            "tgt_ix": "219-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_35",
            "tgt_ix": "219-ARR_v1_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_36",
            "tgt_ix": "219-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_37",
            "tgt_ix": "219-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_37",
            "tgt_ix": "219-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_37",
            "tgt_ix": "219-ARR_v1_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_37",
            "tgt_ix": "219-ARR_v1_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_38",
            "tgt_ix": "219-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_38",
            "tgt_ix": "219-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_38",
            "tgt_ix": "219-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_38",
            "tgt_ix": "219-ARR_v1_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_38",
            "tgt_ix": "219-ARR_v1_38@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_38",
            "tgt_ix": "219-ARR_v1_38@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_38",
            "tgt_ix": "219-ARR_v1_38@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_38",
            "tgt_ix": "219-ARR_v1_38@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_38",
            "tgt_ix": "219-ARR_v1_38@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_38",
            "tgt_ix": "219-ARR_v1_38@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_39",
            "tgt_ix": "219-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_40",
            "tgt_ix": "219-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_40",
            "tgt_ix": "219-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_40",
            "tgt_ix": "219-ARR_v1_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_40",
            "tgt_ix": "219-ARR_v1_40@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_41",
            "tgt_ix": "219-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_41",
            "tgt_ix": "219-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_41",
            "tgt_ix": "219-ARR_v1_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_42",
            "tgt_ix": "219-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_43",
            "tgt_ix": "219-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_44",
            "tgt_ix": "219-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_44",
            "tgt_ix": "219-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_44",
            "tgt_ix": "219-ARR_v1_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_44",
            "tgt_ix": "219-ARR_v1_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_44",
            "tgt_ix": "219-ARR_v1_44@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_45",
            "tgt_ix": "219-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_45",
            "tgt_ix": "219-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_45",
            "tgt_ix": "219-ARR_v1_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_45",
            "tgt_ix": "219-ARR_v1_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_45",
            "tgt_ix": "219-ARR_v1_45@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_45",
            "tgt_ix": "219-ARR_v1_45@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_45",
            "tgt_ix": "219-ARR_v1_45@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_45",
            "tgt_ix": "219-ARR_v1_45@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_46",
            "tgt_ix": "219-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_46",
            "tgt_ix": "219-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_46",
            "tgt_ix": "219-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_46",
            "tgt_ix": "219-ARR_v1_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_46",
            "tgt_ix": "219-ARR_v1_46@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_46",
            "tgt_ix": "219-ARR_v1_46@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_46",
            "tgt_ix": "219-ARR_v1_46@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_46",
            "tgt_ix": "219-ARR_v1_46@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_47",
            "tgt_ix": "219-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_48",
            "tgt_ix": "219-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_48",
            "tgt_ix": "219-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_48",
            "tgt_ix": "219-ARR_v1_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_48",
            "tgt_ix": "219-ARR_v1_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_48",
            "tgt_ix": "219-ARR_v1_48@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_48",
            "tgt_ix": "219-ARR_v1_48@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_49",
            "tgt_ix": "219-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_49",
            "tgt_ix": "219-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_49",
            "tgt_ix": "219-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_49",
            "tgt_ix": "219-ARR_v1_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_49",
            "tgt_ix": "219-ARR_v1_49@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_49",
            "tgt_ix": "219-ARR_v1_49@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_49",
            "tgt_ix": "219-ARR_v1_49@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_49",
            "tgt_ix": "219-ARR_v1_49@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_49",
            "tgt_ix": "219-ARR_v1_49@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_49",
            "tgt_ix": "219-ARR_v1_49@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_49",
            "tgt_ix": "219-ARR_v1_49@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_49",
            "tgt_ix": "219-ARR_v1_49@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_49",
            "tgt_ix": "219-ARR_v1_49@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_49",
            "tgt_ix": "219-ARR_v1_49@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_49",
            "tgt_ix": "219-ARR_v1_49@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_49",
            "tgt_ix": "219-ARR_v1_49@15",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_49",
            "tgt_ix": "219-ARR_v1_49@16",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_50",
            "tgt_ix": "219-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_51",
            "tgt_ix": "219-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_51",
            "tgt_ix": "219-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_51",
            "tgt_ix": "219-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_51",
            "tgt_ix": "219-ARR_v1_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_51",
            "tgt_ix": "219-ARR_v1_51@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_51",
            "tgt_ix": "219-ARR_v1_51@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_51",
            "tgt_ix": "219-ARR_v1_51@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_51",
            "tgt_ix": "219-ARR_v1_51@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_52",
            "tgt_ix": "219-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_52",
            "tgt_ix": "219-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_52",
            "tgt_ix": "219-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_53",
            "tgt_ix": "219-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_54",
            "tgt_ix": "219-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_54",
            "tgt_ix": "219-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_54",
            "tgt_ix": "219-ARR_v1_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_54",
            "tgt_ix": "219-ARR_v1_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_55",
            "tgt_ix": "219-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_56",
            "tgt_ix": "219-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_57",
            "tgt_ix": "219-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_58",
            "tgt_ix": "219-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_59",
            "tgt_ix": "219-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_60",
            "tgt_ix": "219-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_61",
            "tgt_ix": "219-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_62",
            "tgt_ix": "219-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_63",
            "tgt_ix": "219-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_64",
            "tgt_ix": "219-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_65",
            "tgt_ix": "219-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_66",
            "tgt_ix": "219-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_67",
            "tgt_ix": "219-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_68",
            "tgt_ix": "219-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_69",
            "tgt_ix": "219-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_70",
            "tgt_ix": "219-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_71",
            "tgt_ix": "219-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_72",
            "tgt_ix": "219-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_73",
            "tgt_ix": "219-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_74",
            "tgt_ix": "219-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_75",
            "tgt_ix": "219-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_76",
            "tgt_ix": "219-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_77",
            "tgt_ix": "219-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_78",
            "tgt_ix": "219-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_79",
            "tgt_ix": "219-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_80",
            "tgt_ix": "219-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_81",
            "tgt_ix": "219-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_82",
            "tgt_ix": "219-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_83",
            "tgt_ix": "219-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_84",
            "tgt_ix": "219-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_85",
            "tgt_ix": "219-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_86",
            "tgt_ix": "219-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_87",
            "tgt_ix": "219-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_88",
            "tgt_ix": "219-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_89",
            "tgt_ix": "219-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_90",
            "tgt_ix": "219-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_91",
            "tgt_ix": "219-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_92",
            "tgt_ix": "219-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_93",
            "tgt_ix": "219-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_94",
            "tgt_ix": "219-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_95",
            "tgt_ix": "219-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_96",
            "tgt_ix": "219-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_97",
            "tgt_ix": "219-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_98",
            "tgt_ix": "219-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_99",
            "tgt_ix": "219-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "219-ARR_v1_100",
            "tgt_ix": "219-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1090,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "219-ARR",
        "version": 1
    }
}