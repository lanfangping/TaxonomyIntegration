{
    "nodes": [
        {
            "ix": "80-ARR_v2_0",
            "content": "Learning the Beauty in Songs: Neural Singing Voice Beautifier",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_2",
            "content": "We are interested in a novel task, singing voice beautification (SVB). Given the singing voice of an amateur singer, SVB aims to improve the intonation and vocal tone of the voice, while keeping the content and vocal timbre. Current automatic pitch correction techniques are immature, and most of them are restricted to intonation but ignore the overall aesthetic quality. Hence, we introduce Neural Singing Voice Beautifier (NSVB), the first generative model to solve the SVB task, which adopts a conditional variational autoencoder as the backbone and learns the latent representations of vocal tone.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_3",
            "content": "In NSVB, we propose a novel time-warping approach for pitch correction: Shape-Aware Dynamic Time Warping (SADTW), which ameliorates the robustness of existing time-warping approaches, to synchronize the amateur recording with the template pitch curve. Furthermore, we propose a latent-mapping algorithm in the latent space to convert the amateur vocal tone to the professional one. To achieve this, we also propose a new dataset containing parallel singing recordings of both amateur and professional versions. Extensive experiments on both Chinese and English songs demonstrate the effectiveness of our methods in terms of both objective and subjective metrics. Audio samples are available at https://neuralsvb. github.io. Codes: https://github. com/MoonInTheRiver/NeuralSVB.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_4",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "80-ARR_v2_5",
            "content": "The major successes of the artificial intelligent singing voice research are primarily in Singing Voice Synthesis (SVS) (Lee et al., 2019;Blaauw and Bonada, 2020;Ren et al., 2020;Liu et al., 2021a) and Singing Voice Conversion (SVC) (Sisman and Li, 2020;Li et al., 2021;Wang et al., 2021a). However, the Singing Voice Beautification (SVB) remains an important and challenging endeavor for researchers. SVB aims to improve the intonation 1 and the vocal tone of the voice, while keeping the content and vocal timbre 2 . SVB is extensively required both in the professional recording studios and the entertainment industries in our daily life, since it is impractical to record flawless singing audio.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_6",
            "content": "Nowadays in real-life scenarios, SVB is usually performed by professional sound engineers with adequate domain knowledge, who manipulate commercial vocal correction tools such as Melodyne 3 and Autotune 4 (Yong and Nam, 2018). Most current automatic pitch correction works are shown to be an attractive alternative, but they may 1) show weak alignment accuracy (Luo et al., 2018) or pitch accuracy (Wager et al., 2020); 2) cause the tuned recording and the reference recording to be homogeneous in singing style (Yong and Nam, 2018). Besides, they typically focus on the intonation but ignore the overall aesthetic quality (audio quality and vocal tone) (Rosenzweig et al., 2021;Zhuang et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_7",
            "content": "To tackle these challenges, we introduce Neural Singing Voice Beautifier (NSVB), the first generative model to solve the SVB task, which adopts a Conditional Variational AutoEncoder (CVAE) (Kingma and Welling, 2014;Sohn et al., 2015) as the backbone to generate high-quality audio and learns the latent representation of vocal tone. In NSVB, we dichotomize the SVB task into pitch correction and vocal tone improvement: 1) To correct the intonation, a straightforward method is aligning the amateur recording with the template pitch curve, and then putting them together to resynthesize a new singing sample. Previous works (Wada et al., 2017;Luo et al., 2018) implemented this by figuring out the alignment through Dynamic Time Warping (DTW) (M\u00fcller, 2007) or Canonical Time Warping (CTW) (Zhou and Torre, 2009). We propose a novel Shape-Aware DTW algorithm, which ameliorates the robustness of existing time-warping approaches by considering the shape of the pitch curve rather than low-level features when calculating the optimal alignment path.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_8",
            "content": "2) To improve the vocal tone, we propose a latentmapping algorithm in the latent space, which converts the latent variables of the amateur vocal tone to those of the professional ones. This process is optimized by maximizing the log-likelihood of the converted latent variables. To retain the vocal timbre during the vocal tone mapping, we also propose a new dataset named PopBuTFy containing parallel singing recordings of both amateur and professional versions. Besides, thanks to the autoencoder structure, NSVB inherently supports semisupervised learning, where the additional unpaired, unlabeled 5 singing data could be leveraged to facilitate the learning of the latent representations. Extensive experiments on both Chinese and English songs show that NSVB outperforms previous methods by a notable margin, and each component in NSVB is effective, in terms of both objective and subjective metrics. The main contributions of this work are summarized as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_9",
            "content": "\u2022 We propose the first generative model NSVB to solve the SVB task. NSVB not only corrects the pitch of amateur recordings, but also generates the audio with high audio quality and improved vocal tone, to which previous works typically pay little attention. \u2022 We propose Shape-Aware Dynamic Time Warping (SADTW) algorithm to synchronize the amateur recording with the template pitch curve, which ameliorates the robustness of the previous time-warping algorithm. \u2022 We propose a latent-mapping algorithm to convert the latent variable of the amateur vocal tone to the professional one's, and contribute a new dataset PopBuTFyto train the latent-mapping function. \u2022 We design NSVB as a CVAE model, which supports the semi-supervised learning to leverage unpaired, unlabeled singing data for better performance.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_10",
            "content": "2 Related Works",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_11",
            "content": "Singing Voice Conversion",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "80-ARR_v2_12",
            "content": "Singing Voice Conversion (SVC) is a sub-task of Voice Conversion (VC) (Berg-Kirkpatrick and Klein, 2015;Serr\u00e0 et al., 2019;Popov et al., 2021;, which transforms the vocal timbre (or singer identity) of one singer to that of another singer, while preserving the linguistic content and pitch/melody information (Li et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_13",
            "content": "Mainstream SVC models can be grouped into three categories (Zhao et al., 2020): 1) parallel spectral feature mapping models, which learn the conversion function between source and target singers relying on parallel singing data (Villavicencio and Bonada, 2010;Kobayashi et al., 2015;Sisman et al., 2019); 2) Cycle-consistent Generative Adversarial Networks (CycleGAN) (Zhu et al., 2017;Kaneko et al., 2019), where an adversarial loss and a cycleconsistency loss are concurrently used to learn the forward and inverse mappings simultaneously (Sisman and Li, 2020); 3) encoder-decoder models, such as PPG-SVC (Li et al., 2021), which leverage a singing voice synthesis (SVS) system for SVC , and auto-encoder (Qian et al., 2019a;Wang et al., 2021b;Yuan et al., 2020) based SVC (Wang et al., 2021a). The models of the latter two categories can be utilized with nonparallel data. In our work, we aim to convert the intonation and the vocal tone while keeping the content and the vocal timbre, which is quite different from the SVC task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_14",
            "content": "Automatic Pitch Correction",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "80-ARR_v2_15",
            "content": "Automatic Pitch Correction (APC) works attempt to minimize the manual effort in modifying the flawed singing voice (Yong and Nam, 2018). Nam (2018) propose to modify pitch and energy information to improve the singing expressions of an amateur singing recording. However, this method heavily relies on a reference recording, causing the tuned recording and the reference recording to be homogeneous in singing style (Zhuang et al., 2021). Our work adopts the non-parametric and data-free pitch correction method like Luo et al. (2018), but improves the accuracy of alignment.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_16",
            "content": "Methdology",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "80-ARR_v2_17",
            "content": "In this section, we describe the overview of NSVB, which is shown in Figure 1. At Stage 1 in the figure, we reconstruct the input mel-spectrogram through the CVAE backbone (Section 3.1) based on the pitch, content and vocal timbre conditions extracted from the input by the pitch encoder, content encoder and timbre encoder, and optimize the CVAE by maximizing evidence lower bound and adversarial learning. At Stage 2/Inference in the figure, firstly we infer the latent variable z a based on the amateur conditions; secondly we prepare the amateur content vectors aligned with the professional pitch by SADTW algorithm (Section 3.2); thirdly we map z a to z p by the latent-mapping algorithm (Section 3.3); finally, we mix the professional pitch, the aligned amateur content vectors, and the amateur vocal timbre to obtain a new condition, which is leveraged along with the mapped z p by the decoder of CVAE to generate a new beautified mel-spectrogram. The training/inference details and model structure of each component in NSVB are described in Section 3.4 and Section 3.5.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_18",
            "content": "Conditional Variational Generator with Adversarial Learning",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "80-ARR_v2_19",
            "content": "As shown in Figure 2, to generate audio with high quality and learn the latent representations of vocal tone, we introduce a Conditional Variational AutoEncoder (CVAE) (Kingma and Welling, 2014;Sohn et al., 2015) as the mel-spectrogram generator, with the optimizing objective of maximizing the evidence lower bound (ELBO) of the intractable marginal log-likelihood of melspectrogram log p \u03b8 (x|c):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_20",
            "content": "log p \u03b8 (x|c) \u2265 ELBO(\u03d5, \u03b8) \u2261 E z\u223cq \u03d5 (z|x,c) log p \u03b8 (x|z, c) \u2212 log q \u03d5 (z|x, c) p(z) ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_21",
            "content": "where x, c, z denote the input/output melspectrogram, the mix of content, vocal timbre and pitch conditions, and the latent variable representing the vocal tone respectively; \u03d5 and \u03b8 denote the model parameters of CVAE encoder and CVAE decoder; q \u03d5 (z|x, c) is the posterior distribution approximated by the CVAE encoder; p \u03b8 (x|z, c) is the likelihood function that generates mel-spectrograms given latent variable z and condition c; p(z) is the prior distribution of the latent variables z, and we choose the standard normal distribution as p(z) for simplification. Furthermore, to address the over-smoothing problem (Qian et al., 2019b) in CVAE, we utilize an adversarial discriminator (D) (Mao et al., 2017) to refine the output mel-spectrogram:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_22",
            "content": "L adv (\u03d5, \u03b8) = E[(D( x) \u2212 1) 2 ], L adv (D) = E[(D(x) \u2212 1) 2 ] + E[D( x) 2 ], (1",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_23",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_24",
            "content": "where x is the ground-truth and x is the output of CVAE. The descriptions for the model structure of each component are in Section 3.5.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_25",
            "content": "Shape-Aware Dynamic Time Warping",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "80-ARR_v2_26",
            "content": "To implement the pitch correction, a straightforward method is aligning the amateur recording with the template pitch curve, and then concatenating them to resynthesize a new singing sample with improved intonation. Since the source pitch curve of amateur recordings and template one show a high degree of natural correlation along the time axis, applying a proper time-warping algorithm on them is crucial. However, original DTW (M\u00fcller, 2007) could result in a poor alignment when certain parts of the axis move to higher frequencies, and other parts to lower ones, or vice versa (Sundermann and Ney, 2003). Luo et al. (2018) adopt an advanced algorithm CTW (Zhou and Torre, 2009), which combines the canonical correlation analysis (CCA) and DTW to extract the feature sequences of two pitch curves, and then apply DTW on them. However, the alignment accuracy of CTW leaves much to be desired.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_27",
            "content": "We elaborate a non-parametric and data-free algorithm, Shape-Aware DTW (SADTW), based on the prior knowledge that the source pitch curve and the template one have analogous local shape contours. Specifically, we replace the Euclidean distance in the original DTW distance matrix with the shape context descriptor distance. The shape context descriptor of a time point f i in one pitch curve is illustrated in Figure 3. Inspired by (Mori et al., 2005), we divide the data points around f i into m * n bins by m time windows and n angles. We calculate the number of all points falling in the k-th bin. Then the descriptor for f i is defined as the histogram h i \u2208 R m * n :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_28",
            "content": "h i (k) = |{f j \u0338 = f i , f j \u2208 bin(k)}|,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_29",
            "content": "where | \u2022 | means the cardinality of a set. This histogram represents the distribution over relative positions, which is a robust, compact and discriminative descriptor. Then, it is natural to use the X 2 -test statistic on this distribution descriptor as the \"distance\" of two points f a and f p :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_30",
            "content": "C(a, p) = 1 2 m * n k=1 [h a (k) \u2212 h p (k)] 2 h a (k) + h p (k) ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_31",
            "content": "where h a and h p are the normalized histograms corresponding to the point f a from the amateur pitch curve and the point f p from the template pitch curve. C(a, p) ranges from 0 to 1. Finally, we run DTW on the distance matrix C to obtain the alignment with least distance cost between two curves.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_32",
            "content": "4 windows",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_33",
            "content": "\ud835\udc53 ! 30\u00b0F",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_34",
            "content": "igure 3: The shape descriptor in SADTW. The blue curve represents pitch; the horizontal axis means time; the vertical axis means F0-frequency. There are m = 4 windows, n = 6 angles to divide neighbor points of f i .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_35",
            "content": "Latent-mapping Algorithm",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "80-ARR_v2_36",
            "content": "Define a pair of mel-spectrograms (x a , x p ): the contents of x a and y p are the same sentence of a song from the same singer 6 , who sings these two recordings using the amateur tone and the professional tone respectively. Given the CVAE model, we can infer the posterior distribution q \u03d5 (z a |x a , c a ) and q \u03d5 (z p |x p , c p ) corresponding to x a and x p through the encoder of CVAE. To achieve the conversion of vocal tone, we introduce a mapping function M to convert the latent variables from q \u03d5 (z a |x a , c a ) to q \u03d5 (z p |x p , c p ). Concretely, we sample a latent variable of amateur vocal tone z a from q \u03d5 (z a |x a , c a ), and map z a to M(z a ). Then, M can be optimized by minimizing the negative log-likelihood of M(z a ):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_37",
            "content": "L map1 (M) = \u2212 log q \u03d5 (M(z a )|x p , c p ).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_38",
            "content": "Define \u0109p as the mix of 1) the content vectors from the amateur recording aligned by SADTW, 2) vocal timbre embedding encoded by timbre encoder, and 3) template pitch 7 embeddings encoded by pitch encoder. To make sure the converted latent variable could work well together with \u0109p to generate a high-quality audio sample (with the correct pitch and improved vocal tone), we send M(z a ) to the CVAE decoder to generate x, and propose an additional loss:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_39",
            "content": "L map2 (M) = \u2225x \u2212 x p \u2225 1 + \u03bb(D(x) \u2212 1) 2 ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_40",
            "content": "where D has been optimized by Eq. ( 1); \u03bb is a hyper-parameter.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_41",
            "content": "Training and Inference",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "80-ARR_v2_42",
            "content": "There are two training stages for NSVB: in the first training stage, we optimize CVAE by minimizing the following loss function",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_43",
            "content": "L(\u03d5, \u03b8) = \u2212ELBO(\u03d5, \u03b8) + \u03bbL adv (\u03d5, \u03b8),",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_44",
            "content": "and optimize the discriminator (D) by minimizing Eq. ( 1). Note that, the first stage is the reconstruction process of mel-spectrograms, where any unpaired, unlabeled singing data beyond Pop-BuTFy could be leveraged to facilitate the learning of the latent representations. In the second training stage, we optimize M on the parallel dataset Pop-BuTFy by minimizing the following loss function",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_45",
            "content": "L(M) = L map1 (M) + L map2 (M).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_46",
            "content": "\u03d5, \u03b8, and D are not optimized in this stage.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_47",
            "content": "In inference, the encoder of CVAE encodes x a with the condition c a to predict z a . Secondly, we map z a to M(z a ), and run SADTW to align the 7 During training, template pitch is extracted from the waveform corresponding to xp.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_48",
            "content": "amateur recordings with the template pitch curve. The template pitch curve can be derived from a reference recording with good intonation or a pitch predictor with the input of music notes. Then, we obtain \u0109p defined in Section 3.3 and send M(z a ) together with \u0109p in the decoder of CVAE to generate x. Finally, by running a pre-trained vocoder conditioned on x, a new beautified recording is produced.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_49",
            "content": "Model Structure",
            "ntype": "title",
            "meta": {
                "section": "3.5"
            }
        },
        {
            "ix": "80-ARR_v2_50",
            "content": "The encoder of CVAE consists of a 1-D convolutional layer (stride=4), an 8-layer WaveNet structure (Oord et al., 2016;Rethage et al., 2018) and 3 1-D convolutional layers (stride=2) with ReLU activation function and batch normalization followed by a mean pooling, which outputs the mean and log scale standard deviation parameters in the posterior distribution of z. The decoder of CVAE consists of a 4-layer WaveNet structure and a 1-D convolutional layer, which outputs the mel-spectrogram with 80 channels. The discriminator adopts the same structure as , which consists of multiple random window discriminators. The latent-mapping function is composed of 2 linear layers to encode the vocal timbre as the mapping condition, and 3 linear layers to map z a . The pitch encoder is composed of 3 convolutional layers. In addition, given a singing recording, 1) to obtain its content vectors, we train an Automatic Speech Recognition (ASR) model based on Conformer (Gulati et al., 2020) with both speech and singing data, and extract the hidden states from the ASR encoder (viewed as the content encoder) output as the linguistic content information, which are also called phonetic posterior-grams (PPG); 2) to obtain the vocal timbre, we leverage the open-source API resemblyzer 8 as the timbre encoder, which is a deep learning model designed for speaker verification (Wan et al., 2018), to extract the identity information of a singer. More details of model structure can be found in Appendix A.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_51",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "80-ARR_v2_52",
            "content": "Experimental Setup",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "80-ARR_v2_53",
            "content": "In this section, we first introduce PopBuTFy, the dataset for SVB, and then describe the implementation details in our work. Finally, we explain the evaluation method we adopt in this paper.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_54",
            "content": "Dataset Since there is no publicly available highquality, unaccompanied and parallel singing dataset for the SVB task, we collect and annotate a dataset containing both Chinese Mandarin and English pop songs: PopBuTFy. To collect PopBuTFy for SVB, the qualified singers majoring in vocal music are asked to sing a song twice, using the amateur vocal tone for one time and the professional vocal tone for another. Note that some of the amateur recordings are sung off-key by one or more semi-tones for the pitch correction sub-task. The parallel setting could make sure that the personal vocal timbre will keep still during the beautification process. In all, PopBuTFy consists of 99 Chinese pop songs (\u223c10.4 hours in total) from 12 singers and 443 English pop songs (\u223c40.4 hours in total) from 22 singers. All the audio files are recorded in a professional recording studio by qualified singers, male and female. Every song is sampled at 22050 Hz with 16-bit quantization. We randomly choose 6 songs in Chinese and 18 songs in English (from unseen speakers) for validation and test. For subjective evaluations, we choose 60 samples in the test set from different singers, half in Chinese and English. All testing samples are included for objective evaluations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_55",
            "content": "Implementation Details We train the Neural Singing Beautifier on a single 32G Nividia V100 GPU with the batch size of 64 sentences for both 100k steps in Stage 1 and Stage 2 respectively. Besides PopBuTFy, we pre-train the ASR model (used for PPG extraction) leveraging the extra speech datasets: AISHELL-3 (Yao Shi et al., 2020) for Chinese and LibriTTS (Zen et al., 2019) for English. For the semi-supervised learning mentioned in Section 1 and Section 3.4, we leverage an internal Chinese singing dataset (\u223c30 hours without labeled vocal tone) in the first training stage described in Section 3.4 for Chinese experiments. The output melspectrograms of our model are transformed into audio samples using a HiFi-GAN vocoder (Kong et al., 2020) trained with singing data in advance. We set the \u03bb metioned in Section 3.3 to 0.1. We transform the raw waveform with the sampling rate 22050 Hz into mel-spectrograms with the frame size 1024 and the hop size 128. We extract F 0 (fundamental frequency) as pitch information from the raw waveform using Parselmouth 9 , following Wu and Luan (2020); Blaauw and Bonada (2020); Ren et al. (2020). To obtain the ground truth pitch alignment between the amateur recordings and the professional ones for evaluating the accuracy of pitch alignment algorithm, we run the Montreal Forced Aligner tool (McAuliffe et al., 2017) on all the singing recordings to obtain their alignments to lyrics. Then the ground-truth pitch alignment can be derived since the lyrics are shared in a pair of data in PopBuTFy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_56",
            "content": "Performance Evaluation We employ both subjective metrics: Mean Opinion Score (MOS), Comparison Mean Opinion Score (CMOS), and an objective metric: Mean Cepstral Distortion (MCD) to evaluate the audio quality on the test-set. Besides, we use F0 Root Mean Square Error (F0 RMSE) and Pitch Alignment Accuracy (PAA) to estimate the pitch correction performance. For audio, we analyze the MOS and CMOS in two aspects: audio quality (naturalness, pronunciation and sound quality) and vocal tone quality. MOS-Q/CMOS-Q and MOS-V/CMOS-V correspond to the MOS/CMOS of audio quality and vocal tone quality respectively. More details about subjective evaluations are placed in Appendix C.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_57",
            "content": "Main Results",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "80-ARR_v2_58",
            "content": "In this section, we conduct extensive experiments to present our proposed model in regard to 1) the performance of pitch conversion; 2) the audio quality and vocal tone quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_59",
            "content": "Pitch Correction",
            "ntype": "title",
            "meta": {
                "section": "4.2.1"
            }
        },
        {
            "ix": "80-ARR_v2_60",
            "content": "Firstly, we provide the comparison among timewarping algorithms in terms of PAA in Table 1. Normed DTW means two pitch curves will be normalized before running DTW (M\u00fcller, 2007); CTW means the Canonical Time Warping (Zhou and Torre, 2009), which is used for pitch correction in Luo et al. (2018). It can be seen that, SADTW surpasses existing methods by a large margin. We also visualize an alignment example of DTW, CTW, and SADTW in Figure 4.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_61",
            "content": "Secondly, to check whether the amateur recordings are corrected to the good intonation after being beautified by NSVB, we calculate the F0 RMSE metric of the amateur recordings and the audio generated by NSVB, and list the results in Table 2. We can see that F0 RMSE has been improved significantly, which means NSVB successfully achieve pitch correction. To thoroughly evaluate our proposed model in audio quality and vocal tone quality, we compare subjective metric MOS-Q, MOS-V and objective metric MCD of audio samples generated by NVSB with the systems including: 1) GT Mel, amateur (A) and professional (P) version, where we first convert ground truth audio into mel-spectrograms, and then convert the mel-spectrograms back to audio using HiFi-GAN introduced in Section 4.1; 2) Baseline: the baseline model for SVB based on WaveNet with the number of parameters similar to NSVB, which adopts the same pitch correction method (SADTW) as NSVB does, and takes in the condition \u0109p defined in Section 3.3 to generate the mel-spectrogram optimized by the L 1 distance to x p . MCD is calculated using the audio samples of GT Mel P as references.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_62",
            "content": "The subjective and objective results on both Chinese and English datasets are shown in Table 3. We can see that 1) NSVB achieves promising results, with MOS-Q being less than those for ground truth professional recordings by only 0.1 and 0.12 on both datasets; 2) NSVB surpasses the GT Mel A in terms of MOS-V by a large margin, which indicates that NSVB successfully accomplishes the vocal tone improvement. 3) NSVB surpasses the baseline model on all the metrics distinctly, which proves the superiority of our proposed model; 4) GT Mel P, NSVB and Baseline all outperform GT Mel A in terms of MOS-V, which demonstrates that the proposed dataset PopBuTFy is reasonably labeled in respect of vocal tone.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_63",
            "content": "Ablation Studies",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "80-ARR_v2_64",
            "content": "We conduct some ablation studies to demonstrate the effectiveness of our proposed methods and some designs in our model, including latentmapping, additional loss L map2 in the second training stage, and semi-supervised learning with extra unpaired, unlabeled data on Chinese songs. As shown in Table 5, all the compared metrics show the effectiveness of L map2 , which means that the additional loss L map2 is beneficial to optimizing the latent mapping function M, working as a complement to the basic loss L map1 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_65",
            "content": "The details of the adversarial discriminator, the content encoder, and WaveNet structure are shown in Figure 5, Figure 8, and Figure 6. The hidden size of CVAE model, latent variable and discriminator are 256, 128 and 128 respectively. We train NSVB on a single V100 32G GPU for almost 22 hours to finish two-stage training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_66",
            "content": "As shown in Figure 5, our multi-window discriminator consists of 2 unconditional discriminator parts with fixed window sizes. Each unconditional discriminator contains N layers of Conv units. In our model, we set N = 3. The Conv units are all 1-D convolutional networks with ReLU activation and spectral normalization. The outputs of these unconditional discriminators are then concatenated and linearly projected to form the output.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_67",
            "content": "As shown in Figure 6, the WaveNet unit used in the VAE encoder and decoder of NVSB consists of a 1D convolution layer with ReLU to preprocess the input, and a group of sub-layers with residual connection between adjacent ones. Each sub-layer contains a 1 \u00d7 1 convolutional layer to process the input condition and a 3 \u00d7 3 convolutional layer for We visualize four linear-spectrograms generated with the same content. It seems that the professional vocal tone is related to certain patterns in the high-frequency region of the spectrograms. In the future, SVB may be accomplished in a more fine-grained way together with the knowledge in vocal music.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_68",
            "content": "During testing, each audio sample is listened to by at least 10 qualified testers, all majoring in vocal music. We tell all testers to focus on one aspect and ignore the other aspect when scoring MOS/CMOS of each aspect. For MOS, each tester is asked to evaluate the subjective naturalness of a sentence on a 1-5 Likert scale. For CMOS, listeners are asked to compare pairs of audio generated by systems A and B and indicate which of the two audio they prefer and choose one of the following scores: 0 indicating no difference, 1 indicating small difference, 2 indicating a large difference. For audio quality evaluation (MOS-Q and CMOS-Q), we tell listeners to \"focus on examining the naturalness, pronunciation and sound quality, and ignore the differences of singing vocal tone\". For vocal tone evaluations (MOS-V and CMOS-V), we tell listeners to \"focus on examining singing vocal tone of the song, and ignore the differences of audio quality (e.g., environmental noise, timbre)\". We split evaluations for main experiments and ablation studies into several groups for them. They are asked to take a break for 15 minutes between each group of experiments to remain focused during subjective evaluations. All testers get reasonably paid.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_69",
            "content": "SADTW is a kind of advanced APC method, which is designed for fine-tuning the amateur recording, but not for the case when the amateur recordings are completely out of tune. In the latter case, we recommend people to use Singing Voice Synthesis (synthesizing waveform from PPG and MIDI) + Singing Voice Conversion (converting the vocal timbre of the synthesized waveform into the user's), or some Speech to Singing (STS) methods. In addition, SADTW provides a score representing the similarity of two pitch curves, which could be used to determine what kind of SVB solution should be chosen.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_70",
            "content": "This work develops a possible automatic way for singing voice beautification, which may cause unemployment for people with related occupations. In addition, there is the potential for harm from piracy and abuse of our released recordings. Thus, we choose the dataset license: CC by-nc-sa 4.0.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_71",
            "content": "F.1 For every submission F.1.1 Did you discuss the limitations of your work?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_72",
            "content": "Yes. Appendix D.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "80-ARR_v2_73",
            "content": "Taylor Berg, - Kirkpatrick, Dan Klein, Gpufriendly local regression for voice conversion, 2015, Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Taylor Berg",
                    "- Kirkpatrick",
                    "Dan Klein"
                ],
                "title": "Gpufriendly local regression for voice conversion",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_74",
            "content": "Merlijn Blaauw, Jordi Bonada, Sequence-tosequence singing synthesis using the feed-forward transformer, 2020, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Merlijn Blaauw",
                    "Jordi Bonada"
                ],
                "title": "Sequence-tosequence singing synthesis using the feed-forward transformer",
                "pub_date": "2020",
                "pub_title": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "80-ARR_v2_75",
            "content": "Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang, Conformer: Convolution-augmented transformer for speech recognition, 2020-10-29, Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Anmol Gulati",
                    "James Qin",
                    "Chung-Cheng Chiu",
                    "Niki Parmar",
                    "Yu Zhang",
                    "Jiahui Yu",
                    "Wei Han",
                    "Shibo Wang",
                    "Zhengdong Zhang",
                    "Yonghui Wu",
                    "Ruoming Pang"
                ],
                "title": "Conformer: Convolution-augmented transformer for speech recognition",
                "pub_date": "2020-10-29",
                "pub_title": "Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_76",
            "content": "Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka, Nobukatsu Hojo, Cyclegan-vc2: Improved cyclegan-based non-parallel voice conversion, 2019, ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Takuhiro Kaneko",
                    "Hirokazu Kameoka",
                    "Kou Tanaka",
                    "Nobukatsu Hojo"
                ],
                "title": "Cyclegan-vc2: Improved cyclegan-based non-parallel voice conversion",
                "pub_date": "2019",
                "pub_title": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "80-ARR_v2_77",
            "content": "P Diederik, Max Kingma,  Welling, Autoencoding variational bayes, 2014-04-14, 2nd International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "P Diederik",
                    "Max Kingma",
                    " Welling"
                ],
                "title": "Autoencoding variational bayes",
                "pub_date": "2014-04-14",
                "pub_title": "2nd International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_78",
            "content": "Kazuhiro Kobayashi, Tomoki Toda, Graham Neubig, Sakriani Sakti, Satoshi Nakamura, Statistical singing voice conversion based on direct waveform modification with global variance, 2015, Sixteenth Annual Conference of the International Speech Communication Association, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Kazuhiro Kobayashi",
                    "Tomoki Toda",
                    "Graham Neubig",
                    "Sakriani Sakti",
                    "Satoshi Nakamura"
                ],
                "title": "Statistical singing voice conversion based on direct waveform modification with global variance",
                "pub_date": "2015",
                "pub_title": "Sixteenth Annual Conference of the International Speech Communication Association",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_79",
            "content": "Jungil Kong, Jaehyeon Kim, Jaekyoung Bae, Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis, 2020, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Jungil Kong",
                    "Jaehyeon Kim",
                    "Jaekyoung Bae"
                ],
                "title": "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "80-ARR_v2_80",
            "content": "Juheon Lee, Hyeong-Seok Choi, Chang-Bin Jeon, Junghyun Koo, Kyogu Lee, Adversarially trained end-to-end korean singing voice synthesis system, 2019, Proc. Interspeech, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Juheon Lee",
                    "Hyeong-Seok Choi",
                    "Chang-Bin Jeon",
                    "Junghyun Koo",
                    "Kyogu Lee"
                ],
                "title": "Adversarially trained end-to-end korean singing voice synthesis system",
                "pub_date": "2019",
                "pub_title": "Proc. Interspeech",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_81",
            "content": "Zhonghao Li, Benlai Tang, Xiang Yin, Yuan Wan, Ling Xu, Chen Shen, Zejun Ma, Ppg-based singing voice conversion with adversarial representation learning, 2021, ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Zhonghao Li",
                    "Benlai Tang",
                    "Xiang Yin",
                    "Yuan Wan",
                    "Ling Xu",
                    "Chen Shen",
                    "Zejun Ma"
                ],
                "title": "Ppg-based singing voice conversion with adversarial representation learning",
                "pub_date": "2021",
                "pub_title": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "80-ARR_v2_82",
            "content": "UNKNOWN, None, 2021, Diffsinger: Singing voice synthesis via shallow diffusion mechanism, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Diffsinger: Singing voice synthesis via shallow diffusion mechanism",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_83",
            "content": "UNKNOWN, None, 2021, Diffsvc: A diffusion probabilistic model for singing voice conversion, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Diffsvc: A diffusion probabilistic model for singing voice conversion",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_84",
            "content": "Peiling Lu, Jie Wu, Jian Luan, Xu Tan, Li Zhou, Xiaoicesing: A high-quality and integrated singing voice synthesis system, 2020, Proc. Interspeech 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Peiling Lu",
                    "Jie Wu",
                    "Jian Luan",
                    "Xu Tan",
                    "Li Zhou"
                ],
                "title": "Xiaoicesing: A high-quality and integrated singing voice synthesis system",
                "pub_date": "2020",
                "pub_title": "Proc. Interspeech 2020",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_85",
            "content": "Yin-Jyun Luo, Ming-Tso Chen, Tai-Shih Chi, Li Su, Singing voice correction using canonical time warping, 2018, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Yin-Jyun Luo",
                    "Ming-Tso Chen",
                    "Tai-Shih Chi",
                    "Li Su"
                ],
                "title": "Singing voice correction using canonical time warping",
                "pub_date": "2018",
                "pub_title": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "80-ARR_v2_86",
            "content": "Xudong Mao, Qing Li, Haoran Xie, Y Raymond, Zhen Lau, Stephen Wang,  Smolley, Least squares generative adversarial networks, 2017, Proceedings of the IEEE international conference on computer vision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Xudong Mao",
                    "Qing Li",
                    "Haoran Xie",
                    "Y Raymond",
                    "Zhen Lau",
                    "Stephen Wang",
                    " Smolley"
                ],
                "title": "Least squares generative adversarial networks",
                "pub_date": "2017",
                "pub_title": "Proceedings of the IEEE international conference on computer vision",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_87",
            "content": "Michael Mcauliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, Morgan Sonderegger, Montreal forced aligner: Trainable text-speech alignment using kaldi, 2017, Interspeech, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Michael Mcauliffe",
                    "Michaela Socolof",
                    "Sarah Mihuc",
                    "Michael Wagner",
                    "Morgan Sonderegger"
                ],
                "title": "Montreal forced aligner: Trainable text-speech alignment using kaldi",
                "pub_date": "2017",
                "pub_title": "Interspeech",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_88",
            "content": "Greg Mori, Serge Belongie, Jitendra Malik, Efficient shape matching using shape contexts, 2005, IEEE Transactions on Pattern Analysis and Machine Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Greg Mori",
                    "Serge Belongie",
                    "Jitendra Malik"
                ],
                "title": "Efficient shape matching using shape contexts",
                "pub_date": "2005",
                "pub_title": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_89",
            "content": "UNKNOWN, None, 2007, Dynamic time warping. Information retrieval for music and motion, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2007",
                "pub_title": "Dynamic time warping. Information retrieval for music and motion",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_90",
            "content": "Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu, Wavenet: A generative model for raw audio, 2016, 9th ISCA Speech Synthesis Workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Aaron Van Den Oord",
                    "Sander Dieleman",
                    "Heiga Zen",
                    "Karen Simonyan",
                    "Oriol Vinyals",
                    "Alex Graves",
                    "Nal Kalchbrenner",
                    "Andrew Senior",
                    "Koray Kavukcuoglu"
                ],
                "title": "Wavenet: A generative model for raw audio",
                "pub_date": "2016",
                "pub_title": "9th ISCA Speech Synthesis Workshop",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_91",
            "content": "UNKNOWN, None, 2021, Diffusion-based voice conversion with fast maximum likelihood sampling scheme, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Diffusion-based voice conversion with fast maximum likelihood sampling scheme",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_92",
            "content": "Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, Mark Hasegawa-Johnson, Autovc: Zeroshot voice style transfer only autoencoder loss, 2019, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Kaizhi Qian",
                    "Yang Zhang",
                    "Shiyu Chang",
                    "Xuesong Yang",
                    "Mark Hasegawa-Johnson"
                ],
                "title": "Autovc: Zeroshot voice style transfer only autoencoder loss",
                "pub_date": "2019",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "80-ARR_v2_93",
            "content": "Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, Mark Hasegawa-Johnson, AutoVC: Zeroshot voice style transfer with only autoencoder loss, 2019, Proceedings of the 36th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Kaizhi Qian",
                    "Yang Zhang",
                    "Shiyu Chang",
                    "Xuesong Yang",
                    "Mark Hasegawa-Johnson"
                ],
                "title": "AutoVC: Zeroshot voice style transfer with only autoencoder loss",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 36th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "80-ARR_v2_94",
            "content": "UNKNOWN, None, 2020, Deepsinger: Singing voice synthesis with data mined from the web, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Deepsinger: Singing voice synthesis with data mined from the web",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_95",
            "content": "Dario Rethage, Jordi Pons, Xavier Serra, A wavenet for speech denoising, 2018, IEEE International Conference on Acoustics, Speech and Signal Processing, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Dario Rethage",
                    "Jordi Pons",
                    "Xavier Serra"
                ],
                "title": "A wavenet for speech denoising",
                "pub_date": "2018",
                "pub_title": "IEEE International Conference on Acoustics, Speech and Signal Processing",
                "pub": "IEEE"
            }
        },
        {
            "ix": "80-ARR_v2_96",
            "content": "Sebastian Rosenzweig, Simon Schw\u00e4r, Jonathan Driedger, Meinard M\u00fcller, Adaptive pitchshifting with applications to intonation adjustment in a cappella recordings, 2021, Proceedings of the International Conference on Digital Audio Effects (DAFx), .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Sebastian Rosenzweig",
                    "Simon Schw\u00e4r",
                    "Jonathan Driedger",
                    "Meinard M\u00fcller"
                ],
                "title": "Adaptive pitchshifting with applications to intonation adjustment in a cappella recordings",
                "pub_date": "2021",
                "pub_title": "Proceedings of the International Conference on Digital Audio Effects (DAFx)",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_97",
            "content": "Joan Serr\u00e0, Santiago Pascual, Carlos Segura Perales, Blow: a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion, 2019, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Joan Serr\u00e0",
                    "Santiago Pascual",
                    "Carlos Segura Perales"
                ],
                "title": "Blow: a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion",
                "pub_date": "2019",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_98",
            "content": "Berrak Sisman, Haizhou Li, Generative adversarial networks for singing voice conversion with and without parallel data, 2020, Speaker Odyssey, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Berrak Sisman",
                    "Haizhou Li"
                ],
                "title": "Generative adversarial networks for singing voice conversion with and without parallel data",
                "pub_date": "2020",
                "pub_title": "Speaker Odyssey",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_99",
            "content": "UNKNOWN, None, 2019, Singan: Singing voice conversion with generative adversarial networks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Singan: Singing voice conversion with generative adversarial networks",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_100",
            "content": "UNKNOWN, None, , Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "80-ARR_v2_101",
            "content": "Kihyuk Sohn, Honglak Lee, Xinchen Yan, Learning structured output representation using deep conditional generative models, 2015-12-07, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Kihyuk Sohn",
                    "Honglak Lee",
                    "Xinchen Yan"
                ],
                "title": "Learning structured output representation using deep conditional generative models",
                "pub_date": "2015-12-07",
                "pub_title": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_102",
            "content": "David Sundermann, Hermann Ney, Vtlnbased voice conversion, 2003, Proceedings of the 3rd IEEE International Symposium on Signal Processing and Information Technology, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "David Sundermann",
                    "Hermann Ney"
                ],
                "title": "Vtlnbased voice conversion",
                "pub_date": "2003",
                "pub_title": "Proceedings of the 3rd IEEE International Symposium on Signal Processing and Information Technology",
                "pub": "IEEE"
            }
        },
        {
            "ix": "80-ARR_v2_103",
            "content": "Fernando Villavicencio, Jordi Bonada, Applying voice conversion to concatenative singing-voice synthesis, 2010, Eleventh annual conference of the international speech communication association, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Fernando Villavicencio",
                    "Jordi Bonada"
                ],
                "title": "Applying voice conversion to concatenative singing-voice synthesis",
                "pub_date": "2010",
                "pub_title": "Eleventh annual conference of the international speech communication association",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_104",
            "content": "Yusuke Wada, Yoshiaki Bando, Eita Nakamura, Katsutoshi Itoyama, Kazuyoshi Yoshii, An adaptive karaoke system that plays accompaniment parts of music audio signals synchronously with users'singing voices, 2017, SMC, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Yusuke Wada",
                    "Yoshiaki Bando",
                    "Eita Nakamura",
                    "Katsutoshi Itoyama",
                    "Kazuyoshi Yoshii"
                ],
                "title": "An adaptive karaoke system that plays accompaniment parts of music audio signals synchronously with users'singing voices",
                "pub_date": "2017",
                "pub_title": "SMC",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_105",
            "content": "Sanna Wager, George Tzanetakis, Minje Cheng-I Wang,  Kim, Deep autotuner: A pitch correcting network for singing performances, 2020, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Sanna Wager",
                    "George Tzanetakis",
                    "Minje Cheng-I Wang",
                    " Kim"
                ],
                "title": "Deep autotuner: A pitch correcting network for singing performances",
                "pub_date": "2020",
                "pub_title": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "80-ARR_v2_106",
            "content": "Li Wan, Quan Wang, Alan Papir, Ignacio Moreno, Generalized end-to-end loss for speaker verification, 2018, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Li Wan",
                    "Quan Wang",
                    "Alan Papir",
                    "Ignacio Moreno"
                ],
                "title": "Generalized end-to-end loss for speaker verification",
                "pub_date": "2018",
                "pub_title": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "80-ARR_v2_107",
            "content": "UNKNOWN, None, 2021, Towards high-fidelity singing voice conversion with acoustic reference and contrastive predictive coding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Towards high-fidelity singing voice conversion with acoustic reference and contrastive predictive coding",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_108",
            "content": "UNKNOWN, None, 2021, Vqmivc: Vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Vqmivc: Vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_109",
            "content": "UNKNOWN, None, 2020, Adversarially trained multi-singer sequence-to-sequence singing synthesizer, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Adversarially trained multi-singer sequence-to-sequence singing synthesizer",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_110",
            "content": "UNKNOWN, None, 2020, Aishell-3: A multi-speaker mandarin tts corpus and the baselines, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Aishell-3: A multi-speaker mandarin tts corpus and the baselines",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_111",
            "content": "Sangeon Yong, Juhan Nam, Singing expression transfer from one voice to another for a given song, 2018, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Sangeon Yong",
                    "Juhan Nam"
                ],
                "title": "Singing expression transfer from one voice to another for a given song",
                "pub_date": "2018",
                "pub_title": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "80-ARR_v2_112",
            "content": "Siyang Yuan, Pengyu Cheng, Ruiyi Zhang, Weituo Hao, Zhe Gan, Lawrence Carin, Improving zero-shot voice style transfer via disentangled representation learning, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Siyang Yuan",
                    "Pengyu Cheng",
                    "Ruiyi Zhang",
                    "Weituo Hao",
                    "Zhe Gan",
                    "Lawrence Carin"
                ],
                "title": "Improving zero-shot voice style transfer via disentangled representation learning",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_113",
            "content": "Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron Weiss, Ye Jia, Zhifeng Chen, Yonghui Wu, Libritts: A corpus derived from librispeech for textto-speech, 2019-09-19, Interspeech 2019, 20th Annual Conference of the International Speech Communication Association, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Heiga Zen",
                    "Viet Dang",
                    "Rob Clark",
                    "Yu Zhang",
                    "Ron Weiss",
                    "Ye Jia",
                    "Zhifeng Chen",
                    "Yonghui Wu"
                ],
                "title": "Libritts: A corpus derived from librispeech for textto-speech",
                "pub_date": "2019-09-19",
                "pub_title": "Interspeech 2019, 20th Annual Conference of the International Speech Communication Association",
                "pub": null
            }
        },
        {
            "ix": "80-ARR_v2_114",
            "content": "Liqiang Zhang, Chengzhu Yu, Heng Lu, Chao Weng, Chunlei Zhang, Yusong Wu, Xiang Xie, Zijin Li, Dong Yu, Durian-sc: Duration informed attention network based singing voice conversion system, 2020-10-29, Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Liqiang Zhang",
                    "Chengzhu Yu",
                    "Heng Lu",
                    "Chao Weng",
                    "Chunlei Zhang",
                    "Yusong Wu",
                    "Xiang Xie",
                    "Zijin Li",
                    "Dong Yu"
                ],
                "title": "Durian-sc: Duration informed attention network based singing voice conversion system",
                "pub_date": "2020-10-29",
                "pub_title": "Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "80-ARR_v2_0@0",
            "content": "Learning the Beauty in Songs: Neural Singing Voice Beautifier",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_0",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_2@0",
            "content": "We are interested in a novel task, singing voice beautification (SVB).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_2",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_2@1",
            "content": "Given the singing voice of an amateur singer, SVB aims to improve the intonation and vocal tone of the voice, while keeping the content and vocal timbre.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_2",
            "start": 71,
            "end": 223,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_2@2",
            "content": "Current automatic pitch correction techniques are immature, and most of them are restricted to intonation but ignore the overall aesthetic quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_2",
            "start": 225,
            "end": 371,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_2@3",
            "content": "Hence, we introduce Neural Singing Voice Beautifier (NSVB), the first generative model to solve the SVB task, which adopts a conditional variational autoencoder as the backbone and learns the latent representations of vocal tone.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_2",
            "start": 373,
            "end": 601,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_3@0",
            "content": "In NSVB, we propose a novel time-warping approach for pitch correction: Shape-Aware Dynamic Time Warping (SADTW), which ameliorates the robustness of existing time-warping approaches, to synchronize the amateur recording with the template pitch curve.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_3",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_3@1",
            "content": "Furthermore, we propose a latent-mapping algorithm in the latent space to convert the amateur vocal tone to the professional one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_3",
            "start": 252,
            "end": 380,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_3@2",
            "content": "To achieve this, we also propose a new dataset containing parallel singing recordings of both amateur and professional versions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_3",
            "start": 382,
            "end": 509,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_3@3",
            "content": "Extensive experiments on both Chinese and English songs demonstrate the effectiveness of our methods in terms of both objective and subjective metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_3",
            "start": 511,
            "end": 661,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_3@4",
            "content": "Audio samples are available at https://neuralsvb.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_3",
            "start": 663,
            "end": 711,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_3@5",
            "content": "github.io.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_3",
            "start": 713,
            "end": 722,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_3@6",
            "content": "Codes: https://github. com/MoonInTheRiver/NeuralSVB.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_3",
            "start": 724,
            "end": 775,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_4@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_4",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_5@0",
            "content": "The major successes of the artificial intelligent singing voice research are primarily in Singing Voice Synthesis (SVS) (Lee et al., 2019;Blaauw and Bonada, 2020;Ren et al., 2020;Liu et al., 2021a) and Singing Voice Conversion (SVC) (Sisman and Li, 2020;Li et al., 2021;Wang et al., 2021a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_5",
            "start": 0,
            "end": 289,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_5@1",
            "content": "However, the Singing Voice Beautification (SVB) remains an important and challenging endeavor for researchers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_5",
            "start": 291,
            "end": 400,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_5@2",
            "content": "SVB aims to improve the intonation 1 and the vocal tone of the voice, while keeping the content and vocal timbre 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_5",
            "start": 402,
            "end": 517,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_5@3",
            "content": "SVB is extensively required both in the professional recording studios and the entertainment industries in our daily life, since it is impractical to record flawless singing audio.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_5",
            "start": 519,
            "end": 698,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_6@0",
            "content": "Nowadays in real-life scenarios, SVB is usually performed by professional sound engineers with adequate domain knowledge, who manipulate commercial vocal correction tools such as Melodyne 3 and Autotune 4 (Yong and Nam, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_6",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_6@1",
            "content": "Most current automatic pitch correction works are shown to be an attractive alternative, but they may 1) show weak alignment accuracy (Luo et al., 2018) or pitch accuracy (Wager et al., 2020); 2) cause the tuned recording and the reference recording to be homogeneous in singing style (Yong and Nam, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_6",
            "start": 227,
            "end": 532,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_6@2",
            "content": "Besides, they typically focus on the intonation but ignore the overall aesthetic quality (audio quality and vocal tone) (Rosenzweig et al., 2021;Zhuang et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_6",
            "start": 534,
            "end": 699,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_7@0",
            "content": "To tackle these challenges, we introduce Neural Singing Voice Beautifier (NSVB), the first generative model to solve the SVB task, which adopts a Conditional Variational AutoEncoder (CVAE) (Kingma and Welling, 2014;Sohn et al., 2015) as the backbone to generate high-quality audio and learns the latent representation of vocal tone.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_7",
            "start": 0,
            "end": 331,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_7@1",
            "content": "In NSVB, we dichotomize the SVB task into pitch correction and vocal tone improvement: 1) To correct the intonation, a straightforward method is aligning the amateur recording with the template pitch curve, and then putting them together to resynthesize a new singing sample.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_7",
            "start": 333,
            "end": 607,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_7@2",
            "content": "Previous works (Wada et al., 2017;Luo et al., 2018) implemented this by figuring out the alignment through Dynamic Time Warping (DTW) (M\u00fcller, 2007) or Canonical Time Warping (CTW) (Zhou and Torre, 2009).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_7",
            "start": 609,
            "end": 812,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_7@3",
            "content": "We propose a novel Shape-Aware DTW algorithm, which ameliorates the robustness of existing time-warping approaches by considering the shape of the pitch curve rather than low-level features when calculating the optimal alignment path.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_7",
            "start": 814,
            "end": 1047,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_8@0",
            "content": "2) To improve the vocal tone, we propose a latentmapping algorithm in the latent space, which converts the latent variables of the amateur vocal tone to those of the professional ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_8",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_8@1",
            "content": "This process is optimized by maximizing the log-likelihood of the converted latent variables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_8",
            "start": 185,
            "end": 277,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_8@2",
            "content": "To retain the vocal timbre during the vocal tone mapping, we also propose a new dataset named PopBuTFy containing parallel singing recordings of both amateur and professional versions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_8",
            "start": 279,
            "end": 462,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_8@3",
            "content": "Besides, thanks to the autoencoder structure, NSVB inherently supports semisupervised learning, where the additional unpaired, unlabeled 5 singing data could be leveraged to facilitate the learning of the latent representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_8",
            "start": 464,
            "end": 691,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_8@4",
            "content": "Extensive experiments on both Chinese and English songs show that NSVB outperforms previous methods by a notable margin, and each component in NSVB is effective, in terms of both objective and subjective metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_8",
            "start": 693,
            "end": 904,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_8@5",
            "content": "The main contributions of this work are summarized as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_8",
            "start": 906,
            "end": 967,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_9@0",
            "content": "\u2022 We propose the first generative model NSVB to solve the SVB task. NSVB not only corrects the pitch of amateur recordings, but also generates the audio with high audio quality and improved vocal tone, to which previous works typically pay little attention. \u2022 We propose Shape-Aware Dynamic Time Warping (SADTW) algorithm to synchronize the amateur recording with the template pitch curve, which ameliorates the robustness of the previous time-warping algorithm. \u2022 We propose a latent-mapping algorithm to convert the latent variable of the amateur vocal tone to the professional one's, and contribute a new dataset PopBuTFyto train the latent-mapping function. \u2022 We design NSVB as a CVAE model, which supports the semi-supervised learning to leverage unpaired, unlabeled singing data for better performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_9",
            "start": 0,
            "end": 807,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_10@0",
            "content": "2 Related Works",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_10",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_11@0",
            "content": "Singing Voice Conversion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_11",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_12@0",
            "content": "Singing Voice Conversion (SVC) is a sub-task of Voice Conversion (VC) (Berg-Kirkpatrick and Klein, 2015;Serr\u00e0 et al., 2019;Popov et al., 2021;, which transforms the vocal timbre (or singer identity) of one singer to that of another singer, while preserving the linguistic content and pitch/melody information (Li et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_12",
            "start": 0,
            "end": 326,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_13@0",
            "content": "Mainstream SVC models can be grouped into three categories (Zhao et al., 2020):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_13",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_13@1",
            "content": "1) parallel spectral feature mapping models, which learn the conversion function between source and target singers relying on parallel singing data (Villavicencio and Bonada, 2010;Kobayashi et al., 2015;Sisman et al., 2019);",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_13",
            "start": 80,
            "end": 303,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_13@2",
            "content": "2) Cycle-consistent Generative Adversarial Networks (CycleGAN) (Zhu et al., 2017;Kaneko et al., 2019), where an adversarial loss and a cycleconsistency loss are concurrently used to learn the forward and inverse mappings simultaneously (Sisman and Li, 2020);",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_13",
            "start": 305,
            "end": 562,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_13@3",
            "content": "3) encoder-decoder models, such as PPG-SVC (Li et al., 2021), which leverage a singing voice synthesis (SVS) system for SVC , and auto-encoder (Qian et al., 2019a;Wang et al., 2021b;Yuan et al., 2020) based SVC (Wang et al., 2021a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_13",
            "start": 564,
            "end": 795,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_13@4",
            "content": "The models of the latter two categories can be utilized with nonparallel data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_13",
            "start": 797,
            "end": 874,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_13@5",
            "content": "In our work, we aim to convert the intonation and the vocal tone while keeping the content and the vocal timbre, which is quite different from the SVC task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_13",
            "start": 876,
            "end": 1031,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_14@0",
            "content": "Automatic Pitch Correction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_14",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_15@0",
            "content": "Automatic Pitch Correction (APC) works attempt to minimize the manual effort in modifying the flawed singing voice (Yong and Nam, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_15",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_15@1",
            "content": "Nam (2018) propose to modify pitch and energy information to improve the singing expressions of an amateur singing recording.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_15",
            "start": 137,
            "end": 261,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_15@2",
            "content": "However, this method heavily relies on a reference recording, causing the tuned recording and the reference recording to be homogeneous in singing style (Zhuang et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_15",
            "start": 263,
            "end": 437,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_15@3",
            "content": "Our work adopts the non-parametric and data-free pitch correction method like Luo et al. (2018), but improves the accuracy of alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_15",
            "start": 439,
            "end": 574,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_16@0",
            "content": "Methdology",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_16",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_17@0",
            "content": "In this section, we describe the overview of NSVB, which is shown in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_17",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_17@1",
            "content": "At Stage 1 in the figure, we reconstruct the input mel-spectrogram through the CVAE backbone (Section 3.1) based on the pitch, content and vocal timbre conditions extracted from the input by the pitch encoder, content encoder and timbre encoder, and optimize the CVAE by maximizing evidence lower bound and adversarial learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_17",
            "start": 79,
            "end": 406,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_17@2",
            "content": "At Stage 2/Inference in the figure, firstly we infer the latent variable z a based on the amateur conditions; secondly we prepare the amateur content vectors aligned with the professional pitch by SADTW algorithm (Section 3.2); thirdly we map z a to z p by the latent-mapping algorithm (Section 3.3); finally, we mix the professional pitch, the aligned amateur content vectors, and the amateur vocal timbre to obtain a new condition, which is leveraged along with the mapped z p by the decoder of CVAE to generate a new beautified mel-spectrogram.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_17",
            "start": 408,
            "end": 954,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_17@3",
            "content": "The training/inference details and model structure of each component in NSVB are described in Section 3.4 and Section 3.5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_17",
            "start": 956,
            "end": 1077,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_18@0",
            "content": "Conditional Variational Generator with Adversarial Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_18",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_19@0",
            "content": "As shown in Figure 2, to generate audio with high quality and learn the latent representations of vocal tone, we introduce a Conditional Variational AutoEncoder (CVAE) (Kingma and Welling, 2014;Sohn et al., 2015) as the mel-spectrogram generator, with the optimizing objective of maximizing the evidence lower bound (ELBO) of the intractable marginal log-likelihood of melspectrogram log p \u03b8 (x|c):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_19",
            "start": 0,
            "end": 397,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_20@0",
            "content": "log p \u03b8 (x|c) \u2265 ELBO(\u03d5, \u03b8) \u2261 E z\u223cq \u03d5 (z|x,c) log p \u03b8 (x|z, c) \u2212 log q \u03d5 (z|x, c) p(z) ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_20",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_21@0",
            "content": "where x, c, z denote the input/output melspectrogram, the mix of content, vocal timbre and pitch conditions, and the latent variable representing the vocal tone respectively; \u03d5 and \u03b8 denote the model parameters of CVAE encoder and CVAE decoder; q \u03d5 (z|x, c) is the posterior distribution approximated by the CVAE encoder; p \u03b8 (x|z, c) is the likelihood function that generates mel-spectrograms given latent variable z and condition c; p(z) is the prior distribution of the latent variables z, and we choose the standard normal distribution as p(z) for simplification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_21",
            "start": 0,
            "end": 566,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_21@1",
            "content": "Furthermore, to address the over-smoothing problem (Qian et al., 2019b) in CVAE, we utilize an adversarial discriminator (D) (Mao et al., 2017) to refine the output mel-spectrogram:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_21",
            "start": 568,
            "end": 748,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_22@0",
            "content": "L adv (\u03d5, \u03b8) = E[(D( x) \u2212 1) 2 ], L adv (D) = E[(D(x) \u2212 1) 2 ] + E[D( x) 2 ], (1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_22",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_23@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_23",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_24@0",
            "content": "where x is the ground-truth and x is the output of CVAE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_24",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_24@1",
            "content": "The descriptions for the model structure of each component are in Section 3.5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_24",
            "start": 57,
            "end": 134,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_25@0",
            "content": "Shape-Aware Dynamic Time Warping",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_25",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_26@0",
            "content": "To implement the pitch correction, a straightforward method is aligning the amateur recording with the template pitch curve, and then concatenating them to resynthesize a new singing sample with improved intonation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_26",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_26@1",
            "content": "Since the source pitch curve of amateur recordings and template one show a high degree of natural correlation along the time axis, applying a proper time-warping algorithm on them is crucial.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_26",
            "start": 216,
            "end": 406,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_26@2",
            "content": "However, original DTW (M\u00fcller, 2007) could result in a poor alignment when certain parts of the axis move to higher frequencies, and other parts to lower ones, or vice versa (Sundermann and Ney, 2003).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_26",
            "start": 408,
            "end": 608,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_26@3",
            "content": "Luo et al. (2018) adopt an advanced algorithm CTW (Zhou and Torre, 2009), which combines the canonical correlation analysis (CCA) and DTW to extract the feature sequences of two pitch curves, and then apply DTW on them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_26",
            "start": 610,
            "end": 828,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_26@4",
            "content": "However, the alignment accuracy of CTW leaves much to be desired.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_26",
            "start": 830,
            "end": 894,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_27@0",
            "content": "We elaborate a non-parametric and data-free algorithm, Shape-Aware DTW (SADTW), based on the prior knowledge that the source pitch curve and the template one have analogous local shape contours.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_27",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_27@1",
            "content": "Specifically, we replace the Euclidean distance in the original DTW distance matrix with the shape context descriptor distance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_27",
            "start": 195,
            "end": 321,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_27@2",
            "content": "The shape context descriptor of a time point f i in one pitch curve is illustrated in Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_27",
            "start": 323,
            "end": 417,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_27@3",
            "content": "Inspired by (Mori et al., 2005), we divide the data points around f i into m * n bins by m time windows and n angles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_27",
            "start": 419,
            "end": 535,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_27@4",
            "content": "We calculate the number of all points falling in the k-th bin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_27",
            "start": 537,
            "end": 598,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_27@5",
            "content": "Then the descriptor for f i is defined as the histogram h i \u2208 R m * n :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_27",
            "start": 600,
            "end": 670,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_28@0",
            "content": "h i (k) = |{f j \u0338 = f i , f j \u2208 bin(k)}|,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_28",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_29@0",
            "content": "where | \u2022 | means the cardinality of a set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_29",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_29@1",
            "content": "This histogram represents the distribution over relative positions, which is a robust, compact and discriminative descriptor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_29",
            "start": 44,
            "end": 168,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_29@2",
            "content": "Then, it is natural to use the X 2 -test statistic on this distribution descriptor as the \"distance\" of two points f a and f p :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_29",
            "start": 170,
            "end": 297,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_30@0",
            "content": "C(a, p) = 1 2 m * n k=1 [h a (k) \u2212 h p (k)] 2 h a (k) + h p (k) ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_30",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_31@0",
            "content": "where h a and h p are the normalized histograms corresponding to the point f a from the amateur pitch curve and the point f p from the template pitch curve.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_31",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_31@1",
            "content": "C(a, p) ranges from 0 to 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_31",
            "start": 157,
            "end": 183,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_31@2",
            "content": "Finally, we run DTW on the distance matrix C to obtain the alignment with least distance cost between two curves.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_31",
            "start": 185,
            "end": 297,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_32@0",
            "content": "4 windows",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_32",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_33@0",
            "content": "\ud835\udc53 ! 30\u00b0F",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_33",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_34@0",
            "content": "igure 3: The shape descriptor in SADTW.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_34",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_34@1",
            "content": "The blue curve represents pitch; the horizontal axis means time; the vertical axis means F0-frequency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_34",
            "start": 40,
            "end": 141,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_34@2",
            "content": "There are m = 4 windows, n = 6 angles to divide neighbor points of f i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_34",
            "start": 143,
            "end": 214,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_35@0",
            "content": "Latent-mapping Algorithm",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_35",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_36@0",
            "content": "Define a pair of mel-spectrograms (x a , x p ): the contents of x a and y p are the same sentence of a song from the same singer 6 , who sings these two recordings using the amateur tone and the professional tone respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_36",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_36@1",
            "content": "Given the CVAE model, we can infer the posterior distribution q \u03d5 (z a |x a , c a ) and q \u03d5 (z p |x p , c p ) corresponding to x a and x p through the encoder of CVAE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_36",
            "start": 227,
            "end": 393,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_36@2",
            "content": "To achieve the conversion of vocal tone, we introduce a mapping function M to convert the latent variables from q \u03d5 (z a |x a , c a ) to q \u03d5 (z p |x p , c p ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_36",
            "start": 395,
            "end": 553,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_36@3",
            "content": "Concretely, we sample a latent variable of amateur vocal tone z a from q \u03d5 (z a |x a , c a ), and map z a to M(z a ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_36",
            "start": 555,
            "end": 671,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_36@4",
            "content": "Then, M can be optimized by minimizing the negative log-likelihood of M(z a ):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_36",
            "start": 673,
            "end": 750,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_37@0",
            "content": "L map1 (M) = \u2212 log q \u03d5 (M(z a )|x p , c p ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_37",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_38@0",
            "content": "Define \u0109p as the mix of 1) the content vectors from the amateur recording aligned by SADTW, 2) vocal timbre embedding encoded by timbre encoder, and 3) template pitch 7 embeddings encoded by pitch encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_38",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_38@1",
            "content": "To make sure the converted latent variable could work well together with \u0109p to generate a high-quality audio sample (with the correct pitch and improved vocal tone), we send M(z a ) to the CVAE decoder to generate x, and propose an additional loss:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_38",
            "start": 206,
            "end": 453,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_39@0",
            "content": "L map2 (M) = \u2225x \u2212 x p \u2225 1 + \u03bb(D(x) \u2212 1) 2 ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_39",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_40@0",
            "content": "where D has been optimized by Eq. ( 1); \u03bb is a hyper-parameter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_40",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_41@0",
            "content": "Training and Inference",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_41",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_42@0",
            "content": "There are two training stages for NSVB: in the first training stage, we optimize CVAE by minimizing the following loss function",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_42",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_43@0",
            "content": "L(\u03d5, \u03b8) = \u2212ELBO(\u03d5, \u03b8) + \u03bbL adv (\u03d5, \u03b8),",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_43",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_44@0",
            "content": "and optimize the discriminator (D) by minimizing Eq. ( 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_44",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_44@1",
            "content": "Note that, the first stage is the reconstruction process of mel-spectrograms, where any unpaired, unlabeled singing data beyond Pop-BuTFy could be leveraged to facilitate the learning of the latent representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_44",
            "start": 59,
            "end": 272,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_44@2",
            "content": "In the second training stage, we optimize M on the parallel dataset Pop-BuTFy by minimizing the following loss function",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_44",
            "start": 274,
            "end": 392,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_45@0",
            "content": "L(M) = L map1 (M) + L map2 (M).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_45",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_46@0",
            "content": "\u03d5, \u03b8, and D are not optimized in this stage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_46",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_47@0",
            "content": "In inference, the encoder of CVAE encodes x a with the condition c a to predict z a . Secondly, we map z a to M(z a ), and run SADTW to align the 7 During training, template pitch is extracted from the waveform corresponding to xp.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_47",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_48@0",
            "content": "amateur recordings with the template pitch curve.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_48",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_48@1",
            "content": "The template pitch curve can be derived from a reference recording with good intonation or a pitch predictor with the input of music notes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_48",
            "start": 50,
            "end": 188,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_48@2",
            "content": "Then, we obtain \u0109p defined in Section 3.3 and send M(z a ) together with \u0109p in the decoder of CVAE to generate x. Finally, by running a pre-trained vocoder conditioned on x, a new beautified recording is produced.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_48",
            "start": 190,
            "end": 402,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_49@0",
            "content": "Model Structure",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_49",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_50@0",
            "content": "The encoder of CVAE consists of a 1-D convolutional layer (stride=4), an 8-layer WaveNet structure (Oord et al., 2016;Rethage et al., 2018) and 3 1-D convolutional layers (stride=2) with ReLU activation function and batch normalization followed by a mean pooling, which outputs the mean and log scale standard deviation parameters in the posterior distribution of z.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_50",
            "start": 0,
            "end": 365,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_50@1",
            "content": "The decoder of CVAE consists of a 4-layer WaveNet structure and a 1-D convolutional layer, which outputs the mel-spectrogram with 80 channels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_50",
            "start": 367,
            "end": 508,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_50@2",
            "content": "The discriminator adopts the same structure as , which consists of multiple random window discriminators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_50",
            "start": 510,
            "end": 614,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_50@3",
            "content": "The latent-mapping function is composed of 2 linear layers to encode the vocal timbre as the mapping condition, and 3 linear layers to map z a .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_50",
            "start": 616,
            "end": 759,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_50@4",
            "content": "The pitch encoder is composed of 3 convolutional layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_50",
            "start": 761,
            "end": 816,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_50@5",
            "content": "In addition, given a singing recording, 1) to obtain its content vectors, we train an Automatic Speech Recognition (ASR) model based on Conformer (Gulati et al., 2020) with both speech and singing data, and extract the hidden states from the ASR encoder (viewed as the content encoder) output as the linguistic content information, which are also called phonetic posterior-grams (PPG); 2) to obtain the vocal timbre, we leverage the open-source API resemblyzer 8 as the timbre encoder, which is a deep learning model designed for speaker verification (Wan et al., 2018), to extract the identity information of a singer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_50",
            "start": 818,
            "end": 1436,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_50@6",
            "content": "More details of model structure can be found in Appendix A.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_50",
            "start": 1438,
            "end": 1496,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_51@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_51",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_52@0",
            "content": "Experimental Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_52",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_53@0",
            "content": "In this section, we first introduce PopBuTFy, the dataset for SVB, and then describe the implementation details in our work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_53",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_53@1",
            "content": "Finally, we explain the evaluation method we adopt in this paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_53",
            "start": 125,
            "end": 189,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_54@0",
            "content": "Dataset Since there is no publicly available highquality, unaccompanied and parallel singing dataset for the SVB task, we collect and annotate a dataset containing both Chinese Mandarin and English pop songs: PopBuTFy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_54",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_54@1",
            "content": "To collect PopBuTFy for SVB, the qualified singers majoring in vocal music are asked to sing a song twice, using the amateur vocal tone for one time and the professional vocal tone for another.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_54",
            "start": 219,
            "end": 411,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_54@2",
            "content": "Note that some of the amateur recordings are sung off-key by one or more semi-tones for the pitch correction sub-task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_54",
            "start": 413,
            "end": 530,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_54@3",
            "content": "The parallel setting could make sure that the personal vocal timbre will keep still during the beautification process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_54",
            "start": 532,
            "end": 649,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_54@4",
            "content": "In all, PopBuTFy consists of 99 Chinese pop songs (\u223c10.4 hours in total) from 12 singers and 443 English pop songs (\u223c40.4 hours in total) from 22 singers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_54",
            "start": 651,
            "end": 804,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_54@5",
            "content": "All the audio files are recorded in a professional recording studio by qualified singers, male and female.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_54",
            "start": 806,
            "end": 911,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_54@6",
            "content": "Every song is sampled at 22050 Hz with 16-bit quantization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_54",
            "start": 913,
            "end": 971,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_54@7",
            "content": "We randomly choose 6 songs in Chinese and 18 songs in English (from unseen speakers) for validation and test.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_54",
            "start": 973,
            "end": 1081,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_54@8",
            "content": "For subjective evaluations, we choose 60 samples in the test set from different singers, half in Chinese and English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_54",
            "start": 1083,
            "end": 1199,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_54@9",
            "content": "All testing samples are included for objective evaluations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_54",
            "start": 1201,
            "end": 1259,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_55@0",
            "content": "Implementation Details We train the Neural Singing Beautifier on a single 32G Nividia V100 GPU with the batch size of 64 sentences for both 100k steps in Stage 1 and Stage 2 respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_55",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_55@1",
            "content": "Besides PopBuTFy, we pre-train the ASR model (used for PPG extraction) leveraging the extra speech datasets: AISHELL-3 (Yao Shi et al., 2020) for Chinese and LibriTTS (Zen et al., 2019) for English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_55",
            "start": 188,
            "end": 385,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_55@2",
            "content": "For the semi-supervised learning mentioned in Section 1 and Section 3.4, we leverage an internal Chinese singing dataset (\u223c30 hours without labeled vocal tone) in the first training stage described in Section 3.4 for Chinese experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_55",
            "start": 387,
            "end": 623,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_55@3",
            "content": "The output melspectrograms of our model are transformed into audio samples using a HiFi-GAN vocoder (Kong et al., 2020) trained with singing data in advance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_55",
            "start": 625,
            "end": 781,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_55@4",
            "content": "We set the \u03bb metioned in Section 3.3 to 0.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_55",
            "start": 783,
            "end": 826,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_55@5",
            "content": "We transform the raw waveform with the sampling rate 22050 Hz into mel-spectrograms with the frame size 1024 and the hop size 128.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_55",
            "start": 828,
            "end": 957,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_55@6",
            "content": "We extract F 0 (fundamental frequency) as pitch information from the raw waveform using Parselmouth 9 , following Wu and Luan (2020); Blaauw and Bonada (2020); Ren et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_55",
            "start": 959,
            "end": 1136,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_55@7",
            "content": "To obtain the ground truth pitch alignment between the amateur recordings and the professional ones for evaluating the accuracy of pitch alignment algorithm, we run the Montreal Forced Aligner tool (McAuliffe et al., 2017) on all the singing recordings to obtain their alignments to lyrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_55",
            "start": 1138,
            "end": 1427,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_55@8",
            "content": "Then the ground-truth pitch alignment can be derived since the lyrics are shared in a pair of data in PopBuTFy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_55",
            "start": 1429,
            "end": 1539,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_56@0",
            "content": "Performance Evaluation We employ both subjective metrics: Mean Opinion Score (MOS), Comparison Mean Opinion Score (CMOS), and an objective metric: Mean Cepstral Distortion (MCD) to evaluate the audio quality on the test-set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_56",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_56@1",
            "content": "Besides, we use F0 Root Mean Square Error (F0 RMSE) and Pitch Alignment Accuracy (PAA) to estimate the pitch correction performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_56",
            "start": 225,
            "end": 356,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_56@2",
            "content": "For audio, we analyze the MOS and CMOS in two aspects: audio quality (naturalness, pronunciation and sound quality) and vocal tone quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_56",
            "start": 358,
            "end": 496,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_56@3",
            "content": "MOS-Q/CMOS-Q and MOS-V/CMOS-V correspond to the MOS/CMOS of audio quality and vocal tone quality respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_56",
            "start": 498,
            "end": 607,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_56@4",
            "content": "More details about subjective evaluations are placed in Appendix C.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_56",
            "start": 609,
            "end": 675,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_57@0",
            "content": "Main Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_57",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_58@0",
            "content": "In this section, we conduct extensive experiments to present our proposed model in regard to 1) the performance of pitch conversion; 2) the audio quality and vocal tone quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_58",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_59@0",
            "content": "Pitch Correction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_59",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_60@0",
            "content": "Firstly, we provide the comparison among timewarping algorithms in terms of PAA in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_60",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_60@1",
            "content": "Normed DTW means two pitch curves will be normalized before running DTW (M\u00fcller, 2007); CTW means the Canonical Time Warping (Zhou and Torre, 2009), which is used for pitch correction in Luo et al. (2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_60",
            "start": 92,
            "end": 296,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_60@2",
            "content": "It can be seen that, SADTW surpasses existing methods by a large margin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_60",
            "start": 298,
            "end": 369,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_60@3",
            "content": "We also visualize an alignment example of DTW, CTW, and SADTW in Figure 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_60",
            "start": 371,
            "end": 444,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_61@0",
            "content": "Secondly, to check whether the amateur recordings are corrected to the good intonation after being beautified by NSVB, we calculate the F0 RMSE metric of the amateur recordings and the audio generated by NSVB, and list the results in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_61",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_61@1",
            "content": "We can see that F0 RMSE has been improved significantly, which means NSVB successfully achieve pitch correction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_61",
            "start": 243,
            "end": 354,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_61@2",
            "content": "To thoroughly evaluate our proposed model in audio quality and vocal tone quality, we compare subjective metric MOS-Q, MOS-V and objective metric MCD of audio samples generated by NVSB with the systems including:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_61",
            "start": 356,
            "end": 567,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_61@3",
            "content": "1) GT Mel, amateur (A) and professional (P) version, where we first convert ground truth audio into mel-spectrograms, and then convert the mel-spectrograms back to audio using HiFi-GAN introduced in Section 4.1;",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_61",
            "start": 569,
            "end": 779,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_61@4",
            "content": "2) Baseline: the baseline model for SVB based on WaveNet with the number of parameters similar to NSVB, which adopts the same pitch correction method (SADTW) as NSVB does, and takes in the condition \u0109p defined in Section 3.3 to generate the mel-spectrogram optimized by the L 1 distance to x p .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_61",
            "start": 781,
            "end": 1075,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_61@5",
            "content": "MCD is calculated using the audio samples of GT Mel P as references.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_61",
            "start": 1077,
            "end": 1144,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_62@0",
            "content": "The subjective and objective results on both Chinese and English datasets are shown in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_62",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_62@1",
            "content": "We can see that 1) NSVB achieves promising results, with MOS-Q being less than those for ground truth professional recordings by only 0.1 and 0.12 on both datasets; 2) NSVB surpasses the GT Mel A in terms of MOS-V by a large margin, which indicates that NSVB successfully accomplishes the vocal tone improvement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_62",
            "start": 96,
            "end": 407,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_62@2",
            "content": "3) NSVB surpasses the baseline model on all the metrics distinctly, which proves the superiority of our proposed model; 4) GT Mel P, NSVB and Baseline all outperform GT Mel A in terms of MOS-V, which demonstrates that the proposed dataset PopBuTFy is reasonably labeled in respect of vocal tone.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_62",
            "start": 409,
            "end": 703,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_63@0",
            "content": "Ablation Studies",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_63",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_64@0",
            "content": "We conduct some ablation studies to demonstrate the effectiveness of our proposed methods and some designs in our model, including latentmapping, additional loss L map2 in the second training stage, and semi-supervised learning with extra unpaired, unlabeled data on Chinese songs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_64",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_64@1",
            "content": "As shown in Table 5, all the compared metrics show the effectiveness of L map2 , which means that the additional loss L map2 is beneficial to optimizing the latent mapping function M, working as a complement to the basic loss L map1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_64",
            "start": 282,
            "end": 515,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_65@0",
            "content": "The details of the adversarial discriminator, the content encoder, and WaveNet structure are shown in Figure 5, Figure 8, and Figure 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_65",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_65@1",
            "content": "The hidden size of CVAE model, latent variable and discriminator are 256, 128 and 128 respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_65",
            "start": 136,
            "end": 234,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_65@2",
            "content": "We train NSVB on a single V100 32G GPU for almost 22 hours to finish two-stage training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_65",
            "start": 236,
            "end": 323,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_66@0",
            "content": "As shown in Figure 5, our multi-window discriminator consists of 2 unconditional discriminator parts with fixed window sizes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_66",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_66@1",
            "content": "Each unconditional discriminator contains N layers of Conv units.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_66",
            "start": 126,
            "end": 190,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_66@2",
            "content": "In our model, we set N = 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_66",
            "start": 192,
            "end": 218,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_66@3",
            "content": "The Conv units are all 1-D convolutional networks with ReLU activation and spectral normalization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_66",
            "start": 220,
            "end": 317,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_66@4",
            "content": "The outputs of these unconditional discriminators are then concatenated and linearly projected to form the output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_66",
            "start": 319,
            "end": 432,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_67@0",
            "content": "As shown in Figure 6, the WaveNet unit used in the VAE encoder and decoder of NVSB consists of a 1D convolution layer with ReLU to preprocess the input, and a group of sub-layers with residual connection between adjacent ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_67",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_67@1",
            "content": "Each sub-layer contains a 1 \u00d7 1 convolutional layer to process the input condition and a 3 \u00d7 3 convolutional layer for We visualize four linear-spectrograms generated with the same content.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_67",
            "start": 227,
            "end": 415,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_67@2",
            "content": "It seems that the professional vocal tone is related to certain patterns in the high-frequency region of the spectrograms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_67",
            "start": 417,
            "end": 538,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_67@3",
            "content": "In the future, SVB may be accomplished in a more fine-grained way together with the knowledge in vocal music.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_67",
            "start": 540,
            "end": 648,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_68@0",
            "content": "During testing, each audio sample is listened to by at least 10 qualified testers, all majoring in vocal music.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_68",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_68@1",
            "content": "We tell all testers to focus on one aspect and ignore the other aspect when scoring MOS/CMOS of each aspect.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_68",
            "start": 112,
            "end": 219,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_68@2",
            "content": "For MOS, each tester is asked to evaluate the subjective naturalness of a sentence on a 1-5 Likert scale.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_68",
            "start": 221,
            "end": 325,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_68@3",
            "content": "For CMOS, listeners are asked to compare pairs of audio generated by systems A and B and indicate which of the two audio they prefer and choose one of the following scores: 0 indicating no difference, 1 indicating small difference, 2 indicating a large difference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_68",
            "start": 327,
            "end": 590,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_68@4",
            "content": "For audio quality evaluation (MOS-Q and CMOS-Q), we tell listeners to \"focus on examining the naturalness, pronunciation and sound quality, and ignore the differences of singing vocal tone\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_68",
            "start": 592,
            "end": 781,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_68@5",
            "content": "For vocal tone evaluations (MOS-V and CMOS-V), we tell listeners to \"focus on examining singing vocal tone of the song, and ignore the differences of audio quality (e.g., environmental noise, timbre)\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_68",
            "start": 783,
            "end": 983,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_68@6",
            "content": "We split evaluations for main experiments and ablation studies into several groups for them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_68",
            "start": 985,
            "end": 1076,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_68@7",
            "content": "They are asked to take a break for 15 minutes between each group of experiments to remain focused during subjective evaluations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_68",
            "start": 1078,
            "end": 1205,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_68@8",
            "content": "All testers get reasonably paid.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_68",
            "start": 1207,
            "end": 1238,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_69@0",
            "content": "SADTW is a kind of advanced APC method, which is designed for fine-tuning the amateur recording, but not for the case when the amateur recordings are completely out of tune.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_69",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_69@1",
            "content": "In the latter case, we recommend people to use Singing Voice Synthesis (synthesizing waveform from PPG and MIDI) + Singing Voice Conversion (converting the vocal timbre of the synthesized waveform into the user's), or some Speech to Singing (STS) methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_69",
            "start": 174,
            "end": 428,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_69@2",
            "content": "In addition, SADTW provides a score representing the similarity of two pitch curves, which could be used to determine what kind of SVB solution should be chosen.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_69",
            "start": 430,
            "end": 590,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_70@0",
            "content": "This work develops a possible automatic way for singing voice beautification, which may cause unemployment for people with related occupations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_70",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_70@1",
            "content": "In addition, there is the potential for harm from piracy and abuse of our released recordings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_70",
            "start": 144,
            "end": 237,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_70@2",
            "content": "Thus, we choose the dataset license: CC by-nc-sa 4.0.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_70",
            "start": 239,
            "end": 291,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_71@0",
            "content": "F.1 For every submission F.1.1 Did you discuss the limitations of your work?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_71",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_72@0",
            "content": "Yes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_72",
            "start": 0,
            "end": 3,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_72@1",
            "content": "Appendix D.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_72",
            "start": 5,
            "end": 15,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_73@0",
            "content": "Taylor Berg, - Kirkpatrick, Dan Klein, Gpufriendly local regression for voice conversion, 2015, Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_73",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_74@0",
            "content": "Merlijn Blaauw, Jordi Bonada, Sequence-tosequence singing synthesis using the feed-forward transformer, 2020, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_74",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_75@0",
            "content": "Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang, Conformer: Convolution-augmented transformer for speech recognition, 2020-10-29, Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_75",
            "start": 0,
            "end": 333,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_76@0",
            "content": "Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka, Nobukatsu Hojo, Cyclegan-vc2: Improved cyclegan-based non-parallel voice conversion, 2019, ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_76",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_77@0",
            "content": "P Diederik, Max Kingma,  Welling, Autoencoding variational bayes, 2014-04-14, 2nd International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_77",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_78@0",
            "content": "Kazuhiro Kobayashi, Tomoki Toda, Graham Neubig, Sakriani Sakti, Satoshi Nakamura, Statistical singing voice conversion based on direct waveform modification with global variance, 2015, Sixteenth Annual Conference of the International Speech Communication Association, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_78",
            "start": 0,
            "end": 268,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_79@0",
            "content": "Jungil Kong, Jaehyeon Kim, Jaekyoung Bae, Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis, 2020, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_79",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_80@0",
            "content": "Juheon Lee, Hyeong-Seok Choi, Chang-Bin Jeon, Junghyun Koo, Kyogu Lee, Adversarially trained end-to-end korean singing voice synthesis system, 2019, Proc. Interspeech, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_80",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_81@0",
            "content": "Zhonghao Li, Benlai Tang, Xiang Yin, Yuan Wan, Ling Xu, Chen Shen, Zejun Ma, Ppg-based singing voice conversion with adversarial representation learning, 2021, ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_81",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_82@0",
            "content": "UNKNOWN, None, 2021, Diffsinger: Singing voice synthesis via shallow diffusion mechanism, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_82",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_83@0",
            "content": "UNKNOWN, None, 2021, Diffsvc: A diffusion probabilistic model for singing voice conversion, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_83",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_84@0",
            "content": "Peiling Lu, Jie Wu, Jian Luan, Xu Tan, Li Zhou, Xiaoicesing: A high-quality and integrated singing voice synthesis system, 2020, Proc. Interspeech 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_84",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_85@0",
            "content": "Yin-Jyun Luo, Ming-Tso Chen, Tai-Shih Chi, Li Su, Singing voice correction using canonical time warping, 2018, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_85",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_86@0",
            "content": "Xudong Mao, Qing Li, Haoran Xie, Y Raymond, Zhen Lau, Stephen Wang,  Smolley, Least squares generative adversarial networks, 2017, Proceedings of the IEEE international conference on computer vision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_86",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_87@0",
            "content": "Michael Mcauliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, Morgan Sonderegger, Montreal forced aligner: Trainable text-speech alignment using kaldi, 2017, Interspeech, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_87",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_88@0",
            "content": "Greg Mori, Serge Belongie, Jitendra Malik, Efficient shape matching using shape contexts, 2005, IEEE Transactions on Pattern Analysis and Machine Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_88",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_89@0",
            "content": "UNKNOWN, None, 2007, Dynamic time warping. Information retrieval for music and motion, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_89",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_90@0",
            "content": "Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu, Wavenet: A generative model for raw audio, 2016, 9th ISCA Speech Synthesis Workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_90",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_91@0",
            "content": "UNKNOWN, None, 2021, Diffusion-based voice conversion with fast maximum likelihood sampling scheme, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_91",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_92@0",
            "content": "Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, Mark Hasegawa-Johnson, Autovc: Zeroshot voice style transfer only autoencoder loss, 2019, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_92",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_93@0",
            "content": "Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, Mark Hasegawa-Johnson, AutoVC: Zeroshot voice style transfer with only autoencoder loss, 2019, Proceedings of the 36th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_93",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_94@0",
            "content": "UNKNOWN, None, 2020, Deepsinger: Singing voice synthesis with data mined from the web, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_94",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_95@0",
            "content": "Dario Rethage, Jordi Pons, Xavier Serra, A wavenet for speech denoising, 2018, IEEE International Conference on Acoustics, Speech and Signal Processing, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_95",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_96@0",
            "content": "Sebastian Rosenzweig, Simon Schw\u00e4r, Jonathan Driedger, Meinard M\u00fcller, Adaptive pitchshifting with applications to intonation adjustment in a cappella recordings, 2021, Proceedings of the International Conference on Digital Audio Effects (DAFx), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_96",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_97@0",
            "content": "Joan Serr\u00e0, Santiago Pascual, Carlos Segura Perales, Blow: a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion, 2019, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_97",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_98@0",
            "content": "Berrak Sisman, Haizhou Li, Generative adversarial networks for singing voice conversion with and without parallel data, 2020, Speaker Odyssey, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_98",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_99@0",
            "content": "UNKNOWN, None, 2019, Singan: Singing voice conversion with generative adversarial networks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_99",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_100@0",
            "content": "UNKNOWN, None, , Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_100",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_101@0",
            "content": "Kihyuk Sohn, Honglak Lee, Xinchen Yan, Learning structured output representation using deep conditional generative models, 2015-12-07, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_101",
            "start": 0,
            "end": 254,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_102@0",
            "content": "David Sundermann, Hermann Ney, Vtlnbased voice conversion, 2003, Proceedings of the 3rd IEEE International Symposium on Signal Processing and Information Technology, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_102",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_103@0",
            "content": "Fernando Villavicencio, Jordi Bonada, Applying voice conversion to concatenative singing-voice synthesis, 2010, Eleventh annual conference of the international speech communication association, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_103",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_104@0",
            "content": "Yusuke Wada, Yoshiaki Bando, Eita Nakamura, Katsutoshi Itoyama, Kazuyoshi Yoshii, An adaptive karaoke system that plays accompaniment parts of music audio signals synchronously with users'singing voices, 2017, SMC, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_104",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_105@0",
            "content": "Sanna Wager, George Tzanetakis, Minje Cheng-I Wang,  Kim, Deep autotuner: A pitch correcting network for singing performances, 2020, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_105",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_106@0",
            "content": "Li Wan, Quan Wang, Alan Papir, Ignacio Moreno, Generalized end-to-end loss for speaker verification, 2018, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_106",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_107@0",
            "content": "UNKNOWN, None, 2021, Towards high-fidelity singing voice conversion with acoustic reference and contrastive predictive coding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_107",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_108@0",
            "content": "UNKNOWN, None, 2021, Vqmivc: Vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_108",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_109@0",
            "content": "UNKNOWN, None, 2020, Adversarially trained multi-singer sequence-to-sequence singing synthesizer, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_109",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_110@0",
            "content": "UNKNOWN, None, 2020, Aishell-3: A multi-speaker mandarin tts corpus and the baselines, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_110",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_111@0",
            "content": "Sangeon Yong, Juhan Nam, Singing expression transfer from one voice to another for a given song, 2018, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_111",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_112@0",
            "content": "Siyang Yuan, Pengyu Cheng, Ruiyi Zhang, Weituo Hao, Zhe Gan, Lawrence Carin, Improving zero-shot voice style transfer via disentangled representation learning, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_112",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_113@0",
            "content": "Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron Weiss, Ye Jia, Zhifeng Chen, Yonghui Wu, Libritts: A corpus derived from librispeech for textto-speech, 2019-09-19, Interspeech 2019, 20th Annual Conference of the International Speech Communication Association, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_113",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "80-ARR_v2_114@0",
            "content": "Liqiang Zhang, Chengzhu Yu, Heng Lu, Chao Weng, Chunlei Zhang, Yusong Wu, Xiang Xie, Zijin Li, Dong Yu, Durian-sc: Duration informed attention network based singing voice conversion system, 2020-10-29, Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "80-ARR_v2_114",
            "start": 0,
            "end": 313,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "80-ARR_v2_0",
            "tgt_ix": "80-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_0",
            "tgt_ix": "80-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_1",
            "tgt_ix": "80-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_1",
            "tgt_ix": "80-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_1",
            "tgt_ix": "80-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_2",
            "tgt_ix": "80-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_0",
            "tgt_ix": "80-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_3",
            "tgt_ix": "80-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_5",
            "tgt_ix": "80-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_6",
            "tgt_ix": "80-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_7",
            "tgt_ix": "80-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_8",
            "tgt_ix": "80-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_4",
            "tgt_ix": "80-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_4",
            "tgt_ix": "80-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_4",
            "tgt_ix": "80-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_4",
            "tgt_ix": "80-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_4",
            "tgt_ix": "80-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_4",
            "tgt_ix": "80-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_4",
            "tgt_ix": "80-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_0",
            "tgt_ix": "80-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_10",
            "tgt_ix": "80-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_12",
            "tgt_ix": "80-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_11",
            "tgt_ix": "80-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_11",
            "tgt_ix": "80-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_11",
            "tgt_ix": "80-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_0",
            "tgt_ix": "80-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_13",
            "tgt_ix": "80-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_14",
            "tgt_ix": "80-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_14",
            "tgt_ix": "80-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_0",
            "tgt_ix": "80-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_15",
            "tgt_ix": "80-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_16",
            "tgt_ix": "80-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_16",
            "tgt_ix": "80-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_16",
            "tgt_ix": "80-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_17",
            "tgt_ix": "80-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_19",
            "tgt_ix": "80-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_20",
            "tgt_ix": "80-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_21",
            "tgt_ix": "80-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_22",
            "tgt_ix": "80-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_23",
            "tgt_ix": "80-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_18",
            "tgt_ix": "80-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_18",
            "tgt_ix": "80-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_18",
            "tgt_ix": "80-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_18",
            "tgt_ix": "80-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_18",
            "tgt_ix": "80-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_18",
            "tgt_ix": "80-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_18",
            "tgt_ix": "80-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_16",
            "tgt_ix": "80-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_24",
            "tgt_ix": "80-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_26",
            "tgt_ix": "80-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_27",
            "tgt_ix": "80-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_28",
            "tgt_ix": "80-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_29",
            "tgt_ix": "80-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_30",
            "tgt_ix": "80-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_31",
            "tgt_ix": "80-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_32",
            "tgt_ix": "80-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_33",
            "tgt_ix": "80-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_25",
            "tgt_ix": "80-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_25",
            "tgt_ix": "80-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_25",
            "tgt_ix": "80-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_25",
            "tgt_ix": "80-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_25",
            "tgt_ix": "80-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_25",
            "tgt_ix": "80-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_25",
            "tgt_ix": "80-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_25",
            "tgt_ix": "80-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_25",
            "tgt_ix": "80-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_25",
            "tgt_ix": "80-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_16",
            "tgt_ix": "80-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_34",
            "tgt_ix": "80-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_36",
            "tgt_ix": "80-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_37",
            "tgt_ix": "80-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_38",
            "tgt_ix": "80-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_39",
            "tgt_ix": "80-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_35",
            "tgt_ix": "80-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_35",
            "tgt_ix": "80-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_35",
            "tgt_ix": "80-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_35",
            "tgt_ix": "80-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_35",
            "tgt_ix": "80-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_35",
            "tgt_ix": "80-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_16",
            "tgt_ix": "80-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_40",
            "tgt_ix": "80-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_42",
            "tgt_ix": "80-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_43",
            "tgt_ix": "80-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_44",
            "tgt_ix": "80-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_45",
            "tgt_ix": "80-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_46",
            "tgt_ix": "80-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_47",
            "tgt_ix": "80-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_41",
            "tgt_ix": "80-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_41",
            "tgt_ix": "80-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_41",
            "tgt_ix": "80-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_41",
            "tgt_ix": "80-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_41",
            "tgt_ix": "80-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_41",
            "tgt_ix": "80-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_41",
            "tgt_ix": "80-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_41",
            "tgt_ix": "80-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_16",
            "tgt_ix": "80-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_48",
            "tgt_ix": "80-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_49",
            "tgt_ix": "80-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_49",
            "tgt_ix": "80-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_0",
            "tgt_ix": "80-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_50",
            "tgt_ix": "80-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_51",
            "tgt_ix": "80-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_51",
            "tgt_ix": "80-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_53",
            "tgt_ix": "80-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_54",
            "tgt_ix": "80-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_55",
            "tgt_ix": "80-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_52",
            "tgt_ix": "80-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_52",
            "tgt_ix": "80-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_52",
            "tgt_ix": "80-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_52",
            "tgt_ix": "80-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_52",
            "tgt_ix": "80-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_51",
            "tgt_ix": "80-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_56",
            "tgt_ix": "80-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_57",
            "tgt_ix": "80-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_57",
            "tgt_ix": "80-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_51",
            "tgt_ix": "80-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_58",
            "tgt_ix": "80-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_60",
            "tgt_ix": "80-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_61",
            "tgt_ix": "80-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_59",
            "tgt_ix": "80-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_59",
            "tgt_ix": "80-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_59",
            "tgt_ix": "80-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_59",
            "tgt_ix": "80-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_51",
            "tgt_ix": "80-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_62",
            "tgt_ix": "80-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_63",
            "tgt_ix": "80-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_63",
            "tgt_ix": "80-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_63",
            "tgt_ix": "80-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_64",
            "tgt_ix": "80-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_63",
            "tgt_ix": "80-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_65",
            "tgt_ix": "80-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_63",
            "tgt_ix": "80-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_66",
            "tgt_ix": "80-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_63",
            "tgt_ix": "80-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_67",
            "tgt_ix": "80-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_63",
            "tgt_ix": "80-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_68",
            "tgt_ix": "80-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_63",
            "tgt_ix": "80-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_69",
            "tgt_ix": "80-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_71",
            "tgt_ix": "80-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_63",
            "tgt_ix": "80-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_63",
            "tgt_ix": "80-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_70",
            "tgt_ix": "80-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "80-ARR_v2_0",
            "tgt_ix": "80-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_1",
            "tgt_ix": "80-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_2",
            "tgt_ix": "80-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_2",
            "tgt_ix": "80-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_2",
            "tgt_ix": "80-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_2",
            "tgt_ix": "80-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_3",
            "tgt_ix": "80-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_3",
            "tgt_ix": "80-ARR_v2_3@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_3",
            "tgt_ix": "80-ARR_v2_3@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_3",
            "tgt_ix": "80-ARR_v2_3@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_3",
            "tgt_ix": "80-ARR_v2_3@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_3",
            "tgt_ix": "80-ARR_v2_3@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_3",
            "tgt_ix": "80-ARR_v2_3@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_4",
            "tgt_ix": "80-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_5",
            "tgt_ix": "80-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_5",
            "tgt_ix": "80-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_5",
            "tgt_ix": "80-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_5",
            "tgt_ix": "80-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_6",
            "tgt_ix": "80-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_6",
            "tgt_ix": "80-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_6",
            "tgt_ix": "80-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_7",
            "tgt_ix": "80-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_7",
            "tgt_ix": "80-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_7",
            "tgt_ix": "80-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_7",
            "tgt_ix": "80-ARR_v2_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_8",
            "tgt_ix": "80-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_8",
            "tgt_ix": "80-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_8",
            "tgt_ix": "80-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_8",
            "tgt_ix": "80-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_8",
            "tgt_ix": "80-ARR_v2_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_8",
            "tgt_ix": "80-ARR_v2_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_9",
            "tgt_ix": "80-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_10",
            "tgt_ix": "80-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_11",
            "tgt_ix": "80-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_12",
            "tgt_ix": "80-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_13",
            "tgt_ix": "80-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_13",
            "tgt_ix": "80-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_13",
            "tgt_ix": "80-ARR_v2_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_13",
            "tgt_ix": "80-ARR_v2_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_13",
            "tgt_ix": "80-ARR_v2_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_13",
            "tgt_ix": "80-ARR_v2_13@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_14",
            "tgt_ix": "80-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_15",
            "tgt_ix": "80-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_15",
            "tgt_ix": "80-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_15",
            "tgt_ix": "80-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_15",
            "tgt_ix": "80-ARR_v2_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_16",
            "tgt_ix": "80-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_17",
            "tgt_ix": "80-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_17",
            "tgt_ix": "80-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_17",
            "tgt_ix": "80-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_17",
            "tgt_ix": "80-ARR_v2_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_18",
            "tgt_ix": "80-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_19",
            "tgt_ix": "80-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_20",
            "tgt_ix": "80-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_21",
            "tgt_ix": "80-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_21",
            "tgt_ix": "80-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_22",
            "tgt_ix": "80-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_23",
            "tgt_ix": "80-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_24",
            "tgt_ix": "80-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_24",
            "tgt_ix": "80-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_25",
            "tgt_ix": "80-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_26",
            "tgt_ix": "80-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_26",
            "tgt_ix": "80-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_26",
            "tgt_ix": "80-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_26",
            "tgt_ix": "80-ARR_v2_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_26",
            "tgt_ix": "80-ARR_v2_26@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_27",
            "tgt_ix": "80-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_27",
            "tgt_ix": "80-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_27",
            "tgt_ix": "80-ARR_v2_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_27",
            "tgt_ix": "80-ARR_v2_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_27",
            "tgt_ix": "80-ARR_v2_27@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_27",
            "tgt_ix": "80-ARR_v2_27@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_28",
            "tgt_ix": "80-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_29",
            "tgt_ix": "80-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_29",
            "tgt_ix": "80-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_29",
            "tgt_ix": "80-ARR_v2_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_30",
            "tgt_ix": "80-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_31",
            "tgt_ix": "80-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_31",
            "tgt_ix": "80-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_31",
            "tgt_ix": "80-ARR_v2_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_32",
            "tgt_ix": "80-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_33",
            "tgt_ix": "80-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_34",
            "tgt_ix": "80-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_34",
            "tgt_ix": "80-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_34",
            "tgt_ix": "80-ARR_v2_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_35",
            "tgt_ix": "80-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_36",
            "tgt_ix": "80-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_36",
            "tgt_ix": "80-ARR_v2_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_36",
            "tgt_ix": "80-ARR_v2_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_36",
            "tgt_ix": "80-ARR_v2_36@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_36",
            "tgt_ix": "80-ARR_v2_36@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_37",
            "tgt_ix": "80-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_38",
            "tgt_ix": "80-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_38",
            "tgt_ix": "80-ARR_v2_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_39",
            "tgt_ix": "80-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_40",
            "tgt_ix": "80-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_41",
            "tgt_ix": "80-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_42",
            "tgt_ix": "80-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_43",
            "tgt_ix": "80-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_44",
            "tgt_ix": "80-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_44",
            "tgt_ix": "80-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_44",
            "tgt_ix": "80-ARR_v2_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_45",
            "tgt_ix": "80-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_46",
            "tgt_ix": "80-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_47",
            "tgt_ix": "80-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_48",
            "tgt_ix": "80-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_48",
            "tgt_ix": "80-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_48",
            "tgt_ix": "80-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_49",
            "tgt_ix": "80-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_50",
            "tgt_ix": "80-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_50",
            "tgt_ix": "80-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_50",
            "tgt_ix": "80-ARR_v2_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_50",
            "tgt_ix": "80-ARR_v2_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_50",
            "tgt_ix": "80-ARR_v2_50@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_50",
            "tgt_ix": "80-ARR_v2_50@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_50",
            "tgt_ix": "80-ARR_v2_50@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_51",
            "tgt_ix": "80-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_52",
            "tgt_ix": "80-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_53",
            "tgt_ix": "80-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_53",
            "tgt_ix": "80-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_54",
            "tgt_ix": "80-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_54",
            "tgt_ix": "80-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_54",
            "tgt_ix": "80-ARR_v2_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_54",
            "tgt_ix": "80-ARR_v2_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_54",
            "tgt_ix": "80-ARR_v2_54@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_54",
            "tgt_ix": "80-ARR_v2_54@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_54",
            "tgt_ix": "80-ARR_v2_54@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_54",
            "tgt_ix": "80-ARR_v2_54@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_54",
            "tgt_ix": "80-ARR_v2_54@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_54",
            "tgt_ix": "80-ARR_v2_54@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_55",
            "tgt_ix": "80-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_55",
            "tgt_ix": "80-ARR_v2_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_55",
            "tgt_ix": "80-ARR_v2_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_55",
            "tgt_ix": "80-ARR_v2_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_55",
            "tgt_ix": "80-ARR_v2_55@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_55",
            "tgt_ix": "80-ARR_v2_55@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_55",
            "tgt_ix": "80-ARR_v2_55@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_55",
            "tgt_ix": "80-ARR_v2_55@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_55",
            "tgt_ix": "80-ARR_v2_55@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_56",
            "tgt_ix": "80-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_56",
            "tgt_ix": "80-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_56",
            "tgt_ix": "80-ARR_v2_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_56",
            "tgt_ix": "80-ARR_v2_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_56",
            "tgt_ix": "80-ARR_v2_56@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_57",
            "tgt_ix": "80-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_58",
            "tgt_ix": "80-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_59",
            "tgt_ix": "80-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_60",
            "tgt_ix": "80-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_60",
            "tgt_ix": "80-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_60",
            "tgt_ix": "80-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_60",
            "tgt_ix": "80-ARR_v2_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_61",
            "tgt_ix": "80-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_61",
            "tgt_ix": "80-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_61",
            "tgt_ix": "80-ARR_v2_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_61",
            "tgt_ix": "80-ARR_v2_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_61",
            "tgt_ix": "80-ARR_v2_61@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_61",
            "tgt_ix": "80-ARR_v2_61@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_62",
            "tgt_ix": "80-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_62",
            "tgt_ix": "80-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_62",
            "tgt_ix": "80-ARR_v2_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_63",
            "tgt_ix": "80-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_64",
            "tgt_ix": "80-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_64",
            "tgt_ix": "80-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_65",
            "tgt_ix": "80-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_65",
            "tgt_ix": "80-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_65",
            "tgt_ix": "80-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_66",
            "tgt_ix": "80-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_66",
            "tgt_ix": "80-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_66",
            "tgt_ix": "80-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_66",
            "tgt_ix": "80-ARR_v2_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_66",
            "tgt_ix": "80-ARR_v2_66@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_67",
            "tgt_ix": "80-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_67",
            "tgt_ix": "80-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_67",
            "tgt_ix": "80-ARR_v2_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_67",
            "tgt_ix": "80-ARR_v2_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_68",
            "tgt_ix": "80-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_68",
            "tgt_ix": "80-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_68",
            "tgt_ix": "80-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_68",
            "tgt_ix": "80-ARR_v2_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_68",
            "tgt_ix": "80-ARR_v2_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_68",
            "tgt_ix": "80-ARR_v2_68@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_68",
            "tgt_ix": "80-ARR_v2_68@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_68",
            "tgt_ix": "80-ARR_v2_68@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_68",
            "tgt_ix": "80-ARR_v2_68@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_69",
            "tgt_ix": "80-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_69",
            "tgt_ix": "80-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_69",
            "tgt_ix": "80-ARR_v2_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_70",
            "tgt_ix": "80-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_70",
            "tgt_ix": "80-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_70",
            "tgt_ix": "80-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_71",
            "tgt_ix": "80-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_72",
            "tgt_ix": "80-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_72",
            "tgt_ix": "80-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_73",
            "tgt_ix": "80-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_74",
            "tgt_ix": "80-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_75",
            "tgt_ix": "80-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_76",
            "tgt_ix": "80-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_77",
            "tgt_ix": "80-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_78",
            "tgt_ix": "80-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_79",
            "tgt_ix": "80-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_80",
            "tgt_ix": "80-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_81",
            "tgt_ix": "80-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_82",
            "tgt_ix": "80-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_83",
            "tgt_ix": "80-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_84",
            "tgt_ix": "80-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_85",
            "tgt_ix": "80-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_86",
            "tgt_ix": "80-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_87",
            "tgt_ix": "80-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_88",
            "tgt_ix": "80-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_89",
            "tgt_ix": "80-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_90",
            "tgt_ix": "80-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_91",
            "tgt_ix": "80-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_92",
            "tgt_ix": "80-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_93",
            "tgt_ix": "80-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_94",
            "tgt_ix": "80-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_95",
            "tgt_ix": "80-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_96",
            "tgt_ix": "80-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_97",
            "tgt_ix": "80-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_98",
            "tgt_ix": "80-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_99",
            "tgt_ix": "80-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_100",
            "tgt_ix": "80-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_101",
            "tgt_ix": "80-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_102",
            "tgt_ix": "80-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_103",
            "tgt_ix": "80-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_104",
            "tgt_ix": "80-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_105",
            "tgt_ix": "80-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_106",
            "tgt_ix": "80-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_107",
            "tgt_ix": "80-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_108",
            "tgt_ix": "80-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_109",
            "tgt_ix": "80-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_110",
            "tgt_ix": "80-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_111",
            "tgt_ix": "80-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_112",
            "tgt_ix": "80-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_113",
            "tgt_ix": "80-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "80-ARR_v2_114",
            "tgt_ix": "80-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 751,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "80-ARR",
        "version": 2
    }
}