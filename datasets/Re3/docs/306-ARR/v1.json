{
    "nodes": [
        {
            "ix": "306-ARR_v1_0",
            "content": "DialSummEval: Revisiting Summarization Evaluation for Dialogues",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_2",
            "content": "Dialogue summarization is receiving increasing attention from researchers due to its extraordinary difficulty and unique application value. We observe that current dialogue summarization models have flaws that may not be well exposed by frequently used metrics such as ROUGE. In our paper, we re-evaluate 18 categories of metrics in terms of four dimensions: coherence, consistency, fluency and relevance, as well as a unified human evaluation of various models for the first time. Some noteworthy trends which are different from the conventional summarization tasks are identified. We will release DialSummEval, a multifaceted dataset of human judgments containing the outputs of 14 models on SAMSum.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "306-ARR_v1_4",
            "content": "Neural network based approaches and sizable datasets have led to significant progress in researches towards conventional summarization tasks such as news and scientific papers (Lin and Ng, 2019). Compared with conventional summarization tasks, dialogue summarization has received increasing attention from researchers due to its great difficulty and unique application value (Feng et al., 2021a). With the proposal of dialogue summary datasets such as SAMSum (Gliwa et al., 2019), DialogSum and Medi-aSum , a number of models for automatic generation of dialogue summaries have emerged (Feng et al., 2021b;Chen and Yang, 2020;.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_5",
            "content": "There is no denying that these studies have made promising progress, but it remains a challenge to evaluate these advances comprehensively. Current studies generally use the SAMSum dataset and adopt ROUGE (Lin, 2004), an n-grambased automatic evaluation metric using reference summaries, as the overall evaluation criterion for summary quality, complemented by manual evaluation. Schluter (2017) and Graham (2015) illustrate the limitations of ROUGE in evaluating summarization tasks. Also the manual evaluation protocols vary from one research to another based on our observations. We argue that the inadequate evaluation mechanism may have become a major obstacle to the progress of dialogue summarization researches. Many studies, such as Chen and Yang (2020) and Tang et al. (2021), have pointed out that the current dialogue summarization models still have many shortcomings, such as wrong references, incorrect reasoning and improper gender pronouns, and ROUGE may not reflect these problems effectively. For example, Gabriel et al. (2021) note that ROUGE-1 and ROUGE-L fail to accurately measure factual inconsistency across domains. Our case study in Table 1 also illustrates this point. However, it is impractical to perform frequent time-consuming and costly manual evaluation. The alternative is to introduce or propose more reliable automatic evaluation metrics to evaluate the models in a more comprehensive and finegrained manner.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_6",
            "content": "Although there are automatic evaluation metrics for measuring the quality of all aspects of summaries on conventional summarization tasks, especially for factual consistency (Huang et al., 2021), it is difficult to guarantee that they will still perform well on dialogue summarizarion. Recently proposed automatic metrics for evaluating generic natural language generation tasks such as BERTScore , BARTScore have also not been experimented on dialogue summarization. The high abstraction level, low extraction rate, and the requirement for complex reasoning power of the dialogue summarization task present new challenges to automatic evaluation metrics. There Ola should be free by 8. Kurt wants her to call him.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_7",
            "content": "Ola will be late. She should be free by 8. Kurt will call her. 0.69 0.42 0.67 have been a number of manual evaluation datasets and analytical studies for conventional summarization tasks ((Dang and Owczarzak, 2008); Fabbri et al., 2021b;Bhandari et al., 2020), but very little work has been done on systematic analysis of dialogue summarization models and evaluation metrics. Our work will fill the gap in this area and includes the following contributions: 1) We identify evaluation problems in the field of dialogue summarization and point out the urgent need of automatic evaluation metrics that better adapt to dialogue summarization. 2) We collect and provide a sizable, multi-faceted dataset of manual evaluations for dialogue summarization, which contains the output of 14 models, and the dataset will be released. 3) We re-evaluate the performance of 18 types of automatic evaluation metrics on dialogue summarization. 4) We evaluate a variety of dialogue summarization models (extractive, abstractive, and recently based on pre-trained language models) in a unified manner.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_8",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "306-ARR_v1_9",
            "content": "Meta-Evaluation with Human Judgments Automatic evaluation Metrics such as ROUGE (Lin, 2004) and BERTScore )) were compared with other metrics when proposed. However, they are basically not using the dialogue summarization dataset as an experimental corpus, and rarely provide new human judgments data. Bhandari et al. (2020) used pyramid (Nenkova and Passonneau, 2004), a widely used human evaluation method on several conventional summarization datasets to obtain relevance scores for some of the system outputs and re-evaluated the metrics in 6 categories. Similarly, Fabbri et al. (2021b) used CNN/DailyMail dataset (Hermann et al., 2015) and the output of some models for human evaluation covering four facets of relevance, consistency, fluency, and coherence, and then re-evaluated the metrics in 14 categories. None of these involved dialogue summarization datasets. Gabriel et al. (2021) is one of the few current studies using the dialogue summarization dataset SAMSum (Gliwa et al., 2019) for meta-evaluation, but it focuses on factual consistency and selects a small number of metrics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_10",
            "content": "Evaluation for Dialogue Summarization Models Tang et al. (2021) and Chen and Yang (2020) sample the output of models on SAMSum and analyzes the error types when proposing a new model. Due to the different manual evaluation protocols and the small number of models included, it is difficult to comprehensively compare the strengths and weaknesses of different models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_11",
            "content": "Preliminaries",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "306-ARR_v1_12",
            "content": "In this section, we introduce the involved dataset, metrics and models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_13",
            "content": "Dataset",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "306-ARR_v1_14",
            "content": "SAMSum (Gliwa et al., 2019) is the first manually annotated, high-quality chat summarization dataset, containing over 16k dialogues. We use it in this study as it is most widely used and has greatly promoted the research in the field of dialogue summarization, and we are able to collect the outputs of various models on this dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_15",
            "content": "Evaluation Metrics",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "306-ARR_v1_16",
            "content": "We selected a number of evaluation metrics that are frequently used on summarization or other natural language generation tasks. Some are for overall quality; others are specific to a particular aspect. Some require reference summaries or source documents; some only need the summary itself. Here is a brief categorization and description.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_17",
            "content": "Metrics based on n-gram overlap include: ROUGE (Lin, 2004) is the most widely used automatic evaluation metric in summarization. Researchers mainly adopt ROUGE-1, ROUGE-2 and ROUGE-L, which measure the unigram-overlap, bigram-overlap and longest common sequence between two texts respectively. 1 BLEU (Papineni et al., 2002) is the primary evaluation metric for machine translation. It calculates n-gram overlap between texts using precision scores and includes a brevity penalty. 2 METEOR (Banerjee and Lavie, 2005) computes an alignment by mapping unigrams in two texts, based on surface forms, stemmed forms, and meanings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_18",
            "content": "CHRF (Popovi\u0107, 2015) computes character based n-gram overlap between two texts. 3 Metrics based on pre-trained language models include:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_19",
            "content": "BERTScore measures the soft-overlap between two texts at token level using contextual embeddings from BERT. 4 MoverScore (Zhao et al., 2019) applies the semantic distance between two texts at n-gram level using n-gram embeddings pooled from BERT. 5 BARTScore assumes that models trained to convert the generated text to/from a reference output or the source text will achieve higher scores when the generated text is better. It can be flexibly applied to evaluation of text from different perspectives using BART. 6 BLANC (Vasilyev et al., 2020) is a referenceless metric. It hypothesizes that a good summary is beneficial for a pre-trained language model to conduct language understanding tasks on the source document. Specifically, it measures the performance boost of BERT by utilizing the summary in two different ways. 7 PPL, namely perplexity, is often used to evaluate the quality of a language model or the fluency of an utterance. We adopt GPT-2 (Radford et al., 2019) as the language model for computing the perplexity for the whole summary. 8 Metrics based on word embeddings include: SMS (Clark et al., 2019), namely Sentence Mover Similarity, extends Word Movers Distance (Kusner et al., 2015) to measure the distance between two texts which are represented as a bag of sentence embeddings. 9",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_20",
            "content": "Embedding average (Sharma et al., 2017) is an embedding based metric computing the cosine similarity between the embeddings of two texts. A sentence-level embedding is represented by averaging the embeddings of the words composing the sentence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_21",
            "content": "Vector extrema (Forgues et al., 2014) is also an embedding based metric similar to Embedding average. The metric computes a sentence-level embedding by taking the most extreme value of the embeddings of the words composing the sentence for each dimension of the embedding.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_22",
            "content": "Greedy matching (Rus and Lintean, 2012) is another embedding based metric. The metric does not compute a sentence-level embedding. It directly compares the embeddings of words in the two sentences using a greedy matching algorithm to calculate similarity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_23",
            "content": "FEQA (Durmus et al., 2020) employs a BERTbased question-answering model to answer questions using source document. Questions are generated by a fine-tuned BART model using generated summaries with masked named entities as inputs. The metric reports F1 scores against the gold answer, which are often regarded as a measure of factual consistency. 10",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_24",
            "content": "SummaQA (Scialom et al., 2019) is also a QAbased metric. Unlike FEQA, it generates questions from source documents instead of summaries to be evaluated and then uses summaries to answer them. The F1 overlap score and QA-model confidence are reported. 11 QuestEval (Scialom et al., 2021) is another a QA-based metric. This metric can be considered as a combination of FEQA and SummaQA. It takes into account the scores obtained from both styles. For comparison purposes, We use the reference-less mode. 12 Metrics based on entailment classification include:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_25",
            "content": "FactCC (Kryscinski et al., 2020) is an entailment classification metric. We follow the way Pagnoni et al. (2021) used it. Each sentence of the summary is fed into the classifier together with the document to determine whether the facts are consistent, and the proportion of consistent sentences is used to indicate how consistent the summary is.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_26",
            "content": "13 DAE (Goyal and Durrett, 2020;Goyal and Durrett, 2021) is an entailment classification metric based on dependencies. We use it in a similar way to FactCC. When a sentence cannot be parsed by the metric, we default it factually inconsistent. 14",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_27",
            "content": "Summarization Models",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "306-ARR_v1_28",
            "content": "We select some representative models and get the outputs of them on the test set of SAMSum. We choose LEAD-3 and LONGEST-3 as representatives of the simple extractive approaches. PGN (See et al., 2017) and Transformer (Vaswani et al., 2017) are selected as representatives of the earlier neural summarization models. For generic pretrained generative models, we use BART (Lewis et al., 2020), PEGASUS and UniLM (Dong et al., 2019). We retrain these models above to obtain the outputs and the automatic evaluation results are close to Gliwa et al. (2019) and Wu et al. (2021) in default settings. For models specifically designed for dialogue summarization, we choose CODS (Wu et al., 2021), Con-voSumm (Fabbri et al., 2021a), MV-BART (Chen 11 https://github.com/ThomasScialom/ summa-qa 12 https://github.com/ThomasScialom/ QuestEval 13 https://github.com/salesforce/factCC 14 https://github.com/tagoyal/ factuality-datasets and Yang, 2020), PLM-BART (Feng et al., 2021c), Ctrl-DiaSumm ), S-BART and the outputs are all provided by their authors. We also regard the reference summary as a kind of model output.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_29",
            "content": "Data Annotation",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "306-ARR_v1_30",
            "content": "Annotation Setup",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "306-ARR_v1_31",
            "content": "Since human evaluation is expensive and timeconsuming, we decide to randomly sample 100 dialogues from the test set of SAMSum and evaluate the summaries generated by all models on these dialogues. To comprehensively evaluate each metric and model, we perform human evaluation in four aspects, as in Kryscinski et al. (2019):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_32",
            "content": "Coherence measures the quality of all sentences in the summary as a whole. It focuses on whether the summary is coherent and natural.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_33",
            "content": "Consistency measures how well the summary aligns with the dialogue in facts. It focuses on whether the summary contains factual errors.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_34",
            "content": "Fluency measures the quality of individual sentences in the summary compared to Coherence. It focuses on whether the sentences are well-written and grammatically correct.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_35",
            "content": "Relevance measures how well the summary captures the key points of the dialogue. It focuses on whether all and only the important aspects are contained in the summary.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_36",
            "content": "To ensure the quality of the annotation, we tried to annotate some of the data ourselves at the beginning to judge the difficulty of the task and the approximate time spent.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_37",
            "content": "Annotation Process",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "306-ARR_v1_38",
            "content": "We initially tried to annotate the data using crowdsourcing platforms. We published the annotation task on Amazon Mechanical Turk 15 . The interface contained instructions and definitions of the four aspects. A dialogue and a corresponding summary were included in the interface, and the summaries of different models on the same dialogue were presented to the annotators in a sequence to facilitate comparison. For each dimension/aspect, annotators were asked to rate the summary on a Likert scale from 1 to 5. Each summary was evaluated by 5 different annotators, and For each dimension we would receive a total of 100 \u00d7 14 \u00d7 5 = 7000 human annotations. The annotation was done quickly in one day, but the quality was not satisfactory. We calculated the average score of each model in each aspect based on these annotation data and found that the scores of the models are close in each dimension, which is not in the accordance with the reality. For example, in terms of consistency, the reference summary and the extractive approaches should have had a definite advantage, but this failed to be reflected from the data. The result is shown in Table 5. For reliability reasons, we do not use these annotations for our analysis.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_39",
            "content": "Then, we decided to recruit annotators from the school forum who are required to be capable of reading daily conversations and articles in English fluently. We recruited three annotators, using a similar annotation interface and approach as in the crowd-sourcing platforms. These annotators were college students and they are fluent in English. The differences with the crowd-sourcing platform annotation are as follows: 1) For a student who wanted to participate in the annotation, we would ask him to annotate all models on the first 10 conversations (10\u00d714 = 140 annotations), and let her/him continue the annotation only when these annotation results were checked by us to confirm that the annotator had understood the task correctly and could finish the annotation responsibly. Otherwise, we paid the annotator directly for this part and terminated his annotation task. 2) We required each annotator to annotate all data (100 \u00d7 14 = 1400 annotations) to ensure the consistency within the annotator. 3) During the annotation process, we kept in touch with the annotators via email or instant messaging app to answer their questions at any time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_40",
            "content": "It took around 10 days to finish the annotation. We received 100 \u00d7 14 \u00d7 3 = 4200 annotations for each perspective. For each aspect of each summary, if two scores were the same and the other was different from them, we considered the different one as noise. For each dimension, we removed the noise separately and calculated the the Krippendorffs alpha coefficient (Krippendorff, 2011). We found the inter-annotator interval kappa to be within an acceptable range -from 0.5621 to 0.7564, as detailed in Table 2. The raw annotated data will be released and we use the cleaned data for analysis. At last, we use the average of the remaining data to represent the human evaluation score of an summary on a dimension.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_41",
            "content": "Metric Evaluation",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "306-ARR_v1_42",
            "content": "In this section, we will introduce several definitions in meta-evaluation and re-evaluate the metrics mentioned in Section 3.2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_43",
            "content": "Task Formulation",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "306-ARR_v1_44",
            "content": "As mentioned by Bhandari et al. (2020), there are two common ways to measure the correlation of automatic evaluation metrics to manual evaluation: system-level and summary-level.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_45",
            "content": "Assuming there are N dialogues, the i-th dialogue is represented as d i . For a dialogue d i , there are J summaries generated by J models, and we denote each of them as s ij , j = 1 \u2022 \u2022 \u2022 J. There are K evaluation metrics in total, and m k refers to an automatic evaluation metric or human evaluation of a certain dimension. m k (s ij ) means the score of k-th metric towards a pair of dialogue and summary (d i , s ji ). We use R(m i , m j ) to denote the correlation coefficient between two metrics m i and m j .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_46",
            "content": "System-level correlation is defined as follows. The corresponding p-value which indicates statistical significance can be obtained:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_47",
            "content": "R sys (m p , m q ) = R( [ 1 N N \u2211 i=1 m p (s i1 ), \u2022 \u2022 \u2022 , 1 N N \u2211 i=1 m p (s iJ )], [ 1 N N \u2211 i=1 m q (s i1 ), \u2022 \u2022 \u2022 , 1 N N \u2211 i=1 m q (s iJ )])",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_48",
            "content": "Summary-level correlation is defined as follows, and the p-value cannot be derived here because the Summary-level correlation is an average value:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_49",
            "content": "R sum (m p , m q ) = 1 N N \u2211 i=1 R([m p (s i1 ), \u2022 \u2022 \u2022 , m p (s iJ )], [m q (s i1 ), \u2022 \u2022 \u2022 , m q (s iJ )])",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_50",
            "content": "Discussion",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "306-ARR_v1_51",
            "content": "Comparing the performance of various metrics reveals some trends in strengths in all four dimensions. Of all the metrics, QuestEval has the most comprehensive capabilities at the system level. Generally metrics that perform better on coherence and fluency perform worse on consistency and relevance, and vice versa. This can be attributed to the definition of the dimensions, i.e. there is some correlation between the four dimensions themselves. In all dimensions, automatic evaluation metrics based on pre-trained language models generally outperform metrics based on n-gram overlap and contextindependent word embedding. Among them, the recently proposed BARTScore and the increasingly popular QA-based metrics perform the best. This suggests that both directions have the potential to be explored in terms of evaluation for dialogue summarization. Across dimensions, almost all metrics correlate better with human judgments at the system level than at the summary level, and both showed good agreement with each other. This indicates that the summary-level correlations are also worth referring to when enough data are not available for system-level analysis. In addition, metrics such as BLEU and CHRF, which are frequently used in other natural language generation tasks (e.g., machine translation, dialogue, etc.), do not show advantages on dialogue summarization.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_52",
            "content": "The characteristics presented by the automatic evaluation metrics on the dialogue summarization differ from those of the conventional summarization tasks. For ROUGE, we find that increasing the size of n in ROUGE-n is not better in almost all dimensions, which is different from the findings of Rankel et al. ( 2013) and Fabbri et al. (2021b). The ability of ROUGE to reflect content selection, i.e., relevance, as we usually believe, is also questionable. Compared to the results of Fabbri et al. (2021b), metrics based on n-gram overlap such as ROUGE and CHRF perform worse on dialogue summarization, while some metrics that use source documents such as BLANC perform better. We need to focus on the limitations of ROUGE and the role of the source dialogues in evaluating dialogue summaries.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_53",
            "content": "We have also observed some interesting phenomena. Entailment classification metrics such as FactCC and DAE outperform many metrics in terms of consistency, but not as well as BARTScore and QA-based metrics. This may be due to the large gap between the corpus used in training and dialogues, and the need to slice the summaries by sentence when using them. FEQA, which is designed for factual consistency, however, performs best in coherence and fluency, and rather poorly in consistency and relevance. Comparing its performance with QuestEval and SummaQA, generating questions from the original dialogue may be more reliable in measuring consistency, which corroborates with the points of Gabriel et al. (2021). It is surprising that metrics based on the language model such as PPL, BARTScore-h performs poorly in measuring both coherence and fluency. The exact reasons for this need further investigation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_54",
            "content": "Model Evaluation",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "306-ARR_v1_55",
            "content": "In each dimension, we evaluate each model mentioned in Section 3.3 using the average of the human evaluation scores of all summaries. Analyzing Table 4, we conclude the following.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_56",
            "content": "The reference summaries in SAMSum are not perfect, and the annotators felt that they also contained some factual inconsistencies compared to the source dialogues, as well as important elements of the dialogues that were not all captured by them. However, comparing the human evaluation scores of the reference summaries in CNNDM (Fabbri et al., 2021b), the quality is already superior.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_57",
            "content": "Extractive models produce summaries that differ in style from abstractive models, and many conversations contain ungrammatical utterances, which can affect the reading experience and impair their fluency and coherence. In particular, LONGEST-3, which extracts some potentially discontinuous sentences from dialogues, has low coherence. However, since they do not modify the content, they still perform well in terms of con- The correlation (Pearson's r) of annotations computed on system level and summary level along four quality dimensions between automatic metrics and human judgments. For evaluation, all metrics require at least the summaries to be evaluated as input. Metrics with + indicate that the source dialogues are used, metrics withmeans no other input are required, others need to use the reference summaries. The five most-correlated metrics in each column are bolded (For system level, **=significant for p \u2264 0.01, *=significant for p \u2264 0.05). We add suffixes to distinguish the different variants of metrics. For BARTScore, h, r and s are abbreviations of hypothesises, references and source dialogues respectively. BARTScore-s-h measure the probability to generate hypothesises using source dialogues as inputs, while BARTScore-h measures the probability to generate hypothesises without other inputs, and so on. For BLANC, BLANC-tune refers to the way of fine-tuning on a generated summary and then conducting nature language understanding tasks on source dialogues, while BLANC-help refers to the way of inferring with a generated summary concatenated together. For SummaQA, SummaQA-fscore measures the average overlap between predictions and ground truth answers, and SummaQA-conf corresponds to the probability of the true answer. sistency. Since the average length of dialogues in SAMSum is small, extracting a few sentences from it can generally include important contents, so the relevance is also high.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_58",
            "content": "The early neural summrization models represented by PGN and Transformer perform relatively poorly in all dimensions compared to the reference summaries, especially consistency and relevance. This is to be expected because of the high difficulty of dialogue summarization and the small size of SAMSum dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_59",
            "content": "An important finding is that the generic pretrained language models represented by BART, PEGASUS and UniLM, and various recently proposed models specifically designed on the dialogue summarization task do not have significant differences in each dimension. They are already comparable, and in some cases better, in terms of coherence and fluency compared to the reference summaries. They have improved dramatically compared to earlier neural summarization models with respect to consistency and relevance, but there is still some room for enhancement. On the one hand, this finding affirms the capability of these models; On the other hand, it urges us to reflect on how much these recently proposed complex models or fancy techniques are an improvement over the generic pre-trained language models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_60",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "306-ARR_v1_61",
            "content": "We point out the problems with the evaluation in the dialogue summarization and introduce Di-alSummEval, a multi-faceted dataset containing the output of various models and the corresponding human judgments. Based on this dataset, we provide a comprehensive re-evaluation and analysis of the performance of widely used automatic evaluation metrics and each model. There are three important findings: 1) Few metrics are excellent in all dimensions, and the recently proposed BARTScore and QA-based metrics are comparatively outstanding and worth exploring. 2) The automatic evaluation metrics and their variants present some trends that differ from conventional summarization. 3) A variety of models specifically designed for dialogue summarization perform comparably to reference summaries in terms of coherence and fluency, but still have shortcomings in consistency and relevance. We hope that researchers in the field recognize the importance of evaluation in current research, choose some metrics other than ROUGE when evaluating models, propose automatic evaluation metrics that can be better adapted to the field of dialogue summarization based on our work.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_62",
            "content": "Ethical Considerations",
            "ntype": "title",
            "meta": {
                "section": "8"
            }
        },
        {
            "ix": "306-ARR_v1_63",
            "content": "Whether recruiting annotators through Amazon Mechanical Turk or campus, we paid them 15 dollars per hour, more than the local average minimum wage. We removed all content in the dataset that might contain personal information about the annotators. Table 5: Human ratings of summaries along four evaluation dimensions using data from Amazon Mechanical Turk. scores are averaged over five annotators, broken down by the approximate classification in Section 3.3",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "306-ARR_v1_64",
            "content": "Satanjeev Banerjee, Alon Lavie, METEOR: An automatic metric for MT evaluation with improved correlation with human judgments, 2005, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Satanjeev Banerjee",
                    "Alon Lavie"
                ],
                "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
                "pub_date": "2005",
                "pub_title": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_65",
            "content": "Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, Graham Neubig, Reevaluating evaluation in text summarization, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Manik Bhandari",
                    "Pranav Narayan Gour",
                    "Atabak Ashfaq",
                    "Pengfei Liu",
                    "Graham Neubig"
                ],
                "title": "Reevaluating evaluation in text summarization",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_66",
            "content": "Jiaao Chen, Diyi Yang, Multi-view sequenceto-sequence models with conversational structure for abstractive dialogue summarization, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Jiaao Chen",
                    "Diyi Yang"
                ],
                "title": "Multi-view sequenceto-sequence models with conversational structure for abstractive dialogue summarization",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "306-ARR_v1_67",
            "content": "Jiaao Chen, Diyi Yang, Structure-aware abstractive conversation summarization via discourse and action graphs, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Jiaao Chen",
                    "Diyi Yang"
                ],
                "title": "Structure-aware abstractive conversation summarization via discourse and action graphs",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_68",
            "content": "Yulong Chen, Yang Liu, Liang Chen, Yue Zhang, DialogSum: A real-life scenario dialogue summarization dataset, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Yulong Chen",
                    "Yang Liu",
                    "Liang Chen",
                    "Yue Zhang"
                ],
                "title": "DialogSum: A real-life scenario dialogue summarization dataset",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "306-ARR_v1_69",
            "content": "Elizabeth Clark, Asli Celikyilmaz, Noah Smith, Sentence mover's similarity: Automatic evaluation for multi-sentence texts, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Elizabeth Clark",
                    "Asli Celikyilmaz",
                    "Noah Smith"
                ],
                "title": "Sentence mover's similarity: Automatic evaluation for multi-sentence texts",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "306-ARR_v1_70",
            "content": "UNKNOWN, None, 2008, Overview of the tac 2008 update summarization task, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": null,
                "title": null,
                "pub_date": "2008",
                "pub_title": "Overview of the tac 2008 update summarization task",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_71",
            "content": "Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon, Unified language model pre-training for natural language understanding and generation, 2019, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Li Dong",
                    "Nan Yang",
                    "Wenhui Wang",
                    "Furu Wei",
                    "Xiaodong Liu",
                    "Yu Wang",
                    "Jianfeng Gao",
                    "Ming Zhou",
                    "Hsiao-Wuen Hon"
                ],
                "title": "Unified language model pre-training for natural language understanding and generation",
                "pub_date": "2019",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_72",
            "content": "Esin Durmus, He He, Mona Diab, FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Esin Durmus",
                    "He He",
                    "Mona Diab"
                ],
                "title": "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_73",
            "content": "Alexander Fabbri, Faiaz Rahman, Imad Rizvi, Borui Wang, Haoran Li, Yashar Mehdad, Dragomir Radev, ConvoSumm: Conversation summarization benchmark and improved abstractive summarization with argument mining, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Alexander Fabbri",
                    "Faiaz Rahman",
                    "Imad Rizvi",
                    "Borui Wang",
                    "Haoran Li",
                    "Yashar Mehdad",
                    "Dragomir Radev"
                ],
                "title": "ConvoSumm: Conversation summarization benchmark and improved abstractive summarization with argument mining",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "306-ARR_v1_74",
            "content": "Alexander Fabbri, Wojciech Kry\u015bci\u0144ski, Bryan Mccann, Caiming Xiong, Richard Socher, Dragomir Radev, SummEval: Re-evaluating summarization evaluation, 2021, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Alexander Fabbri",
                    "Wojciech Kry\u015bci\u0144ski",
                    "Bryan Mccann",
                    "Caiming Xiong",
                    "Richard Socher",
                    "Dragomir Radev"
                ],
                "title": "SummEval: Re-evaluating summarization evaluation",
                "pub_date": "2021",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_75",
            "content": "UNKNOWN, None, , Xiaocheng Feng, and Bing Qin. 2021a. A survey on dialogue summarization: Recent advances and new frontiers. Computing Research Repository, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Xiaocheng Feng, and Bing Qin. 2021a. A survey on dialogue summarization: Recent advances and new frontiers. Computing Research Repository",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_76",
            "content": "Xiachong Feng, Xiaocheng Feng, Bing Qin, Xinwei Geng, Dialogue discourse-aware graph model and data augmentation for meeting summarization, 2021, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Xiachong Feng",
                    "Xiaocheng Feng",
                    "Bing Qin",
                    "Xinwei Geng"
                ],
                "title": "Dialogue discourse-aware graph model and data augmentation for meeting summarization",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_77",
            "content": "Xiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin, Ting Liu, Language model as an annotator: Exploring DialoGPT for dialogue summarization, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Xiachong Feng",
                    "Xiaocheng Feng",
                    "Libo Qin",
                    "Bing Qin",
                    "Ting Liu"
                ],
                "title": "Language model as an annotator: Exploring DialoGPT for dialogue summarization",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "306-ARR_v1_78",
            "content": "Gabriel Forgues, Joelle Pineau, Jean-Marie Larchev\u00eaque, R\u00e9al Tremblay, Bootstrapping dialog systems with word embeddings, 2014, Nips, modern machine learning and natural language processing workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Gabriel Forgues",
                    "Joelle Pineau",
                    "Jean-Marie Larchev\u00eaque",
                    "R\u00e9al Tremblay"
                ],
                "title": "Bootstrapping dialog systems with word embeddings",
                "pub_date": "2014",
                "pub_title": "Nips, modern machine learning and natural language processing workshop",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_79",
            "content": "Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, Jianfeng Gao, GO FIGURE: A meta evaluation of factuality in summarization, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Saadia Gabriel",
                    "Asli Celikyilmaz",
                    "Rahul Jha",
                    "Yejin Choi",
                    "Jianfeng Gao"
                ],
                "title": "GO FIGURE: A meta evaluation of factuality in summarization",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "306-ARR_v1_80",
            "content": "Bogdan Gliwa, Iwona Mochol, Maciej Biesek, Aleksander Wawer, SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization, 2019, Proceedings of the 2nd Workshop on New Frontiers in Summarization, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Bogdan Gliwa",
                    "Iwona Mochol",
                    "Maciej Biesek",
                    "Aleksander Wawer"
                ],
                "title": "SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2nd Workshop on New Frontiers in Summarization",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "306-ARR_v1_81",
            "content": "Tanya Goyal, Greg Durrett, Evaluating factuality in generation with dependency-level entailment, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Tanya Goyal",
                    "Greg Durrett"
                ],
                "title": "Evaluating factuality in generation with dependency-level entailment",
                "pub_date": "2020",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "306-ARR_v1_82",
            "content": "Tanya Goyal, Greg Durrett, Annotating and modeling fine-grained factuality in summarization, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Tanya Goyal",
                    "Greg Durrett"
                ],
                "title": "Annotating and modeling fine-grained factuality in summarization",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_83",
            "content": "Yvette Graham, Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Yvette Graham"
                ],
                "title": "Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_84",
            "content": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Teaching machines to read and comprehend, 2015, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Karl Moritz Hermann",
                    "Tomas Kocisky",
                    "Edward Grefenstette",
                    "Lasse Espeholt",
                    "Will Kay",
                    "Mustafa Suleyman",
                    "Phil Blunsom"
                ],
                "title": "Teaching machines to read and comprehend",
                "pub_date": "2015",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_85",
            "content": "Yichong Huang, Xiachong Feng, Xiaocheng Feng, and Bing Qin. 2021. The factual inconsistency problem in abstractive text summarization: A survey, , Computing Research Repository, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Yichong Huang",
                    "Xiachong Feng"
                ],
                "title": "Xiaocheng Feng, and Bing Qin. 2021. The factual inconsistency problem in abstractive text summarization: A survey",
                "pub_date": null,
                "pub_title": "Computing Research Repository",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_86",
            "content": "UNKNOWN, None, 2011, Computing krippendorff's alpha-reliability, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": "2011",
                "pub_title": "Computing krippendorff's alpha-reliability",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_87",
            "content": "Wojciech Kryscinski, Nitish Shirish Keskar, Bryan Mc-Cann, Caiming Xiong, Richard Socher, Neural text summarization: A critical evaluation, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Wojciech Kryscinski",
                    "Nitish Shirish Keskar",
                    "Bryan Mc-Cann",
                    "Caiming Xiong",
                    "Richard Socher"
                ],
                "title": "Neural text summarization: A critical evaluation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_88",
            "content": "Wojciech Kryscinski, Bryan Mccann, Caiming Xiong, Richard Socher, Evaluating the factual consistency of abstractive text summarization, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Wojciech Kryscinski",
                    "Bryan Mccann",
                    "Caiming Xiong",
                    "Richard Socher"
                ],
                "title": "Evaluating the factual consistency of abstractive text summarization",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_89",
            "content": "Matt Kusner, Yu Sun, Nicholas Kolkin, Kilian Weinberger ; Lille, France Lewis, Yinhan Liu, Naman Goyal, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension, 2015, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Matt Kusner",
                    "Yu Sun",
                    "Nicholas Kolkin",
                    "Kilian Weinberger ; Lille",
                    "France Lewis",
                    "Yinhan Liu",
                    "Naman Goyal"
                ],
                "title": "Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "306-ARR_v1_90",
            "content": "Chin-Yew Lin, ROUGE: A package for automatic evaluation of summaries, 2004, Text Summarization Branches Out, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Chin-Yew Lin"
                ],
                "title": "ROUGE: A package for automatic evaluation of summaries",
                "pub_date": "2004",
                "pub_title": "Text Summarization Branches Out",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "306-ARR_v1_91",
            "content": "Hui Lin, Vincent Ng, Abstractive summarization: A survey of the state of the art, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Hui Lin",
                    "Vincent Ng"
                ],
                "title": "Abstractive summarization: A survey of the state of the art",
                "pub_date": "2019",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_92",
            "content": "Junpeng Liu, Yanyan Zou, Hainan Zhang, Hongshen Chen, Zhuoye Ding, Caixia Yuan, Xiaojie Wang, Topic-aware contrastive learning for abstractive dialogue summarization, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Junpeng Liu",
                    "Yanyan Zou",
                    "Hainan Zhang",
                    "Hongshen Chen",
                    "Zhuoye Ding",
                    "Caixia Yuan",
                    "Xiaojie Wang"
                ],
                "title": "Topic-aware contrastive learning for abstractive dialogue summarization",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2021",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "306-ARR_v1_93",
            "content": "Zhengyuan Liu, Nancy Chen, Controllable neural dialogue summarization with personal named entity planning, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Zhengyuan Liu",
                    "Nancy Chen"
                ],
                "title": "Controllable neural dialogue summarization with personal named entity planning",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_94",
            "content": "Ani Nenkova, Rebecca Passonneau, Evaluating content selection in summarization: The pyramid method, 2004, Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Ani Nenkova",
                    "Rebecca Passonneau"
                ],
                "title": "Evaluating content selection in summarization: The pyramid method",
                "pub_date": "2004",
                "pub_title": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_95",
            "content": "Artidoro Pagnoni, Vidhisha Balachandran, Yulia Tsvetkov, Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Artidoro Pagnoni",
                    "Vidhisha Balachandran",
                    "Yulia Tsvetkov"
                ],
                "title": "Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_96",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "pub_date": "2002",
                "pub_title": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "306-ARR_v1_97",
            "content": "Maja Popovi\u0107, chrF: character n-gram F-score for automatic MT evaluation, 2015, Proceedings of the Tenth Workshop on Statistical Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Maja Popovi\u0107"
                ],
                "title": "chrF: character n-gram F-score for automatic MT evaluation",
                "pub_date": "2015",
                "pub_title": "Proceedings of the Tenth Workshop on Statistical Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "306-ARR_v1_98",
            "content": "Mengnan Qi, Hao Liu, Yuzhuo Fu, Ting Liu, Improving abstractive dialogue summarization with hierarchical pretraining and topic segment, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Mengnan Qi",
                    "Hao Liu",
                    "Yuzhuo Fu",
                    "Ting Liu"
                ],
                "title": "Improving abstractive dialogue summarization with hierarchical pretraining and topic segment",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_99",
            "content": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Language models are unsupervised multitask learners, 2019, OpenAI blog, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Alec Radford",
                    "Jeffrey Wu",
                    "Rewon Child",
                    "David Luan",
                    "Dario Amodei",
                    "Ilya Sutskever"
                ],
                "title": "Language models are unsupervised multitask learners",
                "pub_date": "2019",
                "pub_title": "OpenAI blog",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_100",
            "content": "A Peter, John Rankel, Hoa Conroy, Ani Dang,  Nenkova, A decade of automatic content evaluation of news summaries: Reassessing the state of the art, 2013, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "A Peter",
                    "John Rankel",
                    "Hoa Conroy",
                    "Ani Dang",
                    " Nenkova"
                ],
                "title": "A decade of automatic content evaluation of news summaries: Reassessing the state of the art",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "306-ARR_v1_101",
            "content": "Vasile Rus, Mihai Lintean, An optimal assessment of natural language student input using word-to-word similarity metrics, 2012, International Conference on Intelligent Tutoring Systems, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Vasile Rus",
                    "Mihai Lintean"
                ],
                "title": "An optimal assessment of natural language student input using word-to-word similarity metrics",
                "pub_date": "2012",
                "pub_title": "International Conference on Intelligent Tutoring Systems",
                "pub": "Springer"
            }
        },
        {
            "ix": "306-ARR_v1_102",
            "content": "Natalie Schluter, The limits of automatic summarisation according to ROUGE, 2017, Proceedings of the 15th Conference of the European Chapter, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Natalie Schluter"
                ],
                "title": "The limits of automatic summarisation according to ROUGE",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 15th Conference of the European Chapter",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "306-ARR_v1_103",
            "content": "Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, Patrick Gallinari, QuestEval: Summarization asks for fact-based evaluation, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Thomas Scialom",
                    "Paul-Alexis Dray",
                    "Sylvain Lamprier",
                    "Benjamin Piwowarski",
                    "Jacopo Staiano",
                    "Alex Wang",
                    "Patrick Gallinari"
                ],
                "title": "QuestEval: Summarization asks for fact-based evaluation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_104",
            "content": "Thomas Scialom, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Answers unite! unsupervised metrics for reinforced summarization models, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Thomas Scialom",
                    "Sylvain Lamprier",
                    "Benjamin Piwowarski",
                    "Jacopo Staiano"
                ],
                "title": "Answers unite! unsupervised metrics for reinforced summarization models",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_105",
            "content": "Abigail See, J Peter, Christopher Liu,  Manning, Get to the point: Summarization with pointergenerator networks, 2017, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Abigail See",
                    "J Peter",
                    "Christopher Liu",
                    " Manning"
                ],
                "title": "Get to the point: Summarization with pointergenerator networks",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "306-ARR_v1_106",
            "content": "UNKNOWN, None, 2017, Relevance of unsupervised metrics in task-oriented dialogue for evaluating natural language generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Relevance of unsupervised metrics in task-oriented dialogue for evaluating natural language generation",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_107",
            "content": "Xiangru Tang, Arjun Nair, Borui Wang, Bingyao Wang, Jai Desai, Aaron Wade, Haoran Li, Confit: Toward faithful dialogue summarization with linguistically-informed contrastive fine-tuning, 2021, Computing Research Repository, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Xiangru Tang",
                    "Arjun Nair",
                    "Borui Wang",
                    "Bingyao Wang",
                    "Jai Desai",
                    "Aaron Wade",
                    "Haoran Li"
                ],
                "title": "Confit: Toward faithful dialogue summarization with linguistically-informed contrastive fine-tuning",
                "pub_date": "2021",
                "pub_title": "Computing Research Repository",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_108",
            "content": "Oleg Vasilyev, Vedant Dharnidharka, John Bohannon, Fill in the BLANC: Human-free quality estimation of document summaries, 2020, Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Oleg Vasilyev",
                    "Vedant Dharnidharka",
                    "John Bohannon"
                ],
                "title": "Fill in the BLANC: Human-free quality estimation of document summaries",
                "pub_date": "2020",
                "pub_title": "Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_109",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_110",
            "content": "Chien-Sheng Wu, Linqing Liu, Wenhao Liu, Controllable abstractive dialogue summarization with sketch supervision, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Chien-Sheng Wu",
                    "Linqing Liu",
                    "Wenhao Liu"
                ],
                "title": "Controllable abstractive dialogue summarization with sketch supervision",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "306-ARR_v1_111",
            "content": "Weizhe Yuan, Graham Neubig, Pengfei Liu, Bartscore: Evaluating generated text as text generation, 2021, Computing Research Repository, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Weizhe Yuan",
                    "Graham Neubig",
                    "Pengfei Liu"
                ],
                "title": "Bartscore: Evaluating generated text as text generation",
                "pub_date": "2021",
                "pub_title": "Computing Research Repository",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_112",
            "content": "Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu, Pegasus: Pre-training with extracted gap-sentences for abstractive summarization, 2020, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Jingqing Zhang",
                    "Yao Zhao",
                    "Mohammad Saleh",
                    "Peter Liu"
                ],
                "title": "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
                "pub_date": "2020",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "306-ARR_v1_113",
            "content": "Tianyi Zhang, * , Varsha Kishore, * , Felix Wu, * , Kilian Weinberger, Yoav Artzi, Bertscore: Evaluating text generation with bert, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Tianyi Zhang",
                    "* ",
                    "Varsha Kishore",
                    "* ",
                    "Felix Wu",
                    "* ",
                    "Kilian Weinberger",
                    "Yoav Artzi"
                ],
                "title": "Bertscore: Evaluating text generation with bert",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_114",
            "content": "Lulu Zhao, Weiran Xu, Improving abstractive dialogue summarization with graph structures and topic words, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Lulu Zhao",
                    "Weiran Xu"
                ],
                "title": "Improving abstractive dialogue summarization with graph structures and topic words",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_115",
            "content": "Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian Meyer, Steffen Eger, MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "Wei Zhao",
                    "Maxime Peyrard",
                    "Fei Liu",
                    "Yang Gao",
                    "Christian Meyer",
                    "Steffen Eger"
                ],
                "title": "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_116",
            "content": "UNKNOWN, None, , Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "306-ARR_v1_117",
            "content": "Chenguang Zhu, Yang Liu, Jie Mei, Michael Zeng, MediaSum: A large-scale media interview dataset for dialogue summarization, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": [
                    "Chenguang Zhu",
                    "Yang Liu",
                    "Jie Mei",
                    "Michael Zeng"
                ],
                "title": "MediaSum: A large-scale media interview dataset for dialogue summarization",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "306-ARR_v1_118",
            "content": "Yicheng Zou, Bolin Zhu, Xingwu Hu, Tao Gui, Qi Zhang, Low-resource dialogue summarization with domain-agnostic multi-source pretraining, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics. A Appendix.",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": [
                    "Yicheng Zou",
                    "Bolin Zhu",
                    "Xingwu Hu",
                    "Tao Gui",
                    "Qi Zhang"
                ],
                "title": "Low-resource dialogue summarization with domain-agnostic multi-source pretraining",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics. A Appendix"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "306-ARR_v1_0@0",
            "content": "DialSummEval: Revisiting Summarization Evaluation for Dialogues",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_0",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_2@0",
            "content": "Dialogue summarization is receiving increasing attention from researchers due to its extraordinary difficulty and unique application value.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_2",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_2@1",
            "content": "We observe that current dialogue summarization models have flaws that may not be well exposed by frequently used metrics such as ROUGE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_2",
            "start": 140,
            "end": 274,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_2@2",
            "content": "In our paper, we re-evaluate 18 categories of metrics in terms of four dimensions: coherence, consistency, fluency and relevance, as well as a unified human evaluation of various models for the first time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_2",
            "start": 276,
            "end": 480,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_2@3",
            "content": "Some noteworthy trends which are different from the conventional summarization tasks are identified.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_2",
            "start": 482,
            "end": 581,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_2@4",
            "content": "We will release DialSummEval, a multifaceted dataset of human judgments containing the outputs of 14 models on SAMSum.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_2",
            "start": 583,
            "end": 700,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_4@0",
            "content": "Neural network based approaches and sizable datasets have led to significant progress in researches towards conventional summarization tasks such as news and scientific papers (Lin and Ng, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_4",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_4@1",
            "content": "Compared with conventional summarization tasks, dialogue summarization has received increasing attention from researchers due to its great difficulty and unique application value (Feng et al., 2021a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_4",
            "start": 196,
            "end": 395,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_4@2",
            "content": "With the proposal of dialogue summary datasets such as SAMSum (Gliwa et al., 2019), DialogSum and Medi-aSum , a number of models for automatic generation of dialogue summaries have emerged (Feng et al., 2021b;Chen and Yang, 2020;.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_4",
            "start": 397,
            "end": 626,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_5@0",
            "content": "There is no denying that these studies have made promising progress, but it remains a challenge to evaluate these advances comprehensively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_5",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_5@1",
            "content": "Current studies generally use the SAMSum dataset and adopt ROUGE (Lin, 2004), an n-grambased automatic evaluation metric using reference summaries, as the overall evaluation criterion for summary quality, complemented by manual evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_5",
            "start": 140,
            "end": 378,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_5@2",
            "content": "Schluter (2017) and Graham (2015) illustrate the limitations of ROUGE in evaluating summarization tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_5",
            "start": 380,
            "end": 483,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_5@3",
            "content": "Also the manual evaluation protocols vary from one research to another based on our observations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_5",
            "start": 485,
            "end": 581,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_5@4",
            "content": "We argue that the inadequate evaluation mechanism may have become a major obstacle to the progress of dialogue summarization researches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_5",
            "start": 583,
            "end": 718,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_5@5",
            "content": "Many studies, such as Chen and Yang (2020) and Tang et al. (2021), have pointed out that the current dialogue summarization models still have many shortcomings, such as wrong references, incorrect reasoning and improper gender pronouns, and ROUGE may not reflect these problems effectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_5",
            "start": 720,
            "end": 1009,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_5@6",
            "content": "For example, Gabriel et al. (2021) note that ROUGE-1 and ROUGE-L fail to accurately measure factual inconsistency across domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_5",
            "start": 1011,
            "end": 1139,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_5@7",
            "content": "Our case study in Table 1 also illustrates this point.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_5",
            "start": 1141,
            "end": 1194,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_5@8",
            "content": "However, it is impractical to perform frequent time-consuming and costly manual evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_5",
            "start": 1196,
            "end": 1286,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_5@9",
            "content": "The alternative is to introduce or propose more reliable automatic evaluation metrics to evaluate the models in a more comprehensive and finegrained manner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_5",
            "start": 1288,
            "end": 1443,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_6@0",
            "content": "Although there are automatic evaluation metrics for measuring the quality of all aspects of summaries on conventional summarization tasks, especially for factual consistency (Huang et al., 2021), it is difficult to guarantee that they will still perform well on dialogue summarizarion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_6",
            "start": 0,
            "end": 284,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_6@1",
            "content": "Recently proposed automatic metrics for evaluating generic natural language generation tasks such as BERTScore , BARTScore have also not been experimented on dialogue summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_6",
            "start": 286,
            "end": 466,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_6@2",
            "content": "The high abstraction level, low extraction rate, and the requirement for complex reasoning power of the dialogue summarization task present new challenges to automatic evaluation metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_6",
            "start": 468,
            "end": 654,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_6@3",
            "content": "There Ola should be free by 8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_6",
            "start": 656,
            "end": 685,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_6@4",
            "content": "Kurt wants her to call him.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_6",
            "start": 687,
            "end": 713,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_7@0",
            "content": "Ola will be late.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_7",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_7@1",
            "content": "She should be free by 8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_7",
            "start": 18,
            "end": 41,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_7@2",
            "content": "Kurt will call her.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_7",
            "start": 43,
            "end": 61,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_7@3",
            "content": "0.69 0.42 0.67 have been a number of manual evaluation datasets and analytical studies for conventional summarization tasks ((Dang and Owczarzak, 2008); Fabbri et al., 2021b;Bhandari et al., 2020), but very little work has been done on systematic analysis of dialogue summarization models and evaluation metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_7",
            "start": 63,
            "end": 374,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_7@4",
            "content": "Our work will fill the gap in this area and includes the following contributions: 1) We identify evaluation problems in the field of dialogue summarization and point out the urgent need of automatic evaluation metrics that better adapt to dialogue summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_7",
            "start": 376,
            "end": 637,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_7@5",
            "content": "2) We collect and provide a sizable, multi-faceted dataset of manual evaluations for dialogue summarization, which contains the output of 14 models, and the dataset will be released.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_7",
            "start": 639,
            "end": 820,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_7@6",
            "content": "3) We re-evaluate the performance of 18 types of automatic evaluation metrics on dialogue summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_7",
            "start": 822,
            "end": 925,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_7@7",
            "content": "4) We evaluate a variety of dialogue summarization models (extractive, abstractive, and recently based on pre-trained language models) in a unified manner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_7",
            "start": 927,
            "end": 1081,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_8@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_8",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_9@0",
            "content": "Meta-Evaluation with Human Judgments Automatic evaluation Metrics such as ROUGE (Lin, 2004) and BERTScore )) were compared with other metrics when proposed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_9",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_9@1",
            "content": "However, they are basically not using the dialogue summarization dataset as an experimental corpus, and rarely provide new human judgments data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_9",
            "start": 157,
            "end": 300,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_9@2",
            "content": "Bhandari et al. (2020) used pyramid (Nenkova and Passonneau, 2004), a widely used human evaluation method on several conventional summarization datasets to obtain relevance scores for some of the system outputs and re-evaluated the metrics in 6 categories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_9",
            "start": 302,
            "end": 557,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_9@3",
            "content": "Similarly, Fabbri et al. (2021b) used CNN/DailyMail dataset (Hermann et al., 2015) and the output of some models for human evaluation covering four facets of relevance, consistency, fluency, and coherence, and then re-evaluated the metrics in 14 categories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_9",
            "start": 559,
            "end": 815,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_9@4",
            "content": "None of these involved dialogue summarization datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_9",
            "start": 817,
            "end": 871,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_9@5",
            "content": "Gabriel et al. (2021) is one of the few current studies using the dialogue summarization dataset SAMSum (Gliwa et al., 2019) for meta-evaluation, but it focuses on factual consistency and selects a small number of metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_9",
            "start": 873,
            "end": 1094,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_10@0",
            "content": "Evaluation for Dialogue Summarization Models Tang et al. (2021) and Chen and Yang (2020) sample the output of models on SAMSum and analyzes the error types when proposing a new model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_10",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_10@1",
            "content": "Due to the different manual evaluation protocols and the small number of models included, it is difficult to comprehensively compare the strengths and weaknesses of different models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_10",
            "start": 184,
            "end": 365,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_11@0",
            "content": "Preliminaries",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_11",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_12@0",
            "content": "In this section, we introduce the involved dataset, metrics and models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_12",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_13@0",
            "content": "Dataset",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_13",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_14@0",
            "content": "SAMSum (Gliwa et al., 2019) is the first manually annotated, high-quality chat summarization dataset, containing over 16k dialogues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_14",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_14@1",
            "content": "We use it in this study as it is most widely used and has greatly promoted the research in the field of dialogue summarization, and we are able to collect the outputs of various models on this dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_14",
            "start": 133,
            "end": 333,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_15@0",
            "content": "Evaluation Metrics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_15",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_16@0",
            "content": "We selected a number of evaluation metrics that are frequently used on summarization or other natural language generation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_16",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_16@1",
            "content": "Some are for overall quality; others are specific to a particular aspect.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_16",
            "start": 129,
            "end": 201,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_16@2",
            "content": "Some require reference summaries or source documents; some only need the summary itself.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_16",
            "start": 203,
            "end": 290,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_16@3",
            "content": "Here is a brief categorization and description.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_16",
            "start": 292,
            "end": 338,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_17@0",
            "content": "Metrics based on n-gram overlap include: ROUGE (Lin, 2004) is the most widely used automatic evaluation metric in summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_17",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_17@1",
            "content": "Researchers mainly adopt ROUGE-1, ROUGE-2 and ROUGE-L, which measure the unigram-overlap, bigram-overlap and longest common sequence between two texts respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_17",
            "start": 129,
            "end": 292,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_17@2",
            "content": "1 BLEU (Papineni et al., 2002) is the primary evaluation metric for machine translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_17",
            "start": 294,
            "end": 381,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_17@3",
            "content": "It calculates n-gram overlap between texts using precision scores and includes a brevity penalty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_17",
            "start": 383,
            "end": 479,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_17@4",
            "content": "2 METEOR (Banerjee and Lavie, 2005) computes an alignment by mapping unigrams in two texts, based on surface forms, stemmed forms, and meanings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_17",
            "start": 481,
            "end": 624,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_18@0",
            "content": "CHRF (Popovi\u0107, 2015) computes character based n-gram overlap between two texts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_18",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_18@1",
            "content": "3 Metrics based on pre-trained language models include:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_18",
            "start": 80,
            "end": 134,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_19@0",
            "content": "BERTScore measures the soft-overlap between two texts at token level using contextual embeddings from BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_19",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_19@1",
            "content": "4 MoverScore (Zhao et al., 2019) applies the semantic distance between two texts at n-gram level using n-gram embeddings pooled from BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_19",
            "start": 108,
            "end": 245,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_19@2",
            "content": "5 BARTScore assumes that models trained to convert the generated text to/from a reference output or the source text will achieve higher scores when the generated text is better.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_19",
            "start": 247,
            "end": 423,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_19@3",
            "content": "It can be flexibly applied to evaluation of text from different perspectives using BART. 6 BLANC (Vasilyev et al., 2020) is a referenceless metric.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_19",
            "start": 425,
            "end": 571,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_19@4",
            "content": "It hypothesizes that a good summary is beneficial for a pre-trained language model to conduct language understanding tasks on the source document.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_19",
            "start": 573,
            "end": 718,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_19@5",
            "content": "Specifically, it measures the performance boost of BERT by utilizing the summary in two different ways.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_19",
            "start": 720,
            "end": 822,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_19@6",
            "content": "7 PPL, namely perplexity, is often used to evaluate the quality of a language model or the fluency of an utterance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_19",
            "start": 824,
            "end": 938,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_19@7",
            "content": "We adopt GPT-2 (Radford et al., 2019) as the language model for computing the perplexity for the whole summary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_19",
            "start": 940,
            "end": 1050,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_19@8",
            "content": "8 Metrics based on word embeddings include: SMS (Clark et al., 2019), namely Sentence Mover Similarity, extends Word Movers Distance (Kusner et al., 2015) to measure the distance between two texts which are represented as a bag of sentence embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_19",
            "start": 1052,
            "end": 1302,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_19@9",
            "content": "9",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_19",
            "start": 1304,
            "end": 1304,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_20@0",
            "content": "Embedding average (Sharma et al., 2017) is an embedding based metric computing the cosine similarity between the embeddings of two texts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_20",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_20@1",
            "content": "A sentence-level embedding is represented by averaging the embeddings of the words composing the sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_20",
            "start": 138,
            "end": 243,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_21@0",
            "content": "Vector extrema (Forgues et al., 2014) is also an embedding based metric similar to Embedding average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_21",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_21@1",
            "content": "The metric computes a sentence-level embedding by taking the most extreme value of the embeddings of the words composing the sentence for each dimension of the embedding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_21",
            "start": 102,
            "end": 271,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_22@0",
            "content": "Greedy matching (Rus and Lintean, 2012) is another embedding based metric.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_22",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_22@1",
            "content": "The metric does not compute a sentence-level embedding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_22",
            "start": 75,
            "end": 129,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_22@2",
            "content": "It directly compares the embeddings of words in the two sentences using a greedy matching algorithm to calculate similarity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_22",
            "start": 131,
            "end": 254,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_23@0",
            "content": "FEQA (Durmus et al., 2020) employs a BERTbased question-answering model to answer questions using source document.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_23",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_23@1",
            "content": "Questions are generated by a fine-tuned BART model using generated summaries with masked named entities as inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_23",
            "start": 115,
            "end": 228,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_23@2",
            "content": "The metric reports F1 scores against the gold answer, which are often regarded as a measure of factual consistency. 10",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_23",
            "start": 230,
            "end": 347,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_24@0",
            "content": "SummaQA (Scialom et al., 2019) is also a QAbased metric.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_24",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_24@1",
            "content": "Unlike FEQA, it generates questions from source documents instead of summaries to be evaluated and then uses summaries to answer them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_24",
            "start": 57,
            "end": 190,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_24@2",
            "content": "The F1 overlap score and QA-model confidence are reported.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_24",
            "start": 192,
            "end": 249,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_24@3",
            "content": "11 QuestEval (Scialom et al., 2021) is another a QA-based metric.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_24",
            "start": 251,
            "end": 315,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_24@4",
            "content": "This metric can be considered as a combination of FEQA and SummaQA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_24",
            "start": 317,
            "end": 383,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_24@5",
            "content": "It takes into account the scores obtained from both styles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_24",
            "start": 385,
            "end": 443,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_24@6",
            "content": "For comparison purposes, We use the reference-less mode.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_24",
            "start": 445,
            "end": 500,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_24@7",
            "content": "12 Metrics based on entailment classification include:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_24",
            "start": 502,
            "end": 555,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_25@0",
            "content": "FactCC (Kryscinski et al., 2020) is an entailment classification metric.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_25",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_25@1",
            "content": "We follow the way Pagnoni et al. (2021) used it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_25",
            "start": 73,
            "end": 120,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_25@2",
            "content": "Each sentence of the summary is fed into the classifier together with the document to determine whether the facts are consistent, and the proportion of consistent sentences is used to indicate how consistent the summary is.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_25",
            "start": 122,
            "end": 344,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_26@0",
            "content": "13 DAE (Goyal and Durrett, 2020;Goyal and Durrett, 2021) is an entailment classification metric based on dependencies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_26",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_26@1",
            "content": "We use it in a similar way to FactCC.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_26",
            "start": 119,
            "end": 155,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_26@2",
            "content": "When a sentence cannot be parsed by the metric, we default it factually inconsistent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_26",
            "start": 157,
            "end": 241,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_26@3",
            "content": "14",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_26",
            "start": 243,
            "end": 244,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_27@0",
            "content": "Summarization Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_27",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_28@0",
            "content": "We select some representative models and get the outputs of them on the test set of SAMSum.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_28",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_28@1",
            "content": "We choose LEAD-3 and LONGEST-3 as representatives of the simple extractive approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_28",
            "start": 92,
            "end": 177,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_28@2",
            "content": "PGN (See et al., 2017) and Transformer (Vaswani et al., 2017) are selected as representatives of the earlier neural summarization models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_28",
            "start": 179,
            "end": 315,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_28@3",
            "content": "For generic pretrained generative models, we use BART (Lewis et al., 2020), PEGASUS and UniLM (Dong et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_28",
            "start": 317,
            "end": 430,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_28@4",
            "content": "We retrain these models above to obtain the outputs and the automatic evaluation results are close to Gliwa et al. (2019) and Wu et al. (2021) in default settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_28",
            "start": 432,
            "end": 594,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_28@5",
            "content": "For models specifically designed for dialogue summarization, we choose CODS (Wu et al., 2021), Con-voSumm (Fabbri et al., 2021a), MV-BART (Chen 11 https://github.com/ThomasScialom/ summa-qa 12 https://github.com/ThomasScialom/ QuestEval 13 https://github.com/salesforce/factCC 14 https://github.com/tagoyal/ factuality-datasets and Yang, 2020), PLM-BART (Feng et al., 2021c), Ctrl-DiaSumm ), S-BART and the outputs are all provided by their authors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_28",
            "start": 596,
            "end": 1044,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_28@6",
            "content": "We also regard the reference summary as a kind of model output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_28",
            "start": 1046,
            "end": 1108,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_29@0",
            "content": "Data Annotation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_29",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_30@0",
            "content": "Annotation Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_30",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_31@0",
            "content": "Since human evaluation is expensive and timeconsuming, we decide to randomly sample 100 dialogues from the test set of SAMSum and evaluate the summaries generated by all models on these dialogues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_31",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_31@1",
            "content": "To comprehensively evaluate each metric and model, we perform human evaluation in four aspects, as in Kryscinski et al. (2019):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_31",
            "start": 197,
            "end": 323,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_32@0",
            "content": "Coherence measures the quality of all sentences in the summary as a whole.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_32",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_32@1",
            "content": "It focuses on whether the summary is coherent and natural.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_32",
            "start": 75,
            "end": 132,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_33@0",
            "content": "Consistency measures how well the summary aligns with the dialogue in facts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_33",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_33@1",
            "content": "It focuses on whether the summary contains factual errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_33",
            "start": 77,
            "end": 134,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_34@0",
            "content": "Fluency measures the quality of individual sentences in the summary compared to Coherence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_34",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_34@1",
            "content": "It focuses on whether the sentences are well-written and grammatically correct.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_34",
            "start": 91,
            "end": 169,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_35@0",
            "content": "Relevance measures how well the summary captures the key points of the dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_35",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_35@1",
            "content": "It focuses on whether all and only the important aspects are contained in the summary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_35",
            "start": 81,
            "end": 166,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_36@0",
            "content": "To ensure the quality of the annotation, we tried to annotate some of the data ourselves at the beginning to judge the difficulty of the task and the approximate time spent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_36",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_37@0",
            "content": "Annotation Process",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_37",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_38@0",
            "content": "We initially tried to annotate the data using crowdsourcing platforms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_38",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_38@1",
            "content": "We published the annotation task on Amazon Mechanical Turk 15 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_38",
            "start": 71,
            "end": 133,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_38@2",
            "content": "The interface contained instructions and definitions of the four aspects.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_38",
            "start": 135,
            "end": 207,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_38@3",
            "content": "A dialogue and a corresponding summary were included in the interface, and the summaries of different models on the same dialogue were presented to the annotators in a sequence to facilitate comparison.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_38",
            "start": 209,
            "end": 410,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_38@4",
            "content": "For each dimension/aspect, annotators were asked to rate the summary on a Likert scale from 1 to 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_38",
            "start": 412,
            "end": 510,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_38@5",
            "content": "Each summary was evaluated by 5 different annotators, and For each dimension we would receive a total of 100 \u00d7 14 \u00d7 5 = 7000 human annotations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_38",
            "start": 512,
            "end": 654,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_38@6",
            "content": "The annotation was done quickly in one day, but the quality was not satisfactory.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_38",
            "start": 656,
            "end": 736,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_38@7",
            "content": "We calculated the average score of each model in each aspect based on these annotation data and found that the scores of the models are close in each dimension, which is not in the accordance with the reality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_38",
            "start": 738,
            "end": 946,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_38@8",
            "content": "For example, in terms of consistency, the reference summary and the extractive approaches should have had a definite advantage, but this failed to be reflected from the data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_38",
            "start": 948,
            "end": 1121,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_38@9",
            "content": "The result is shown in Table 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_38",
            "start": 1123,
            "end": 1153,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_38@10",
            "content": "For reliability reasons, we do not use these annotations for our analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_38",
            "start": 1155,
            "end": 1228,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_39@0",
            "content": "Then, we decided to recruit annotators from the school forum who are required to be capable of reading daily conversations and articles in English fluently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_39",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_39@1",
            "content": "We recruited three annotators, using a similar annotation interface and approach as in the crowd-sourcing platforms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_39",
            "start": 157,
            "end": 272,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_39@2",
            "content": "These annotators were college students and they are fluent in English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_39",
            "start": 274,
            "end": 343,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_39@3",
            "content": "The differences with the crowd-sourcing platform annotation are as follows: 1) For a student who wanted to participate in the annotation, we would ask him to annotate all models on the first 10 conversations (10\u00d714 = 140 annotations), and let her/him continue the annotation only when these annotation results were checked by us to confirm that the annotator had understood the task correctly and could finish the annotation responsibly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_39",
            "start": 345,
            "end": 781,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_39@4",
            "content": "Otherwise, we paid the annotator directly for this part and terminated his annotation task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_39",
            "start": 783,
            "end": 873,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_39@5",
            "content": "2) We required each annotator to annotate all data (100 \u00d7 14 = 1400 annotations) to ensure the consistency within the annotator.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_39",
            "start": 875,
            "end": 1002,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_39@6",
            "content": "3) During the annotation process, we kept in touch with the annotators via email or instant messaging app to answer their questions at any time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_39",
            "start": 1004,
            "end": 1147,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_40@0",
            "content": "It took around 10 days to finish the annotation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_40",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_40@1",
            "content": "We received 100 \u00d7 14 \u00d7 3 = 4200 annotations for each perspective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_40",
            "start": 49,
            "end": 113,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_40@2",
            "content": "For each aspect of each summary, if two scores were the same and the other was different from them, we considered the different one as noise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_40",
            "start": 115,
            "end": 255,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_40@3",
            "content": "For each dimension, we removed the noise separately and calculated the the Krippendorffs alpha coefficient (Krippendorff, 2011).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_40",
            "start": 257,
            "end": 384,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_40@4",
            "content": "We found the inter-annotator interval kappa to be within an acceptable range -from 0.5621 to 0.7564, as detailed in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_40",
            "start": 386,
            "end": 509,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_40@5",
            "content": "The raw annotated data will be released and we use the cleaned data for analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_40",
            "start": 511,
            "end": 591,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_40@6",
            "content": "At last, we use the average of the remaining data to represent the human evaluation score of an summary on a dimension.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_40",
            "start": 593,
            "end": 711,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_41@0",
            "content": "Metric Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_41",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_42@0",
            "content": "In this section, we will introduce several definitions in meta-evaluation and re-evaluate the metrics mentioned in Section 3.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_42",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_43@0",
            "content": "Task Formulation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_43",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_44@0",
            "content": "As mentioned by Bhandari et al. (2020), there are two common ways to measure the correlation of automatic evaluation metrics to manual evaluation: system-level and summary-level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_44",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_45@0",
            "content": "Assuming there are N dialogues, the i-th dialogue is represented as d i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_45",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_45@1",
            "content": "For a dialogue d i , there are J summaries generated by J models, and we denote each of them as s ij , j = 1 \u2022 \u2022 \u2022 J. There are K evaluation metrics in total, and m k refers to an automatic evaluation metric or human evaluation of a certain dimension.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_45",
            "start": 74,
            "end": 324,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_45@2",
            "content": "m k (s ij ) means the score of k-th metric towards a pair of dialogue and summary (d i , s ji ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_45",
            "start": 326,
            "end": 421,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_45@3",
            "content": "We use R(m i , m j ) to denote the correlation coefficient between two metrics m i and m j .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_45",
            "start": 423,
            "end": 514,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_46@0",
            "content": "System-level correlation is defined as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_46",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_46@1",
            "content": "The corresponding p-value which indicates statistical significance can be obtained:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_46",
            "start": 48,
            "end": 130,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_47@0",
            "content": "R sys (m p , m q ) = R( [ 1 N N \u2211 i=1 m p (s i1 ), \u2022 \u2022 \u2022 , 1 N N \u2211 i=1 m p (s iJ )], [ 1 N N \u2211 i=1 m q (s i1 ), \u2022 \u2022 \u2022 , 1 N N \u2211 i=1 m q (s iJ )])",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_47",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_48@0",
            "content": "Summary-level correlation is defined as follows, and the p-value cannot be derived here because the Summary-level correlation is an average value:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_48",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_49@0",
            "content": "R sum (m p , m q ) = 1 N N \u2211 i=1 R([m p (s i1 ), \u2022 \u2022 \u2022 , m p (s iJ )], [m q (s i1 ), \u2022 \u2022 \u2022 , m q (s iJ )])",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_49",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_50@0",
            "content": "Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_50",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_51@0",
            "content": "Comparing the performance of various metrics reveals some trends in strengths in all four dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_51",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_51@1",
            "content": "Of all the metrics, QuestEval has the most comprehensive capabilities at the system level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_51",
            "start": 102,
            "end": 191,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_51@2",
            "content": "Generally metrics that perform better on coherence and fluency perform worse on consistency and relevance, and vice versa.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_51",
            "start": 193,
            "end": 314,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_51@3",
            "content": "This can be attributed to the definition of the dimensions, i.e. there is some correlation between the four dimensions themselves.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_51",
            "start": 316,
            "end": 445,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_51@4",
            "content": "In all dimensions, automatic evaluation metrics based on pre-trained language models generally outperform metrics based on n-gram overlap and contextindependent word embedding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_51",
            "start": 447,
            "end": 622,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_51@5",
            "content": "Among them, the recently proposed BARTScore and the increasingly popular QA-based metrics perform the best.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_51",
            "start": 624,
            "end": 730,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_51@6",
            "content": "This suggests that both directions have the potential to be explored in terms of evaluation for dialogue summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_51",
            "start": 732,
            "end": 850,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_51@7",
            "content": "Across dimensions, almost all metrics correlate better with human judgments at the system level than at the summary level, and both showed good agreement with each other.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_51",
            "start": 852,
            "end": 1021,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_51@8",
            "content": "This indicates that the summary-level correlations are also worth referring to when enough data are not available for system-level analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_51",
            "start": 1023,
            "end": 1162,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_51@9",
            "content": "In addition, metrics such as BLEU and CHRF, which are frequently used in other natural language generation tasks (e.g., machine translation, dialogue, etc.), do not show advantages on dialogue summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_51",
            "start": 1164,
            "end": 1370,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_52@0",
            "content": "The characteristics presented by the automatic evaluation metrics on the dialogue summarization differ from those of the conventional summarization tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_52",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_52@1",
            "content": "For ROUGE, we find that increasing the size of n in ROUGE-n is not better in almost all dimensions, which is different from the findings of Rankel et al. ( 2013) and Fabbri et al. (2021b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_52",
            "start": 155,
            "end": 342,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_52@2",
            "content": "The ability of ROUGE to reflect content selection, i.e., relevance, as we usually believe, is also questionable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_52",
            "start": 344,
            "end": 455,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_52@3",
            "content": "Compared to the results of Fabbri et al. (2021b), metrics based on n-gram overlap such as ROUGE and CHRF perform worse on dialogue summarization, while some metrics that use source documents such as BLANC perform better.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_52",
            "start": 457,
            "end": 676,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_52@4",
            "content": "We need to focus on the limitations of ROUGE and the role of the source dialogues in evaluating dialogue summaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_52",
            "start": 678,
            "end": 792,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_53@0",
            "content": "We have also observed some interesting phenomena.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_53",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_53@1",
            "content": "Entailment classification metrics such as FactCC and DAE outperform many metrics in terms of consistency, but not as well as BARTScore and QA-based metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_53",
            "start": 50,
            "end": 205,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_53@2",
            "content": "This may be due to the large gap between the corpus used in training and dialogues, and the need to slice the summaries by sentence when using them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_53",
            "start": 207,
            "end": 354,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_53@3",
            "content": "FEQA, which is designed for factual consistency, however, performs best in coherence and fluency, and rather poorly in consistency and relevance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_53",
            "start": 356,
            "end": 500,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_53@4",
            "content": "Comparing its performance with QuestEval and SummaQA, generating questions from the original dialogue may be more reliable in measuring consistency, which corroborates with the points of Gabriel et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_53",
            "start": 502,
            "end": 710,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_53@5",
            "content": "It is surprising that metrics based on the language model such as PPL, BARTScore-h performs poorly in measuring both coherence and fluency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_53",
            "start": 712,
            "end": 850,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_53@6",
            "content": "The exact reasons for this need further investigation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_53",
            "start": 852,
            "end": 905,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_54@0",
            "content": "Model Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_54",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_55@0",
            "content": "In each dimension, we evaluate each model mentioned in Section 3.3 using the average of the human evaluation scores of all summaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_55",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_55@1",
            "content": "Analyzing Table 4, we conclude the following.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_55",
            "start": 134,
            "end": 178,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_56@0",
            "content": "The reference summaries in SAMSum are not perfect, and the annotators felt that they also contained some factual inconsistencies compared to the source dialogues, as well as important elements of the dialogues that were not all captured by them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_56",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_56@1",
            "content": "However, comparing the human evaluation scores of the reference summaries in CNNDM (Fabbri et al., 2021b), the quality is already superior.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_56",
            "start": 246,
            "end": 384,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_57@0",
            "content": "Extractive models produce summaries that differ in style from abstractive models, and many conversations contain ungrammatical utterances, which can affect the reading experience and impair their fluency and coherence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_57",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_57@1",
            "content": "In particular, LONGEST-3, which extracts some potentially discontinuous sentences from dialogues, has low coherence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_57",
            "start": 219,
            "end": 334,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_57@2",
            "content": "However, since they do not modify the content, they still perform well in terms of con- The correlation (Pearson's r) of annotations computed on system level and summary level along four quality dimensions between automatic metrics and human judgments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_57",
            "start": 336,
            "end": 587,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_57@3",
            "content": "For evaluation, all metrics require at least the summaries to be evaluated as input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_57",
            "start": 589,
            "end": 672,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_57@4",
            "content": "Metrics with + indicate that the source dialogues are used, metrics withmeans no other input are required, others need to use the reference summaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_57",
            "start": 674,
            "end": 823,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_57@5",
            "content": "The five most-correlated metrics in each column are bolded (For system level, **=significant for p \u2264 0.01, *=significant for p \u2264 0.05).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_57",
            "start": 825,
            "end": 959,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_57@6",
            "content": "We add suffixes to distinguish the different variants of metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_57",
            "start": 961,
            "end": 1025,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_57@7",
            "content": "For BARTScore, h, r and s are abbreviations of hypothesises, references and source dialogues respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_57",
            "start": 1027,
            "end": 1132,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_57@8",
            "content": "BARTScore-s-h measure the probability to generate hypothesises using source dialogues as inputs, while BARTScore-h measures the probability to generate hypothesises without other inputs, and so on.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_57",
            "start": 1134,
            "end": 1330,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_57@9",
            "content": "For BLANC, BLANC-tune refers to the way of fine-tuning on a generated summary and then conducting nature language understanding tasks on source dialogues, while BLANC-help refers to the way of inferring with a generated summary concatenated together.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_57",
            "start": 1332,
            "end": 1581,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_57@10",
            "content": "For SummaQA, SummaQA-fscore measures the average overlap between predictions and ground truth answers, and SummaQA-conf corresponds to the probability of the true answer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_57",
            "start": 1583,
            "end": 1752,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_57@11",
            "content": "sistency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_57",
            "start": 1754,
            "end": 1762,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_57@12",
            "content": "Since the average length of dialogues in SAMSum is small, extracting a few sentences from it can generally include important contents, so the relevance is also high.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_57",
            "start": 1764,
            "end": 1928,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_58@0",
            "content": "The early neural summrization models represented by PGN and Transformer perform relatively poorly in all dimensions compared to the reference summaries, especially consistency and relevance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_58",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_58@1",
            "content": "This is to be expected because of the high difficulty of dialogue summarization and the small size of SAMSum dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_58",
            "start": 191,
            "end": 307,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_59@0",
            "content": "An important finding is that the generic pretrained language models represented by BART, PEGASUS and UniLM, and various recently proposed models specifically designed on the dialogue summarization task do not have significant differences in each dimension.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_59",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_59@1",
            "content": "They are already comparable, and in some cases better, in terms of coherence and fluency compared to the reference summaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_59",
            "start": 257,
            "end": 381,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_59@2",
            "content": "They have improved dramatically compared to earlier neural summarization models with respect to consistency and relevance, but there is still some room for enhancement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_59",
            "start": 383,
            "end": 550,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_59@3",
            "content": "On the one hand, this finding affirms the capability of these models; On the other hand, it urges us to reflect on how much these recently proposed complex models or fancy techniques are an improvement over the generic pre-trained language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_59",
            "start": 552,
            "end": 798,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_60@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_60",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_61@0",
            "content": "We point out the problems with the evaluation in the dialogue summarization and introduce Di-alSummEval, a multi-faceted dataset containing the output of various models and the corresponding human judgments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_61",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_61@1",
            "content": "Based on this dataset, we provide a comprehensive re-evaluation and analysis of the performance of widely used automatic evaluation metrics and each model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_61",
            "start": 208,
            "end": 362,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_61@2",
            "content": "There are three important findings: 1) Few metrics are excellent in all dimensions, and the recently proposed BARTScore and QA-based metrics are comparatively outstanding and worth exploring.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_61",
            "start": 364,
            "end": 554,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_61@3",
            "content": "2) The automatic evaluation metrics and their variants present some trends that differ from conventional summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_61",
            "start": 556,
            "end": 674,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_61@4",
            "content": "3) A variety of models specifically designed for dialogue summarization perform comparably to reference summaries in terms of coherence and fluency, but still have shortcomings in consistency and relevance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_61",
            "start": 676,
            "end": 881,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_61@5",
            "content": "We hope that researchers in the field recognize the importance of evaluation in current research, choose some metrics other than ROUGE when evaluating models, propose automatic evaluation metrics that can be better adapted to the field of dialogue summarization based on our work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_61",
            "start": 883,
            "end": 1162,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_62@0",
            "content": "Ethical Considerations",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_62",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_63@0",
            "content": "Whether recruiting annotators through Amazon Mechanical Turk or campus, we paid them 15 dollars per hour, more than the local average minimum wage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_63",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_63@1",
            "content": "We removed all content in the dataset that might contain personal information about the annotators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_63",
            "start": 148,
            "end": 246,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_63@2",
            "content": "Table 5: Human ratings of summaries along four evaluation dimensions using data from Amazon Mechanical Turk. scores are averaged over five annotators, broken down by the approximate classification in Section 3.3",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_63",
            "start": 248,
            "end": 458,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_64@0",
            "content": "Satanjeev Banerjee, Alon Lavie, METEOR: An automatic metric for MT evaluation with improved correlation with human judgments, 2005, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_64",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_65@0",
            "content": "Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, Graham Neubig, Reevaluating evaluation in text summarization, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_65",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_66@0",
            "content": "Jiaao Chen, Diyi Yang, Multi-view sequenceto-sequence models with conversational structure for abstractive dialogue summarization, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_66",
            "start": 0,
            "end": 282,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_67@0",
            "content": "Jiaao Chen, Diyi Yang, Structure-aware abstractive conversation summarization via discourse and action graphs, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_67",
            "start": 0,
            "end": 261,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_68@0",
            "content": "Yulong Chen, Yang Liu, Liang Chen, Yue Zhang, DialogSum: A real-life scenario dialogue summarization dataset, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_68",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_69@0",
            "content": "Elizabeth Clark, Asli Celikyilmaz, Noah Smith, Sentence mover's similarity: Automatic evaluation for multi-sentence texts, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_69",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_70@0",
            "content": "UNKNOWN, None, 2008, Overview of the tac 2008 update summarization task, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_70",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_71@0",
            "content": "Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon, Unified language model pre-training for natural language understanding and generation, 2019, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_71",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_72@0",
            "content": "Esin Durmus, He He, Mona Diab, FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_72",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_73@0",
            "content": "Alexander Fabbri, Faiaz Rahman, Imad Rizvi, Borui Wang, Haoran Li, Yashar Mehdad, Dragomir Radev, ConvoSumm: Conversation summarization benchmark and improved abstractive summarization with argument mining, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_73",
            "start": 0,
            "end": 388,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_74@0",
            "content": "Alexander Fabbri, Wojciech Kry\u015bci\u0144ski, Bryan Mccann, Caiming Xiong, Richard Socher, Dragomir Radev, SummEval: Re-evaluating summarization evaluation, 2021, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_74",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_75@0",
            "content": "UNKNOWN, None, , Xiaocheng Feng, and Bing Qin. 2021a. A survey on dialogue summarization: Recent advances and new frontiers. Computing Research Repository, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_75",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_76@0",
            "content": "Xiachong Feng, Xiaocheng Feng, Bing Qin, Xinwei Geng, Dialogue discourse-aware graph model and data augmentation for meeting summarization, 2021, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_76",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_77@0",
            "content": "Xiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin, Ting Liu, Language model as an annotator: Exploring DialoGPT for dialogue summarization, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_77",
            "start": 0,
            "end": 321,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_78@0",
            "content": "Gabriel Forgues, Joelle Pineau, Jean-Marie Larchev\u00eaque, R\u00e9al Tremblay, Bootstrapping dialog systems with word embeddings, 2014, Nips, modern machine learning and natural language processing workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_78",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_79@0",
            "content": "Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, Jianfeng Gao, GO FIGURE: A meta evaluation of factuality in summarization, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_79",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_80@0",
            "content": "Bogdan Gliwa, Iwona Mochol, Maciej Biesek, Aleksander Wawer, SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization, 2019, Proceedings of the 2nd Workshop on New Frontiers in Summarization, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_80",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_81@0",
            "content": "Tanya Goyal, Greg Durrett, Evaluating factuality in generation with dependency-level entailment, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_81",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_82@0",
            "content": "Tanya Goyal, Greg Durrett, Annotating and modeling fine-grained factuality in summarization, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_82",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_83@0",
            "content": "Yvette Graham, Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_83",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_84@0",
            "content": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Teaching machines to read and comprehend, 2015, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_84",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_85@0",
            "content": "Yichong Huang, Xiachong Feng, Xiaocheng Feng, and Bing Qin. 2021. The factual inconsistency problem in abstractive text summarization: A survey, , Computing Research Repository, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_85",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_86@0",
            "content": "UNKNOWN, None, 2011, Computing krippendorff's alpha-reliability, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_86",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_87@0",
            "content": "Wojciech Kryscinski, Nitish Shirish Keskar, Bryan Mc-Cann, Caiming Xiong, Richard Socher, Neural text summarization: A critical evaluation, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_87",
            "start": 0,
            "end": 323,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_88@0",
            "content": "Wojciech Kryscinski, Bryan Mccann, Caiming Xiong, Richard Socher, Evaluating the factual consistency of abstractive text summarization, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_88",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_89@0",
            "content": "Matt Kusner, Yu Sun, Nicholas Kolkin, Kilian Weinberger ; Lille, France Lewis, Yinhan Liu, Naman Goyal, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension, 2015, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_89",
            "start": 0,
            "end": 411,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_90@0",
            "content": "Chin-Yew Lin, ROUGE: A package for automatic evaluation of summaries, 2004, Text Summarization Branches Out, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_90",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_91@0",
            "content": "Hui Lin, Vincent Ng, Abstractive summarization: A survey of the state of the art, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_91",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_92@0",
            "content": "Junpeng Liu, Yanyan Zou, Hainan Zhang, Hongshen Chen, Zhuoye Ding, Caixia Yuan, Xiaojie Wang, Topic-aware contrastive learning for abstractive dialogue summarization, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_92",
            "start": 0,
            "end": 285,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_93@0",
            "content": "Zhengyuan Liu, Nancy Chen, Controllable neural dialogue summarization with personal named entity planning, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_93",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_94@0",
            "content": "Ani Nenkova, Rebecca Passonneau, Evaluating content selection in summarization: The pyramid method, 2004, Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_94",
            "start": 0,
            "end": 258,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_95@0",
            "content": "Artidoro Pagnoni, Vidhisha Balachandran, Yulia Tsvetkov, Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_95",
            "start": 0,
            "end": 309,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_96@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_96",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_97@0",
            "content": "Maja Popovi\u0107, chrF: character n-gram F-score for automatic MT evaluation, 2015, Proceedings of the Tenth Workshop on Statistical Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_97",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_98@0",
            "content": "Mengnan Qi, Hao Liu, Yuzhuo Fu, Ting Liu, Improving abstractive dialogue summarization with hierarchical pretraining and topic segment, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_98",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_99@0",
            "content": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Language models are unsupervised multitask learners, 2019, OpenAI blog, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_99",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_100@0",
            "content": "A Peter, John Rankel, Hoa Conroy, Ani Dang,  Nenkova, A decade of automatic content evaluation of news summaries: Reassessing the state of the art, 2013, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_100",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_101@0",
            "content": "Vasile Rus, Mihai Lintean, An optimal assessment of natural language student input using word-to-word similarity metrics, 2012, International Conference on Intelligent Tutoring Systems, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_101",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_102@0",
            "content": "Natalie Schluter, The limits of automatic summarisation according to ROUGE, 2017, Proceedings of the 15th Conference of the European Chapter, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_102",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_103@0",
            "content": "Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, Patrick Gallinari, QuestEval: Summarization asks for fact-based evaluation, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_103",
            "start": 0,
            "end": 270,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_104@0",
            "content": "Thomas Scialom, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Answers unite! unsupervised metrics for reinforced summarization models, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_104",
            "start": 0,
            "end": 327,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_105@0",
            "content": "Abigail See, J Peter, Christopher Liu,  Manning, Get to the point: Summarization with pointergenerator networks, 2017, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_105",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_106@0",
            "content": "UNKNOWN, None, 2017, Relevance of unsupervised metrics in task-oriented dialogue for evaluating natural language generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_106",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_107@0",
            "content": "Xiangru Tang, Arjun Nair, Borui Wang, Bingyao Wang, Jai Desai, Aaron Wade, Haoran Li, Confit: Toward faithful dialogue summarization with linguistically-informed contrastive fine-tuning, 2021, Computing Research Repository, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_107",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_108@0",
            "content": "Oleg Vasilyev, Vedant Dharnidharka, John Bohannon, Fill in the BLANC: Human-free quality estimation of document summaries, 2020, Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_108",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_109@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_109",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_110@0",
            "content": "Chien-Sheng Wu, Linqing Liu, Wenhao Liu, Controllable abstractive dialogue summarization with sketch supervision, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_110",
            "start": 0,
            "end": 245,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_111@0",
            "content": "Weizhe Yuan, Graham Neubig, Pengfei Liu, Bartscore: Evaluating generated text as text generation, 2021, Computing Research Repository, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_111",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_112@0",
            "content": "Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu, Pegasus: Pre-training with extracted gap-sentences for abstractive summarization, 2020, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_112",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_113@0",
            "content": "Tianyi Zhang, * , Varsha Kishore, * , Felix Wu, * , Kilian Weinberger, Yoav Artzi, Bertscore: Evaluating text generation with bert, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_113",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_114@0",
            "content": "Lulu Zhao, Weiran Xu, Improving abstractive dialogue summarization with graph structures and topic words, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_114",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_115@0",
            "content": "Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian Meyer, Steffen Eger, MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_115",
            "start": 0,
            "end": 355,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_116@0",
            "content": "UNKNOWN, None, , Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_116",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_117@0",
            "content": "Chenguang Zhu, Yang Liu, Jie Mei, Michael Zeng, MediaSum: A large-scale media interview dataset for dialogue summarization, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_117",
            "start": 0,
            "end": 323,
            "label": {}
        },
        {
            "ix": "306-ARR_v1_118@0",
            "content": "Yicheng Zou, Bolin Zhu, Xingwu Hu, Tao Gui, Qi Zhang, Low-resource dialogue summarization with domain-agnostic multi-source pretraining, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics. A Appendix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "306-ARR_v1_118",
            "start": 0,
            "end": 284,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "306-ARR_v1_0",
            "tgt_ix": "306-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_0",
            "tgt_ix": "306-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_1",
            "tgt_ix": "306-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_1",
            "tgt_ix": "306-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_0",
            "tgt_ix": "306-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_2",
            "tgt_ix": "306-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_4",
            "tgt_ix": "306-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_5",
            "tgt_ix": "306-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_6",
            "tgt_ix": "306-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_3",
            "tgt_ix": "306-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_3",
            "tgt_ix": "306-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_3",
            "tgt_ix": "306-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_3",
            "tgt_ix": "306-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_3",
            "tgt_ix": "306-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_0",
            "tgt_ix": "306-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_7",
            "tgt_ix": "306-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_9",
            "tgt_ix": "306-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_8",
            "tgt_ix": "306-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_8",
            "tgt_ix": "306-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_8",
            "tgt_ix": "306-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_0",
            "tgt_ix": "306-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_10",
            "tgt_ix": "306-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_11",
            "tgt_ix": "306-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_11",
            "tgt_ix": "306-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_11",
            "tgt_ix": "306-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_12",
            "tgt_ix": "306-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_13",
            "tgt_ix": "306-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_13",
            "tgt_ix": "306-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_11",
            "tgt_ix": "306-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_14",
            "tgt_ix": "306-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_16",
            "tgt_ix": "306-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_17",
            "tgt_ix": "306-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_18",
            "tgt_ix": "306-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_19",
            "tgt_ix": "306-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_20",
            "tgt_ix": "306-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_21",
            "tgt_ix": "306-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_15",
            "tgt_ix": "306-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_15",
            "tgt_ix": "306-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_15",
            "tgt_ix": "306-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_15",
            "tgt_ix": "306-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_15",
            "tgt_ix": "306-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_15",
            "tgt_ix": "306-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_15",
            "tgt_ix": "306-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_15",
            "tgt_ix": "306-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_23",
            "tgt_ix": "306-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_24",
            "tgt_ix": "306-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_25",
            "tgt_ix": "306-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_15",
            "tgt_ix": "306-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_15",
            "tgt_ix": "306-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_15",
            "tgt_ix": "306-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_15",
            "tgt_ix": "306-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_22",
            "tgt_ix": "306-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_11",
            "tgt_ix": "306-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_26",
            "tgt_ix": "306-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_27",
            "tgt_ix": "306-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_27",
            "tgt_ix": "306-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_0",
            "tgt_ix": "306-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_28",
            "tgt_ix": "306-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_29",
            "tgt_ix": "306-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_29",
            "tgt_ix": "306-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_31",
            "tgt_ix": "306-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_32",
            "tgt_ix": "306-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_33",
            "tgt_ix": "306-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_34",
            "tgt_ix": "306-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_35",
            "tgt_ix": "306-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_30",
            "tgt_ix": "306-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_30",
            "tgt_ix": "306-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_30",
            "tgt_ix": "306-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_30",
            "tgt_ix": "306-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_30",
            "tgt_ix": "306-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_30",
            "tgt_ix": "306-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_30",
            "tgt_ix": "306-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_29",
            "tgt_ix": "306-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_36",
            "tgt_ix": "306-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_38",
            "tgt_ix": "306-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_39",
            "tgt_ix": "306-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_37",
            "tgt_ix": "306-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_37",
            "tgt_ix": "306-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_37",
            "tgt_ix": "306-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_37",
            "tgt_ix": "306-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_0",
            "tgt_ix": "306-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_40",
            "tgt_ix": "306-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_41",
            "tgt_ix": "306-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_41",
            "tgt_ix": "306-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_41",
            "tgt_ix": "306-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_42",
            "tgt_ix": "306-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_44",
            "tgt_ix": "306-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_45",
            "tgt_ix": "306-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_46",
            "tgt_ix": "306-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_47",
            "tgt_ix": "306-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_48",
            "tgt_ix": "306-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_43",
            "tgt_ix": "306-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_43",
            "tgt_ix": "306-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_43",
            "tgt_ix": "306-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_43",
            "tgt_ix": "306-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_43",
            "tgt_ix": "306-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_43",
            "tgt_ix": "306-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_43",
            "tgt_ix": "306-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_41",
            "tgt_ix": "306-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_49",
            "tgt_ix": "306-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_51",
            "tgt_ix": "306-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_52",
            "tgt_ix": "306-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_50",
            "tgt_ix": "306-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_50",
            "tgt_ix": "306-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_50",
            "tgt_ix": "306-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_50",
            "tgt_ix": "306-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_0",
            "tgt_ix": "306-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_53",
            "tgt_ix": "306-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_55",
            "tgt_ix": "306-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_56",
            "tgt_ix": "306-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_57",
            "tgt_ix": "306-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_58",
            "tgt_ix": "306-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_54",
            "tgt_ix": "306-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_54",
            "tgt_ix": "306-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_54",
            "tgt_ix": "306-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_54",
            "tgt_ix": "306-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_54",
            "tgt_ix": "306-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_54",
            "tgt_ix": "306-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_0",
            "tgt_ix": "306-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_59",
            "tgt_ix": "306-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_60",
            "tgt_ix": "306-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_60",
            "tgt_ix": "306-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_0",
            "tgt_ix": "306-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_61",
            "tgt_ix": "306-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_62",
            "tgt_ix": "306-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_62",
            "tgt_ix": "306-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "306-ARR_v1_0",
            "tgt_ix": "306-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_1",
            "tgt_ix": "306-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_2",
            "tgt_ix": "306-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_2",
            "tgt_ix": "306-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_2",
            "tgt_ix": "306-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_2",
            "tgt_ix": "306-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_2",
            "tgt_ix": "306-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_3",
            "tgt_ix": "306-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_4",
            "tgt_ix": "306-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_4",
            "tgt_ix": "306-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_4",
            "tgt_ix": "306-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_5",
            "tgt_ix": "306-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_5",
            "tgt_ix": "306-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_5",
            "tgt_ix": "306-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_5",
            "tgt_ix": "306-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_5",
            "tgt_ix": "306-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_5",
            "tgt_ix": "306-ARR_v1_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_5",
            "tgt_ix": "306-ARR_v1_5@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_5",
            "tgt_ix": "306-ARR_v1_5@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_5",
            "tgt_ix": "306-ARR_v1_5@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_5",
            "tgt_ix": "306-ARR_v1_5@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_6",
            "tgt_ix": "306-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_6",
            "tgt_ix": "306-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_6",
            "tgt_ix": "306-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_6",
            "tgt_ix": "306-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_6",
            "tgt_ix": "306-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_7",
            "tgt_ix": "306-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_7",
            "tgt_ix": "306-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_7",
            "tgt_ix": "306-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_7",
            "tgt_ix": "306-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_7",
            "tgt_ix": "306-ARR_v1_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_7",
            "tgt_ix": "306-ARR_v1_7@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_7",
            "tgt_ix": "306-ARR_v1_7@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_7",
            "tgt_ix": "306-ARR_v1_7@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_8",
            "tgt_ix": "306-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_9",
            "tgt_ix": "306-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_9",
            "tgt_ix": "306-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_9",
            "tgt_ix": "306-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_9",
            "tgt_ix": "306-ARR_v1_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_9",
            "tgt_ix": "306-ARR_v1_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_9",
            "tgt_ix": "306-ARR_v1_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_10",
            "tgt_ix": "306-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_10",
            "tgt_ix": "306-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_11",
            "tgt_ix": "306-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_12",
            "tgt_ix": "306-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_13",
            "tgt_ix": "306-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_14",
            "tgt_ix": "306-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_14",
            "tgt_ix": "306-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_15",
            "tgt_ix": "306-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_16",
            "tgt_ix": "306-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_16",
            "tgt_ix": "306-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_16",
            "tgt_ix": "306-ARR_v1_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_16",
            "tgt_ix": "306-ARR_v1_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_17",
            "tgt_ix": "306-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_17",
            "tgt_ix": "306-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_17",
            "tgt_ix": "306-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_17",
            "tgt_ix": "306-ARR_v1_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_17",
            "tgt_ix": "306-ARR_v1_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_18",
            "tgt_ix": "306-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_18",
            "tgt_ix": "306-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_19",
            "tgt_ix": "306-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_19",
            "tgt_ix": "306-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_19",
            "tgt_ix": "306-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_19",
            "tgt_ix": "306-ARR_v1_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_19",
            "tgt_ix": "306-ARR_v1_19@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_19",
            "tgt_ix": "306-ARR_v1_19@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_19",
            "tgt_ix": "306-ARR_v1_19@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_19",
            "tgt_ix": "306-ARR_v1_19@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_19",
            "tgt_ix": "306-ARR_v1_19@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_19",
            "tgt_ix": "306-ARR_v1_19@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_20",
            "tgt_ix": "306-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_20",
            "tgt_ix": "306-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_21",
            "tgt_ix": "306-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_21",
            "tgt_ix": "306-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_22",
            "tgt_ix": "306-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_22",
            "tgt_ix": "306-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_22",
            "tgt_ix": "306-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_23",
            "tgt_ix": "306-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_23",
            "tgt_ix": "306-ARR_v1_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_23",
            "tgt_ix": "306-ARR_v1_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_24",
            "tgt_ix": "306-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_24",
            "tgt_ix": "306-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_24",
            "tgt_ix": "306-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_24",
            "tgt_ix": "306-ARR_v1_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_24",
            "tgt_ix": "306-ARR_v1_24@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_24",
            "tgt_ix": "306-ARR_v1_24@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_24",
            "tgt_ix": "306-ARR_v1_24@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_24",
            "tgt_ix": "306-ARR_v1_24@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_25",
            "tgt_ix": "306-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_25",
            "tgt_ix": "306-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_25",
            "tgt_ix": "306-ARR_v1_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_26",
            "tgt_ix": "306-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_26",
            "tgt_ix": "306-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_26",
            "tgt_ix": "306-ARR_v1_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_26",
            "tgt_ix": "306-ARR_v1_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_27",
            "tgt_ix": "306-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_28",
            "tgt_ix": "306-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_28",
            "tgt_ix": "306-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_28",
            "tgt_ix": "306-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_28",
            "tgt_ix": "306-ARR_v1_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_28",
            "tgt_ix": "306-ARR_v1_28@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_28",
            "tgt_ix": "306-ARR_v1_28@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_28",
            "tgt_ix": "306-ARR_v1_28@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_29",
            "tgt_ix": "306-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_30",
            "tgt_ix": "306-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_31",
            "tgt_ix": "306-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_31",
            "tgt_ix": "306-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_32",
            "tgt_ix": "306-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_32",
            "tgt_ix": "306-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_33",
            "tgt_ix": "306-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_33",
            "tgt_ix": "306-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_34",
            "tgt_ix": "306-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_34",
            "tgt_ix": "306-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_35",
            "tgt_ix": "306-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_35",
            "tgt_ix": "306-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_36",
            "tgt_ix": "306-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_37",
            "tgt_ix": "306-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_38",
            "tgt_ix": "306-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_38",
            "tgt_ix": "306-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_38",
            "tgt_ix": "306-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_38",
            "tgt_ix": "306-ARR_v1_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_38",
            "tgt_ix": "306-ARR_v1_38@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_38",
            "tgt_ix": "306-ARR_v1_38@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_38",
            "tgt_ix": "306-ARR_v1_38@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_38",
            "tgt_ix": "306-ARR_v1_38@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_38",
            "tgt_ix": "306-ARR_v1_38@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_38",
            "tgt_ix": "306-ARR_v1_38@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_38",
            "tgt_ix": "306-ARR_v1_38@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_39",
            "tgt_ix": "306-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_39",
            "tgt_ix": "306-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_39",
            "tgt_ix": "306-ARR_v1_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_39",
            "tgt_ix": "306-ARR_v1_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_39",
            "tgt_ix": "306-ARR_v1_39@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_39",
            "tgt_ix": "306-ARR_v1_39@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_39",
            "tgt_ix": "306-ARR_v1_39@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_40",
            "tgt_ix": "306-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_40",
            "tgt_ix": "306-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_40",
            "tgt_ix": "306-ARR_v1_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_40",
            "tgt_ix": "306-ARR_v1_40@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_40",
            "tgt_ix": "306-ARR_v1_40@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_40",
            "tgt_ix": "306-ARR_v1_40@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_40",
            "tgt_ix": "306-ARR_v1_40@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_41",
            "tgt_ix": "306-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_42",
            "tgt_ix": "306-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_43",
            "tgt_ix": "306-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_44",
            "tgt_ix": "306-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_45",
            "tgt_ix": "306-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_45",
            "tgt_ix": "306-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_45",
            "tgt_ix": "306-ARR_v1_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_45",
            "tgt_ix": "306-ARR_v1_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_46",
            "tgt_ix": "306-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_46",
            "tgt_ix": "306-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_47",
            "tgt_ix": "306-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_48",
            "tgt_ix": "306-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_49",
            "tgt_ix": "306-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_50",
            "tgt_ix": "306-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_51",
            "tgt_ix": "306-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_51",
            "tgt_ix": "306-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_51",
            "tgt_ix": "306-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_51",
            "tgt_ix": "306-ARR_v1_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_51",
            "tgt_ix": "306-ARR_v1_51@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_51",
            "tgt_ix": "306-ARR_v1_51@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_51",
            "tgt_ix": "306-ARR_v1_51@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_51",
            "tgt_ix": "306-ARR_v1_51@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_51",
            "tgt_ix": "306-ARR_v1_51@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_51",
            "tgt_ix": "306-ARR_v1_51@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_52",
            "tgt_ix": "306-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_52",
            "tgt_ix": "306-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_52",
            "tgt_ix": "306-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_52",
            "tgt_ix": "306-ARR_v1_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_52",
            "tgt_ix": "306-ARR_v1_52@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_53",
            "tgt_ix": "306-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_53",
            "tgt_ix": "306-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_53",
            "tgt_ix": "306-ARR_v1_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_53",
            "tgt_ix": "306-ARR_v1_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_53",
            "tgt_ix": "306-ARR_v1_53@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_53",
            "tgt_ix": "306-ARR_v1_53@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_53",
            "tgt_ix": "306-ARR_v1_53@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_54",
            "tgt_ix": "306-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_55",
            "tgt_ix": "306-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_55",
            "tgt_ix": "306-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_56",
            "tgt_ix": "306-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_56",
            "tgt_ix": "306-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_57",
            "tgt_ix": "306-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_57",
            "tgt_ix": "306-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_57",
            "tgt_ix": "306-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_57",
            "tgt_ix": "306-ARR_v1_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_57",
            "tgt_ix": "306-ARR_v1_57@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_57",
            "tgt_ix": "306-ARR_v1_57@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_57",
            "tgt_ix": "306-ARR_v1_57@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_57",
            "tgt_ix": "306-ARR_v1_57@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_57",
            "tgt_ix": "306-ARR_v1_57@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_57",
            "tgt_ix": "306-ARR_v1_57@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_57",
            "tgt_ix": "306-ARR_v1_57@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_57",
            "tgt_ix": "306-ARR_v1_57@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_57",
            "tgt_ix": "306-ARR_v1_57@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_58",
            "tgt_ix": "306-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_58",
            "tgt_ix": "306-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_59",
            "tgt_ix": "306-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_59",
            "tgt_ix": "306-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_59",
            "tgt_ix": "306-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_59",
            "tgt_ix": "306-ARR_v1_59@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_60",
            "tgt_ix": "306-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_61",
            "tgt_ix": "306-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_61",
            "tgt_ix": "306-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_61",
            "tgt_ix": "306-ARR_v1_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_61",
            "tgt_ix": "306-ARR_v1_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_61",
            "tgt_ix": "306-ARR_v1_61@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_61",
            "tgt_ix": "306-ARR_v1_61@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_62",
            "tgt_ix": "306-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_63",
            "tgt_ix": "306-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_63",
            "tgt_ix": "306-ARR_v1_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_63",
            "tgt_ix": "306-ARR_v1_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_64",
            "tgt_ix": "306-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_65",
            "tgt_ix": "306-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_66",
            "tgt_ix": "306-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_67",
            "tgt_ix": "306-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_68",
            "tgt_ix": "306-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_69",
            "tgt_ix": "306-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_70",
            "tgt_ix": "306-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_71",
            "tgt_ix": "306-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_72",
            "tgt_ix": "306-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_73",
            "tgt_ix": "306-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_74",
            "tgt_ix": "306-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_75",
            "tgt_ix": "306-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_76",
            "tgt_ix": "306-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_77",
            "tgt_ix": "306-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_78",
            "tgt_ix": "306-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_79",
            "tgt_ix": "306-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_80",
            "tgt_ix": "306-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_81",
            "tgt_ix": "306-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_82",
            "tgt_ix": "306-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_83",
            "tgt_ix": "306-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_84",
            "tgt_ix": "306-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_85",
            "tgt_ix": "306-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_86",
            "tgt_ix": "306-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_87",
            "tgt_ix": "306-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_88",
            "tgt_ix": "306-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_89",
            "tgt_ix": "306-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_90",
            "tgt_ix": "306-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_91",
            "tgt_ix": "306-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_92",
            "tgt_ix": "306-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_93",
            "tgt_ix": "306-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_94",
            "tgt_ix": "306-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_95",
            "tgt_ix": "306-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_96",
            "tgt_ix": "306-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_97",
            "tgt_ix": "306-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_98",
            "tgt_ix": "306-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_99",
            "tgt_ix": "306-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_100",
            "tgt_ix": "306-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_101",
            "tgt_ix": "306-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_102",
            "tgt_ix": "306-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_103",
            "tgt_ix": "306-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_104",
            "tgt_ix": "306-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_105",
            "tgt_ix": "306-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_106",
            "tgt_ix": "306-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_107",
            "tgt_ix": "306-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_108",
            "tgt_ix": "306-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_109",
            "tgt_ix": "306-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_110",
            "tgt_ix": "306-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_111",
            "tgt_ix": "306-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_112",
            "tgt_ix": "306-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_113",
            "tgt_ix": "306-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_114",
            "tgt_ix": "306-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_115",
            "tgt_ix": "306-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_116",
            "tgt_ix": "306-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_117",
            "tgt_ix": "306-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "306-ARR_v1_118",
            "tgt_ix": "306-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1247,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "306-ARR",
        "version": 1
    }
}