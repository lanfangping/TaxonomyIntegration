{
    "nodes": [
        {
            "ix": "255-ARR_v1_0",
            "content": "Table-based Fact Verification with Self-adaptive Mixture of Experts",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_2",
            "content": "The table-based fact verification task has recently gained widespread attention and yet remains to be a very challenging problem. It inherently requires informative reasoning over natural language together with different numerical and logical reasoning on tables (e.g., count, superlative, comparative). In this paper, we present a Self-adaptive Mixture-of-Experts Network (SaMoE), a novel framework built on this fundamental property. Specifically, we have developed a mixture-of-experts neural network to recognize and execute different types of reasoning-the network is composed of multiple experts, each handling a specific part of the semantics for reasoning, whereas a management module is applied to decide the contribution of each expert network to the verification result. A self-adaptive method is developed to teach the management module combining results of different experts more efficiently without external knowledge. The experimental results illustrate that our framework achieves 85.1% accuracy on the benchmark dataset TAB-FACT, comparable with the previous state-ofthe-art models. We hope our framework can serve as a new baseline for table-based verification. Our code will be available at (URL to be released here).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "255-ARR_v1_4",
            "content": "Fact Verification, aiming to determine the consistency between a statement and given evidence, has become a crucial part of various applications such as fake news detection, rumor detection (Rashkin et al., 2017;Thorne et al., 2018;Goodrich et al., 2019;Vaibhav et al., 2019;Kryscinski et al., 2020). While most existing research focuses on verification based on unstructured text, a new trend is extending the scope to structured evidence (e.g., tables), which is informative and ubiquitous in our daily lives. Table-based verification is more challenging than unstructured-text-based due to the complexity of the requirements, including sophisticated textual, numerical, and logical reasoning across evidence tables; even for some statements, multiple types of reasoning are indispensable to complete the verification. An example is presented in Figure 1. To tackle the challenges above, previous work established two kinds of methods: (1) programenhanced methods Zhong et al., 2020;Shi et al., 2020; and (2) table-based pre-trained models Liu et al., 2021). The program-enhanced methods mainly leverage programs generated by the semantic parser. Specifically, statements are parsed into executable programs to extract the logical/numerical semantics, which is further be leveraged together with contextual semantics learned by a language model (e.g., BERT) in inference. However, the semantic parsers that generate semanticconsistent programs must be trained in a weak supervision setting, which brings difficulties in training. Furthermore, generalizing this method to other datasets is almost impossible without the API set modification according to the reasoning requirements on the new datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_5",
            "content": "The table-based pre-trained models leverage elaborate model structure (Herzig et al., 2020) and pre-training tasks Liu et al., 2021) to enhance the reasoning skills on structured data. Nevertheless, two significant shortcomings remain. Firstly, the process is demanding due to the tremendous computing resources required by pre-training. Moreover, the effectiveness of pretraining to its downstream tasks mainly depends on the adaptability between these two tasks. Therefore, implementing pre-training tasks may fail to meet the requirements when facing the unseen reasoning types demanded by new datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_6",
            "content": "In this paper, we introduce an innovative framework, Self-adaptive Mixture of Experts (SaMoE), to address the previously mentioned problems. The entire framework is illustrated in Figure 2. SaMoE consists of 3 components: feature extractor, experts, and management module, which is the combination of manager and supervisor networks. Each expert initially takes the same feature as input and then learns to deal with different parts of the reasoning types (e.g., contextual/logical/numerical) required by table-based verification. A management module is designed to guide the training of experts and combine experts' verification results effectively. The manager network in this module assigns each expert a unique attention score, allowing each individual to focus on different kinds of reasoning types and summarizes experts' entire outputs as the final verification result. However, managers failed to allocate the highest attention score to the expert who performs best on the current reasoning type in most circumstances. To alleviate this problem, we introduce a supervisor network to adjust the attention score given by the manager. The supervisor network is trained selfadaptively (i.e., it learns directly from experts' performance on the train set) without prior knowledge of the task or dataset. Extensive experiments are conducted to show that our proposed framework, implemented with a general pre-trained language model RoBERTa , outperforms previous state-of-the-art methods, including tablebased pre-trained models. The main contributions of this work are as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_7",
            "content": "\u2022 We innovatively implement mixture-ofexperts for table-based verification, aiming to arrange each expert to different types of reasoning. This method can also be easily generalized to other datasets. \u2022 We investigate a self-adaptive method to adjust suitable attention score to each expert according to its performance on different reasoning types, achieving more efficient cooperation across experts. \u2022 Our framework achieves better performance on the TABFACT dataset without the assistance of table-based pre-trained models.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_8",
            "content": "Task Formulation",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "255-ARR_v1_9",
            "content": "The table-based verification task expects one to determine whether a statement S is entailed or refuted by an evidence table T . The process above can be regarded as a binary classification task and thus denoted as f (S, T ) = \u0177, where f is the verification model and \u0177 \u2208 {0, 1} its prediction.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_10",
            "content": "Methods",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "255-ARR_v1_11",
            "content": "We present the proposed framework (SaMoE), which leverages a set of experts to deal with different parts of the reasoning types involved in These two processes will be further interpreted in the following subsections.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_12",
            "content": "Table Pre-processing",
            "ntype": "title",
            "meta": {
                "section": "3.1.1"
            }
        },
        {
            "ix": "255-ARR_v1_13",
            "content": "As for Tables, the pre-processing (pruning and serializing) before the joint representation learning provides convenience for subsequent processing of the existing language model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_14",
            "content": "Table pruning discards some parts of the table that do not participate in the verification, according to the input size limit of the language model. We take advantage of the tablepruning algorithm proposed in and further enhance its performance. The original algorithm matches the entities in statements with cells in tables by a heuristic method and selects the columns that include matched cells to form the pruned table. Noticed that the algorithm always fails to select the critical columns of verification while there is still room left for the input sequence of the language model, we further add a greedy strategy on the algorithm that keeps adding columns that are not selected to the pruned table until reaching the maximum input size of the downstream model to make the best use of its capacity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_15",
            "content": "Table Serializing Tables are further serialized to a 1-D sequence after pruning to be compatible with the input format of the language model. We follow the serializing method used in TABLE-BERT that paraphrases tables with a natural language template. Specifically, a table with m rows and n columns is paraphrased as \"row 1 is: h 1 is T 11 ; ... ; h n is T 1n . row 2 is: ... row m is: h 1 is T m1 ; ... ; h n is T mn .\", where h i refers the i th header and T ij the value in the (i, j) \u2212 th cell of table T . We find that such template-serialized tables are more suitable for language models pre-trained on unstructured text to process.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_16",
            "content": "Joint Representation Learning",
            "ntype": "title",
            "meta": {
                "section": "3.1.2"
            }
        },
        {
            "ix": "255-ARR_v1_17",
            "content": "After the table pre-processing, the serialized table and the statement are further passed to a language model to learn the joint contextual representation of each token. The learned representation vectors are then transmitted to the experts and the management module for inference and management. Specifically, the serialized table and the statement are initially tokenized into two token sequences T and S. Then the joint token sequence X is formed as X = [\u27e8s\u27e9, S, \u27e8/s\u27e9, T, \u27e8/s\u27e9], where \u27e8s\u27e9 and \u27e8/s\u27e9 are the separators that identify the beginning and the end of each token sequence. The token sequence X will be padded or truncated to fit the maximum input length of the language model. Finally, a transformer model is applied to learn the contextual representation of X :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_18",
            "content": "H = f LM (X)(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_19",
            "content": "where H \u2208 R n\u00d7d refers to the learned joint representation, n is the maximum input length and d the dimension of the representation vector. f LM denotes the contextual representation learning process of the language model. In this paper, we implement it with transformer (Vaswani et al., 2017), the most popular contextual representation model in recent years.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_20",
            "content": "Experts",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "255-ARR_v1_21",
            "content": "A group of experts is applied to verify the statements separately based on the same statement-table joint semantics extracted by the feature extractor module. Experts share the same model structure, while the parameter learning strategy of SaMoE gives expert differentiation. Specifically, each expert is implemented with a stack of transformer encoding layers. An MLP classifier that calculates the probability of the statement is entailed by the evidence table based on the encoded semantics. We implement experts with the same general structure rather than different structures specially designed for certain reasoning types since we anticipate that the proposed framework can be smoothly generalized to other datasets. The process above can be formulated as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_22",
            "content": "h i = f Enc i (H)(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_23",
            "content": "p i = sof tmax(tanh(h i W i 1 )W i 2 ) (3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_24",
            "content": "where h i \u2208 R d is the token \u27e8s\u27e9's final representation vector encoded by the i th expert's encoder Enc i . It implies the i th expert's whole understanding to the statement-table pair. W i 1 \u2208 R d\u00d7d and W i 2 \u2208 R d\u00d72 are the trainable parameters of i th expert's classifier, which projects h i to the probabilities p i \u2208 R 2 that the statement is entailed/refuted by the table. tanh and sof tmax are activation functions. n e refers to the number of experts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_25",
            "content": "Management Module",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "255-ARR_v1_26",
            "content": "Learning the joint semantics parsed in Sec.3.1, the management module intends to generate attention scores to bias experts' training and combine experts' results efficiently. The module consists of two components: manager and supervisor, both of them are implemented based on transformer model. The manager is mainly designed to guide experts' training, while the supervisor is applied to combine experts' results efficiently.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_27",
            "content": "Manager The manager guides the training of experts and forms a preliminary assumption to the expert combination. It encodes the joint representation matrix and generates attention scores a M to guide the experts' training process:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_28",
            "content": "h M = f Enc M (H) (4) e M = tanh(h M W M 1 )W M 2 (5) a M = sof tmax(e M )(6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_29",
            "content": "where Enc M denotes the manager's encoder, W M 1 \u2208 R d\u00d7d and W M 2 \u2208 R d\u00d7ne are trainable parameters. The network structures of the manager and experts are basically the same, only different in the layers of the encoder and the output dimension.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_30",
            "content": "After preceding calculation, the normalized attention scores a M are used to guide the training of experts by a specially designed verification loss, which will be introduced in Sec.4.1.1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_31",
            "content": "Supervisor The supervisor adjusts the attention scores submitted by the manager to improve the cooperative efficiency among experts (i.e. assigning higher weights to experts who perform better on the current input pair). The network predicts the deviation between the preliminary assumption (i.e., the attention) and the ideal combination weights based on the knowledge encoded in the joint representation matrix H:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_32",
            "content": "h S = f Enc S (H) (7) e S = tanh(h S W S 1 )W S 2 (8) a S = sof tmax(e M + e S )(9)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_33",
            "content": "where W S 1 \u2208 R d\u00d7d , W S 2 \u2208 R d\u00d7ne are trainable parameters and Enc S refers to the encoder of the supervisor. Parameters of the supervisor are optimized self-adaptively based on experts' performance on the train set. More details of this learning strategy will be presented in Sec.4.2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_34",
            "content": "Parameter Learning",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "255-ARR_v1_35",
            "content": "Parameters in SaMoE are learned in two consecutive stages: 1) Supervised learning: parameters in the feature extractor, experts and the manager are end-to-end optimized under the supervision of labels; 2) Self-adaptive learning: parameters in the supervisor are self-optimized by observing experts' performance on the train set (other parameters are fixed simultaneously). A weighted sum of two losses is minimized in the first stage to achieve diverse and balanced training of experts. For the second stage, we minimize a self-adaptive loss calculated based on the experts' classification loss. Subsequent sections introduce these two learning stages in detail.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_36",
            "content": "Supervised Learning",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "255-ARR_v1_37",
            "content": "Supervised learning guides each expert on dealing with different reasoning types while maintaining balanced training across experts. To achieve the goals above, we develop two loss functions: 1) verification loss L V that measures the weighted sum of each expert's classification loss, differentiating experts' learning with different attention scores assigned by the manager; 2) manager assumption loss L M that is applied to prevent the occurrence of imbalanced training across experts. The overall loss of this state is calculated by a weighted sum of these two terms: L 1 = L V + \u03bbL M , where \u03bb is a hyperparameter that controls the ratio of L M . Subsequent sections provide detailed introduction to these two terms.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_38",
            "content": "Verification Loss",
            "ntype": "title",
            "meta": {
                "section": "4.1.1"
            }
        },
        {
            "ix": "255-ARR_v1_39",
            "content": "The verification loss L V is designed based on the loss function proposed in Jacobs et al. (1991). It is calculated by the weighted sum of each expert's cross-entropy:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_40",
            "content": "L V = ne i=1 (a M ) i \u2022 CE(p i , l) (10)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_41",
            "content": "where (a M ) i is the i th element of the attention scores a M , l \u2208 {0, 1} is the label of the statementtable pair and CE(\u2022, \u2022) the cross-entropy function.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_42",
            "content": "Note that it is necessary to calculate each expert's cross-entropy independently. We want each expert to behave like an independent expert (i.e., complete the verification without the help of other experts).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_43",
            "content": "The attention vector a M acts as a \"training scheduler\" in this loss function: experts that are assigned with larger attention scores receive a larger gradient than other experts on the current input, resulting in diverse experts' performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_44",
            "content": "Manager Assumption Loss",
            "ntype": "title",
            "meta": {
                "section": "4.1.2"
            }
        },
        {
            "ix": "255-ARR_v1_45",
            "content": "We have trained the MoE with only the verification loss L V and observe a severe \"imbalanced experts\" phenomenon that only one expert is well-trained (i.e., the expert performance is improved by training) and the manager keeps assigning a close-to-1 attention score to this expert, which is also reported in previous research (Eigen et al., 2013;. To avoid this problem, we develop another loss function that forces the manager to assign reasonable attention scores to experts:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_46",
            "content": "L M = D(a P ||a M )(11)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_47",
            "content": "where D(\u2022||\u2022) denotes the Kullback-Leibler divergence and a P a prior assumption that is generated with a simple heuristic algorithm (to be introduced in the next paragraph) which requires limited prior knowledge of the reasoning types. By minimizing L M , the manager learns to assign each expert with a reasonable attention score, resulting in a balanced training across experts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_48",
            "content": "Prior Assumption Generation The prior assumption a P is generated to represent the probabilities that the statement involves different reasoning types that we are interested in. Specifically, we develop a trigger-word-based heuristic algorithm to form the prior assumption for each statement automatically:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_49",
            "content": "1. Initialize the prior assumption with e 0 \u2208 R ne , which is empirically set as (0.1, 0.1, ..., 0.6) T .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_50",
            "content": "The (e 0 ) ne represents the probability that the statement does not involve any predefined reasoning types and thus is set higher than other values in advance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_51",
            "content": "2. Traverse the trigger-word set of each reasoning type (n e \u2212 1 types in total). If a trigger word/pattern w that belongs to i th reasoning type is detected in the statement, the trigger's weight s w (set empirically) is accumulated to the i th dimension of a zero-initialized bias vector \u03b4 \u2208 R ne : \u03b4 i \u2190 \u03b4 i + s w .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_52",
            "content": "3. Add the bias vector \u03b4 to the prior assumption e 0 and normalize to get the prior assumption: a P = sof tmax(e 0 + \u03b4).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_53",
            "content": "Figure 3 presents an example of this process. Learning to imitate the prior assumption, the manager guides each expert to focus on different reasoning types and thus achieves diverse experts. We implement a relatively small trigger-word pool in experiments and find the method works effectively, indicating that the method can be smoothly generalized to other datasets with little modification to the predefined reasoning types and trigger-word pool.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_54",
            "content": "Self-adaptive Learning",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "255-ARR_v1_55",
            "content": "Self-adaptive learning aims to enhance further the expert combining efficiency with only the knowledge of the expert's performance on the train set. Specifically, an \"expert ability\" vector a E \u2208 R ne is calculated based on the \"expert loss\" vector m \u2208 R ne , where m i = CE(p i , l) is the crossentropy loss of the i th expert. Note that the crossentropy of the expert is negatively correlated with its performance. Then the expert ability vector a E is calculated as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_56",
            "content": "a E = sof tmax(\u2212\u03b1 \u2022 m)(12)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_57",
            "content": "where \u03b1 = \u03b2/V ar(m) is a variancenormalizing coefficient and \u03b2 is a hyperparameter that decides the variance of the expert ability vector before the activation (i.e., V ar(\u2212\u03b1\u2022m) = \u03b2). Such normalization is designed based on the observation that m tends to have a extreme small variance and sof tmax(\u2212m) often generates a close-to-uniform distribution. Note that the generated a E is positively correlated with the experts' performance (e.g., if the i th expert outperforms the j th expert on the input pair then we have (a E ) i > (a E ) j ).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_58",
            "content": "Based on a E , we develop the loss function that has the same form with L M in Sec 4.1.2:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_59",
            "content": "L S = D(a E ||a S )(13)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_60",
            "content": "By minimizing the loss above, the higher attention scores are assigned to the best-performed experts after the supervisor's adjustment, resulting in more efficient cooperation across experts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_61",
            "content": "Experiment Setup",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "255-ARR_v1_62",
            "content": "Data and Metric",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "255-ARR_v1_63",
            "content": "We conduct the experiments on TABFACT, a large scale benchmark dataset of the table-based fact verification task 1 . TABFACT contains a total of 117k statements and 16k Wikipedia tables. The test set is further divided into a simple and complex subset based on verification difficulty, for verifying some statements on TABFACT requires more logical/numerical reasoning skills. We choose accuracy as the evaluation metric following the existing work to make our experiment results comparable. More details of TABFACT are presented in Appendix A.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_64",
            "content": "Implementation Details",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "255-ARR_v1_65",
            "content": "Training Details We set n e = 5 expert networks in our implementation of SaMoE. The transformer layers are 12 for encoders in the feature extractor and experts and 2 for encoders in the manager and supervisor. The hidden states' dimension d, the maximum input length n, the \u03bb in Sec.4.1, and the \u03b2 in Sec.4.2 are set to 1024, 512, 0.1 and 0.1 respectively. We applied RoBERTa-Large to initialize the feature extractor and experts in our framework. The details of parameter initialization can be found in Appendix B.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_66",
            "content": "We apply Adam optimizer (Kingma and Ba, 2015) in training with learning rate 2e-5, dropout rate 0.1, warmup step 17,304, and batch size 32.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_67",
            "content": "SaMoE is first trained in the supervised learning stage for 57,680 steps (20 epochs). Then the supervisor is trained in the self-adaptive learning stage for another 5,000 steps, while the best parameters of other parts in the framework are loaded and fixed. The model is evaluated every 1000 steps, and the model that achieves the highest performance on the development set is saved. All the codes are implemented with Pytorch (Paszke et al., 2019) and the transformers package (Wolf et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_68",
            "content": "We choose the top 4 types of reasoning types that appear most frequently in TABFACT 2 (count, comparative, superlative, negation). We apply a small trigger-word pool containing only 26 trigger words, injecting limited prior knowledge of the dataset. More details of this part are presented in Appendix C.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_69",
            "content": "Baselines",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "255-ARR_v1_70",
            "content": "We compared our proposed framework with different kinds of baselines on TABFACT: (1) Programenhanced methods: LPA , Log-icalFactChecker (Zhong et al., 2020), HeterTFV (Shi et al., 2020), ProgVGAT and Decomp (Yang and Zhu, 2021); (2) Tablebased pre-trained models: TAPAS and TAPEX (Liu et al., 2021); (3) Other methods: Table-BERT and SAT .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_71",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "255-ARR_v1_72",
            "content": "Overall Performance",
            "ntype": "title",
            "meta": {
                "section": "6.1"
            }
        },
        {
            "ix": "255-ARR_v1_73",
            "content": "We compare the proposed SaMoE with different kinds of baselines, and the results are listed in Table 1. Baselines are presented with the best performance reported in the corresponding papers. SaMoE obtains an accuracy of 85.1% on the test set, achieving a new state-of-the-art on the dataset. Results show that our method consistently outperforms all the program-enhanced methods with a significant 2.4% improvement compared with the Decomp method (the best performed programenhanced method). Note that SaMoE performs similar with Decomp-LARGE on the simple subset of the test set (93.6% vs. 93.6%) while outperforms Decomp-LARGE with a remarkable 3.5% on the complex subset (80.9% vs. 77.4%). Such analysis indicates that the performance improvement is mainly derived from successfully verifying complex statements, which required more sophisticated reasoning than statements in the simple set. SaMoE even shows comparable performance with the previous SOTA TAPEX that is pre-trained to execute SQL queries on tables. Our method outperforms TAPEX with a 0.9% improvement on the test set and a further 1.3% improvement on the complex subset, indicating that SaMoE, based on a text-based pre-trained model, performs even better than table-based pre-trained models on a variety of complex reasoning types demanded by the",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_74",
            "content": "Ablation Study",
            "ntype": "title",
            "meta": {
                "section": "6.2"
            }
        },
        {
            "ix": "255-ARR_v1_75",
            "content": "We further investigate the effectiveness of the MoE structure and self-adaptive learning with an ablation study. We conduct two experiments: one reduces the number of experts to 1 to disable the contribution from the MoE structure (SaMoE w/o Sa (n e = 1)); the other trains the proposed framework with only the supervised learning stage (SaMoE w/o Sa). Results are presented in Table 2. The MoE structure achieves a 0.7% improvement on the test set (84.7% vs. 84.0%), and self-adaptive learning further improves the performance slightly (85.1% vs. 84.7%). Note that the slight improvement of self-adaptive learning is expected since the experts and the feature extractor are fixed in this stage. The results demonstrate the effectiveness of both the MoE structure and the self-adaptive learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_76",
            "content": "Effectiveness Analysis",
            "ntype": "title",
            "meta": {
                "section": "6.3"
            }
        },
        {
            "ix": "255-ARR_v1_77",
            "content": "We show in this section that the effectiveness of the proposed framework is derived from two aspects: the differentiation of experts (each expert outperforms others on a specific part of reasoning types) and the effective attention assignment by the management module (the best-performed experts are assigned with higher attention scores).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_78",
            "content": "Expert Differentiation",
            "ntype": "title",
            "meta": {
                "section": "6.3.1"
            }
        },
        {
            "ix": "255-ARR_v1_79",
            "content": "We first investigate the proposed manager assumption loss L M and find that it achieves balanced training across experts, which is the premise of expert differentiation. We further show that the proposed framework achieves differentiation across experts. Figure 5 presents the proportion of statements in the test set that are verified correctly by at least k experts (k varies from 1 to 5). Note that the proportion increases rapidly as k decreases (76.2% to 90.7% for k from 5 to 1), which illustrates that experts behave differently on a large proportion of statements. The results indicate that SaMoE successfully achieves expert differentiation, which expands the original performance upper bound considerably (90.7%).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_80",
            "content": "Effective Attention Assignment",
            "ntype": "title",
            "meta": {
                "section": "6.3.2"
            }
        },
        {
            "ix": "255-ARR_v1_81",
            "content": "We conduct a detailed analysis to investigate whether the management module assigns higher attention scores to experts with the best performance after self-adaptive learning. To achieve this goal, we regard the management module as a n e -class classifier and calculate the top-k accuracy of predicting the best-performed expert (the one with the smallest cross-entropy) on the test set where k is chosen in [1,2,3]. The results of the analysis are presented in Table 3. The top-k accuracy is improved significantly after self-adaptive learning (+6.6%, +14.2%, +8.4% respectively), indicating that the management module successfully assigns higher attention scores to the best-performed experts by self-adaptive learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_82",
            "content": "Based on the significant performance upper bound expanded by the expert differentiation, the effective attention assignment achieves more efficient cooperation across these diverse experts, thus improving the verification performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_83",
            "content": "Related Works",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "255-ARR_v1_84",
            "content": "Table-Based Fact Verification Most of the current models utilize programs to improve the model's ability to handle various types of numerical and logical reasoning Zhong et al., 2020;Shi et al., 2020;Yang and Zhu, 2021) Mixture of Experts Mixture of experts is a special model combining method. Jacobs et al. (1991) first introduces this method and proposes a loss that encourages competitive learning across expert models. We develop a self-adapted mixture-ofexperts framework that achieves a more effective combination of experts by learning from the experts' performance on the train set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_85",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "8"
            }
        },
        {
            "ix": "255-ARR_v1_86",
            "content": "This paper proposes a framework that leverages the mixture of experts to recognize and execute different types of reasoning required for table-based fact verification. We propose an MoE model guided with limited prior knowledge to handle different parts of the reasoning types required by table-based verification with diverse experts. Moreover, we design a novel supervisor network to adjust the imprecise attention score and achieve a more efficient combination across experts. A self-adaptive learning strategy is further applied to train the proposed supervisor network without prior knowledge of the task or dataset. The experiments show that the proposed model achieves a new state-of-the-art performance of 85.1% accuracy on the benchmark dataset TABFACT. The ablation studies and analysis further indicate the effectiveness of the proposed MoE structure and self-adaptive learning strategy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_87",
            "content": "For parameter initialization, We leverage RoBERTa-Large, a pre-trained language model that has 24 transformer encoding layers. We initial parameters of the feature extractor with the embedding layer and the bottom 12 encoding layers of RoBERTa-Large and each expert with the upper 12 encoding layers of RoBERTa-Large, respectively. We use PyTorch to initialize other parameters randomly.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_88",
            "content": "We choose four reasoning types that appear most frequently in TABFACT: count, comparative, superlative, and negation. The detailed definitions of four reasoning types chosen in our implementation are listed below:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_89",
            "content": "1. Count: counting the number of specific rows in the table, such as \"xxx be listed a total of 3 times\", \"xxx win only 1 time in ...\", etc.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_90",
            "content": "2. Comparative: comparing two values in the statement or cells, such as \"xxx play in more than 1 game during ...\", \"xxx has a larger yyy than zzz\", etc.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_91",
            "content": "3. Superlative: finding the highest/lowest value of the specific column, such as \"the longest xxx be yyy\", \"the lowest score at xxx be yyy\", etc.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_92",
            "content": "4. Negation: negating the original semantics of the statement, such as \"xxx has never lost a game in ...\", \"xxx never score 0 points\", etc. A small trigger-word pool that contains only 26 trigger words/patterns is applied for the prior assumption generation: 11 triggers for the \"count\" type, 15 for \"negation\"; and for the rest types (i.e., \"comparative\" and \"superlative\" types), the NLTK package is employed to recognize the comparative and superlative words automatically. Such a small trigger-word pool injects limit prior knowledge of the dataset, indicating that the proposed method can be generalized to other datasets by simply modifying the pool of trigger words. Table 5 presents some words/patterns in the trigger-word pool applied in our experiments. x+[number] denotes a combination of a word and a number that is served as a trigger (e.g., for the statement \"xxx win 3 times in ...\", we match the phrase \"3 times\" with the trigger \"[number]+times\").",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "255-ARR_v1_93",
            "content": "Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, William Wang, Tabfact: A large-scale dataset for table-based fact verification, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Wenhu Chen",
                    "Hongmin Wang",
                    "Jianshu Chen",
                    "Yunkai Zhang",
                    "Hong Wang",
                    "Shiyang Li",
                    "Xiyou Zhou",
                    "William Wang"
                ],
                "title": "Tabfact: A large-scale dataset for table-based fact verification",
                "pub_date": "2020-04-26",
                "pub_title": "8th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "255-ARR_v1_94",
            "content": "UNKNOWN, None, 2013, Learning factored representations in a deep mixture of experts, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2013",
                "pub_title": "Learning factored representations in a deep mixture of experts",
                "pub": null
            }
        },
        {
            "ix": "255-ARR_v1_95",
            "content": "Julian Eisenschlos, Syrine Krichene, Thomas M\u00fcller, Understanding tables with intermediate pre-training, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Julian Eisenschlos",
                    "Syrine Krichene",
                    "Thomas M\u00fcller"
                ],
                "title": "Understanding tables with intermediate pre-training",
                "pub_date": "2020",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "pub": null
            }
        },
        {
            "ix": "255-ARR_v1_96",
            "content": "Ben Goodrich, Vinay Rao, J Peter, Mohammad Liu,  Saleh, Assessing the factual accuracy of generated text, 2019, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Ben Goodrich",
                    "Vinay Rao",
                    "J Peter",
                    "Mohammad Liu",
                    " Saleh"
                ],
                "title": "Assessing the factual accuracy of generated text",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
                "pub": null
            }
        },
        {
            "ix": "255-ARR_v1_97",
            "content": "Vivek Gupta, Maitrey Mehta, INFOTABS: Inference on tables as semi-structured data, 2020, Proceedings of the 58th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Vivek Gupta",
                    "Maitrey Mehta"
                ],
                "title": "INFOTABS: Inference on tables as semi-structured data",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th",
                "pub": null
            }
        },
        {
            "ix": "255-ARR_v1_98",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "255-ARR_v1_99",
            "content": "Jonathan Herzig, Krzysztof Nowak, Thomas M\u00fcller, Francesco Piccinno, Julian Eisenschlos, TaPas: Weakly supervised table parsing via pre-training, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Jonathan Herzig",
                    "Krzysztof Nowak",
                    "Thomas M\u00fcller",
                    "Francesco Piccinno",
                    "Julian Eisenschlos"
                ],
                "title": "TaPas: Weakly supervised table parsing via pre-training",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "255-ARR_v1_100",
            "content": "A Robert, Michael Jacobs,  Jordan, J Steven, Geoffrey Nowlan,  Hinton, Adaptive mixtures of local experts, 1991, Neural computation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "A Robert",
                    "Michael Jacobs",
                    " Jordan",
                    "J Steven",
                    "Geoffrey Nowlan",
                    " Hinton"
                ],
                "title": "Adaptive mixtures of local experts",
                "pub_date": "1991",
                "pub_title": "Neural computation",
                "pub": null
            }
        },
        {
            "ix": "255-ARR_v1_101",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A method for stochastic optimization, 2015-05-07, 3rd International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "P Diederik",
                    "Jimmy Kingma",
                    " Ba"
                ],
                "title": "Adam: A method for stochastic optimization",
                "pub_date": "2015-05-07",
                "pub_title": "3rd International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "255-ARR_v1_102",
            "content": "Wojciech Kryscinski, Bryan Mccann, Caiming Xiong, Richard Socher, Evaluating the factual consistency of abstractive text summarization, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Wojciech Kryscinski",
                    "Bryan Mccann",
                    "Caiming Xiong",
                    "Richard Socher"
                ],
                "title": "Evaluating the factual consistency of abstractive text summarization",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "255-ARR_v1_103",
            "content": "UNKNOWN, None, 2021, Tapex: Table pre-training via learning a neural sql executor, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Tapex: Table pre-training via learning a neural sql executor",
                "pub": null
            }
        },
        {
            "ix": "255-ARR_v1_104",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "255-ARR_v1_105",
            "content": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, Pytorch: An imperative style, high-performance deep learning library, 2019-12-08, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Adam Paszke",
                    "Sam Gross",
                    "Francisco Massa",
                    "Adam Lerer",
                    "James Bradbury",
                    "Gregory Chanan",
                    "Trevor Killeen",
                    "Zeming Lin",
                    "Natalia Gimelshein",
                    "Luca Antiga",
                    "Alban Desmaison",
                    "Andreas K\u00f6pf",
                    "Edward Yang",
                    "Zachary Devito",
                    "Martin Raison",
                    "Alykhan Tejani",
                    "Sasank Chilamkurthy",
                    "Benoit Steiner",
                    "Lu Fang",
                    "Junjie Bai",
                    "Soumith Chintala"
                ],
                "title": "Pytorch: An imperative style, high-performance deep learning library",
                "pub_date": "2019-12-08",
                "pub_title": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "255-ARR_v1_106",
            "content": "Eunsol Hannah Rashkin, Jin Choi, Svitlana Jang, Yejin Volkova,  Choi, Truth of varying shades: Analyzing language in fake news and political fact-checking, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Eunsol Hannah Rashkin",
                    "Jin Choi",
                    "Svitlana Jang",
                    "Yejin Volkova",
                    " Choi"
                ],
                "title": "Truth of varying shades: Analyzing language in fake news and political fact-checking",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "255-ARR_v1_107",
            "content": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean, Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017-04-24, 5th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Noam Shazeer",
                    "Azalia Mirhoseini",
                    "Krzysztof Maziarz",
                    "Andy Davis",
                    "Quoc Le",
                    "Geoffrey Hinton",
                    "Jeff Dean"
                ],
                "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
                "pub_date": "2017-04-24",
                "pub_title": "5th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "255-ARR_v1_108",
            "content": "Qi Shi, Yu Zhang, Qingyu Yin, Ting Liu, Learn to combine linguistic and symbolic information for table-based fact verification, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Qi Shi",
                    "Yu Zhang",
                    "Qingyu Yin",
                    "Ting Liu"
                ],
                "title": "Learn to combine linguistic and symbolic information for table-based fact verification",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "255-ARR_v1_109",
            "content": "James Thorne, Andreas Vlachos, FEVER: a large-scale dataset for fact extraction and VERification, 2018, NAACL-HLT, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "James Thorne",
                    "Andreas Vlachos"
                ],
                "title": "FEVER: a large-scale dataset for fact extraction and VERification",
                "pub_date": "2018",
                "pub_title": "NAACL-HLT",
                "pub": null
            }
        },
        {
            "ix": "255-ARR_v1_110",
            "content": "Vaibhav Vaibhav, Raghuram Mandyam, Eduard Hovy, Do sentence interactions matter? leveraging sentence level representations for fake news classification, 2019, Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Vaibhav Vaibhav",
                    "Raghuram Mandyam",
                    "Eduard Hovy"
                ],
                "title": "Do sentence interactions matter? leveraging sentence level representations for fake news classification",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "255-ARR_v1_111",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Illia Kaiser,  Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "Illia Kaiser",
                    " Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "255-ARR_v1_112",
            "content": "X Nancy, Diwakar Wang, Marina Mahajan, Sara Danilevsky,  Rosenthal, SemEval-2021 task 9: Fact verification and evidence finding for tabular data in scientific documents (SEM-TAB-FACTS), 2021, Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "X Nancy",
                    "Diwakar Wang",
                    "Marina Mahajan",
                    "Sara Danilevsky",
                    " Rosenthal"
                ],
                "title": "SemEval-2021 task 9: Fact verification and evidence finding for tabular data in scientific documents (SEM-TAB-FACTS)",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
                "pub": null
            }
        },
        {
            "ix": "255-ARR_v1_113",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest,  Rush, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Thomas Wolf",
                    "Lysandre Debut",
                    "Victor Sanh",
                    "Julien Chaumond",
                    "Clement Delangue",
                    "Anthony Moi",
                    "Pierric Cistac",
                    "Tim Rault",
                    "Remi Louf",
                    "Morgan Funtowicz",
                    "Joe Davison",
                    "Sam Shleifer",
                    "Clara Patrick Von Platen",
                    "Yacine Ma",
                    "Julien Jernite",
                    "Canwen Plu",
                    "Teven Xu",
                    "Sylvain Scao",
                    "Mariama Gugger",
                    "Quentin Drame",
                    "Alexander Lhoest",
                    " Rush"
                ],
                "title": "Transformers: State-of-the-art natural language processing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "255-ARR_v1_114",
            "content": "Xiaoyu Yang, Feng Nie, Yufei Feng, Quan Liu, Zhigang Chen, Xiaodan Zhu, Program enhanced fact verification with verbalization and graph attention network, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Xiaoyu Yang",
                    "Feng Nie",
                    "Yufei Feng",
                    "Quan Liu",
                    "Zhigang Chen",
                    "Xiaodan Zhu"
                ],
                "title": "Program enhanced fact verification with verbalization and graph attention network",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "255-ARR_v1_115",
            "content": "UNKNOWN, None, 2021, Exploring decomposition for table-based fact verification, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Exploring decomposition for table-based fact verification",
                "pub": null
            }
        },
        {
            "ix": "255-ARR_v1_116",
            "content": "Hongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi Cao, Fuzheng Zhang, Zhongyuan Wang, Table fact verification with structure-aware transformer, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Hongzhi Zhang",
                    "Yingyao Wang",
                    "Sirui Wang",
                    "Xuezhi Cao",
                    "Fuzheng Zhang",
                    "Zhongyuan Wang"
                ],
                "title": "Table fact verification with structure-aware transformer",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "255-ARR_v1_117",
            "content": "Wanjun Zhong, Duyu Tang, Zhangyin Feng, Nan Duan, Ming Zhou, Ming Gong, Linjun Shou, Daxin Jiang, Jiahai Wang, Jian Yin, Logical-FactChecker: Leveraging logical operations for fact checking with graph module network, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Wanjun Zhong",
                    "Duyu Tang",
                    "Zhangyin Feng",
                    "Nan Duan",
                    "Ming Zhou",
                    "Ming Gong",
                    "Linjun Shou",
                    "Daxin Jiang",
                    "Jiahai Wang",
                    "Jian Yin"
                ],
                "title": "Logical-FactChecker: Leveraging logical operations for fact checking with graph module network",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "255-ARR_v1_0@0",
            "content": "Table-based Fact Verification with Self-adaptive Mixture of Experts",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_0",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_2@0",
            "content": "The table-based fact verification task has recently gained widespread attention and yet remains to be a very challenging problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_2",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_2@1",
            "content": "It inherently requires informative reasoning over natural language together with different numerical and logical reasoning on tables (e.g., count, superlative, comparative).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_2",
            "start": 130,
            "end": 302,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_2@2",
            "content": "In this paper, we present a Self-adaptive Mixture-of-Experts Network (SaMoE), a novel framework built on this fundamental property.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_2",
            "start": 304,
            "end": 434,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_2@3",
            "content": "Specifically, we have developed a mixture-of-experts neural network to recognize and execute different types of reasoning-the network is composed of multiple experts, each handling a specific part of the semantics for reasoning, whereas a management module is applied to decide the contribution of each expert network to the verification result.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_2",
            "start": 436,
            "end": 780,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_2@4",
            "content": "A self-adaptive method is developed to teach the management module combining results of different experts more efficiently without external knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_2",
            "start": 782,
            "end": 931,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_2@5",
            "content": "The experimental results illustrate that our framework achieves 85.1% accuracy on the benchmark dataset TAB-FACT, comparable with the previous state-ofthe-art models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_2",
            "start": 933,
            "end": 1098,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_2@6",
            "content": "We hope our framework can serve as a new baseline for table-based verification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_2",
            "start": 1100,
            "end": 1178,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_2@7",
            "content": "Our code will be available at (URL to be released here).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_2",
            "start": 1180,
            "end": 1235,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_4@0",
            "content": "Fact Verification, aiming to determine the consistency between a statement and given evidence, has become a crucial part of various applications such as fake news detection, rumor detection (Rashkin et al., 2017;Thorne et al., 2018;Goodrich et al., 2019;Vaibhav et al., 2019;Kryscinski et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_4",
            "start": 0,
            "end": 299,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_4@1",
            "content": "While most existing research focuses on verification based on unstructured text, a new trend is extending the scope to structured evidence (e.g., tables), which is informative and ubiquitous in our daily lives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_4",
            "start": 301,
            "end": 510,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_4@2",
            "content": "Table-based verification is more challenging than unstructured-text-based due to the complexity of the requirements, including sophisticated textual, numerical, and logical reasoning across evidence tables; even for some statements, multiple types of reasoning are indispensable to complete the verification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_4",
            "start": 512,
            "end": 819,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_4@3",
            "content": "An example is presented in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_4",
            "start": 821,
            "end": 856,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_4@4",
            "content": "To tackle the challenges above, previous work established two kinds of methods: (1) programenhanced methods Zhong et al., 2020;Shi et al., 2020; and (2) table-based pre-trained models Liu et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_4",
            "start": 858,
            "end": 1059,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_4@5",
            "content": "The program-enhanced methods mainly leverage programs generated by the semantic parser.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_4",
            "start": 1061,
            "end": 1147,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_4@6",
            "content": "Specifically, statements are parsed into executable programs to extract the logical/numerical semantics, which is further be leveraged together with contextual semantics learned by a language model (e.g., BERT) in inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_4",
            "start": 1149,
            "end": 1372,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_4@7",
            "content": "However, the semantic parsers that generate semanticconsistent programs must be trained in a weak supervision setting, which brings difficulties in training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_4",
            "start": 1374,
            "end": 1530,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_4@8",
            "content": "Furthermore, generalizing this method to other datasets is almost impossible without the API set modification according to the reasoning requirements on the new datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_4",
            "start": 1532,
            "end": 1701,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_5@0",
            "content": "The table-based pre-trained models leverage elaborate model structure (Herzig et al., 2020) and pre-training tasks Liu et al., 2021) to enhance the reasoning skills on structured data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_5",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_5@1",
            "content": "Nevertheless, two significant shortcomings remain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_5",
            "start": 185,
            "end": 234,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_5@2",
            "content": "Firstly, the process is demanding due to the tremendous computing resources required by pre-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_5",
            "start": 236,
            "end": 336,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_5@3",
            "content": "Moreover, the effectiveness of pretraining to its downstream tasks mainly depends on the adaptability between these two tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_5",
            "start": 338,
            "end": 463,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_5@4",
            "content": "Therefore, implementing pre-training tasks may fail to meet the requirements when facing the unseen reasoning types demanded by new datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_5",
            "start": 465,
            "end": 605,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_6@0",
            "content": "In this paper, we introduce an innovative framework, Self-adaptive Mixture of Experts (SaMoE), to address the previously mentioned problems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_6",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_6@1",
            "content": "The entire framework is illustrated in Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_6",
            "start": 141,
            "end": 188,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_6@2",
            "content": "SaMoE consists of 3 components: feature extractor, experts, and management module, which is the combination of manager and supervisor networks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_6",
            "start": 190,
            "end": 332,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_6@3",
            "content": "Each expert initially takes the same feature as input and then learns to deal with different parts of the reasoning types (e.g., contextual/logical/numerical) required by table-based verification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_6",
            "start": 334,
            "end": 529,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_6@4",
            "content": "A management module is designed to guide the training of experts and combine experts' verification results effectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_6",
            "start": 531,
            "end": 649,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_6@5",
            "content": "The manager network in this module assigns each expert a unique attention score, allowing each individual to focus on different kinds of reasoning types and summarizes experts' entire outputs as the final verification result.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_6",
            "start": 651,
            "end": 875,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_6@6",
            "content": "However, managers failed to allocate the highest attention score to the expert who performs best on the current reasoning type in most circumstances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_6",
            "start": 877,
            "end": 1025,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_6@7",
            "content": "To alleviate this problem, we introduce a supervisor network to adjust the attention score given by the manager.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_6",
            "start": 1027,
            "end": 1138,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_6@8",
            "content": "The supervisor network is trained selfadaptively (i.e., it learns directly from experts' performance on the train set) without prior knowledge of the task or dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_6",
            "start": 1140,
            "end": 1305,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_6@9",
            "content": "Extensive experiments are conducted to show that our proposed framework, implemented with a general pre-trained language model RoBERTa , outperforms previous state-of-the-art methods, including tablebased pre-trained models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_6",
            "start": 1307,
            "end": 1530,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_6@10",
            "content": "The main contributions of this work are as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_6",
            "start": 1532,
            "end": 1582,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_7@0",
            "content": "\u2022 We innovatively implement mixture-ofexperts for table-based verification, aiming to arrange each expert to different types of reasoning. This method can also be easily generalized to other datasets. \u2022 We investigate a self-adaptive method to adjust suitable attention score to each expert according to its performance on different reasoning types, achieving more efficient cooperation across experts. \u2022 Our framework achieves better performance on the TABFACT dataset without the assistance of table-based pre-trained models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_7",
            "start": 0,
            "end": 526,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_8@0",
            "content": "Task Formulation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_8",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_9@0",
            "content": "The table-based verification task expects one to determine whether a statement S is entailed or refuted by an evidence table T .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_9",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_9@1",
            "content": "The process above can be regarded as a binary classification task and thus denoted as f (S, T ) = \u0177, where f is the verification model and \u0177 \u2208 {0, 1} its prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_9",
            "start": 129,
            "end": 293,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_10@0",
            "content": "Methods",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_10",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_11@0",
            "content": "We present the proposed framework (SaMoE), which leverages a set of experts to deal with different parts of the reasoning types involved in These two processes will be further interpreted in the following subsections.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_11",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_12@0",
            "content": "Table Pre-processing",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_12",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_13@0",
            "content": "As for Tables, the pre-processing (pruning and serializing) before the joint representation learning provides convenience for subsequent processing of the existing language model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_13",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_14@0",
            "content": "Table pruning discards some parts of the table that do not participate in the verification, according to the input size limit of the language model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_14",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_14@1",
            "content": "We take advantage of the tablepruning algorithm proposed in and further enhance its performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_14",
            "start": 149,
            "end": 244,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_14@2",
            "content": "The original algorithm matches the entities in statements with cells in tables by a heuristic method and selects the columns that include matched cells to form the pruned table.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_14",
            "start": 246,
            "end": 422,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_14@3",
            "content": "Noticed that the algorithm always fails to select the critical columns of verification while there is still room left for the input sequence of the language model, we further add a greedy strategy on the algorithm that keeps adding columns that are not selected to the pruned table until reaching the maximum input size of the downstream model to make the best use of its capacity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_14",
            "start": 424,
            "end": 804,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_15@0",
            "content": "Table Serializing Tables are further serialized to a 1-D sequence after pruning to be compatible with the input format of the language model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_15",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_15@1",
            "content": "We follow the serializing method used in TABLE-BERT that paraphrases tables with a natural language template.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_15",
            "start": 142,
            "end": 250,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_15@2",
            "content": "Specifically, a table with m rows and n columns is paraphrased as \"row 1 is: h 1 is T 11 ; ... ; h n is T 1n . row 2 is: ... row m is: h 1 is T m1 ; ... ; h n is T mn .\", where h i refers the i th header and T ij the value in the (i, j) \u2212 th cell of table T .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_15",
            "start": 252,
            "end": 510,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_15@3",
            "content": "We find that such template-serialized tables are more suitable for language models pre-trained on unstructured text to process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_15",
            "start": 512,
            "end": 638,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_16@0",
            "content": "Joint Representation Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_16",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_17@0",
            "content": "After the table pre-processing, the serialized table and the statement are further passed to a language model to learn the joint contextual representation of each token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_17",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_17@1",
            "content": "The learned representation vectors are then transmitted to the experts and the management module for inference and management.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_17",
            "start": 170,
            "end": 295,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_17@2",
            "content": "Specifically, the serialized table and the statement are initially tokenized into two token sequences T and S. Then the joint token sequence X is formed as X = [\u27e8s\u27e9, S, \u27e8/s\u27e9, T, \u27e8/s\u27e9], where \u27e8s\u27e9 and \u27e8/s\u27e9 are the separators that identify the beginning and the end of each token sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_17",
            "start": 297,
            "end": 582,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_17@3",
            "content": "The token sequence X will be padded or truncated to fit the maximum input length of the language model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_17",
            "start": 584,
            "end": 686,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_17@4",
            "content": "Finally, a transformer model is applied to learn the contextual representation of X :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_17",
            "start": 688,
            "end": 772,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_18@0",
            "content": "H = f LM (X)(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_18",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_19@0",
            "content": "where H \u2208 R n\u00d7d refers to the learned joint representation, n is the maximum input length and d the dimension of the representation vector.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_19",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_19@1",
            "content": "f LM denotes the contextual representation learning process of the language model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_19",
            "start": 140,
            "end": 221,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_19@2",
            "content": "In this paper, we implement it with transformer (Vaswani et al., 2017), the most popular contextual representation model in recent years.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_19",
            "start": 223,
            "end": 359,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_20@0",
            "content": "Experts",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_20",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_21@0",
            "content": "A group of experts is applied to verify the statements separately based on the same statement-table joint semantics extracted by the feature extractor module.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_21",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_21@1",
            "content": "Experts share the same model structure, while the parameter learning strategy of SaMoE gives expert differentiation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_21",
            "start": 159,
            "end": 274,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_21@2",
            "content": "Specifically, each expert is implemented with a stack of transformer encoding layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_21",
            "start": 276,
            "end": 360,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_21@3",
            "content": "An MLP classifier that calculates the probability of the statement is entailed by the evidence table based on the encoded semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_21",
            "start": 362,
            "end": 493,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_21@4",
            "content": "We implement experts with the same general structure rather than different structures specially designed for certain reasoning types since we anticipate that the proposed framework can be smoothly generalized to other datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_21",
            "start": 495,
            "end": 721,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_21@5",
            "content": "The process above can be formulated as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_21",
            "start": 723,
            "end": 769,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_22@0",
            "content": "h i = f Enc i (H)(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_22",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_23@0",
            "content": "p i = sof tmax(tanh(h i W i 1 )W i 2 ) (3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_23",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_24@0",
            "content": "where h i \u2208 R d is the token \u27e8s\u27e9's final representation vector encoded by the i th expert's encoder Enc i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_24",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_24@1",
            "content": "It implies the i th expert's whole understanding to the statement-table pair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_24",
            "start": 108,
            "end": 184,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_24@2",
            "content": "W i 1 \u2208 R d\u00d7d and W i 2 \u2208 R d\u00d72 are the trainable parameters of i th expert's classifier, which projects h i to the probabilities p i \u2208 R 2 that the statement is entailed/refuted by the table.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_24",
            "start": 186,
            "end": 377,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_24@3",
            "content": "tanh and sof tmax are activation functions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_24",
            "start": 379,
            "end": 421,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_24@4",
            "content": "n e refers to the number of experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_24",
            "start": 423,
            "end": 458,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_25@0",
            "content": "Management Module",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_25",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_26@0",
            "content": "Learning the joint semantics parsed in Sec.3.1, the management module intends to generate attention scores to bias experts' training and combine experts' results efficiently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_26",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_26@1",
            "content": "The module consists of two components: manager and supervisor, both of them are implemented based on transformer model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_26",
            "start": 175,
            "end": 293,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_26@2",
            "content": "The manager is mainly designed to guide experts' training, while the supervisor is applied to combine experts' results efficiently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_26",
            "start": 295,
            "end": 425,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_27@0",
            "content": "Manager The manager guides the training of experts and forms a preliminary assumption to the expert combination.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_27",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_27@1",
            "content": "It encodes the joint representation matrix and generates attention scores a M to guide the experts' training process:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_27",
            "start": 113,
            "end": 229,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_28@0",
            "content": "h M = f Enc M (H) (4) e M = tanh(h M W M 1 )W M 2 (5) a M = sof tmax(e M )(6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_28",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_29@0",
            "content": "where Enc M denotes the manager's encoder, W M 1 \u2208 R d\u00d7d and W M 2 \u2208 R d\u00d7ne are trainable parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_29",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_29@1",
            "content": "The network structures of the manager and experts are basically the same, only different in the layers of the encoder and the output dimension.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_29",
            "start": 102,
            "end": 244,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_30@0",
            "content": "After preceding calculation, the normalized attention scores a M are used to guide the training of experts by a specially designed verification loss, which will be introduced in Sec.4.1.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_30",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_31@0",
            "content": "Supervisor The supervisor adjusts the attention scores submitted by the manager to improve the cooperative efficiency among experts (i.e. assigning higher weights to experts who perform better on the current input pair).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_31",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_31@1",
            "content": "The network predicts the deviation between the preliminary assumption (i.e., the attention) and the ideal combination weights based on the knowledge encoded in the joint representation matrix H:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_31",
            "start": 221,
            "end": 414,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_32@0",
            "content": "h S = f Enc S (H) (7) e S = tanh(h S W S 1 )W S 2 (8) a S = sof tmax(e M + e S )(9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_32",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_33@0",
            "content": "where W S 1 \u2208 R d\u00d7d , W S 2 \u2208 R d\u00d7ne are trainable parameters and Enc S refers to the encoder of the supervisor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_33",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_33@1",
            "content": "Parameters of the supervisor are optimized self-adaptively based on experts' performance on the train set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_33",
            "start": 113,
            "end": 218,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_33@2",
            "content": "More details of this learning strategy will be presented in Sec.4.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_33",
            "start": 220,
            "end": 287,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_34@0",
            "content": "Parameter Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_34",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_35@0",
            "content": "Parameters in SaMoE are learned in two consecutive stages:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_35",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_35@1",
            "content": "1) Supervised learning: parameters in the feature extractor, experts and the manager are end-to-end optimized under the supervision of labels;",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_35",
            "start": 59,
            "end": 200,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_35@2",
            "content": "2) Self-adaptive learning: parameters in the supervisor are self-optimized by observing experts' performance on the train set (other parameters are fixed simultaneously).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_35",
            "start": 202,
            "end": 371,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_35@3",
            "content": "A weighted sum of two losses is minimized in the first stage to achieve diverse and balanced training of experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_35",
            "start": 373,
            "end": 485,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_35@4",
            "content": "For the second stage, we minimize a self-adaptive loss calculated based on the experts' classification loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_35",
            "start": 487,
            "end": 594,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_35@5",
            "content": "Subsequent sections introduce these two learning stages in detail.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_35",
            "start": 596,
            "end": 661,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_36@0",
            "content": "Supervised Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_36",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_37@0",
            "content": "Supervised learning guides each expert on dealing with different reasoning types while maintaining balanced training across experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_37",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_37@1",
            "content": "To achieve the goals above, we develop two loss functions: 1) verification loss L V that measures the weighted sum of each expert's classification loss, differentiating experts' learning with different attention scores assigned by the manager; 2) manager assumption loss L M that is applied to prevent the occurrence of imbalanced training across experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_37",
            "start": 133,
            "end": 487,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_37@2",
            "content": "The overall loss of this state is calculated by a weighted sum of these two terms: L 1 = L V + \u03bbL M , where \u03bb is a hyperparameter that controls the ratio of L M .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_37",
            "start": 489,
            "end": 650,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_37@3",
            "content": "Subsequent sections provide detailed introduction to these two terms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_37",
            "start": 652,
            "end": 720,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_38@0",
            "content": "Verification Loss",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_38",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_39@0",
            "content": "The verification loss L V is designed based on the loss function proposed in Jacobs et al. (1991).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_39",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_39@1",
            "content": "It is calculated by the weighted sum of each expert's cross-entropy:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_39",
            "start": 99,
            "end": 166,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_40@0",
            "content": "L V = ne i=1 (a M ) i \u2022 CE(p i , l) (10)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_40",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_41@0",
            "content": "where (a M ) i is the i th element of the attention scores a M , l \u2208 {0, 1} is the label of the statementtable pair and CE(\u2022, \u2022) the cross-entropy function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_41",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_42@0",
            "content": "Note that it is necessary to calculate each expert's cross-entropy independently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_42",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_42@1",
            "content": "We want each expert to behave like an independent expert (i.e., complete the verification without the help of other experts).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_42",
            "start": 82,
            "end": 206,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_43@0",
            "content": "The attention vector a M acts as a \"training scheduler\" in this loss function: experts that are assigned with larger attention scores receive a larger gradient than other experts on the current input, resulting in diverse experts' performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_43",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_44@0",
            "content": "Manager Assumption Loss",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_44",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_45@0",
            "content": "We have trained the MoE with only the verification loss L V and observe a severe \"imbalanced experts\" phenomenon that only one expert is well-trained (i.e., the expert performance is improved by training) and the manager keeps assigning a close-to-1 attention score to this expert, which is also reported in previous research (Eigen et al., 2013;. To avoid this problem, we develop another loss function that forces the manager to assign reasonable attention scores to experts:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_45",
            "start": 0,
            "end": 476,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_46@0",
            "content": "L M = D(a P ||a M )(11)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_46",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_47@0",
            "content": "where D(\u2022||\u2022) denotes the Kullback-Leibler divergence and a P a prior assumption that is generated with a simple heuristic algorithm (to be introduced in the next paragraph) which requires limited prior knowledge of the reasoning types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_47",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_47@1",
            "content": "By minimizing L M , the manager learns to assign each expert with a reasonable attention score, resulting in a balanced training across experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_47",
            "start": 237,
            "end": 380,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_48@0",
            "content": "Prior Assumption Generation The prior assumption a P is generated to represent the probabilities that the statement involves different reasoning types that we are interested in.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_48",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_48@1",
            "content": "Specifically, we develop a trigger-word-based heuristic algorithm to form the prior assumption for each statement automatically:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_48",
            "start": 178,
            "end": 305,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_49@0",
            "content": "1. Initialize the prior assumption with e 0 \u2208 R ne , which is empirically set as (0.1, 0.1, ..., 0.6) T .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_49",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_50@0",
            "content": "The (e 0 ) ne represents the probability that the statement does not involve any predefined reasoning types and thus is set higher than other values in advance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_50",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_51@0",
            "content": "2. Traverse the trigger-word set of each reasoning type (n e \u2212 1 types in total).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_51",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_51@1",
            "content": "If a trigger word/pattern w that belongs to i th reasoning type is detected in the statement, the trigger's weight s w (set empirically) is accumulated to the i th dimension of a zero-initialized bias vector \u03b4 \u2208 R ne : \u03b4 i \u2190 \u03b4 i + s w .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_51",
            "start": 82,
            "end": 317,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_52@0",
            "content": "3. Add the bias vector \u03b4 to the prior assumption e 0 and normalize to get the prior assumption: a P = sof tmax(e 0 + \u03b4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_52",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_53@0",
            "content": "Figure 3 presents an example of this process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_53",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_53@1",
            "content": "Learning to imitate the prior assumption, the manager guides each expert to focus on different reasoning types and thus achieves diverse experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_53",
            "start": 46,
            "end": 190,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_53@2",
            "content": "We implement a relatively small trigger-word pool in experiments and find the method works effectively, indicating that the method can be smoothly generalized to other datasets with little modification to the predefined reasoning types and trigger-word pool.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_53",
            "start": 192,
            "end": 449,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_54@0",
            "content": "Self-adaptive Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_54",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_55@0",
            "content": "Self-adaptive learning aims to enhance further the expert combining efficiency with only the knowledge of the expert's performance on the train set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_55",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_55@1",
            "content": "Specifically, an \"expert ability\" vector a E \u2208 R ne is calculated based on the \"expert loss\" vector m \u2208 R ne , where m i = CE(p i , l) is the crossentropy loss of the i th expert.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_55",
            "start": 149,
            "end": 327,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_55@2",
            "content": "Note that the crossentropy of the expert is negatively correlated with its performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_55",
            "start": 329,
            "end": 415,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_55@3",
            "content": "Then the expert ability vector a E is calculated as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_55",
            "start": 417,
            "end": 476,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_56@0",
            "content": "a E = sof tmax(\u2212\u03b1 \u2022 m)(12)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_56",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_57@0",
            "content": "where \u03b1 = \u03b2/V ar(m) is a variancenormalizing coefficient and \u03b2 is a hyperparameter that decides the variance of the expert ability vector before the activation (i.e., V ar(\u2212\u03b1\u2022m) = \u03b2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_57",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_57@1",
            "content": "Such normalization is designed based on the observation that m tends to have a extreme small variance and sof tmax(\u2212m) often generates a close-to-uniform distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_57",
            "start": 184,
            "end": 350,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_57@2",
            "content": "Note that the generated a E is positively correlated with the experts' performance (e.g., if the i th expert outperforms the j th expert on the input pair then we have (a E ) i > (a E ) j ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_57",
            "start": 352,
            "end": 541,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_58@0",
            "content": "Based on a E , we develop the loss function that has the same form with L M in Sec 4.1.2:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_58",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_59@0",
            "content": "L S = D(a E ||a S )(13)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_59",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_60@0",
            "content": "By minimizing the loss above, the higher attention scores are assigned to the best-performed experts after the supervisor's adjustment, resulting in more efficient cooperation across experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_60",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_61@0",
            "content": "Experiment Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_61",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_62@0",
            "content": "Data and Metric",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_62",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_63@0",
            "content": "We conduct the experiments on TABFACT, a large scale benchmark dataset of the table-based fact verification task 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_63",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_63@1",
            "content": "TABFACT contains a total of 117k statements and 16k Wikipedia tables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_63",
            "start": 117,
            "end": 185,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_63@2",
            "content": "The test set is further divided into a simple and complex subset based on verification difficulty, for verifying some statements on TABFACT requires more logical/numerical reasoning skills.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_63",
            "start": 187,
            "end": 375,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_63@3",
            "content": "We choose accuracy as the evaluation metric following the existing work to make our experiment results comparable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_63",
            "start": 377,
            "end": 490,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_63@4",
            "content": "More details of TABFACT are presented in Appendix A.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_63",
            "start": 492,
            "end": 543,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_64@0",
            "content": "Implementation Details",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_64",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_65@0",
            "content": "Training Details We set n e = 5 expert networks in our implementation of SaMoE. The transformer layers are 12 for encoders in the feature extractor and experts and 2 for encoders in the manager and supervisor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_65",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_65@1",
            "content": "The hidden states' dimension d, the maximum input length n, the \u03bb in Sec.4.1, and the \u03b2 in Sec.4.2 are set to 1024, 512, 0.1 and 0.1 respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_65",
            "start": 210,
            "end": 355,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_65@2",
            "content": "We applied RoBERTa-Large to initialize the feature extractor and experts in our framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_65",
            "start": 357,
            "end": 446,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_65@3",
            "content": "The details of parameter initialization can be found in Appendix B.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_65",
            "start": 448,
            "end": 514,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_66@0",
            "content": "We apply Adam optimizer (Kingma and Ba, 2015) in training with learning rate 2e-5, dropout rate 0.1, warmup step 17,304, and batch size 32.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_66",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_67@0",
            "content": "SaMoE is first trained in the supervised learning stage for 57,680 steps (20 epochs).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_67",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_67@1",
            "content": "Then the supervisor is trained in the self-adaptive learning stage for another 5,000 steps, while the best parameters of other parts in the framework are loaded and fixed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_67",
            "start": 86,
            "end": 256,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_67@2",
            "content": "The model is evaluated every 1000 steps, and the model that achieves the highest performance on the development set is saved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_67",
            "start": 258,
            "end": 382,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_67@3",
            "content": "All the codes are implemented with Pytorch (Paszke et al., 2019) and the transformers package (Wolf et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_67",
            "start": 384,
            "end": 497,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_68@0",
            "content": "We choose the top 4 types of reasoning types that appear most frequently in TABFACT 2 (count, comparative, superlative, negation).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_68",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_68@1",
            "content": "We apply a small trigger-word pool containing only 26 trigger words, injecting limited prior knowledge of the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_68",
            "start": 131,
            "end": 248,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_68@2",
            "content": "More details of this part are presented in Appendix C.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_68",
            "start": 250,
            "end": 303,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_69@0",
            "content": "Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_69",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_70@0",
            "content": "We compared our proposed framework with different kinds of baselines on TABFACT: (1) Programenhanced methods: LPA , Log-icalFactChecker (Zhong et al., 2020), HeterTFV (Shi et al., 2020), ProgVGAT and Decomp (Yang and Zhu, 2021); (2) Tablebased pre-trained models: TAPAS and TAPEX (Liu et al., 2021); (3) Other methods: Table-BERT and SAT .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_70",
            "start": 0,
            "end": 338,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_71@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_71",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_72@0",
            "content": "Overall Performance",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_72",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_73@0",
            "content": "We compare the proposed SaMoE with different kinds of baselines, and the results are listed in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_73",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_73@1",
            "content": "Baselines are presented with the best performance reported in the corresponding papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_73",
            "start": 104,
            "end": 190,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_73@2",
            "content": "SaMoE obtains an accuracy of 85.1% on the test set, achieving a new state-of-the-art on the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_73",
            "start": 192,
            "end": 291,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_73@3",
            "content": "Results show that our method consistently outperforms all the program-enhanced methods with a significant 2.4% improvement compared with the Decomp method (the best performed programenhanced method).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_73",
            "start": 293,
            "end": 491,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_73@4",
            "content": "Note that SaMoE performs similar with Decomp-LARGE on the simple subset of the test set (93.6% vs. 93.6%) while outperforms Decomp-LARGE with a remarkable 3.5% on the complex subset (80.9% vs. 77.4%).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_73",
            "start": 493,
            "end": 692,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_73@5",
            "content": "Such analysis indicates that the performance improvement is mainly derived from successfully verifying complex statements, which required more sophisticated reasoning than statements in the simple set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_73",
            "start": 694,
            "end": 894,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_73@6",
            "content": "SaMoE even shows comparable performance with the previous SOTA TAPEX that is pre-trained to execute SQL queries on tables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_73",
            "start": 896,
            "end": 1017,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_73@7",
            "content": "Our method outperforms TAPEX with a 0.9% improvement on the test set and a further 1.3% improvement on the complex subset, indicating that SaMoE, based on a text-based pre-trained model, performs even better than table-based pre-trained models on a variety of complex reasoning types demanded by the",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_73",
            "start": 1019,
            "end": 1317,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_74@0",
            "content": "Ablation Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_74",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_75@0",
            "content": "We further investigate the effectiveness of the MoE structure and self-adaptive learning with an ablation study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_75",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_75@1",
            "content": "We conduct two experiments: one reduces the number of experts to 1 to disable the contribution from the MoE structure (SaMoE w/o Sa (n e = 1)); the other trains the proposed framework with only the supervised learning stage (SaMoE w/o Sa).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_75",
            "start": 113,
            "end": 351,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_75@2",
            "content": "Results are presented in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_75",
            "start": 353,
            "end": 385,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_75@3",
            "content": "The MoE structure achieves a 0.7% improvement on the test set (84.7% vs. 84.0%), and self-adaptive learning further improves the performance slightly (85.1% vs. 84.7%).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_75",
            "start": 387,
            "end": 554,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_75@4",
            "content": "Note that the slight improvement of self-adaptive learning is expected since the experts and the feature extractor are fixed in this stage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_75",
            "start": 556,
            "end": 694,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_75@5",
            "content": "The results demonstrate the effectiveness of both the MoE structure and the self-adaptive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_75",
            "start": 696,
            "end": 794,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_76@0",
            "content": "Effectiveness Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_76",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_77@0",
            "content": "We show in this section that the effectiveness of the proposed framework is derived from two aspects: the differentiation of experts (each expert outperforms others on a specific part of reasoning types) and the effective attention assignment by the management module (the best-performed experts are assigned with higher attention scores).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_77",
            "start": 0,
            "end": 338,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_78@0",
            "content": "Expert Differentiation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_78",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_79@0",
            "content": "We first investigate the proposed manager assumption loss L M and find that it achieves balanced training across experts, which is the premise of expert differentiation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_79",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_79@1",
            "content": "We further show that the proposed framework achieves differentiation across experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_79",
            "start": 170,
            "end": 253,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_79@2",
            "content": "Figure 5 presents the proportion of statements in the test set that are verified correctly by at least k experts (k varies from 1 to 5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_79",
            "start": 255,
            "end": 390,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_79@3",
            "content": "Note that the proportion increases rapidly as k decreases (76.2% to 90.7% for k from 5 to 1), which illustrates that experts behave differently on a large proportion of statements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_79",
            "start": 392,
            "end": 571,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_79@4",
            "content": "The results indicate that SaMoE successfully achieves expert differentiation, which expands the original performance upper bound considerably (90.7%).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_79",
            "start": 573,
            "end": 722,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_80@0",
            "content": "Effective Attention Assignment",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_80",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_81@0",
            "content": "We conduct a detailed analysis to investigate whether the management module assigns higher attention scores to experts with the best performance after self-adaptive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_81",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_81@1",
            "content": "To achieve this goal, we regard the management module as a n e -class classifier and calculate the top-k accuracy of predicting the best-performed expert (the one with the smallest cross-entropy) on the test set where k is chosen in [1,2,3].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_81",
            "start": 175,
            "end": 415,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_81@2",
            "content": "The results of the analysis are presented in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_81",
            "start": 417,
            "end": 469,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_81@3",
            "content": "The top-k accuracy is improved significantly after self-adaptive learning (+6.6%, +14.2%, +8.4% respectively), indicating that the management module successfully assigns higher attention scores to the best-performed experts by self-adaptive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_81",
            "start": 471,
            "end": 720,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_82@0",
            "content": "Based on the significant performance upper bound expanded by the expert differentiation, the effective attention assignment achieves more efficient cooperation across these diverse experts, thus improving the verification performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_82",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_83@0",
            "content": "Related Works",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_83",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_84@0",
            "content": "Table-Based Fact Verification Most of the current models utilize programs to improve the model's ability to handle various types of numerical and logical reasoning Zhong et al., 2020;Shi et al., 2020;Yang and Zhu, 2021) Mixture of Experts Mixture of experts is a special model combining method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_84",
            "start": 0,
            "end": 293,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_84@1",
            "content": "Jacobs et al. (1991) first introduces this method and proposes a loss that encourages competitive learning across expert models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_84",
            "start": 295,
            "end": 422,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_84@2",
            "content": "We develop a self-adapted mixture-ofexperts framework that achieves a more effective combination of experts by learning from the experts' performance on the train set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_84",
            "start": 424,
            "end": 590,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_85@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_85",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_86@0",
            "content": "This paper proposes a framework that leverages the mixture of experts to recognize and execute different types of reasoning required for table-based fact verification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_86",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_86@1",
            "content": "We propose an MoE model guided with limited prior knowledge to handle different parts of the reasoning types required by table-based verification with diverse experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_86",
            "start": 168,
            "end": 334,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_86@2",
            "content": "Moreover, we design a novel supervisor network to adjust the imprecise attention score and achieve a more efficient combination across experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_86",
            "start": 336,
            "end": 478,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_86@3",
            "content": "A self-adaptive learning strategy is further applied to train the proposed supervisor network without prior knowledge of the task or dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_86",
            "start": 480,
            "end": 620,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_86@4",
            "content": "The experiments show that the proposed model achieves a new state-of-the-art performance of 85.1% accuracy on the benchmark dataset TABFACT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_86",
            "start": 622,
            "end": 761,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_86@5",
            "content": "The ablation studies and analysis further indicate the effectiveness of the proposed MoE structure and self-adaptive learning strategy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_86",
            "start": 763,
            "end": 897,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_87@0",
            "content": "For parameter initialization, We leverage RoBERTa-Large, a pre-trained language model that has 24 transformer encoding layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_87",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_87@1",
            "content": "We initial parameters of the feature extractor with the embedding layer and the bottom 12 encoding layers of RoBERTa-Large and each expert with the upper 12 encoding layers of RoBERTa-Large, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_87",
            "start": 127,
            "end": 330,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_87@2",
            "content": "We use PyTorch to initialize other parameters randomly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_87",
            "start": 332,
            "end": 386,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_88@0",
            "content": "We choose four reasoning types that appear most frequently in TABFACT: count, comparative, superlative, and negation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_88",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_88@1",
            "content": "The detailed definitions of four reasoning types chosen in our implementation are listed below:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_88",
            "start": 118,
            "end": 212,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_89@0",
            "content": "1. Count: counting the number of specific rows in the table, such as \"xxx be listed a total of 3 times\", \"xxx win only 1 time in ...\", etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_89",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_90@0",
            "content": "2. Comparative: comparing two values in the statement or cells, such as \"xxx play in more than 1 game during ...\", \"xxx has a larger yyy than zzz\", etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_90",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_91@0",
            "content": "3. Superlative: finding the highest/lowest value of the specific column, such as \"the longest xxx be yyy\", \"the lowest score at xxx be yyy\", etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_91",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_92@0",
            "content": "4. Negation: negating the original semantics of the statement, such as \"xxx has never lost a game in ...\", \"xxx never score 0 points\", etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_92",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_92@1",
            "content": "A small trigger-word pool that contains only 26 trigger words/patterns is applied for the prior assumption generation: 11 triggers for the \"count\" type, 15 for \"negation\"; and for the rest types (i.e., \"comparative\" and \"superlative\" types), the NLTK package is employed to recognize the comparative and superlative words automatically.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_92",
            "start": 140,
            "end": 475,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_92@2",
            "content": "Such a small trigger-word pool injects limit prior knowledge of the dataset, indicating that the proposed method can be generalized to other datasets by simply modifying the pool of trigger words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_92",
            "start": 477,
            "end": 672,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_92@3",
            "content": "Table 5 presents some words/patterns in the trigger-word pool applied in our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_92",
            "start": 674,
            "end": 762,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_92@4",
            "content": "x+[number] denotes a combination of a word and a number that is served as a trigger (e.g., for the statement \"xxx win 3 times in ...\", we match the phrase \"3 times\" with the trigger \"[number]+times\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_92",
            "start": 764,
            "end": 963,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_93@0",
            "content": "Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, William Wang, Tabfact: A large-scale dataset for table-based fact verification, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_93",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_94@0",
            "content": "UNKNOWN, None, 2013, Learning factored representations in a deep mixture of experts, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_94",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_95@0",
            "content": "Julian Eisenschlos, Syrine Krichene, Thomas M\u00fcller, Understanding tables with intermediate pre-training, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_95",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_96@0",
            "content": "Ben Goodrich, Vinay Rao, J Peter, Mohammad Liu,  Saleh, Assessing the factual accuracy of generated text, 2019, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_96",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_97@0",
            "content": "Vivek Gupta, Maitrey Mehta, INFOTABS: Inference on tables as semi-structured data, 2020, Proceedings of the 58th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_97",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_98@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_98",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_99@0",
            "content": "Jonathan Herzig, Krzysztof Nowak, Thomas M\u00fcller, Francesco Piccinno, Julian Eisenschlos, TaPas: Weakly supervised table parsing via pre-training, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_99",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_100@0",
            "content": "A Robert, Michael Jacobs,  Jordan, J Steven, Geoffrey Nowlan,  Hinton, Adaptive mixtures of local experts, 1991, Neural computation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_100",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_101@0",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A method for stochastic optimization, 2015-05-07, 3rd International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_101",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_102@0",
            "content": "Wojciech Kryscinski, Bryan Mccann, Caiming Xiong, Richard Socher, Evaluating the factual consistency of abstractive text summarization, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_102",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_103@0",
            "content": "UNKNOWN, None, 2021, Tapex: Table pre-training via learning a neural sql executor, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_103",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_104@0",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_104",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_105@0",
            "content": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, Pytorch: An imperative style, high-performance deep learning library, 2019-12-08, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_105",
            "start": 0,
            "end": 509,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_106@0",
            "content": "Eunsol Hannah Rashkin, Jin Choi, Svitlana Jang, Yejin Volkova,  Choi, Truth of varying shades: Analyzing language in fake news and political fact-checking, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_106",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_107@0",
            "content": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean, Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017-04-24, 5th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_107",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_108@0",
            "content": "Qi Shi, Yu Zhang, Qingyu Yin, Ting Liu, Learn to combine linguistic and symbolic information for table-based fact verification, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_108",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_109@0",
            "content": "James Thorne, Andreas Vlachos, FEVER: a large-scale dataset for fact extraction and VERification, 2018, NAACL-HLT, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_109",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_110@0",
            "content": "Vaibhav Vaibhav, Raghuram Mandyam, Eduard Hovy, Do sentence interactions matter? leveraging sentence level representations for fake news classification, 2019, Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_110",
            "start": 0,
            "end": 311,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_111@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Illia Kaiser,  Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_111",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_112@0",
            "content": "X Nancy, Diwakar Wang, Marina Mahajan, Sara Danilevsky,  Rosenthal, SemEval-2021 task 9: Fact verification and evidence finding for tabular data in scientific documents (SEM-TAB-FACTS), 2021, Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_112",
            "start": 0,
            "end": 278,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_113@0",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest,  Rush, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_113",
            "start": 0,
            "end": 536,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_114@0",
            "content": "Xiaoyu Yang, Feng Nie, Yufei Feng, Quan Liu, Zhigang Chen, Xiaodan Zhu, Program enhanced fact verification with verbalization and graph attention network, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_114",
            "start": 0,
            "end": 306,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_115@0",
            "content": "UNKNOWN, None, 2021, Exploring decomposition for table-based fact verification, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_115",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_116@0",
            "content": "Hongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi Cao, Fuzheng Zhang, Zhongyuan Wang, Table fact verification with structure-aware transformer, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_116",
            "start": 0,
            "end": 293,
            "label": {}
        },
        {
            "ix": "255-ARR_v1_117@0",
            "content": "Wanjun Zhong, Duyu Tang, Zhangyin Feng, Nan Duan, Ming Zhou, Ming Gong, Linjun Shou, Daxin Jiang, Jiahai Wang, Jian Yin, Logical-FactChecker: Leveraging logical operations for fact checking with graph module network, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "255-ARR_v1_117",
            "start": 0,
            "end": 353,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "255-ARR_v1_0",
            "tgt_ix": "255-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_0",
            "tgt_ix": "255-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_1",
            "tgt_ix": "255-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_1",
            "tgt_ix": "255-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_0",
            "tgt_ix": "255-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_2",
            "tgt_ix": "255-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_4",
            "tgt_ix": "255-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_5",
            "tgt_ix": "255-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_6",
            "tgt_ix": "255-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_3",
            "tgt_ix": "255-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_3",
            "tgt_ix": "255-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_3",
            "tgt_ix": "255-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_3",
            "tgt_ix": "255-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_3",
            "tgt_ix": "255-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_0",
            "tgt_ix": "255-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_8",
            "tgt_ix": "255-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_8",
            "tgt_ix": "255-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_0",
            "tgt_ix": "255-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_9",
            "tgt_ix": "255-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_10",
            "tgt_ix": "255-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_10",
            "tgt_ix": "255-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_10",
            "tgt_ix": "255-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_11",
            "tgt_ix": "255-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_12",
            "tgt_ix": "255-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_12",
            "tgt_ix": "255-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_14",
            "tgt_ix": "255-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_12",
            "tgt_ix": "255-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_12",
            "tgt_ix": "255-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_13",
            "tgt_ix": "255-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_10",
            "tgt_ix": "255-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_15",
            "tgt_ix": "255-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_17",
            "tgt_ix": "255-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_18",
            "tgt_ix": "255-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_16",
            "tgt_ix": "255-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_16",
            "tgt_ix": "255-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_16",
            "tgt_ix": "255-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_16",
            "tgt_ix": "255-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_10",
            "tgt_ix": "255-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_19",
            "tgt_ix": "255-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_21",
            "tgt_ix": "255-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_22",
            "tgt_ix": "255-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_23",
            "tgt_ix": "255-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_20",
            "tgt_ix": "255-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_20",
            "tgt_ix": "255-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_20",
            "tgt_ix": "255-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_20",
            "tgt_ix": "255-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_20",
            "tgt_ix": "255-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_10",
            "tgt_ix": "255-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_24",
            "tgt_ix": "255-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_26",
            "tgt_ix": "255-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_27",
            "tgt_ix": "255-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_28",
            "tgt_ix": "255-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_29",
            "tgt_ix": "255-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_30",
            "tgt_ix": "255-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_31",
            "tgt_ix": "255-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_32",
            "tgt_ix": "255-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_25",
            "tgt_ix": "255-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_25",
            "tgt_ix": "255-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_25",
            "tgt_ix": "255-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_25",
            "tgt_ix": "255-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_25",
            "tgt_ix": "255-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_25",
            "tgt_ix": "255-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_25",
            "tgt_ix": "255-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_25",
            "tgt_ix": "255-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_25",
            "tgt_ix": "255-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_0",
            "tgt_ix": "255-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_33",
            "tgt_ix": "255-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_34",
            "tgt_ix": "255-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_34",
            "tgt_ix": "255-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_34",
            "tgt_ix": "255-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_35",
            "tgt_ix": "255-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_36",
            "tgt_ix": "255-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_36",
            "tgt_ix": "255-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_34",
            "tgt_ix": "255-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_37",
            "tgt_ix": "255-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_39",
            "tgt_ix": "255-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_40",
            "tgt_ix": "255-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_41",
            "tgt_ix": "255-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_42",
            "tgt_ix": "255-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_38",
            "tgt_ix": "255-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_38",
            "tgt_ix": "255-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_38",
            "tgt_ix": "255-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_38",
            "tgt_ix": "255-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_38",
            "tgt_ix": "255-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_38",
            "tgt_ix": "255-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_34",
            "tgt_ix": "255-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_43",
            "tgt_ix": "255-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_45",
            "tgt_ix": "255-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_46",
            "tgt_ix": "255-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_47",
            "tgt_ix": "255-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_48",
            "tgt_ix": "255-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_49",
            "tgt_ix": "255-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_50",
            "tgt_ix": "255-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_51",
            "tgt_ix": "255-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_52",
            "tgt_ix": "255-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_44",
            "tgt_ix": "255-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_44",
            "tgt_ix": "255-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_44",
            "tgt_ix": "255-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_44",
            "tgt_ix": "255-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_44",
            "tgt_ix": "255-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_44",
            "tgt_ix": "255-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_44",
            "tgt_ix": "255-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_44",
            "tgt_ix": "255-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_44",
            "tgt_ix": "255-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_44",
            "tgt_ix": "255-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_34",
            "tgt_ix": "255-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_53",
            "tgt_ix": "255-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_55",
            "tgt_ix": "255-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_56",
            "tgt_ix": "255-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_57",
            "tgt_ix": "255-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_58",
            "tgt_ix": "255-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_59",
            "tgt_ix": "255-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_54",
            "tgt_ix": "255-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_54",
            "tgt_ix": "255-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_54",
            "tgt_ix": "255-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_54",
            "tgt_ix": "255-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_54",
            "tgt_ix": "255-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_54",
            "tgt_ix": "255-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_54",
            "tgt_ix": "255-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_0",
            "tgt_ix": "255-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_60",
            "tgt_ix": "255-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_61",
            "tgt_ix": "255-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_61",
            "tgt_ix": "255-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_62",
            "tgt_ix": "255-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_62",
            "tgt_ix": "255-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_61",
            "tgt_ix": "255-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_63",
            "tgt_ix": "255-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_65",
            "tgt_ix": "255-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_66",
            "tgt_ix": "255-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_64",
            "tgt_ix": "255-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_64",
            "tgt_ix": "255-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_64",
            "tgt_ix": "255-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_64",
            "tgt_ix": "255-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_64",
            "tgt_ix": "255-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_67",
            "tgt_ix": "255-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_61",
            "tgt_ix": "255-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_68",
            "tgt_ix": "255-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_69",
            "tgt_ix": "255-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_69",
            "tgt_ix": "255-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_0",
            "tgt_ix": "255-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_70",
            "tgt_ix": "255-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_71",
            "tgt_ix": "255-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_71",
            "tgt_ix": "255-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_72",
            "tgt_ix": "255-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_72",
            "tgt_ix": "255-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_71",
            "tgt_ix": "255-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_73",
            "tgt_ix": "255-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_74",
            "tgt_ix": "255-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_74",
            "tgt_ix": "255-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_71",
            "tgt_ix": "255-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_75",
            "tgt_ix": "255-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_76",
            "tgt_ix": "255-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_76",
            "tgt_ix": "255-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_71",
            "tgt_ix": "255-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_77",
            "tgt_ix": "255-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_78",
            "tgt_ix": "255-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_78",
            "tgt_ix": "255-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_71",
            "tgt_ix": "255-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_79",
            "tgt_ix": "255-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_81",
            "tgt_ix": "255-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_80",
            "tgt_ix": "255-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_80",
            "tgt_ix": "255-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_80",
            "tgt_ix": "255-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_0",
            "tgt_ix": "255-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_82",
            "tgt_ix": "255-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_83",
            "tgt_ix": "255-ARR_v1_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_83",
            "tgt_ix": "255-ARR_v1_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_0",
            "tgt_ix": "255-ARR_v1_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_84",
            "tgt_ix": "255-ARR_v1_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_85",
            "tgt_ix": "255-ARR_v1_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_85",
            "tgt_ix": "255-ARR_v1_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_85",
            "tgt_ix": "255-ARR_v1_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_86",
            "tgt_ix": "255-ARR_v1_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_88",
            "tgt_ix": "255-ARR_v1_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_89",
            "tgt_ix": "255-ARR_v1_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_90",
            "tgt_ix": "255-ARR_v1_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_91",
            "tgt_ix": "255-ARR_v1_92",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_85",
            "tgt_ix": "255-ARR_v1_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_85",
            "tgt_ix": "255-ARR_v1_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_85",
            "tgt_ix": "255-ARR_v1_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_85",
            "tgt_ix": "255-ARR_v1_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_85",
            "tgt_ix": "255-ARR_v1_92",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_87",
            "tgt_ix": "255-ARR_v1_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "255-ARR_v1_0",
            "tgt_ix": "255-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_1",
            "tgt_ix": "255-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_2",
            "tgt_ix": "255-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_2",
            "tgt_ix": "255-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_2",
            "tgt_ix": "255-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_2",
            "tgt_ix": "255-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_2",
            "tgt_ix": "255-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_2",
            "tgt_ix": "255-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_2",
            "tgt_ix": "255-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_2",
            "tgt_ix": "255-ARR_v1_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_3",
            "tgt_ix": "255-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_4",
            "tgt_ix": "255-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_4",
            "tgt_ix": "255-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_4",
            "tgt_ix": "255-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_4",
            "tgt_ix": "255-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_4",
            "tgt_ix": "255-ARR_v1_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_4",
            "tgt_ix": "255-ARR_v1_4@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_4",
            "tgt_ix": "255-ARR_v1_4@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_4",
            "tgt_ix": "255-ARR_v1_4@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_4",
            "tgt_ix": "255-ARR_v1_4@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_5",
            "tgt_ix": "255-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_5",
            "tgt_ix": "255-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_5",
            "tgt_ix": "255-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_5",
            "tgt_ix": "255-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_5",
            "tgt_ix": "255-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_6",
            "tgt_ix": "255-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_6",
            "tgt_ix": "255-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_6",
            "tgt_ix": "255-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_6",
            "tgt_ix": "255-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_6",
            "tgt_ix": "255-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_6",
            "tgt_ix": "255-ARR_v1_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_6",
            "tgt_ix": "255-ARR_v1_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_6",
            "tgt_ix": "255-ARR_v1_6@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_6",
            "tgt_ix": "255-ARR_v1_6@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_6",
            "tgt_ix": "255-ARR_v1_6@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_6",
            "tgt_ix": "255-ARR_v1_6@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_7",
            "tgt_ix": "255-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_8",
            "tgt_ix": "255-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_9",
            "tgt_ix": "255-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_9",
            "tgt_ix": "255-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_10",
            "tgt_ix": "255-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_11",
            "tgt_ix": "255-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_12",
            "tgt_ix": "255-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_13",
            "tgt_ix": "255-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_14",
            "tgt_ix": "255-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_14",
            "tgt_ix": "255-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_14",
            "tgt_ix": "255-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_14",
            "tgt_ix": "255-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_15",
            "tgt_ix": "255-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_15",
            "tgt_ix": "255-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_15",
            "tgt_ix": "255-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_15",
            "tgt_ix": "255-ARR_v1_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_16",
            "tgt_ix": "255-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_17",
            "tgt_ix": "255-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_17",
            "tgt_ix": "255-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_17",
            "tgt_ix": "255-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_17",
            "tgt_ix": "255-ARR_v1_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_17",
            "tgt_ix": "255-ARR_v1_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_18",
            "tgt_ix": "255-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_19",
            "tgt_ix": "255-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_19",
            "tgt_ix": "255-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_19",
            "tgt_ix": "255-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_20",
            "tgt_ix": "255-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_21",
            "tgt_ix": "255-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_21",
            "tgt_ix": "255-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_21",
            "tgt_ix": "255-ARR_v1_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_21",
            "tgt_ix": "255-ARR_v1_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_21",
            "tgt_ix": "255-ARR_v1_21@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_21",
            "tgt_ix": "255-ARR_v1_21@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_22",
            "tgt_ix": "255-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_23",
            "tgt_ix": "255-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_24",
            "tgt_ix": "255-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_24",
            "tgt_ix": "255-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_24",
            "tgt_ix": "255-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_24",
            "tgt_ix": "255-ARR_v1_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_24",
            "tgt_ix": "255-ARR_v1_24@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_25",
            "tgt_ix": "255-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_26",
            "tgt_ix": "255-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_26",
            "tgt_ix": "255-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_26",
            "tgt_ix": "255-ARR_v1_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_27",
            "tgt_ix": "255-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_27",
            "tgt_ix": "255-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_28",
            "tgt_ix": "255-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_29",
            "tgt_ix": "255-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_29",
            "tgt_ix": "255-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_30",
            "tgt_ix": "255-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_31",
            "tgt_ix": "255-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_31",
            "tgt_ix": "255-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_32",
            "tgt_ix": "255-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_33",
            "tgt_ix": "255-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_33",
            "tgt_ix": "255-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_33",
            "tgt_ix": "255-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_34",
            "tgt_ix": "255-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_35",
            "tgt_ix": "255-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_35",
            "tgt_ix": "255-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_35",
            "tgt_ix": "255-ARR_v1_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_35",
            "tgt_ix": "255-ARR_v1_35@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_35",
            "tgt_ix": "255-ARR_v1_35@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_35",
            "tgt_ix": "255-ARR_v1_35@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_36",
            "tgt_ix": "255-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_37",
            "tgt_ix": "255-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_37",
            "tgt_ix": "255-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_37",
            "tgt_ix": "255-ARR_v1_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_37",
            "tgt_ix": "255-ARR_v1_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_38",
            "tgt_ix": "255-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_39",
            "tgt_ix": "255-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_39",
            "tgt_ix": "255-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_40",
            "tgt_ix": "255-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_41",
            "tgt_ix": "255-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_42",
            "tgt_ix": "255-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_42",
            "tgt_ix": "255-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_43",
            "tgt_ix": "255-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_44",
            "tgt_ix": "255-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_45",
            "tgt_ix": "255-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_46",
            "tgt_ix": "255-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_47",
            "tgt_ix": "255-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_47",
            "tgt_ix": "255-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_48",
            "tgt_ix": "255-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_48",
            "tgt_ix": "255-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_49",
            "tgt_ix": "255-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_50",
            "tgt_ix": "255-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_51",
            "tgt_ix": "255-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_51",
            "tgt_ix": "255-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_52",
            "tgt_ix": "255-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_53",
            "tgt_ix": "255-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_53",
            "tgt_ix": "255-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_53",
            "tgt_ix": "255-ARR_v1_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_54",
            "tgt_ix": "255-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_55",
            "tgt_ix": "255-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_55",
            "tgt_ix": "255-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_55",
            "tgt_ix": "255-ARR_v1_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_55",
            "tgt_ix": "255-ARR_v1_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_56",
            "tgt_ix": "255-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_57",
            "tgt_ix": "255-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_57",
            "tgt_ix": "255-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_57",
            "tgt_ix": "255-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_58",
            "tgt_ix": "255-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_59",
            "tgt_ix": "255-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_60",
            "tgt_ix": "255-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_61",
            "tgt_ix": "255-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_62",
            "tgt_ix": "255-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_63",
            "tgt_ix": "255-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_63",
            "tgt_ix": "255-ARR_v1_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_63",
            "tgt_ix": "255-ARR_v1_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_63",
            "tgt_ix": "255-ARR_v1_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_63",
            "tgt_ix": "255-ARR_v1_63@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_64",
            "tgt_ix": "255-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_65",
            "tgt_ix": "255-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_65",
            "tgt_ix": "255-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_65",
            "tgt_ix": "255-ARR_v1_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_65",
            "tgt_ix": "255-ARR_v1_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_66",
            "tgt_ix": "255-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_67",
            "tgt_ix": "255-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_67",
            "tgt_ix": "255-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_67",
            "tgt_ix": "255-ARR_v1_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_67",
            "tgt_ix": "255-ARR_v1_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_68",
            "tgt_ix": "255-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_68",
            "tgt_ix": "255-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_68",
            "tgt_ix": "255-ARR_v1_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_69",
            "tgt_ix": "255-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_70",
            "tgt_ix": "255-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_71",
            "tgt_ix": "255-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_72",
            "tgt_ix": "255-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_73",
            "tgt_ix": "255-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_73",
            "tgt_ix": "255-ARR_v1_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_73",
            "tgt_ix": "255-ARR_v1_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_73",
            "tgt_ix": "255-ARR_v1_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_73",
            "tgt_ix": "255-ARR_v1_73@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_73",
            "tgt_ix": "255-ARR_v1_73@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_73",
            "tgt_ix": "255-ARR_v1_73@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_73",
            "tgt_ix": "255-ARR_v1_73@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_74",
            "tgt_ix": "255-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_75",
            "tgt_ix": "255-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_75",
            "tgt_ix": "255-ARR_v1_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_75",
            "tgt_ix": "255-ARR_v1_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_75",
            "tgt_ix": "255-ARR_v1_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_75",
            "tgt_ix": "255-ARR_v1_75@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_75",
            "tgt_ix": "255-ARR_v1_75@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_76",
            "tgt_ix": "255-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_77",
            "tgt_ix": "255-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_78",
            "tgt_ix": "255-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_79",
            "tgt_ix": "255-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_79",
            "tgt_ix": "255-ARR_v1_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_79",
            "tgt_ix": "255-ARR_v1_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_79",
            "tgt_ix": "255-ARR_v1_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_79",
            "tgt_ix": "255-ARR_v1_79@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_80",
            "tgt_ix": "255-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_81",
            "tgt_ix": "255-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_81",
            "tgt_ix": "255-ARR_v1_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_81",
            "tgt_ix": "255-ARR_v1_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_81",
            "tgt_ix": "255-ARR_v1_81@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_82",
            "tgt_ix": "255-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_83",
            "tgt_ix": "255-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_84",
            "tgt_ix": "255-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_84",
            "tgt_ix": "255-ARR_v1_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_84",
            "tgt_ix": "255-ARR_v1_84@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_85",
            "tgt_ix": "255-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_86",
            "tgt_ix": "255-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_86",
            "tgt_ix": "255-ARR_v1_86@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_86",
            "tgt_ix": "255-ARR_v1_86@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_86",
            "tgt_ix": "255-ARR_v1_86@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_86",
            "tgt_ix": "255-ARR_v1_86@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_86",
            "tgt_ix": "255-ARR_v1_86@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_87",
            "tgt_ix": "255-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_87",
            "tgt_ix": "255-ARR_v1_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_87",
            "tgt_ix": "255-ARR_v1_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_88",
            "tgt_ix": "255-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_88",
            "tgt_ix": "255-ARR_v1_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_89",
            "tgt_ix": "255-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_90",
            "tgt_ix": "255-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_91",
            "tgt_ix": "255-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_92",
            "tgt_ix": "255-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_92",
            "tgt_ix": "255-ARR_v1_92@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_92",
            "tgt_ix": "255-ARR_v1_92@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_92",
            "tgt_ix": "255-ARR_v1_92@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_92",
            "tgt_ix": "255-ARR_v1_92@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_93",
            "tgt_ix": "255-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_94",
            "tgt_ix": "255-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_95",
            "tgt_ix": "255-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_96",
            "tgt_ix": "255-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_97",
            "tgt_ix": "255-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_98",
            "tgt_ix": "255-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_99",
            "tgt_ix": "255-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_100",
            "tgt_ix": "255-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_101",
            "tgt_ix": "255-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_102",
            "tgt_ix": "255-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_103",
            "tgt_ix": "255-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_104",
            "tgt_ix": "255-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_105",
            "tgt_ix": "255-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_106",
            "tgt_ix": "255-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_107",
            "tgt_ix": "255-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_108",
            "tgt_ix": "255-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_109",
            "tgt_ix": "255-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_110",
            "tgt_ix": "255-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_111",
            "tgt_ix": "255-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_112",
            "tgt_ix": "255-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_113",
            "tgt_ix": "255-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_114",
            "tgt_ix": "255-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_115",
            "tgt_ix": "255-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_116",
            "tgt_ix": "255-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "255-ARR_v1_117",
            "tgt_ix": "255-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1215,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "255-ARR",
        "version": 1
    }
}