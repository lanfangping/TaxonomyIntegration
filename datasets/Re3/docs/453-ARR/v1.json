{
    "nodes": [
        {
            "ix": "453-ARR_v1_0",
            "content": "Controllable Natural Language Generation with Contrastive Prefixes",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_2",
            "content": "To guide the generation of large pretrained language models (LM), previous work has focused on directly fine-tuning the language model or utilizing an attribute discriminator. In this work, we propose a novel lightweight framework for controllable GPT2 (Radford et al., 2019) generation, which utilizes a set of small attribute-specific vectors, called prefixes (Li and Liang, 2021), to steer natural language generation. Different from Li and Liang (2021), where each prefix is trained independently, we take the relationship among prefixes into consideration and train multiple prefixes simultaneously, as illustrated in Figure 1. We propose a novel supervised method and also an unsupervised method to train the prefixes for single-aspect control while the combination of these two methods can achieve multi-aspect control. Experimental results on both singleaspect and multi-aspect control show that our methods can guide generation towards the desired attributes while keeping high linguistic quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "453-ARR_v1_4",
            "content": "The goal of controllable Natural Language Generation (NLG) is to guide generation towards the desired attributes in the concerned aspects of the text. For example, the aspect can be topic or sentiment, and sentiment may have two attributes: positive and negative. Previous work has focused on directly fine-tuning the existing models (Keskar et al., 2019;Hu et al., 2017;Ficler and Goldberg, 2017) or using a discriminator to guide generation (Dathathri et al., 2020;Krause et al., 2020;Holtzman et al., 2018). CTRL (Keskar et al., 2019) achieves controllability at the expense of training a large conditional LM. GeDi (Krause et al., 2020) also trains conditional LMs but uses them as discriminators to guide generation, introducing additional 345M parameters. Besides, GeDi focuses on single-aspect control, ignoring the need for multi-aspect control. PPLM (Dathathri et al., 2020) guides generation by iteratively updating the LM's hidden activations. However, this decoding strategy is extremely computationally intensive, resulting in a slow generation speed (Gehman et al., 2020). Prefix-tuning (Li and Liang, 2021) proposes to optimize a prefix, which is a small continuous taskspecific vector, as a lightweight alternative to finetuning an NLG task, such as table-to-text generation or summarization. Inspired by Li and Liang (2021), we propose to use prefixes, a set of small continuous attribute-specific vectors, to steer NLG. Compared with using an attribute model or a generative discriminator (Dathathri et al., 2020;Krause et al., 2020), using learned prefixes to achieve controllability has the following benefits. First, it introduces fewer additional parameters (~0.2%-2% of GPT2 parameters in our experiments). Second, using prefixes keeps the inference speed comparable to that of the original GPT2 model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_5",
            "content": "In a general sense, prefix-tuning (Li and Liang, 2021) can be considered as controlling the generation of language models. Prefix-tuning views each prefix as an independent control task thus trains each prefix separately (top in Figure 1). However, one aspect of controllability in NLG involves multiple attributes, which might have a relationship with each other. For example, the sentiment aspect usually has two attributes: positive and negative, which are in opposition to each other. We think that this opposite relationship can be helpful to improve the controllability of a prefix. Therefore, we propose a novel supervised method and a novel unsupervised one in our framework, which takes the relationship among prefixes into consideration and trains multiple prefixes simultaneously with novel training objectives, as illustrated in Figure 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_6",
            "content": "Experimental results on the single-aspect control tasks (sentiment control, detoxification, and topic control) show that our proposed methods can guide generation towards the target attribute while keeping high linguistic quality, even when only several dozen labeled examples are available. In addition to single-aspect control, multi-aspect control can be achieved by combining the proposed supervised method with the unsupervised method in our framework. Experimental results on the sentiment and topic control show that the prefixes trained with our method can successfully control these two aspects simultaneously.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_7",
            "content": "Our main contributions are as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_8",
            "content": "\u2022 We propose a novel framework that utilizes prefixes with frozen LMs as a lightweight alternative for controllable GPT2 generation. \u2022 We propose a supervised method and an unsupervised method with novel objectives for prefix training, where the relationship among prefixes are considered and multiple prefixes are trained simultaneously. \u2022 This work provides a unified perspective for single-aspect control and multi-aspect control.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_9",
            "content": "Experimental results show that our methods can effectively guide generation in both single-aspect control and multi-aspect control.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_10",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "453-ARR_v1_11",
            "content": "Ficler and Goldberg (2017) control the stylistic aspects of the generated text with a conditioned RNN (Recurrent Neural Network) LM. Holtzman et al. (2018) compose a committee of discriminators to guide an RNN generator towards the generations with the desired linguistic quality. Hu et al. (2017) aim at controlling the sentiment and tense of the generated text by combining variational autoencoders (VAE) and attribute discriminators. Controlling these attributes of text generation has manifold applications, such as knowledge-grounded conversation (Dinan et al., 2019) and poetry generation (Ghazvininejad et al., 2017). More recently, with the advent of Transformers and large pretrained language models, such as GPT2, an extensive body of work has focused on controlling the generation of these Transformerbased models. Keskar et al. (2019) train a 1.63 billion-parameter conditional transformer LM from scratch with 55 attribute control codes to guide generation. However, this method is expensive and lacks flexibility since the control codes are fixed. Dathathri et al. (2020) address these limitations by developing a plug-and-play model which leverages an attribute discriminator to perturb the LM's hidden activations. However, updating gradients at the token level results in slow inference. Instead of updating the hidden activations, Krause et al. (2020); Yang and Klein (2021); Lin and Riedl (2021) introduce generative discriminators to re-weight the next token distributions on the fly during inference, thus improving the inference speed.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_12",
            "content": "Our work is mostly related to Yu et al. (2021); Li and Liang (2021). Yu et al. (2021) use a pretrained LM followed by an attribute alignment function to encode the tokens of the target attributes and the resulting hidden states are used to control generation. Different from their work, we do not take the tokens of the target attributes as input. Instead, we directly train a set of parameters, which acts as the prepended hidden states of GPT2, to control generation. Avoiding using attribute tokens can circumvent the problems when it is difficult to describe the desired attribute with only one word. Besides, Yu et al. (2021) focus on attributes disentanglement, which is not a focus in our work, so our training methods are different. Prefix-tuning (Li and Liang, 2021) can, in a general sense, be viewed as controlling the generation of LMs, where the LM is controlled to depict a specific NLG task, while in this work, the LM is controlled to carry specific attributes in a generation. Besides, our proposed methods for prefix training are different from Li and Liang (2021), as stated in Section 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_13",
            "content": "Method",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "453-ARR_v1_14",
            "content": "Our method uses prefixes to guide GPT2 generation, where a prefix is a continuous attributespecific vector prepended to the activations of the GPT2 model. Prefixes are free parameters denoted as H \u03b8 . Different from Li and Liang (2021), where each prefix is trained independently, we consider the relationship among attributes and train multiple prefixes simultaneously, so H \u03b8 is of dimension N \u00d7 M \u00d7 D, where N is the number of prefixes. In single-aspect control, N equals the number of attributes in the concerned aspect while in multiaspect control, N equals the product of the number of attributes in each aspect. M is the length of a prefix, and D is the dimension of the activation in GPT2. Following Li and Liang (2021), we reparametrize H \u03b8 [i, j, :] = W i H \u03b8 [i, j, :] by a smaller parameter (H \u03b8 ) composed with a large matrix (W i ). After the training finishes, only H \u03b8 needs to be saved for generation while W and H \u03b8 can be discarded. Since the GPT2 parameters are kept frozen during training, they do not need to be saved either. Figure 2 shows an example of the generation process under the control of a trained prefix. The prefixes can be trained in a supervised, semi-supervised, or unsupervised way. Since the semi-supervised method is a combination of the supervised and the unsupervised method, we introduce the supervised and the unsupervised method in this section. For clarity, we introduce these methods under the single-aspect control setting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_15",
            "content": "Supervised Method",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "453-ARR_v1_16",
            "content": "Suppose the concerned aspect has the attribute set Y , each training example is a pair of (x, y) where x is the input text and y \u2208 Y is the attribute label of x. Note that the attribute label also indicates the ground truth index of the prefix in H \u03b8 , so y also refers to the prefix index in the following description. As mentioned in Section 1, we introduce an additional discriminative loss to train multiple prefixes simultaneously. Therefore, the training loss L sup is a weighted sum of the language model loss L LM and the discriminative loss L d :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_17",
            "content": "L sup = \u03c9 1 L LM + \u03c9 2 L d(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_18",
            "content": "L LM = \u2212 T t=1 log p(x t |x <t , y)(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_19",
            "content": "L d = \u2212 log p(y)p(x|y) y \u2208Y p(y )p(x|y )(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_20",
            "content": "The computation of log p(x t |x <t , y) is parameterized as log p \u03b8,\u03b3 (x t |x <t , H \u03b8 [y, :, :]), where \u03b3 is the set of fixed GPT2 parameters, and \u03b8 represents learnable prefix parameters. log p(x|y) = over t.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_21",
            "content": "Note that each prefix can be trained independently using L LM alone, which would be the same as prefix-tuning (Li and Liang, 2021). Intuitively, prefixes trained by L LM are infused with the information of what is encouraged to generate. However, we observe that in controllable NLG, it is helpful to also infuse a prefix with the information of what is discouraged to generate. Given a training example (x, y), the prefix H \u03b8 [y, :, :] should be optimized towards generating x, while the other prefixes should be discouraged to generate x. To achieve this goal, all the prefixes in H \u03b8 should be trained simultaneously. Therefore, the discriminative loss L d is introduced. As in equation 3, optimizing L d improves the attribute alignment p(y|x) by increasing p(x|y) and lowering p(x|\u0233), \u0233 \u2208 Y \\{y} at the same time. We assume uniform prior, so p(y) and p(y ) can be canceled out in Equation 3. Figure 3 illustrates the training process with two prefixes.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_22",
            "content": "Unsupervised Method",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "453-ARR_v1_23",
            "content": "In the unsupervised setting, we assume the attribute set Y of the concerned aspect is known. The training example consists of input text x only. The attribute label y is no longer available and thus the index of the prefix associated with x is unknown. Inspired by VQ-VAE (van den Oord et al., 2017), we consider the index of the prefix as a latent variable z. We take the backbone model in the above supervised method as the decoder and introduce an encoder to parameterize the categorical distribution q(z|x). According to q(z|x), a prefix index z is sampled and the prefix H \u03b8 [z, :, :] is then fed into the decoder to reconstruct the input text x. Since the sampling process of the prefixes is non-differentiable, we use Gumbel-Softmax (GS) relaxation (Jang et al., 2017;Maddison et al., 2017) following S\u00f8nderby et al. ( 2017); Ramesh et al. (2021). Formally, q(z|x) is computed as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_24",
            "content": "q(z|x) = GS(\u2212 Enc(x) \u2212 H \u03b8 2 , \u03c4 ) (4) where \u03c4 is the temperature of Gumbel-Softmax, and Enc is the encoder function. To train the prefixes, the loss function is a weighted sum of the three loss terms:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_25",
            "content": "L uns = \u03c9 1 L LM + \u03c9 2 L KL + \u03c9 3 L c (5) L LM = \u2212 T t=1 log p(x t |x <t , z)(6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_26",
            "content": "L KL = KL[q(z|x)||p(z)](7",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_27",
            "content": ") where L LM is the language model loss. Similar as that in the supervised method, the computation of log p(x t |x <t , z) is parameterized as log p \u03b8,\u03b3 (x t |x <t , H \u03b8 [z, :, :]). L KL is the Kullback-Leibler divergence, where we assume the prior p(z) to be uniform. Note that these two terms constitute the loss function of VAE. Optimizing these two loss terms improves the Evidence Lower BOund (ELBO) of log p(x). Similar to the intuition behind L d in the supervised method, if the ground truth prefix for x is H \u03b8 [y, :, :], then the other prefixes should be discouraged to generate x. However, L d requires the ground truth attribute label y for computation. Instead, we introduce an unsupervised contrastive loss L c during training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_28",
            "content": "L c = max(m \u2212 p(z|x) \u2212 p(z|x) 2 , 0) 2 (8) where m is a pre-set margin and z is another latent variable indicating the index of the opposite prefix of x. q(z|x) is computed as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_29",
            "content": "q(z|x) = GS( Enc(x) \u2212 H \u03b8 2 , \u03c4 )(9",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_30",
            "content": ") L c is aimed at increasing the attribute alignment by pushing p(z|x) away from p(z|x) by a margin. The computation of p(z|x) is as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_31",
            "content": "p(z|x) = p(z)p(x|z) z \u2208Y p(z )p(x|z )(10)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_32",
            "content": "We assume uniform prior, so p(z) and p(z ) can be canceled out. Similar as the parameterization of log p(x|y) in the supervised method, the parameterization of log p(x|z) is the sum of log p \u03b8,\u03b3 (x t |x <t , H \u03b8 [z, :, :]) over t. The training process is illustrated in Figure 4.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_33",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "453-ARR_v1_34",
            "content": "We experiment with three tasks: sentiment control, detoxification, and topic control. We compare our method to GPT2, PPLM, and GeDi. We experiment with GPT2-medium (345M parameters) for all the methods. We use the original implementation of PPLM and GeDi released by Dathathri et al. (2020) and Krause et al. (2020), and the hyperparameters are set to the reported value in the original paper. The detailed hyperparameters in each task are listed in appendix A. For the GPT2 model, we do experiments under two settings. First, the GPT2 model generates completions of each prompt in the evaluation dataset, which is denoted as GPT2medium. Second, GPT2-medium + prompt engineering prepends a guiding sentence to each testing prompt and then generates completions of each augmented prompt. We evaluate the linguistic quality and attribute alignment of the generation. The linguistic quality is evaluated using the perplexity calculated by GPT2-large (774M parameters).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_35",
            "content": "To evaluate the robustness of our supervised method with the size of the training dataset, we experiment with the following three different settings: 1) using the complete training dataset; 2) using 1,000 examples per attribute for training; 3) using 24 examples per attribute for training. We evaluate our unsupervised method on the sentiment control task and the detoxification task, which are binary tasks. Note that different from the supervised method, our unsupervised method does not use any attribute labels, so the order of the attributes in the trained prefixes is undetermined. After the prefixes finish training using the unsupervised method, we manually check the order of the attributes.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_36",
            "content": "Single-Aspect Control",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "453-ARR_v1_37",
            "content": "Tasks",
            "ntype": "title",
            "meta": {
                "section": "4.1.1"
            }
        },
        {
            "ix": "453-ARR_v1_38",
            "content": "Sentiment Control Same as GeDi, we use IMDb movie reviews (Maas et al., 2011) to train our model. The number of prefixes is 2. Note that GeDi only uses 11.25k examples from the dataset for training. To be a fair comparison, we randomly sample 11.25k examples from the dataset to train our model. To evaluate the sentiment alignment of the generated text, we finetune a RoBERTa (Liu et al., 2019) classifier using the Yelp Review dataset (Zhang et al., 2015). The prompts used for evaluation are the same as those in the PPLM experiment (Dathathri et al., 2020). For each of the 15 prompts, 45 completions are gen-erated. In the GPT2-medium + prompt engineering setting, we prepend each prompt with the guiding sentence \"This is a negative review:\" for negative sentiment control, and similarly, we prepend each prompt with \"This is a positive review:\" for positive sentiment control.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_39",
            "content": "Detoxification We use Jigsaw Toxic Comment Classification Challenge Dataset 1 to train our model. The number of prefixes is 2. Google Perspective API 2 is used for toxicity evaluation. The testing prompts are collected from RealTox-icityPrompts (Gehman et al., 2020). We use the prompts categorized as \"challenging\" in the dataset. We further filter out the prompts with toxicity larger than 0.5, scored by Perspective. The resulted evaluation dataset consists of 203 prompts. For each of these prompts, 20 completions are generated. In the GPT2-medium + prompt engineering setting, we prepend each prompt with the guiding sentence \"This is a non-toxic comment:\".",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_40",
            "content": "We experiment with the AGNews dataset and DBPedia dataset (Zhang et al., 2015). The number of prefixes is 4 and 14, respectively. The prompts used for evaluation are the same as those in the PPLM experiment (Dathathri et al., 2020). For each of the 20 prompts, 45 completions are generated. Same as that in GeDi, we split each of the original training datasets in half. One half is used to train prefixes, while the other half is used to train a RoBERTa topic classifier for topic relevance evaluation. In the GPT2-medium + prompt engineering setting, the guiding sentence follows the template \"The following is about [TOPIC]\". We do not compare with PPLM in the topic control task since PPLM uses a bag-of-words attribute model to do topic control, where the 7 predefined topics are different from the topics in the AGNews dataset or the DBPedia dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_41",
            "content": "All the experiments are conducted on NVIDIA Tesla V100 GPUs. The detailed hyper-parameters for each experiment are listed in appendix A.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_42",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "4.1.2"
            }
        },
        {
            "ix": "453-ARR_v1_43",
            "content": "In the unsupervised setting, GPT2-medium + prompt engineering shows controllability on sentiment control (Table 1) and topic control (Table 3). However, this method does not work on the detoxification task ( significantly lowers the toxicity on the detoxification task and the ablation study shows that the contrastive loss L c is crucial. On the sentiment control task, our unsupervised method does not achieve good attribute alignment when the target sentiment is negative, but it performs well when the target sentiment is positive. One possible reason is that compared with the differences between toxic and normal sentences, the difference between positive sentiment and negative sentiment is more subtle, so it is more challenging for the GPT2 encoder in our unsupervised model to accurately separate the unlabeled data into two sentiments. As a result, the encoder's implicit criterion to categorize the input text may not be exactly the sentiment, which is also the reason that after removing the contrastive loss L c in the unsupervised loss function, the attribute relevance on the negative sentiment is higher while that on the positive sentiment is lower.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_44",
            "content": "In the supervised setting with full data, our supervised method consistently achieves better controllability than PPLM while maintaining the linguistic quality of the generations (Table 1, 2). Although GeDi achieves a high attribute alignment score on the three tasks, it severely sacrifices the linguistic quality, as indicated by the high perplexity. In the few-shot setting, where the number of labeled training examples is reduced to 1000 or 24 examples per attribute, our supervised method can still maintain good controllability on the three tasks, showing the robustness of our method to the size of the training data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_45",
            "content": "Ablation study shows the importance of the discriminative loss L d in our supervised method. As mentioned in section 3, training without L d is equivalent to prefix-tuning. Comparing the results of Ours\u2212L d and GPT2-medium show that directly using prefix-tuning can achieve controllability on the sentiment or the topic. However, it is less effective on detoxification. The reason is that different from topic control or sentiment control, detoxification requires the model to avoid generating some words or phrases according to the context, which can not be achieved by prefix-tuning. L d fills this gap by increasing p(x|y) and lowering p(x|\u0233) at the same time. Therefore, incorporating L d is of critical importance to the detoxification task. In the DBPedia topic control task, adding L d also achieves a large improvement on attribute alignment. The number of attributes in this task is much larger than that in the other tasks, so incorporating L d can effectively push the prefixes to capture the unique features of each topic.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_46",
            "content": "We compare the average inference speed of our methods with the baselines (Table 5). The inference speed of PPLM is several dozen times slower than that of the original GPT2 model. GeDi's inference speed is much faster than that of PPLM. The inference speed of our method is the closest to that of the original GPT2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_47",
            "content": "Human Evaluation",
            "ntype": "title",
            "meta": {
                "section": "4.1.3"
            }
        },
        {
            "ix": "453-ARR_v1_48",
            "content": "Besides automatic evaluation, we also conduct human evaluations on Amazon Mechanical Turk to compare the performance of the baselines and our methods. In each task, workers are presented with a prompt along with the completions generated by different methods. Workers are instructed to answer two questions:\"Which one has the best linguistic quality?\" and \"The target attribute is [ATT]. Which one aligns best with the target attribute?\". [ATT] is the control attribute used when generating the completions. In order to evaluate the linguistic quality and the attribute alignment separately, the workers are instructed not to consider the control aspect or the factual errors when answering the first question and not to consider the linguistic quality when answering the second question. The user interface provided to the workers is shown in the appendix (Figure 5). We conduct human evaluations on the results of the sentiment control experiment and those of the AGNews topic control experiment separately. 100 tasks are randomly sampled from the results of each control experiment. Each task is assigned to 3 different Mechanical Turk workers and the annotations are aggregated by majority voting. To ensure data quality, we restrict the workers to be in Canada or United States with a HIT approval rate higher than 95%. In total, 81 workers participated in the human evaluation. For the sentiment control task, we compare the results of GPT2-medium + prompt engineering, PPLM, GeDi, and our supervised method (with full training dataset). For the AGNews topic control task, PPLM is not evaluated as explained above. The results are shown in Table 4. The inter-annotator agreement on the sentiment task and the AGNews task is 0.39 and 0.30 in Fleiss' \u03ba, respectively. Appendix B lists other details of the human evaluation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_49",
            "content": "In the sentiment control task, the result of human evaluation on linguistic quality is generally consistent with the result of automatic evaluation. However, different from the result of the automatic evaluation, annotators are more inclined to select Ours and GPT2 + prompt engineering when evaluating attribute alignment. Although the annotators are instructed not to consider linguistic quality when evaluating sentiment alignment, they tend to select the one with better linguistic quality when multiple completions exhibits equally good attribute alignment. In the AGNews topic control task, the result of human evaluation on attribute alignment is generally consistent with the result of automatic evaluation. However, in more than half of the linguistic quality questions, the annotators select Ours, although GPT2-medium + prompt engineering achieves lower perplexity than Ours. On inspection, we find that GPT2-medium + prompt engineering in this task exhibits a more severe repetition problem compared to that in the sentiment control task. This inconsistency shows the limitation of using automatic evaluations, as alluded to in Welbl et al. (2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_50",
            "content": "Both human evaluation and automatic evaluation show that the linguistic quality of GeDi is inferior to that of the other methods. One possible reason is the length of the prompt. In the original experiment in Krause et al. (2020), each prompt is at least 150 characters for sentiment control evaluation and at least 30 characters for topic control evaluation. However, we use the prompts as in Dathathri et al. (2020), where the average prompt length is 11.8 characters for sentiment control evaluation and 14.5 characters for topic control evaluation. The generated examples are shown in the appendix (Table 7).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_51",
            "content": "Multi-Aspect Control",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "453-ARR_v1_52",
            "content": "Our method can also be applied to multi-aspect control. Directly applying our supervised method training examples with single-aspect labels from multiple aspects, which can be utilized to achieve multi-aspect control. One method is to train a set of prefixes for each aspect separately using our supervised method and then concatenate the prefixes from different aspects for generation. This is denoted as Ours (concatenation) in the result table. Another method is to train the prefixes of multiple aspects simultaneously by considering each single-aspect labeled example as partially labeled. We use a semi-supervised method for training, which is a combination of our supervised method and unsupervised method in Section 3. The model structure is the same as in the unsupervised method (Figure 4). The loss function is as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_53",
            "content": "L = \u03c9 1 L LM + \u03c9 2 L d + \u03c9 3 L enc (11) L enc = \u2212 log q(z sup = y|x) (12) q(z|x) = \u03c3(\u2212 Enc(x) \u2212 H \u03b8 2 )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_54",
            "content": "(13) where the latent variable z is the concatenation of the latent variable of each aspect, including both the supervised aspects and the unsupervised ones z = [z sup ; z uns ]. L enc is used to train the encoder. It is introduced because the partially labeled examples imply the ground truth indexes of the prefixes in the labeled aspect, providing supervision for both the prefix and the encoder. \u03c3 is the softmax function.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_55",
            "content": "We experiment with controlling the following two aspects simultaneously: sentiment and topic. We use the binary sentiment dataset from Amazon review (Zhang et al., 2015) and the DBPedia topic dataset. The prompts used for evaluation are the same as those in the topic control experiment. For each of the 20 prompts, 45 completions are generated. In the GPT2-medium + prompt engineering setting, the guiding sentence follows the template \"This is a [SENTIMENT] review on [TOPIC]:\". In Ours (concatenation), the sentiment prefixes and the topic prefixes are trained separately using our supervised method and then concatenated as multiaspect prefixes. In Ours (semi-supervised), we reuse the prefixes trained in the single-aspect control tasks to initialize H \u03b8 . For example, if the target sentiment is positive and the target topic is an album, the prepended guiding sentence is \"This is a positive review on an album:\". All the experiments are conducted on NVIDIA Tesla V100 GPUs. The hyper-parameters are listed in appendix A.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_56",
            "content": "Experimental results on multi-aspect control (Table 6) show that simply concatenating the prefixes trained for single-aspect control can effectively control the sentiment and topic simultaneously, and our experiments show that the order of the prefixes does not impact the result. On the other hand, training using the combination of our supervised and unsupervised methods can further improve the attribute alignment without sacrificing too much linguistic quality. Same as the observations stated in Section 4.1.2, removing the discriminative loss L d will significantly degrade the attribute relevance, especially the topic relevance. Removing the encoder loss L enc may achieve higher overall attribute relevance at the cost of linguistic quality, indicated by a higher perplexity. We present the generated examples in the appendix (Table 7).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_57",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "453-ARR_v1_58",
            "content": "We propose a novel framework for controllable GPT2 generation with frozen LMs, which utilizes contrastive prefixes to guide generation. Experimental results show that our framework can not only successfully guide generation from a single aspect but also achieve promising results on multiaspect control tasks. Besides the control tasks we experimented with, our proposed framework can be freely applied to other desired attributes. We intend to make our implementation freely available online to facilitate future research and downstream applications.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "453-ARR_v1_59",
            "content": "Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu, Plug and play language models: A simple approach to controlled text generation, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Sumanth Dathathri",
                    "Andrea Madotto",
                    "Janice Lan",
                    "Jane Hung",
                    "Eric Frank",
                    "Piero Molino",
                    "Jason Yosinski",
                    "Rosanne Liu"
                ],
                "title": "Plug and play language models: A simple approach to controlled text generation",
                "pub_date": "2020-04-26",
                "pub_title": "8th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "453-ARR_v1_60",
            "content": "Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston, Wizard of wikipedia: Knowledge-powered conversational agents, 2019-05-06, 7th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Emily Dinan",
                    "Stephen Roller",
                    "Kurt Shuster",
                    "Angela Fan",
                    "Michael Auli",
                    "Jason Weston"
                ],
                "title": "Wizard of wikipedia: Knowledge-powered conversational agents",
                "pub_date": "2019-05-06",
                "pub_title": "7th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "453-ARR_v1_61",
            "content": "UNKNOWN, None, 2017, Controlling linguistic style aspects in neural language generation, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Controlling linguistic style aspects in neural language generation",
                "pub": "CoRR"
            }
        },
        {
            "ix": "453-ARR_v1_62",
            "content": "Suchin Samuel Gehman, Maarten Gururangan, Yejin Sap, Noah Choi,  Smith, Realtoxicityprompts: Evaluating neural toxic degeneration in language models, 2020-11-20, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Suchin Samuel Gehman",
                    "Maarten Gururangan",
                    "Yejin Sap",
                    "Noah Choi",
                    " Smith"
                ],
                "title": "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
                "pub_date": "2020-11-20",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "453-ARR_v1_63",
            "content": "Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi, Kevin Knight, Hafez: an interactive poetry generation system, 2017-07-30, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Marjan Ghazvininejad",
                    "Xing Shi",
                    "Jay Priyadarshi",
                    "Kevin Knight"
                ],
                "title": "Hafez: an interactive poetry generation system",
                "pub_date": "2017-07-30",
                "pub_title": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "453-ARR_v1_64",
            "content": "Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, Yejin Choi, Learning to write with cooperative discriminators, 2018-07-15, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Ari Holtzman",
                    "Jan Buys",
                    "Maxwell Forbes",
                    "Antoine Bosselut",
                    "David Golub",
                    "Yejin Choi"
                ],
                "title": "Learning to write with cooperative discriminators",
                "pub_date": "2018-07-15",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "453-ARR_v1_65",
            "content": "Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric Xing, Toward controlled generation of text, 2017-08-11, Proceedings of the 34th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Zhiting Hu",
                    "Zichao Yang",
                    "Xiaodan Liang",
                    "Ruslan Salakhutdinov",
                    "Eric Xing"
                ],
                "title": "Toward controlled generation of text",
                "pub_date": "2017-08-11",
                "pub_title": "Proceedings of the 34th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "453-ARR_v1_66",
            "content": "Eric Jang, Shixiang Gu, Ben Poole, Categorical reparameterization with gumbel-softmax, 2017-04-24, 5th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Eric Jang",
                    "Shixiang Gu",
                    "Ben Poole"
                ],
                "title": "Categorical reparameterization with gumbel-softmax",
                "pub_date": "2017-04-24",
                "pub_title": "5th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "453-ARR_v1_67",
            "content": "UNKNOWN, None, 2019, CTRL: A conditional transformer language model for controllable generation, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "CTRL: A conditional transformer language model for controllable generation",
                "pub": "CoRR"
            }
        },
        {
            "ix": "453-ARR_v1_68",
            "content": "UNKNOWN, None, 2009, Gedi: Generative discriminator guided sequence generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": null,
                "title": null,
                "pub_date": "2009",
                "pub_title": "Gedi: Generative discriminator guided sequence generation",
                "pub": null
            }
        },
        {
            "ix": "453-ARR_v1_69",
            "content": "Lisa Xiang, Percy Li,  Liang, Prefix-tuning: Optimizing continuous prompts for generation, 2021-08-01, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Lisa Xiang",
                    "Percy Li",
                    " Liang"
                ],
                "title": "Prefix-tuning: Optimizing continuous prompts for generation",
                "pub_date": "2021-08-01",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "453-ARR_v1_70",
            "content": "UNKNOWN, None, 2021, Plug-and-blend: A framework for controllable story generation with blended control codes, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Plug-and-blend: A framework for controllable story generation with blended control codes",
                "pub": null
            }
        },
        {
            "ix": "453-ARR_v1_71",
            "content": "UNKNOWN, None, 1907, Roberta: A robustly optimized BERT pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": null,
                "title": null,
                "pub_date": "1907",
                "pub_title": "Roberta: A robustly optimized BERT pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "453-ARR_v1_72",
            "content": "Andrew Maas, Raymond Daly, Peter Pham, Dan Huang, Andrew Ng, Christopher Potts, Learning word vectors for sentiment analysis, 2011-06-24, The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Andrew Maas",
                    "Raymond Daly",
                    "Peter Pham",
                    "Dan Huang",
                    "Andrew Ng",
                    "Christopher Potts"
                ],
                "title": "Learning word vectors for sentiment analysis",
                "pub_date": "2011-06-24",
                "pub_title": "The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",
                "pub": null
            }
        },
        {
            "ix": "453-ARR_v1_73",
            "content": "Chris Maddison, Andriy Mnih, Yee Whye Teh, The concrete distribution: A continuous relaxation of discrete random variables, 2017-04-24, 5th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Chris Maddison",
                    "Andriy Mnih",
                    "Yee Whye Teh"
                ],
                "title": "The concrete distribution: A continuous relaxation of discrete random variables",
                "pub_date": "2017-04-24",
                "pub_title": "5th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "453-ARR_v1_74",
            "content": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Language models are unsupervised multitask learners, 2019, OpenAI blog, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Alec Radford",
                    "Jeffrey Wu",
                    "Rewon Child",
                    "David Luan",
                    "Dario Amodei",
                    "Ilya Sutskever"
                ],
                "title": "Language models are unsupervised multitask learners",
                "pub_date": "2019",
                "pub_title": "OpenAI blog",
                "pub": null
            }
        },
        {
            "ix": "453-ARR_v1_75",
            "content": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, Zero-shot text-to-image generation, 2021-07-24, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Aditya Ramesh",
                    "Mikhail Pavlov",
                    "Gabriel Goh",
                    "Scott Gray",
                    "Chelsea Voss",
                    "Alec Radford",
                    "Mark Chen",
                    "Ilya Sutskever"
                ],
                "title": "Zero-shot text-to-image generation",
                "pub_date": "2021-07-24",
                "pub_title": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021",
                "pub": "PMLR"
            }
        },
        {
            "ix": "453-ARR_v1_76",
            "content": "Ben Casper Kaae S\u00f8nderby, Andriy Poole,  Mnih, Continuous relaxation training of discrete latent variable image models, 2017, Beysian DeepLearning workshop, NIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Ben Casper Kaae S\u00f8nderby",
                    "Andriy Poole",
                    " Mnih"
                ],
                "title": "Continuous relaxation training of discrete latent variable image models",
                "pub_date": "2017",
                "pub_title": "Beysian DeepLearning workshop, NIPS",
                "pub": null
            }
        },
        {
            "ix": "453-ARR_v1_77",
            "content": "A\u00e4ron Van Den Oord, Oriol Vinyals, Koray Kavukcuoglu, Neural discrete representation learning, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "A\u00e4ron Van Den Oord",
                    "Oriol Vinyals",
                    "Koray Kavukcuoglu"
                ],
                "title": "Neural discrete representation learning",
                "pub_date": "2017-12-04",
                "pub_title": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "453-ARR_v1_78",
            "content": "Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. 2021. Challenges in detoxifying language models, , Findings of the Association for Computational Linguistics: EMNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Johannes Welbl",
                    "Amelia Glaese",
                    "Jonathan Uesato",
                    "Sumanth Dathathri",
                    "John Mellor"
                ],
                "title": "Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. 2021. Challenges in detoxifying language models",
                "pub_date": null,
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "453-ARR_v1_79",
            "content": "Kevin Yang, Dan Klein, FUDGE: controlled text generation with future discriminators, 2021-06-06, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Kevin Yang",
                    "Dan Klein"
                ],
                "title": "FUDGE: controlled text generation with future discriminators",
                "pub_date": "2021-06-06",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021",
                "pub": null
            }
        },
        {
            "ix": "453-ARR_v1_80",
            "content": "UNKNOWN, None, 2021, Attribute alignment: Controlling text generation from pretrained language models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Attribute alignment: Controlling text generation from pretrained language models",
                "pub": null
            }
        },
        {
            "ix": "453-ARR_v1_81",
            "content": "Xiang Zhang, Junbo Zhao, Yann Lecun, Character-level convolutional networks for text classification, 2015-12-07, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Xiang Zhang",
                    "Junbo Zhao",
                    "Yann Lecun"
                ],
                "title": "Character-level convolutional networks for text classification",
                "pub_date": "2015-12-07",
                "pub_title": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "453-ARR_v1_0@0",
            "content": "Controllable Natural Language Generation with Contrastive Prefixes",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_0",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_2@0",
            "content": "To guide the generation of large pretrained language models (LM), previous work has focused on directly fine-tuning the language model or utilizing an attribute discriminator.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_2",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_2@1",
            "content": "In this work, we propose a novel lightweight framework for controllable GPT2 (Radford et al., 2019) generation, which utilizes a set of small attribute-specific vectors, called prefixes (Li and Liang, 2021), to steer natural language generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_2",
            "start": 176,
            "end": 420,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_2@2",
            "content": "Different from Li and Liang (2021), where each prefix is trained independently, we take the relationship among prefixes into consideration and train multiple prefixes simultaneously, as illustrated in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_2",
            "start": 422,
            "end": 631,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_2@3",
            "content": "We propose a novel supervised method and also an unsupervised method to train the prefixes for single-aspect control while the combination of these two methods can achieve multi-aspect control.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_2",
            "start": 633,
            "end": 825,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_2@4",
            "content": "Experimental results on both singleaspect and multi-aspect control show that our methods can guide generation towards the desired attributes while keeping high linguistic quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_2",
            "start": 827,
            "end": 1005,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_4@0",
            "content": "The goal of controllable Natural Language Generation (NLG) is to guide generation towards the desired attributes in the concerned aspects of the text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_4",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_4@1",
            "content": "For example, the aspect can be topic or sentiment, and sentiment may have two attributes: positive and negative.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_4",
            "start": 151,
            "end": 262,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_4@2",
            "content": "Previous work has focused on directly fine-tuning the existing models (Keskar et al., 2019;Hu et al., 2017;Ficler and Goldberg, 2017) or using a discriminator to guide generation (Dathathri et al., 2020;Krause et al., 2020;Holtzman et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_4",
            "start": 264,
            "end": 509,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_4@3",
            "content": "CTRL (Keskar et al., 2019) achieves controllability at the expense of training a large conditional LM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_4",
            "start": 511,
            "end": 612,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_4@4",
            "content": "GeDi (Krause et al., 2020) also trains conditional LMs but uses them as discriminators to guide generation, introducing additional 345M parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_4",
            "start": 614,
            "end": 760,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_4@5",
            "content": "Besides, GeDi focuses on single-aspect control, ignoring the need for multi-aspect control.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_4",
            "start": 762,
            "end": 852,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_4@6",
            "content": "PPLM (Dathathri et al., 2020) guides generation by iteratively updating the LM's hidden activations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_4",
            "start": 854,
            "end": 953,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_4@7",
            "content": "However, this decoding strategy is extremely computationally intensive, resulting in a slow generation speed (Gehman et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_4",
            "start": 955,
            "end": 1085,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_4@8",
            "content": "Prefix-tuning (Li and Liang, 2021) proposes to optimize a prefix, which is a small continuous taskspecific vector, as a lightweight alternative to finetuning an NLG task, such as table-to-text generation or summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_4",
            "start": 1087,
            "end": 1307,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_4@9",
            "content": "Inspired by Li and Liang (2021), we propose to use prefixes, a set of small continuous attribute-specific vectors, to steer NLG.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_4",
            "start": 1309,
            "end": 1436,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_4@10",
            "content": "Compared with using an attribute model or a generative discriminator (Dathathri et al., 2020;Krause et al., 2020), using learned prefixes to achieve controllability has the following benefits.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_4",
            "start": 1438,
            "end": 1629,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_4@11",
            "content": "First, it introduces fewer additional parameters (~0.2%-2% of GPT2 parameters in our experiments).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_4",
            "start": 1631,
            "end": 1728,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_4@12",
            "content": "Second, using prefixes keeps the inference speed comparable to that of the original GPT2 model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_4",
            "start": 1730,
            "end": 1824,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_5@0",
            "content": "In a general sense, prefix-tuning (Li and Liang, 2021) can be considered as controlling the generation of language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_5",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_5@1",
            "content": "Prefix-tuning views each prefix as an independent control task thus trains each prefix separately (top in Figure 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_5",
            "start": 123,
            "end": 238,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_5@2",
            "content": "However, one aspect of controllability in NLG involves multiple attributes, which might have a relationship with each other.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_5",
            "start": 240,
            "end": 363,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_5@3",
            "content": "For example, the sentiment aspect usually has two attributes: positive and negative, which are in opposition to each other.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_5",
            "start": 365,
            "end": 487,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_5@4",
            "content": "We think that this opposite relationship can be helpful to improve the controllability of a prefix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_5",
            "start": 489,
            "end": 587,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_5@5",
            "content": "Therefore, we propose a novel supervised method and a novel unsupervised one in our framework, which takes the relationship among prefixes into consideration and trains multiple prefixes simultaneously with novel training objectives, as illustrated in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_5",
            "start": 589,
            "end": 849,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_6@0",
            "content": "Experimental results on the single-aspect control tasks (sentiment control, detoxification, and topic control) show that our proposed methods can guide generation towards the target attribute while keeping high linguistic quality, even when only several dozen labeled examples are available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_6",
            "start": 0,
            "end": 290,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_6@1",
            "content": "In addition to single-aspect control, multi-aspect control can be achieved by combining the proposed supervised method with the unsupervised method in our framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_6",
            "start": 292,
            "end": 456,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_6@2",
            "content": "Experimental results on the sentiment and topic control show that the prefixes trained with our method can successfully control these two aspects simultaneously.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_6",
            "start": 458,
            "end": 618,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_7@0",
            "content": "Our main contributions are as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_7",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_8@0",
            "content": "\u2022 We propose a novel framework that utilizes prefixes with frozen LMs as a lightweight alternative for controllable GPT2 generation. \u2022 We propose a supervised method and an unsupervised method with novel objectives for prefix training, where the relationship among prefixes are considered and multiple prefixes are trained simultaneously. \u2022 This work provides a unified perspective for single-aspect control and multi-aspect control.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_8",
            "start": 0,
            "end": 432,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_9@0",
            "content": "Experimental results show that our methods can effectively guide generation in both single-aspect control and multi-aspect control.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_9",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_10@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_10",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_11@0",
            "content": "Ficler and Goldberg (2017) control the stylistic aspects of the generated text with a conditioned RNN (Recurrent Neural Network) LM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_11",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_11@1",
            "content": "Holtzman et al. (2018) compose a committee of discriminators to guide an RNN generator towards the generations with the desired linguistic quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_11",
            "start": 133,
            "end": 279,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_11@2",
            "content": "Hu et al. (2017) aim at controlling the sentiment and tense of the generated text by combining variational autoencoders (VAE) and attribute discriminators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_11",
            "start": 281,
            "end": 435,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_11@3",
            "content": "Controlling these attributes of text generation has manifold applications, such as knowledge-grounded conversation (Dinan et al., 2019) and poetry generation (Ghazvininejad et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_11",
            "start": 437,
            "end": 623,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_11@4",
            "content": "More recently, with the advent of Transformers and large pretrained language models, such as GPT2, an extensive body of work has focused on controlling the generation of these Transformerbased models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_11",
            "start": 625,
            "end": 824,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_11@5",
            "content": "Keskar et al. (2019) train a 1.63 billion-parameter conditional transformer LM from scratch with 55 attribute control codes to guide generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_11",
            "start": 826,
            "end": 969,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_11@6",
            "content": "However, this method is expensive and lacks flexibility since the control codes are fixed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_11",
            "start": 971,
            "end": 1060,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_11@7",
            "content": "Dathathri et al. (2020) address these limitations by developing a plug-and-play model which leverages an attribute discriminator to perturb the LM's hidden activations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_11",
            "start": 1062,
            "end": 1229,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_11@8",
            "content": "However, updating gradients at the token level results in slow inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_11",
            "start": 1231,
            "end": 1303,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_11@9",
            "content": "Instead of updating the hidden activations, Krause et al. (2020); Yang and Klein (2021); Lin and Riedl (2021) introduce generative discriminators to re-weight the next token distributions on the fly during inference, thus improving the inference speed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_11",
            "start": 1305,
            "end": 1556,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_12@0",
            "content": "Our work is mostly related to Yu et al. (2021); Li and Liang (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_12",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_12@1",
            "content": "Yu et al. (2021) use a pretrained LM followed by an attribute alignment function to encode the tokens of the target attributes and the resulting hidden states are used to control generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_12",
            "start": 69,
            "end": 258,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_12@2",
            "content": "Different from their work, we do not take the tokens of the target attributes as input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_12",
            "start": 260,
            "end": 346,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_12@3",
            "content": "Instead, we directly train a set of parameters, which acts as the prepended hidden states of GPT2, to control generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_12",
            "start": 348,
            "end": 468,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_12@4",
            "content": "Avoiding using attribute tokens can circumvent the problems when it is difficult to describe the desired attribute with only one word.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_12",
            "start": 470,
            "end": 603,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_12@5",
            "content": "Besides, Yu et al. (2021) focus on attributes disentanglement, which is not a focus in our work, so our training methods are different.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_12",
            "start": 605,
            "end": 739,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_12@6",
            "content": "Prefix-tuning (Li and Liang, 2021) can, in a general sense, be viewed as controlling the generation of LMs, where the LM is controlled to depict a specific NLG task, while in this work, the LM is controlled to carry specific attributes in a generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_12",
            "start": 741,
            "end": 992,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_12@7",
            "content": "Besides, our proposed methods for prefix training are different from Li and Liang (2021), as stated in Section 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_12",
            "start": 994,
            "end": 1106,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_13@0",
            "content": "Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_13",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_14@0",
            "content": "Our method uses prefixes to guide GPT2 generation, where a prefix is a continuous attributespecific vector prepended to the activations of the GPT2 model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_14",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_14@1",
            "content": "Prefixes are free parameters denoted as H \u03b8 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_14",
            "start": 155,
            "end": 199,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_14@2",
            "content": "Different from Li and Liang (2021), where each prefix is trained independently, we consider the relationship among attributes and train multiple prefixes simultaneously, so H \u03b8 is of dimension N \u00d7 M \u00d7 D, where N is the number of prefixes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_14",
            "start": 201,
            "end": 438,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_14@3",
            "content": "In single-aspect control, N equals the number of attributes in the concerned aspect while in multiaspect control, N equals the product of the number of attributes in each aspect.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_14",
            "start": 440,
            "end": 617,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_14@4",
            "content": "M is the length of a prefix, and D is the dimension of the activation in GPT2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_14",
            "start": 619,
            "end": 696,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_14@5",
            "content": "Following Li and Liang (2021), we reparametrize H \u03b8 [i, j, :] = W i H \u03b8 [i, j, :] by a smaller parameter (H \u03b8 ) composed with a large matrix (W i ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_14",
            "start": 698,
            "end": 845,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_14@6",
            "content": "After the training finishes, only H \u03b8 needs to be saved for generation while W and H \u03b8 can be discarded.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_14",
            "start": 847,
            "end": 950,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_14@7",
            "content": "Since the GPT2 parameters are kept frozen during training, they do not need to be saved either.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_14",
            "start": 952,
            "end": 1046,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_14@8",
            "content": "Figure 2 shows an example of the generation process under the control of a trained prefix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_14",
            "start": 1048,
            "end": 1137,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_14@9",
            "content": "The prefixes can be trained in a supervised, semi-supervised, or unsupervised way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_14",
            "start": 1139,
            "end": 1220,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_14@10",
            "content": "Since the semi-supervised method is a combination of the supervised and the unsupervised method, we introduce the supervised and the unsupervised method in this section.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_14",
            "start": 1222,
            "end": 1390,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_14@11",
            "content": "For clarity, we introduce these methods under the single-aspect control setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_14",
            "start": 1392,
            "end": 1471,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_15@0",
            "content": "Supervised Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_15",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_16@0",
            "content": "Suppose the concerned aspect has the attribute set Y , each training example is a pair of (x, y) where x is the input text and y \u2208 Y is the attribute label of x.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_16",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_16@1",
            "content": "Note that the attribute label also indicates the ground truth index of the prefix in H \u03b8 , so y also refers to the prefix index in the following description.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_16",
            "start": 162,
            "end": 318,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_16@2",
            "content": "As mentioned in Section 1, we introduce an additional discriminative loss to train multiple prefixes simultaneously.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_16",
            "start": 320,
            "end": 435,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_16@3",
            "content": "Therefore, the training loss L sup is a weighted sum of the language model loss L LM and the discriminative loss L d :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_16",
            "start": 437,
            "end": 554,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_17@0",
            "content": "L sup = \u03c9 1 L LM + \u03c9 2 L d(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_17",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_18@0",
            "content": "L LM = \u2212 T t=1 log p(x t |x <t , y)(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_18",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_19@0",
            "content": "L d = \u2212 log p(y)p(x|y) y \u2208Y p(y )p(x|y )(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_19",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_20@0",
            "content": "The computation of log p(x t |x <t , y) is parameterized as log p \u03b8,\u03b3 (x t |x <t , H \u03b8 [y, :, :]), where \u03b3 is the set of fixed GPT2 parameters, and \u03b8 represents learnable prefix parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_20",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_20@1",
            "content": "log p(x|y) = over t.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_20",
            "start": 190,
            "end": 209,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_21@0",
            "content": "Note that each prefix can be trained independently using L LM alone, which would be the same as prefix-tuning (Li and Liang, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_21",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_21@1",
            "content": "Intuitively, prefixes trained by L LM are infused with the information of what is encouraged to generate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_21",
            "start": 132,
            "end": 236,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_21@2",
            "content": "However, we observe that in controllable NLG, it is helpful to also infuse a prefix with the information of what is discouraged to generate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_21",
            "start": 238,
            "end": 377,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_21@3",
            "content": "Given a training example (x, y), the prefix H \u03b8 [y, :, :] should be optimized towards generating x, while the other prefixes should be discouraged to generate x. To achieve this goal, all the prefixes in H \u03b8 should be trained simultaneously.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_21",
            "start": 379,
            "end": 619,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_21@4",
            "content": "Therefore, the discriminative loss L d is introduced.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_21",
            "start": 621,
            "end": 673,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_21@5",
            "content": "As in equation 3, optimizing L d improves the attribute alignment p(y|x) by increasing p(x|y) and lowering p(x|\u0233), \u0233 \u2208 Y \\{y} at the same time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_21",
            "start": 675,
            "end": 817,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_21@6",
            "content": "We assume uniform prior, so p(y) and p(y ) can be canceled out in Equation 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_21",
            "start": 819,
            "end": 895,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_21@7",
            "content": "Figure 3 illustrates the training process with two prefixes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_21",
            "start": 897,
            "end": 956,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_22@0",
            "content": "Unsupervised Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_22",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_23@0",
            "content": "In the unsupervised setting, we assume the attribute set Y of the concerned aspect is known.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_23",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_23@1",
            "content": "The training example consists of input text x only.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_23",
            "start": 93,
            "end": 143,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_23@2",
            "content": "The attribute label y is no longer available and thus the index of the prefix associated with x is unknown.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_23",
            "start": 145,
            "end": 251,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_23@3",
            "content": "Inspired by VQ-VAE (van den Oord et al., 2017), we consider the index of the prefix as a latent variable z.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_23",
            "start": 253,
            "end": 359,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_23@4",
            "content": "We take the backbone model in the above supervised method as the decoder and introduce an encoder to parameterize the categorical distribution q(z|x).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_23",
            "start": 361,
            "end": 510,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_23@5",
            "content": "According to q(z|x), a prefix index z is sampled and the prefix H \u03b8 [z, :, :] is then fed into the decoder to reconstruct the input text x.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_23",
            "start": 512,
            "end": 650,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_23@6",
            "content": "Since the sampling process of the prefixes is non-differentiable, we use Gumbel-Softmax (GS) relaxation (Jang et al., 2017;Maddison et al., 2017) following S\u00f8nderby et al. ( 2017); Ramesh et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_23",
            "start": 652,
            "end": 853,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_23@7",
            "content": "Formally, q(z|x) is computed as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_23",
            "start": 855,
            "end": 894,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_24@0",
            "content": "q(z|x) = GS(\u2212 Enc(x) \u2212 H \u03b8 2 , \u03c4 ) (4) where \u03c4 is the temperature of Gumbel-Softmax, and Enc is the encoder function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_24",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_24@1",
            "content": "To train the prefixes, the loss function is a weighted sum of the three loss terms:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_24",
            "start": 118,
            "end": 200,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_25@0",
            "content": "L uns = \u03c9 1 L LM + \u03c9 2 L KL + \u03c9 3 L c (5) L LM = \u2212 T t=1 log p(x t |x <t , z)(6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_25",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_26@0",
            "content": "L KL = KL[q(z|x)||p(z)](7",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_26",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_27@0",
            "content": ") where L LM is the language model loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_27",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_27@1",
            "content": "Similar as that in the supervised method, the computation of log p(x t |x <t , z) is parameterized as log p \u03b8,\u03b3 (x t |x <t , H \u03b8 [z, :, :]).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_27",
            "start": 41,
            "end": 180,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_27@2",
            "content": "L KL is the Kullback-Leibler divergence, where we assume the prior p(z) to be uniform.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_27",
            "start": 182,
            "end": 267,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_27@3",
            "content": "Note that these two terms constitute the loss function of VAE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_27",
            "start": 269,
            "end": 330,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_27@4",
            "content": "Optimizing these two loss terms improves the Evidence Lower BOund (ELBO) of log p(x).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_27",
            "start": 332,
            "end": 416,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_27@5",
            "content": "Similar to the intuition behind L d in the supervised method, if the ground truth prefix for x is H \u03b8 [y, :, :], then the other prefixes should be discouraged to generate x. However, L d requires the ground truth attribute label y for computation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_27",
            "start": 418,
            "end": 664,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_27@6",
            "content": "Instead, we introduce an unsupervised contrastive loss L c during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_27",
            "start": 666,
            "end": 740,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_28@0",
            "content": "L c = max(m \u2212 p(z|x) \u2212 p(z|x) 2 , 0) 2 (8) where m is a pre-set margin and z is another latent variable indicating the index of the opposite prefix of x. q(z|x) is computed as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_28",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_29@0",
            "content": "q(z|x) = GS( Enc(x) \u2212 H \u03b8 2 , \u03c4 )(9",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_29",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_30@0",
            "content": ") L c is aimed at increasing the attribute alignment by pushing p(z|x) away from p(z|x) by a margin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_30",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_30@1",
            "content": "The computation of p(z|x) is as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_30",
            "start": 101,
            "end": 140,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_31@0",
            "content": "p(z|x) = p(z)p(x|z) z \u2208Y p(z )p(x|z )(10)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_31",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_32@0",
            "content": "We assume uniform prior, so p(z) and p(z ) can be canceled out.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_32",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_32@1",
            "content": "Similar as the parameterization of log p(x|y) in the supervised method, the parameterization of log p(x|z) is the sum of log p \u03b8,\u03b3 (x t |x <t , H \u03b8 [z, :, :]) over t. The training process is illustrated in Figure 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_32",
            "start": 64,
            "end": 278,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_33@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_33",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_34@0",
            "content": "We experiment with three tasks: sentiment control, detoxification, and topic control.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_34",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_34@1",
            "content": "We compare our method to GPT2, PPLM, and GeDi.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_34",
            "start": 86,
            "end": 131,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_34@2",
            "content": "We experiment with GPT2-medium (345M parameters) for all the methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_34",
            "start": 133,
            "end": 201,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_34@3",
            "content": "We use the original implementation of PPLM and GeDi released by Dathathri et al. (2020) and Krause et al. (2020), and the hyperparameters are set to the reported value in the original paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_34",
            "start": 203,
            "end": 392,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_34@4",
            "content": "The detailed hyperparameters in each task are listed in appendix A. For the GPT2 model, we do experiments under two settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_34",
            "start": 394,
            "end": 518,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_34@5",
            "content": "First, the GPT2 model generates completions of each prompt in the evaluation dataset, which is denoted as GPT2medium.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_34",
            "start": 520,
            "end": 636,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_34@6",
            "content": "Second, GPT2-medium + prompt engineering prepends a guiding sentence to each testing prompt and then generates completions of each augmented prompt.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_34",
            "start": 638,
            "end": 785,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_34@7",
            "content": "We evaluate the linguistic quality and attribute alignment of the generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_34",
            "start": 787,
            "end": 863,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_34@8",
            "content": "The linguistic quality is evaluated using the perplexity calculated by GPT2-large (774M parameters).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_34",
            "start": 865,
            "end": 964,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_35@0",
            "content": "To evaluate the robustness of our supervised method with the size of the training dataset, we experiment with the following three different settings: 1) using the complete training dataset; 2) using 1,000 examples per attribute for training; 3) using 24 examples per attribute for training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_35",
            "start": 0,
            "end": 289,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_35@1",
            "content": "We evaluate our unsupervised method on the sentiment control task and the detoxification task, which are binary tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_35",
            "start": 291,
            "end": 408,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_35@2",
            "content": "Note that different from the supervised method, our unsupervised method does not use any attribute labels, so the order of the attributes in the trained prefixes is undetermined.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_35",
            "start": 410,
            "end": 587,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_35@3",
            "content": "After the prefixes finish training using the unsupervised method, we manually check the order of the attributes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_35",
            "start": 589,
            "end": 700,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_36@0",
            "content": "Single-Aspect Control",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_36",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_37@0",
            "content": "Tasks",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_37",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_38@0",
            "content": "Sentiment Control Same as GeDi, we use IMDb movie reviews (Maas et al., 2011) to train our model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_38",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_38@1",
            "content": "The number of prefixes is 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_38",
            "start": 98,
            "end": 125,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_38@2",
            "content": "Note that GeDi only uses 11.25k examples from the dataset for training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_38",
            "start": 127,
            "end": 197,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_38@3",
            "content": "To be a fair comparison, we randomly sample 11.25k examples from the dataset to train our model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_38",
            "start": 199,
            "end": 294,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_38@4",
            "content": "To evaluate the sentiment alignment of the generated text, we finetune a RoBERTa (Liu et al., 2019) classifier using the Yelp Review dataset (Zhang et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_38",
            "start": 296,
            "end": 457,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_38@5",
            "content": "The prompts used for evaluation are the same as those in the PPLM experiment (Dathathri et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_38",
            "start": 459,
            "end": 560,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_38@6",
            "content": "For each of the 15 prompts, 45 completions are gen-erated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_38",
            "start": 562,
            "end": 619,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_38@7",
            "content": "In the GPT2-medium + prompt engineering setting, we prepend each prompt with the guiding sentence \"This is a negative review:\" for negative sentiment control, and similarly, we prepend each prompt with \"This is a positive review:\" for positive sentiment control.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_38",
            "start": 621,
            "end": 882,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_39@0",
            "content": "Detoxification We use Jigsaw Toxic Comment Classification Challenge Dataset 1 to train our model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_39",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_39@1",
            "content": "The number of prefixes is 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_39",
            "start": 98,
            "end": 125,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_39@2",
            "content": "Google Perspective API 2 is used for toxicity evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_39",
            "start": 127,
            "end": 183,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_39@3",
            "content": "The testing prompts are collected from RealTox-icityPrompts (Gehman et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_39",
            "start": 185,
            "end": 266,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_39@4",
            "content": "We use the prompts categorized as \"challenging\" in the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_39",
            "start": 268,
            "end": 330,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_39@5",
            "content": "We further filter out the prompts with toxicity larger than 0.5, scored by Perspective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_39",
            "start": 332,
            "end": 418,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_39@6",
            "content": "The resulted evaluation dataset consists of 203 prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_39",
            "start": 420,
            "end": 475,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_39@7",
            "content": "For each of these prompts, 20 completions are generated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_39",
            "start": 477,
            "end": 532,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_39@8",
            "content": "In the GPT2-medium + prompt engineering setting, we prepend each prompt with the guiding sentence \"This is a non-toxic comment:\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_39",
            "start": 534,
            "end": 662,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_40@0",
            "content": "We experiment with the AGNews dataset and DBPedia dataset (Zhang et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_40",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_40@1",
            "content": "The number of prefixes is 4 and 14, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_40",
            "start": 80,
            "end": 128,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_40@2",
            "content": "The prompts used for evaluation are the same as those in the PPLM experiment (Dathathri et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_40",
            "start": 130,
            "end": 231,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_40@3",
            "content": "For each of the 20 prompts, 45 completions are generated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_40",
            "start": 233,
            "end": 289,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_40@4",
            "content": "Same as that in GeDi, we split each of the original training datasets in half.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_40",
            "start": 291,
            "end": 368,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_40@5",
            "content": "One half is used to train prefixes, while the other half is used to train a RoBERTa topic classifier for topic relevance evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_40",
            "start": 370,
            "end": 501,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_40@6",
            "content": "In the GPT2-medium + prompt engineering setting, the guiding sentence follows the template \"The following is about [TOPIC]\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_40",
            "start": 503,
            "end": 626,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_40@7",
            "content": "We do not compare with PPLM in the topic control task since PPLM uses a bag-of-words attribute model to do topic control, where the 7 predefined topics are different from the topics in the AGNews dataset or the DBPedia dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_40",
            "start": 628,
            "end": 854,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_41@0",
            "content": "All the experiments are conducted on NVIDIA Tesla V100 GPUs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_41",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_41@1",
            "content": "The detailed hyper-parameters for each experiment are listed in appendix A.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_41",
            "start": 61,
            "end": 135,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_42@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_42",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_43@0",
            "content": "In the unsupervised setting, GPT2-medium + prompt engineering shows controllability on sentiment control (Table 1) and topic control (Table 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_43",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_43@1",
            "content": "However, this method does not work on the detoxification task ( significantly lowers the toxicity on the detoxification task and the ablation study shows that the contrastive loss L c is crucial.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_43",
            "start": 144,
            "end": 338,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_43@2",
            "content": "On the sentiment control task, our unsupervised method does not achieve good attribute alignment when the target sentiment is negative, but it performs well when the target sentiment is positive.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_43",
            "start": 340,
            "end": 534,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_43@3",
            "content": "One possible reason is that compared with the differences between toxic and normal sentences, the difference between positive sentiment and negative sentiment is more subtle, so it is more challenging for the GPT2 encoder in our unsupervised model to accurately separate the unlabeled data into two sentiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_43",
            "start": 536,
            "end": 845,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_43@4",
            "content": "As a result, the encoder's implicit criterion to categorize the input text may not be exactly the sentiment, which is also the reason that after removing the contrastive loss L c in the unsupervised loss function, the attribute relevance on the negative sentiment is higher while that on the positive sentiment is lower.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_43",
            "start": 847,
            "end": 1166,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_44@0",
            "content": "In the supervised setting with full data, our supervised method consistently achieves better controllability than PPLM while maintaining the linguistic quality of the generations (Table 1, 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_44",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_44@1",
            "content": "Although GeDi achieves a high attribute alignment score on the three tasks, it severely sacrifices the linguistic quality, as indicated by the high perplexity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_44",
            "start": 193,
            "end": 351,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_44@2",
            "content": "In the few-shot setting, where the number of labeled training examples is reduced to 1000 or 24 examples per attribute, our supervised method can still maintain good controllability on the three tasks, showing the robustness of our method to the size of the training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_44",
            "start": 353,
            "end": 624,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_45@0",
            "content": "Ablation study shows the importance of the discriminative loss L d in our supervised method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_45",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_45@1",
            "content": "As mentioned in section 3, training without L d is equivalent to prefix-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_45",
            "start": 93,
            "end": 171,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_45@2",
            "content": "Comparing the results of Ours\u2212L d and GPT2-medium show that directly using prefix-tuning can achieve controllability on the sentiment or the topic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_45",
            "start": 173,
            "end": 319,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_45@3",
            "content": "However, it is less effective on detoxification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_45",
            "start": 321,
            "end": 368,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_45@4",
            "content": "The reason is that different from topic control or sentiment control, detoxification requires the model to avoid generating some words or phrases according to the context, which can not be achieved by prefix-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_45",
            "start": 370,
            "end": 584,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_45@5",
            "content": "L d fills this gap by increasing p(x|y) and lowering p(x|\u0233) at the same time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_45",
            "start": 586,
            "end": 662,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_45@6",
            "content": "Therefore, incorporating L d is of critical importance to the detoxification task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_45",
            "start": 664,
            "end": 745,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_45@7",
            "content": "In the DBPedia topic control task, adding L d also achieves a large improvement on attribute alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_45",
            "start": 747,
            "end": 849,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_45@8",
            "content": "The number of attributes in this task is much larger than that in the other tasks, so incorporating L d can effectively push the prefixes to capture the unique features of each topic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_45",
            "start": 851,
            "end": 1033,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_46@0",
            "content": "We compare the average inference speed of our methods with the baselines (Table 5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_46",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_46@1",
            "content": "The inference speed of PPLM is several dozen times slower than that of the original GPT2 model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_46",
            "start": 84,
            "end": 178,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_46@2",
            "content": "GeDi's inference speed is much faster than that of PPLM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_46",
            "start": 180,
            "end": 235,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_46@3",
            "content": "The inference speed of our method is the closest to that of the original GPT2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_46",
            "start": 237,
            "end": 314,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_47@0",
            "content": "Human Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_47",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_48@0",
            "content": "Besides automatic evaluation, we also conduct human evaluations on Amazon Mechanical Turk to compare the performance of the baselines and our methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_48",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_48@1",
            "content": "In each task, workers are presented with a prompt along with the completions generated by different methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_48",
            "start": 151,
            "end": 258,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_48@2",
            "content": "Workers are instructed to answer two questions:\"Which one has the best linguistic quality?\" and \"The target attribute is [ATT]. Which one aligns best with the target attribute?\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_48",
            "start": 260,
            "end": 437,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_48@3",
            "content": "[ATT] is the control attribute used when generating the completions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_48",
            "start": 439,
            "end": 506,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_48@4",
            "content": "In order to evaluate the linguistic quality and the attribute alignment separately, the workers are instructed not to consider the control aspect or the factual errors when answering the first question and not to consider the linguistic quality when answering the second question.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_48",
            "start": 508,
            "end": 787,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_48@5",
            "content": "The user interface provided to the workers is shown in the appendix (Figure 5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_48",
            "start": 789,
            "end": 867,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_48@6",
            "content": "We conduct human evaluations on the results of the sentiment control experiment and those of the AGNews topic control experiment separately.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_48",
            "start": 869,
            "end": 1008,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_48@7",
            "content": "100 tasks are randomly sampled from the results of each control experiment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_48",
            "start": 1010,
            "end": 1084,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_48@8",
            "content": "Each task is assigned to 3 different Mechanical Turk workers and the annotations are aggregated by majority voting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_48",
            "start": 1086,
            "end": 1200,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_48@9",
            "content": "To ensure data quality, we restrict the workers to be in Canada or United States with a HIT approval rate higher than 95%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_48",
            "start": 1202,
            "end": 1323,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_48@10",
            "content": "In total, 81 workers participated in the human evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_48",
            "start": 1325,
            "end": 1382,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_48@11",
            "content": "For the sentiment control task, we compare the results of GPT2-medium + prompt engineering, PPLM, GeDi, and our supervised method (with full training dataset).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_48",
            "start": 1384,
            "end": 1542,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_48@12",
            "content": "For the AGNews topic control task, PPLM is not evaluated as explained above.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_48",
            "start": 1544,
            "end": 1619,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_48@13",
            "content": "The results are shown in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_48",
            "start": 1621,
            "end": 1653,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_48@14",
            "content": "The inter-annotator agreement on the sentiment task and the AGNews task is 0.39 and 0.30 in Fleiss' \u03ba, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_48",
            "start": 1655,
            "end": 1770,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_48@15",
            "content": "Appendix B lists other details of the human evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_48",
            "start": 1772,
            "end": 1826,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_49@0",
            "content": "In the sentiment control task, the result of human evaluation on linguistic quality is generally consistent with the result of automatic evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_49",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_49@1",
            "content": "However, different from the result of the automatic evaluation, annotators are more inclined to select Ours and GPT2 + prompt engineering when evaluating attribute alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_49",
            "start": 149,
            "end": 322,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_49@2",
            "content": "Although the annotators are instructed not to consider linguistic quality when evaluating sentiment alignment, they tend to select the one with better linguistic quality when multiple completions exhibits equally good attribute alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_49",
            "start": 324,
            "end": 561,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_49@3",
            "content": "In the AGNews topic control task, the result of human evaluation on attribute alignment is generally consistent with the result of automatic evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_49",
            "start": 563,
            "end": 714,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_49@4",
            "content": "However, in more than half of the linguistic quality questions, the annotators select Ours, although GPT2-medium + prompt engineering achieves lower perplexity than Ours.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_49",
            "start": 716,
            "end": 885,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_49@5",
            "content": "On inspection, we find that GPT2-medium + prompt engineering in this task exhibits a more severe repetition problem compared to that in the sentiment control task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_49",
            "start": 887,
            "end": 1049,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_49@6",
            "content": "This inconsistency shows the limitation of using automatic evaluations, as alluded to in Welbl et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_49",
            "start": 1051,
            "end": 1159,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_50@0",
            "content": "Both human evaluation and automatic evaluation show that the linguistic quality of GeDi is inferior to that of the other methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_50",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_50@1",
            "content": "One possible reason is the length of the prompt.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_50",
            "start": 130,
            "end": 177,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_50@2",
            "content": "In the original experiment in Krause et al. (2020), each prompt is at least 150 characters for sentiment control evaluation and at least 30 characters for topic control evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_50",
            "start": 179,
            "end": 358,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_50@3",
            "content": "However, we use the prompts as in Dathathri et al. (2020), where the average prompt length is 11.8 characters for sentiment control evaluation and 14.5 characters for topic control evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_50",
            "start": 360,
            "end": 551,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_50@4",
            "content": "The generated examples are shown in the appendix (Table 7).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_50",
            "start": 553,
            "end": 611,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_51@0",
            "content": "Multi-Aspect Control",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_51",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_52@0",
            "content": "Our method can also be applied to multi-aspect control.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_52",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_52@1",
            "content": "Directly applying our supervised method training examples with single-aspect labels from multiple aspects, which can be utilized to achieve multi-aspect control.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_52",
            "start": 56,
            "end": 216,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_52@2",
            "content": "One method is to train a set of prefixes for each aspect separately using our supervised method and then concatenate the prefixes from different aspects for generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_52",
            "start": 218,
            "end": 385,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_52@3",
            "content": "This is denoted as Ours (concatenation) in the result table.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_52",
            "start": 387,
            "end": 446,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_52@4",
            "content": "Another method is to train the prefixes of multiple aspects simultaneously by considering each single-aspect labeled example as partially labeled.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_52",
            "start": 448,
            "end": 593,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_52@5",
            "content": "We use a semi-supervised method for training, which is a combination of our supervised method and unsupervised method in Section 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_52",
            "start": 595,
            "end": 725,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_52@6",
            "content": "The model structure is the same as in the unsupervised method (Figure 4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_52",
            "start": 727,
            "end": 799,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_52@7",
            "content": "The loss function is as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_52",
            "start": 801,
            "end": 832,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_53@0",
            "content": "L = \u03c9 1 L LM + \u03c9 2 L d + \u03c9 3 L enc (11) L enc = \u2212 log q(z sup = y|x) (12) q(z|x) = \u03c3(\u2212 Enc(x) \u2212 H \u03b8 2 )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_53",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_54@0",
            "content": "(13) where the latent variable z is the concatenation of the latent variable of each aspect, including both the supervised aspects and the unsupervised ones z = [z sup ; z uns ].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_54",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_54@1",
            "content": "L enc is used to train the encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_54",
            "start": 179,
            "end": 213,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_54@2",
            "content": "It is introduced because the partially labeled examples imply the ground truth indexes of the prefixes in the labeled aspect, providing supervision for both the prefix and the encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_54",
            "start": 215,
            "end": 398,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_54@3",
            "content": "\u03c3 is the softmax function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_54",
            "start": 400,
            "end": 425,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_55@0",
            "content": "We experiment with controlling the following two aspects simultaneously: sentiment and topic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_55",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_55@1",
            "content": "We use the binary sentiment dataset from Amazon review (Zhang et al., 2015) and the DBPedia topic dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_55",
            "start": 94,
            "end": 199,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_55@2",
            "content": "The prompts used for evaluation are the same as those in the topic control experiment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_55",
            "start": 201,
            "end": 286,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_55@3",
            "content": "For each of the 20 prompts, 45 completions are generated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_55",
            "start": 288,
            "end": 344,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_55@4",
            "content": "In the GPT2-medium + prompt engineering setting, the guiding sentence follows the template \"This is a [SENTIMENT] review on [TOPIC]:\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_55",
            "start": 346,
            "end": 479,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_55@5",
            "content": "In Ours (concatenation), the sentiment prefixes and the topic prefixes are trained separately using our supervised method and then concatenated as multiaspect prefixes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_55",
            "start": 481,
            "end": 648,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_55@6",
            "content": "In Ours (semi-supervised), we reuse the prefixes trained in the single-aspect control tasks to initialize H \u03b8 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_55",
            "start": 650,
            "end": 760,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_55@7",
            "content": "For example, if the target sentiment is positive and the target topic is an album, the prepended guiding sentence is \"This is a positive review on an album:\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_55",
            "start": 762,
            "end": 919,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_55@8",
            "content": "All the experiments are conducted on NVIDIA Tesla V100 GPUs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_55",
            "start": 921,
            "end": 980,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_55@9",
            "content": "The hyper-parameters are listed in appendix A.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_55",
            "start": 982,
            "end": 1027,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_56@0",
            "content": "Experimental results on multi-aspect control (Table 6) show that simply concatenating the prefixes trained for single-aspect control can effectively control the sentiment and topic simultaneously, and our experiments show that the order of the prefixes does not impact the result.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_56",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_56@1",
            "content": "On the other hand, training using the combination of our supervised and unsupervised methods can further improve the attribute alignment without sacrificing too much linguistic quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_56",
            "start": 281,
            "end": 465,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_56@2",
            "content": "Same as the observations stated in Section 4.1.2, removing the discriminative loss L d will significantly degrade the attribute relevance, especially the topic relevance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_56",
            "start": 467,
            "end": 636,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_56@3",
            "content": "Removing the encoder loss L enc may achieve higher overall attribute relevance at the cost of linguistic quality, indicated by a higher perplexity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_56",
            "start": 638,
            "end": 784,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_56@4",
            "content": "We present the generated examples in the appendix (Table 7).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_56",
            "start": 786,
            "end": 845,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_57@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_57",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_58@0",
            "content": "We propose a novel framework for controllable GPT2 generation with frozen LMs, which utilizes contrastive prefixes to guide generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_58",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_58@1",
            "content": "Experimental results show that our framework can not only successfully guide generation from a single aspect but also achieve promising results on multiaspect control tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_58",
            "start": 136,
            "end": 308,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_58@2",
            "content": "Besides the control tasks we experimented with, our proposed framework can be freely applied to other desired attributes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_58",
            "start": 310,
            "end": 430,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_58@3",
            "content": "We intend to make our implementation freely available online to facilitate future research and downstream applications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_58",
            "start": 432,
            "end": 550,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_59@0",
            "content": "Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu, Plug and play language models: A simple approach to controlled text generation, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_59",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_60@0",
            "content": "Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston, Wizard of wikipedia: Knowledge-powered conversational agents, 2019-05-06, 7th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_60",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_61@0",
            "content": "UNKNOWN, None, 2017, Controlling linguistic style aspects in neural language generation, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_61",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_62@0",
            "content": "Suchin Samuel Gehman, Maarten Gururangan, Yejin Sap, Noah Choi,  Smith, Realtoxicityprompts: Evaluating neural toxic degeneration in language models, 2020-11-20, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_62",
            "start": 0,
            "end": 313,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_63@0",
            "content": "Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi, Kevin Knight, Hafez: an interactive poetry generation system, 2017-07-30, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_63",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_64@0",
            "content": "Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, Yejin Choi, Learning to write with cooperative discriminators, 2018-07-15, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_64",
            "start": 0,
            "end": 276,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_65@0",
            "content": "Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric Xing, Toward controlled generation of text, 2017-08-11, Proceedings of the 34th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_65",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_66@0",
            "content": "Eric Jang, Shixiang Gu, Ben Poole, Categorical reparameterization with gumbel-softmax, 2017-04-24, 5th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_66",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_67@0",
            "content": "UNKNOWN, None, 2019, CTRL: A conditional transformer language model for controllable generation, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_67",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_68@0",
            "content": "UNKNOWN, None, 2009, Gedi: Generative discriminator guided sequence generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_68",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_69@0",
            "content": "Lisa Xiang, Percy Li,  Liang, Prefix-tuning: Optimizing continuous prompts for generation, 2021-08-01, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_69",
            "start": 0,
            "end": 325,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_70@0",
            "content": "UNKNOWN, None, 2021, Plug-and-blend: A framework for controllable story generation with blended control codes, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_70",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_71@0",
            "content": "UNKNOWN, None, 1907, Roberta: A robustly optimized BERT pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_71",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_72@0",
            "content": "Andrew Maas, Raymond Daly, Peter Pham, Dan Huang, Andrew Ng, Christopher Potts, Learning word vectors for sentiment analysis, 2011-06-24, The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_72",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_73@0",
            "content": "Chris Maddison, Andriy Mnih, Yee Whye Teh, The concrete distribution: A continuous relaxation of discrete random variables, 2017-04-24, 5th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_73",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_74@0",
            "content": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Language models are unsupervised multitask learners, 2019, OpenAI blog, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_74",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_75@0",
            "content": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, Zero-shot text-to-image generation, 2021-07-24, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_75",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_76@0",
            "content": "Ben Casper Kaae S\u00f8nderby, Andriy Poole,  Mnih, Continuous relaxation training of discrete latent variable image models, 2017, Beysian DeepLearning workshop, NIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_76",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_77@0",
            "content": "A\u00e4ron Van Den Oord, Oriol Vinyals, Koray Kavukcuoglu, Neural discrete representation learning, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_77",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_78@0",
            "content": "Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. 2021. Challenges in detoxifying language models, , Findings of the Association for Computational Linguistics: EMNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_78",
            "start": 0,
            "end": 286,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_79@0",
            "content": "Kevin Yang, Dan Klein, FUDGE: controlled text generation with future discriminators, 2021-06-06, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_79",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_80@0",
            "content": "UNKNOWN, None, 2021, Attribute alignment: Controlling text generation from pretrained language models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_80",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "453-ARR_v1_81@0",
            "content": "Xiang Zhang, Junbo Zhao, Yann Lecun, Character-level convolutional networks for text classification, 2015-12-07, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "453-ARR_v1_81",
            "start": 0,
            "end": 227,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "453-ARR_v1_0",
            "tgt_ix": "453-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_0",
            "tgt_ix": "453-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_1",
            "tgt_ix": "453-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_1",
            "tgt_ix": "453-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_0",
            "tgt_ix": "453-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_2",
            "tgt_ix": "453-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_4",
            "tgt_ix": "453-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_5",
            "tgt_ix": "453-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_6",
            "tgt_ix": "453-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_7",
            "tgt_ix": "453-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_3",
            "tgt_ix": "453-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_3",
            "tgt_ix": "453-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_3",
            "tgt_ix": "453-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_3",
            "tgt_ix": "453-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_3",
            "tgt_ix": "453-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_3",
            "tgt_ix": "453-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_3",
            "tgt_ix": "453-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_0",
            "tgt_ix": "453-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_9",
            "tgt_ix": "453-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_11",
            "tgt_ix": "453-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_10",
            "tgt_ix": "453-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_10",
            "tgt_ix": "453-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_10",
            "tgt_ix": "453-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_0",
            "tgt_ix": "453-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_12",
            "tgt_ix": "453-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_13",
            "tgt_ix": "453-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_13",
            "tgt_ix": "453-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_13",
            "tgt_ix": "453-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_14",
            "tgt_ix": "453-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_16",
            "tgt_ix": "453-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_17",
            "tgt_ix": "453-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_18",
            "tgt_ix": "453-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_19",
            "tgt_ix": "453-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_20",
            "tgt_ix": "453-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_15",
            "tgt_ix": "453-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_15",
            "tgt_ix": "453-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_15",
            "tgt_ix": "453-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_15",
            "tgt_ix": "453-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_15",
            "tgt_ix": "453-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_15",
            "tgt_ix": "453-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_15",
            "tgt_ix": "453-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_13",
            "tgt_ix": "453-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_21",
            "tgt_ix": "453-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_23",
            "tgt_ix": "453-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_24",
            "tgt_ix": "453-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_25",
            "tgt_ix": "453-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_26",
            "tgt_ix": "453-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_27",
            "tgt_ix": "453-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_28",
            "tgt_ix": "453-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_29",
            "tgt_ix": "453-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_30",
            "tgt_ix": "453-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_31",
            "tgt_ix": "453-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_22",
            "tgt_ix": "453-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_22",
            "tgt_ix": "453-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_22",
            "tgt_ix": "453-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_22",
            "tgt_ix": "453-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_22",
            "tgt_ix": "453-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_22",
            "tgt_ix": "453-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_22",
            "tgt_ix": "453-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_22",
            "tgt_ix": "453-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_22",
            "tgt_ix": "453-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_22",
            "tgt_ix": "453-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_22",
            "tgt_ix": "453-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_0",
            "tgt_ix": "453-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_32",
            "tgt_ix": "453-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_34",
            "tgt_ix": "453-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_33",
            "tgt_ix": "453-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_33",
            "tgt_ix": "453-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_33",
            "tgt_ix": "453-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_33",
            "tgt_ix": "453-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_35",
            "tgt_ix": "453-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_33",
            "tgt_ix": "453-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_36",
            "tgt_ix": "453-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_38",
            "tgt_ix": "453-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_37",
            "tgt_ix": "453-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_37",
            "tgt_ix": "453-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_37",
            "tgt_ix": "453-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_40",
            "tgt_ix": "453-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_37",
            "tgt_ix": "453-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_37",
            "tgt_ix": "453-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_39",
            "tgt_ix": "453-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_33",
            "tgt_ix": "453-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_41",
            "tgt_ix": "453-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_43",
            "tgt_ix": "453-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_44",
            "tgt_ix": "453-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_45",
            "tgt_ix": "453-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_42",
            "tgt_ix": "453-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_42",
            "tgt_ix": "453-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_42",
            "tgt_ix": "453-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_42",
            "tgt_ix": "453-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_42",
            "tgt_ix": "453-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_33",
            "tgt_ix": "453-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_46",
            "tgt_ix": "453-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_48",
            "tgt_ix": "453-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_49",
            "tgt_ix": "453-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_47",
            "tgt_ix": "453-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_47",
            "tgt_ix": "453-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_47",
            "tgt_ix": "453-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_47",
            "tgt_ix": "453-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_33",
            "tgt_ix": "453-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_50",
            "tgt_ix": "453-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_52",
            "tgt_ix": "453-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_53",
            "tgt_ix": "453-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_54",
            "tgt_ix": "453-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_55",
            "tgt_ix": "453-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_51",
            "tgt_ix": "453-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_51",
            "tgt_ix": "453-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_51",
            "tgt_ix": "453-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_51",
            "tgt_ix": "453-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_51",
            "tgt_ix": "453-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_51",
            "tgt_ix": "453-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_0",
            "tgt_ix": "453-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_56",
            "tgt_ix": "453-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_57",
            "tgt_ix": "453-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_57",
            "tgt_ix": "453-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "453-ARR_v1_0",
            "tgt_ix": "453-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_1",
            "tgt_ix": "453-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_2",
            "tgt_ix": "453-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_2",
            "tgt_ix": "453-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_2",
            "tgt_ix": "453-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_2",
            "tgt_ix": "453-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_2",
            "tgt_ix": "453-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_3",
            "tgt_ix": "453-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_4",
            "tgt_ix": "453-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_4",
            "tgt_ix": "453-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_4",
            "tgt_ix": "453-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_4",
            "tgt_ix": "453-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_4",
            "tgt_ix": "453-ARR_v1_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_4",
            "tgt_ix": "453-ARR_v1_4@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_4",
            "tgt_ix": "453-ARR_v1_4@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_4",
            "tgt_ix": "453-ARR_v1_4@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_4",
            "tgt_ix": "453-ARR_v1_4@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_4",
            "tgt_ix": "453-ARR_v1_4@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_4",
            "tgt_ix": "453-ARR_v1_4@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_4",
            "tgt_ix": "453-ARR_v1_4@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_4",
            "tgt_ix": "453-ARR_v1_4@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_5",
            "tgt_ix": "453-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_5",
            "tgt_ix": "453-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_5",
            "tgt_ix": "453-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_5",
            "tgt_ix": "453-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_5",
            "tgt_ix": "453-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_5",
            "tgt_ix": "453-ARR_v1_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_6",
            "tgt_ix": "453-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_6",
            "tgt_ix": "453-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_6",
            "tgt_ix": "453-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_7",
            "tgt_ix": "453-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_8",
            "tgt_ix": "453-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_9",
            "tgt_ix": "453-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_10",
            "tgt_ix": "453-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_11",
            "tgt_ix": "453-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_11",
            "tgt_ix": "453-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_11",
            "tgt_ix": "453-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_11",
            "tgt_ix": "453-ARR_v1_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_11",
            "tgt_ix": "453-ARR_v1_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_11",
            "tgt_ix": "453-ARR_v1_11@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_11",
            "tgt_ix": "453-ARR_v1_11@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_11",
            "tgt_ix": "453-ARR_v1_11@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_11",
            "tgt_ix": "453-ARR_v1_11@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_11",
            "tgt_ix": "453-ARR_v1_11@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_12",
            "tgt_ix": "453-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_12",
            "tgt_ix": "453-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_12",
            "tgt_ix": "453-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_12",
            "tgt_ix": "453-ARR_v1_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_12",
            "tgt_ix": "453-ARR_v1_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_12",
            "tgt_ix": "453-ARR_v1_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_12",
            "tgt_ix": "453-ARR_v1_12@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_12",
            "tgt_ix": "453-ARR_v1_12@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_13",
            "tgt_ix": "453-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_14",
            "tgt_ix": "453-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_14",
            "tgt_ix": "453-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_14",
            "tgt_ix": "453-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_14",
            "tgt_ix": "453-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_14",
            "tgt_ix": "453-ARR_v1_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_14",
            "tgt_ix": "453-ARR_v1_14@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_14",
            "tgt_ix": "453-ARR_v1_14@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_14",
            "tgt_ix": "453-ARR_v1_14@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_14",
            "tgt_ix": "453-ARR_v1_14@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_14",
            "tgt_ix": "453-ARR_v1_14@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_14",
            "tgt_ix": "453-ARR_v1_14@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_14",
            "tgt_ix": "453-ARR_v1_14@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_15",
            "tgt_ix": "453-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_16",
            "tgt_ix": "453-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_16",
            "tgt_ix": "453-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_16",
            "tgt_ix": "453-ARR_v1_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_16",
            "tgt_ix": "453-ARR_v1_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_17",
            "tgt_ix": "453-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_18",
            "tgt_ix": "453-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_19",
            "tgt_ix": "453-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_20",
            "tgt_ix": "453-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_20",
            "tgt_ix": "453-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_21",
            "tgt_ix": "453-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_21",
            "tgt_ix": "453-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_21",
            "tgt_ix": "453-ARR_v1_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_21",
            "tgt_ix": "453-ARR_v1_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_21",
            "tgt_ix": "453-ARR_v1_21@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_21",
            "tgt_ix": "453-ARR_v1_21@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_21",
            "tgt_ix": "453-ARR_v1_21@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_21",
            "tgt_ix": "453-ARR_v1_21@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_22",
            "tgt_ix": "453-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_23",
            "tgt_ix": "453-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_23",
            "tgt_ix": "453-ARR_v1_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_23",
            "tgt_ix": "453-ARR_v1_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_23",
            "tgt_ix": "453-ARR_v1_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_23",
            "tgt_ix": "453-ARR_v1_23@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_23",
            "tgt_ix": "453-ARR_v1_23@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_23",
            "tgt_ix": "453-ARR_v1_23@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_23",
            "tgt_ix": "453-ARR_v1_23@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_24",
            "tgt_ix": "453-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_24",
            "tgt_ix": "453-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_25",
            "tgt_ix": "453-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_26",
            "tgt_ix": "453-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_27",
            "tgt_ix": "453-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_27",
            "tgt_ix": "453-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_27",
            "tgt_ix": "453-ARR_v1_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_27",
            "tgt_ix": "453-ARR_v1_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_27",
            "tgt_ix": "453-ARR_v1_27@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_27",
            "tgt_ix": "453-ARR_v1_27@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_27",
            "tgt_ix": "453-ARR_v1_27@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_28",
            "tgt_ix": "453-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_29",
            "tgt_ix": "453-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_30",
            "tgt_ix": "453-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_30",
            "tgt_ix": "453-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_31",
            "tgt_ix": "453-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_32",
            "tgt_ix": "453-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_32",
            "tgt_ix": "453-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_33",
            "tgt_ix": "453-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_34",
            "tgt_ix": "453-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_34",
            "tgt_ix": "453-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_34",
            "tgt_ix": "453-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_34",
            "tgt_ix": "453-ARR_v1_34@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_34",
            "tgt_ix": "453-ARR_v1_34@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_34",
            "tgt_ix": "453-ARR_v1_34@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_34",
            "tgt_ix": "453-ARR_v1_34@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_34",
            "tgt_ix": "453-ARR_v1_34@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_34",
            "tgt_ix": "453-ARR_v1_34@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_35",
            "tgt_ix": "453-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_35",
            "tgt_ix": "453-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_35",
            "tgt_ix": "453-ARR_v1_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_35",
            "tgt_ix": "453-ARR_v1_35@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_36",
            "tgt_ix": "453-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_37",
            "tgt_ix": "453-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_38",
            "tgt_ix": "453-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_38",
            "tgt_ix": "453-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_38",
            "tgt_ix": "453-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_38",
            "tgt_ix": "453-ARR_v1_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_38",
            "tgt_ix": "453-ARR_v1_38@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_38",
            "tgt_ix": "453-ARR_v1_38@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_38",
            "tgt_ix": "453-ARR_v1_38@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_38",
            "tgt_ix": "453-ARR_v1_38@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_39",
            "tgt_ix": "453-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_39",
            "tgt_ix": "453-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_39",
            "tgt_ix": "453-ARR_v1_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_39",
            "tgt_ix": "453-ARR_v1_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_39",
            "tgt_ix": "453-ARR_v1_39@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_39",
            "tgt_ix": "453-ARR_v1_39@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_39",
            "tgt_ix": "453-ARR_v1_39@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_39",
            "tgt_ix": "453-ARR_v1_39@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_39",
            "tgt_ix": "453-ARR_v1_39@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_40",
            "tgt_ix": "453-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_40",
            "tgt_ix": "453-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_40",
            "tgt_ix": "453-ARR_v1_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_40",
            "tgt_ix": "453-ARR_v1_40@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_40",
            "tgt_ix": "453-ARR_v1_40@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_40",
            "tgt_ix": "453-ARR_v1_40@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_40",
            "tgt_ix": "453-ARR_v1_40@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_40",
            "tgt_ix": "453-ARR_v1_40@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_41",
            "tgt_ix": "453-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_41",
            "tgt_ix": "453-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_42",
            "tgt_ix": "453-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_43",
            "tgt_ix": "453-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_43",
            "tgt_ix": "453-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_43",
            "tgt_ix": "453-ARR_v1_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_43",
            "tgt_ix": "453-ARR_v1_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_43",
            "tgt_ix": "453-ARR_v1_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_44",
            "tgt_ix": "453-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_44",
            "tgt_ix": "453-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_44",
            "tgt_ix": "453-ARR_v1_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_45",
            "tgt_ix": "453-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_45",
            "tgt_ix": "453-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_45",
            "tgt_ix": "453-ARR_v1_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_45",
            "tgt_ix": "453-ARR_v1_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_45",
            "tgt_ix": "453-ARR_v1_45@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_45",
            "tgt_ix": "453-ARR_v1_45@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_45",
            "tgt_ix": "453-ARR_v1_45@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_45",
            "tgt_ix": "453-ARR_v1_45@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_45",
            "tgt_ix": "453-ARR_v1_45@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_46",
            "tgt_ix": "453-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_46",
            "tgt_ix": "453-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_46",
            "tgt_ix": "453-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_46",
            "tgt_ix": "453-ARR_v1_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_47",
            "tgt_ix": "453-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_48",
            "tgt_ix": "453-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_48",
            "tgt_ix": "453-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_48",
            "tgt_ix": "453-ARR_v1_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_48",
            "tgt_ix": "453-ARR_v1_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_48",
            "tgt_ix": "453-ARR_v1_48@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_48",
            "tgt_ix": "453-ARR_v1_48@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_48",
            "tgt_ix": "453-ARR_v1_48@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_48",
            "tgt_ix": "453-ARR_v1_48@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_48",
            "tgt_ix": "453-ARR_v1_48@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_48",
            "tgt_ix": "453-ARR_v1_48@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_48",
            "tgt_ix": "453-ARR_v1_48@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_48",
            "tgt_ix": "453-ARR_v1_48@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_48",
            "tgt_ix": "453-ARR_v1_48@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_48",
            "tgt_ix": "453-ARR_v1_48@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_48",
            "tgt_ix": "453-ARR_v1_48@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_48",
            "tgt_ix": "453-ARR_v1_48@15",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_49",
            "tgt_ix": "453-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_49",
            "tgt_ix": "453-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_49",
            "tgt_ix": "453-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_49",
            "tgt_ix": "453-ARR_v1_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_49",
            "tgt_ix": "453-ARR_v1_49@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_49",
            "tgt_ix": "453-ARR_v1_49@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_49",
            "tgt_ix": "453-ARR_v1_49@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_50",
            "tgt_ix": "453-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_50",
            "tgt_ix": "453-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_50",
            "tgt_ix": "453-ARR_v1_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_50",
            "tgt_ix": "453-ARR_v1_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_50",
            "tgt_ix": "453-ARR_v1_50@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_51",
            "tgt_ix": "453-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_52",
            "tgt_ix": "453-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_52",
            "tgt_ix": "453-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_52",
            "tgt_ix": "453-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_52",
            "tgt_ix": "453-ARR_v1_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_52",
            "tgt_ix": "453-ARR_v1_52@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_52",
            "tgt_ix": "453-ARR_v1_52@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_52",
            "tgt_ix": "453-ARR_v1_52@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_52",
            "tgt_ix": "453-ARR_v1_52@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_53",
            "tgt_ix": "453-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_54",
            "tgt_ix": "453-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_54",
            "tgt_ix": "453-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_54",
            "tgt_ix": "453-ARR_v1_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_54",
            "tgt_ix": "453-ARR_v1_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_55",
            "tgt_ix": "453-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_55",
            "tgt_ix": "453-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_55",
            "tgt_ix": "453-ARR_v1_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_55",
            "tgt_ix": "453-ARR_v1_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_55",
            "tgt_ix": "453-ARR_v1_55@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_55",
            "tgt_ix": "453-ARR_v1_55@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_55",
            "tgt_ix": "453-ARR_v1_55@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_55",
            "tgt_ix": "453-ARR_v1_55@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_55",
            "tgt_ix": "453-ARR_v1_55@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_55",
            "tgt_ix": "453-ARR_v1_55@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_56",
            "tgt_ix": "453-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_56",
            "tgt_ix": "453-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_56",
            "tgt_ix": "453-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_56",
            "tgt_ix": "453-ARR_v1_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_56",
            "tgt_ix": "453-ARR_v1_56@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_57",
            "tgt_ix": "453-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_58",
            "tgt_ix": "453-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_58",
            "tgt_ix": "453-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_58",
            "tgt_ix": "453-ARR_v1_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_58",
            "tgt_ix": "453-ARR_v1_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_59",
            "tgt_ix": "453-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_60",
            "tgt_ix": "453-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_61",
            "tgt_ix": "453-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_62",
            "tgt_ix": "453-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_63",
            "tgt_ix": "453-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_64",
            "tgt_ix": "453-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_65",
            "tgt_ix": "453-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_66",
            "tgt_ix": "453-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_67",
            "tgt_ix": "453-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_68",
            "tgt_ix": "453-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_69",
            "tgt_ix": "453-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_70",
            "tgt_ix": "453-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_71",
            "tgt_ix": "453-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_72",
            "tgt_ix": "453-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_73",
            "tgt_ix": "453-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_74",
            "tgt_ix": "453-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_75",
            "tgt_ix": "453-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_76",
            "tgt_ix": "453-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_77",
            "tgt_ix": "453-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_78",
            "tgt_ix": "453-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_79",
            "tgt_ix": "453-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_80",
            "tgt_ix": "453-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "453-ARR_v1_81",
            "tgt_ix": "453-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1151,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "453-ARR",
        "version": 1
    }
}