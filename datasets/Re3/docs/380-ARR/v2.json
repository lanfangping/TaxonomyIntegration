{
    "nodes": [
        {
            "ix": "380-ARR_v2_0",
            "content": "EPiDA: An Easy Plug-in Data Augmentation Framework for High Performance Text Classification",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_2",
            "content": "Recent works have empirically shown the effectiveness of data augmentation (DA) for NLP tasks, especially for those suffering from data scarcity. Intuitively, given the size of generated data, their diversity and quality are crucial to the performance of targeted tasks. However, to the best of our knowledge, most existing methods consider only either the diversity or the quality of augmented data, thus cannot fully tap the potential of DA for NLP. In this paper, we present an easy and plug-in data augmentation framework EPiDA to support effective text classification. EPiDA employs two mechanisms: relative entropy maximization (REM) and conditional entropy minimization (CEM) to control data generation, where REM is designed to enhance the diversity of augmented data while CEM is exploited to ensure their semantic consistency. EPiDA can support efficient and continuous data generation for effective classifier training. Extensive experiments show that EP-iDA outperforms existing SOTA methods in most cases, though not using any agent network or pre-trained generation network, and it works well with various DA algorithms and classification models. Code is available at https: //github.com/zhaominyiz/EPiDA.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "380-ARR_v2_4",
            "content": "Data augmentation (DA) is widely-used in classification tasks (Shorten and Khoshgoftaar, 2019;Feng et al., 2021;. In computer vision (CV), (Krizhevsky et al., 2012;Chatfield et al., 2014;Szegedy et al., 2015) adopt strategies like flipping, cropping, tilting to perform DA. In natural language processing (NLP), (Xie et al., 2017;Coulombe, 2018;Niu and Bansal, 2018;Wei and Zou, 2019) find that native augmentation skills such as spelling errors, synonym replacement, deleting and swapping, can bring considerable performance improvement. All these methods use * Corresponding author.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_5",
            "content": "various transformations for data augmentation, but they do not achieve equal success in different NLP tasks . Sometimes, they fail to guarantee semantic consistency, and may even bring semantic errors that are harmful to classification. The reason lies in that data augmentation for NLP is in discrete space, so it can easily incur large deviation of semantics (e.g. in sentiment classification task, deleting emotional words from a sentence will make its meaning completely different).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_6",
            "content": "Generally, given the size of generated data, their diversity and quality are crucial to the performance of targeted tasks (Ash et al., 2019). Recent works have begun to emphasize the diversity or quality of augmented data. For example, in CV, AA (Cubuk et al., 2019), Fast-AA (Lim et al., 2019) and LTA (Luo et al., 2020) employ agent networks to learn how to enhance diversity. In NLP, language models are widely used to control generation quality, including Back-translation (Sennrich et al., 2016;Yu et al., 2018), Seq2seq models (Kobayashi, 2018;Kumar et al., 2019;, GPT-2 (Radford et al., 2019;Anaby-Tavor et al., 2020;Quteineh et al., 2020;Liu et al., 2020) and T5 (Dong et al., 2021). In addition, some works (Morris et al., 2020) in NLP utilize adversarial augmentation to enrich the diversity of the samples. However, to the best of our knowledge, most existing works consider only either the quality or the diversity of augmented data, so cannot fully exploit the potential of data augmentation for NLP tasks. Besides, recent existing DA methods for NLP tasks usually resort to pre-trained language models, are extremely inefficient due to huge model complexity and tedious finetuning, which limits the scope of their applications.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_7",
            "content": "In this paper, we propose a new data augmentation framework for text classification. This framework is called EPiDA (the abbreviation of Easy Plug-in Data Augmentation), which employs two mechanisms to control the diversity and quality of augmented data: relative entropy maximization (REM) and conditional entropy minimization (CEM), where the former is for boosting diversity while the latter for ensuring quality. Fig. 1 shows the pipeline of EPiDA The main contributions of this paper are as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_8",
            "content": "1. We propose an easy plug-in data augmentation framework EPiDA for text classification.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_9",
            "content": "EPiDA can work with various existing DA algorithms and classification models, it is general, efficient, and easy-to-deploy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_10",
            "content": "2. We design two mechanisms relative entropy maximization (REM) and conditional entropy minimization (CEM) to boost the diversity and quality of augmented data simultaneously in an explicit and controllable way.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_11",
            "content": "3. We conduct extensive experiments to evaluate EPiDA. Experimental results show that EP-iDA outperforms existing DA methods, and works well with different DA algorithms and classification models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_12",
            "content": "The rest of this paper is organized as follows: Sec. 2 reviews related work and highlights the differences between our work and major existing methods. Sec. 3 introduce our method in details. Sec. 4 presents the results of performance evaluation, and Sec. 5 concludes the paper.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_13",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "380-ARR_v2_14",
            "content": "In this section, we first review the related work of DA for NLP, then expound the differences between our method and the major existing ones. According to the methodology of data generation, existing methods can be categorized into three types: rule-based, interpolation-based, and model-based, respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_15",
            "content": "Rule-Based Methods",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "380-ARR_v2_16",
            "content": "These works use easy and predetermined transformations without model components. (Kolomiyets et al., 2011;Zhang et al., 2015;Wang and Yang, 2015) use synonyms to replace words. EDA (Wei and Zou, 2019) and AEDA (Karimi et al., 2021) introduce random insertions, swaps, and deletions. Xie et al. (2017) employed spelling errors to augment sentences. \u015eahin and Steedman (2018) conducted sentence rotating via dependency tree morphing. proposed a multi-task view of DA. SUB 2 (Shi et al., 2021) generates new examples by substituting substructures via constituency parse trees. Although these methods are easy to implement, they do not consider controlling data quality and diversity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_17",
            "content": "Interpolation-Based Methods",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "380-ARR_v2_18",
            "content": "MIXUP (Zhang et al., 2017) pioneers this type of works by interpolating the input and labels of two or more real examples. Recently, many MIXUP strategies (Verma et al., 2019;Yun et al., 2019) were proposed in CV. Due to the discrete nature of inputs of NLP tasks, such methods can be applied to NLP tasks only via padding and mixing embeddings or higher hidden layers (Chen et al., 2020;Si et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_19",
            "content": "Model-Based Methods",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "380-ARR_v2_20",
            "content": "Seq2seq and language models have been used to generate high quality samples. Among these approaches, Back-translation (Sennrich et al., 2016;Yu et al., 2018) translates sentences into another language and then translates it back to the original language. RNNs and transformers are used to reconstruct sub-parts of real data with contextual information (Kobayashi, 2018;Gao et al., 2019;. Recently, we have witnessed the great success of large-scale pre-trained language models (PLMs) such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), GPT-2 (Radford et al., 2019) are also widely used to augment samples (Ng et al., 2020;Nie et al., 2020;Anaby-Tavor et al., 2020;Quteineh et al., 2020;Liu et al., 2020;Dong et al., 2021). For example, DataBoost (Liu et al., 2020) develops a reinforcement learning strategy to guide the conditional generation without changing the architecture of GPT-2. Besides, adversarial augmentation (i.e., attack, GANs) are also used to enrich the diversity of the generated samples (Morris et al., 2020;Simoncini and Spanakis, 2021). Although model-based methods can control generation quality well via PLMs, they are computationally inefficient, which limits their applications.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_21",
            "content": "Differences between EPiDA and Existing Methods",
            "ntype": "title",
            "meta": {
                "section": "2.4"
            }
        },
        {
            "ix": "380-ARR_v2_22",
            "content": "To expound the differences between EPiDA and typical existing methods, in Tab. 1 we present a qualitative comparison from four dimensions: whether controlling the diversity and quality of the augmented data, whether using pre-trained model (language model or agent network), and whether using the feedback from the classifier.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_23",
            "content": "As shown in Tab. 1, among the existing methods, most control only either diversity (e.g. AA and EDA) or quality (e.g. DataBoost) of augmented data, thus cannot completely leverage the potential of data augmentation. And most use language model or agent network, which is beneficial to data quality but also inefficient. Only the recent LearnDA (Zuo et al., 2021) and VDA consider both diversity and quality, and only AA uses feedback of the classifier. Our EPiDA addresses both diversity and quality of augmented data via the feedback of the classifier in an explicit and controllable way, without the help of any additional model components, which makes it not only more effective but also more efficient.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_24",
            "content": "Note that in addition to the differences listed in Tab. 1, our method EPiDA differs from LearnDA in at least three other aspects: 1) LearnDA employs perplexity score (PPL) and cosine similarity to measure diversity and quality respectively, while EPiDA adopts two mechanisms relative entropy maximization (REM) and conditional entropy minimization (CEM) to control diversity and quality, which is theoretically more rational and solid. 2) LearnDA is for event causality identification, while EPiDA is mainly for text classification. 3) LearnDA needs knowledge guidance, while EP-iDA does not. These make it difficult to evaluate LearnDA in our experimental settings. Thus, we do not conduct performance comparison between EPiDA and LearnDA. Nevertheless, in our ablation study, we replace REM and CEM with PPL and cosine similarity in EPiDA, and our experimental results show that EPiDA with REM and CEM performs better than that with PPL and cosine similarity. Besides, comparing with VDA that requires PLM to provide substitution probability, EPiDA is free of PLMs, and is more effective, efficient and practical.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_25",
            "content": "Method",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "380-ARR_v2_26",
            "content": "As shown in Fig. 1, EPiDA consists of three components: a DA algorithm T , a classifier or classification model C, and a Sample Evaluation and Selection (SEAS) module that is the core component of EPiDA. Generally, the DA algorithm and the classifier can be any of existing DA algorithms and classifiers. With the feedback of the classifier, SEAS evaluates candidate samples generated by the DA algorithm in terms of diversity and quality via the Relative Entropy Maximization (REM) mechanism and the Conditional Entropy Minimization (CEM) mechanism, and outputs the qualified samples to further train the classifier. So EPiDA can serve as a plug-in component to boost existing DA algorithms for training better target models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_27",
            "content": "The Rationale to Control DA",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "380-ARR_v2_28",
            "content": "Consider a classification task with a dataset X of n samples: X={(x 1 , y 1 ),(x 2 , y 2 ),...,(x n , y n )}. Here, x i is a sample, y i is its label. The loss function is",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_29",
            "content": "L o (\u03c9) = 1 n n i=1 l(\u03c9 \u22a4 \u03d5(x i ); y i ).(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_30",
            "content": "where \u03d5 : R d \u2192 R D is a finite-dimensional feature map, \u03c9 \u2208 R D means learnable parameters, and l can be a common loss function like cross-entropy. Now we employ a DA algorithm T to conduct augmentation for each sample in X. Let t j i be the j-th sample generated by T with x i as input, and m samples are generated from x i , the loss function for the generated samples can be written as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_31",
            "content": "L g (\u03c9) = 1 n n i=1 1 m m j=1 l(\u03c9 \u22a4 \u03d5(t j i ); y i ).(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_32",
            "content": "Here, we assume 1) t j i and x i have the same label y i , so we can use y i to optimize the new loss function; 2) Data augmentation does not significantly change the feature map \u03d5, that is, augmentation can maintain semantic consistency of the sample space. Now we combine the augmented samples into the original samples, thus the total loss function of EPiDA can be written as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_33",
            "content": "L(\u03c9) = L o (\u03c9) + L g (\u03c9).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_34",
            "content": "(3)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_35",
            "content": "Recall that we use the feedback of the classifier C to select samples. Specifically, we use the original training samples X to pre-train the classifier C, and for each generated sample t j i , the feedback signal about t j i from the classifier is used for evaluating t j i . When the generation process is over, all generated samples {t j i } are used to train C again. First, we consider how to generate samples of high diversity. Intuitively, generated samples should be different from the original samples. Recalling that the classifier C is pretrained by X, so for generated sample t j i , its loss l(\u03c9 \u22a4 \u03d5(t j i ); y i ) should be large. In this sense, given the classifier C (\u03c9 is fixed), we select samples that meet the following objective function:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_36",
            "content": "max t j i L g (\u03c9, \u03d5(t j i )),(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_37",
            "content": "which means that we are to generate \"hard\" samples for the classifier to cope with. Second, we consider how to control the quality of augmented data. Recall that we assume for each augmented sample t j i , its label y i keeps unchanged, so we can use the original label to evaluate the loss function. However, due to the discrete nature of language, it is nontrivial for augmented samples to meet this assumption. Taking the sentiment analysis task for example, suppose we use EDA (Wei and Zou, 2019) to augment x i =\"you'll probably love it\", EDA may delete the word \"love\". Obviously, the resulting sentence breaks the semantic consistency. To guarantee semantic consistency, we limit the semantic deviation of \u03d5(t j i ) from \u03d5(x i ). Let M and \u03c1 be a metric function to measure semantic difference between samples and a threshold respectively, we impose the following constraint on \u03d5(t j i ):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_38",
            "content": "|M (\u03c9 \u22a4 \u03d5(t j i ), \u03c9 \u22a4 \u03d5(x i ))| \u2264 \u03c1.(5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_39",
            "content": "Thus, we can enhance data diversity by optimizing Eq. ( 4), and improve data quality using Eq. ( 5). The problem turns to solve Eq. ( 4) and Eq. ( 5).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_40",
            "content": "Relative Entropy Maximization",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "380-ARR_v2_41",
            "content": "We rewrite the objective function in Eq. ( 4) via:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_42",
            "content": "Lg(\u03c9, \u03d5(t j i )) = 1 nm n i=1 m j=1 l(\u03c9 \u22a4 \u03d5(t j i ); yi) = 1 nm n i=1 m j=1 D(p(\u03c9 \u22a4 \u03d5(t j i )), p(yi)) + H(p(yi)),(6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_43",
            "content": "where p, H, D indicate probability distribution, Shannon entropy, and relative entropy respectively, and actually H(p(y i )) = 0 since p(y i ) is a onehot vector. According to Eq. ( 6), we try to augment samples with large relative entropy under the given labels. Thus, we call this method relative entropy maximization (REM) mechanism. As relative entropy measures the difference between the two distributions p(\u03c9 \u22a4 \u03d5(t j i )) and p(y i ), the larger the difference is, the more diverse the augmented sample is. Therefore, we define the diversity score s ij div of augmented sample t j i as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_44",
            "content": "s ij div = D(p(\u03c9 \u22a4 \u03d5(t j i )), p(y i )).(7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_45",
            "content": "Conditional Entropy Minimization",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "380-ARR_v2_46",
            "content": "We use conditional entropy as the metric function in Eq. ( 5) to constrain the semantic deviation of",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_47",
            "content": "\u03d5(t j i ) from \u03d5(x i ), i.e., M (\u2022, \u2022) := H(\u2022|\u2022).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_48",
            "content": "Then, Eq. ( 5) can be rewritten to",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_49",
            "content": "H(p(\u03c9 \u22a4 \u03d5(t j i ))|p(\u03c9 \u22a4 \u03d5(x i ))) \u2264 \u03c1. (8",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_50",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_51",
            "content": "where H(\u2022, \u2022) is conditional entropy. Furthermore, to meet Eq. ( 8), we select samples {t j i } by solving the following optimization problem:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_52",
            "content": "min t j i H(p(\u03c9 \u22a4 \u03d5(t j i ))|p(\u03c9 \u22a4 \u03d5(x i ))).(9)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_53",
            "content": "We call this conditional entropy minimization (CEM) mechanism.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_54",
            "content": "The smallest value of",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_55",
            "content": "H(p(\u03c9 \u22a4 \u03d5(t j i ))|p(\u03c9 \u22a4 \u03d5(x i ))) is 0, indicating that given p(\u03c9 \u22a4 \u03d5(x i )), p(\u03c9 \u22a4 \u03d5(t j i )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_56",
            "content": ") is exactly predictable. Eq. ( 9) can also be expanded to the difference between Shannon entropy H and mutual information I, i.e., H(X|Y )=H(X)-I(X, Y ). In other words, CEM minimizes the entropy of the selected sample t j i and maximizes the mutual information between t j i and the original sample x i , which means that CEM tries to augment samples of high prediction probability and high similarity with the original sample. As in REM, we define the quality score s ij qua of augmented sample t j i as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_57",
            "content": "s ij qua = \u2212H(p(\u03c9 \u22a4 \u03d5(t j i ))|p(\u03c9 \u22a4 \u03d5(x i ))). (10",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_58",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_59",
            "content": "Algorithm 1: EPiDA Data Augmentation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_60",
            "content": "Input: Classification model \u03c9, input sample x i and its label y i , DA algorithm T , augmentation number m and amplification factor K. Output: m augmented samples. // assign the set T (xi) of mK candidates to array ti ti \u2190 T (x i );",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_61",
            "content": "s div , s qua , s tot = R K * m , R K * m , R K * m ; for j = 1, 2, . . . , K * m do",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_62",
            "content": "Calculate s ij div via Eq. (7) ; Calculate s ij qua via Eq. ( 10) ; end Take Min_Max_Norm for s div and s qua ; s tot = s div + s qua ; // find the subscripts of the top m small elements id = argtopm(\u2212s tot ) ; Return ti [id] ;",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_63",
            "content": "Algorithm and Implementation",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "380-ARR_v2_64",
            "content": "The procedure of EPiDA is presented in Alg. 1. For each input sample x i , EPiDA outputs m augmented samples. First, we employ T to generate K * m candidate augmented samples for x i , where K is a hyperparameter to amplify the number of candidate samples, which is called amplification factor. Then, for each augmented sample, we use REM and CEM to evaluate its diversity score (s div ) and quality score (s qua ), respectively. Next, we adopt Min_Max_Norm to make s div and s qua fall in [0,1]. After that, we add them together as the overall score of the sample, and sort all the augmented samples in descending order according to their scores. Finally, we take the top m samples from all the K * m candidate samples as the output, and utilize them to train the classifier. By nature, the goals of REM and CEM are conflicting, i.e., a sample of high diversity is more probably of low quality, and vice versa. We give an example in Tab. 2 to demonstrate this point. REM encourages to change salient words, which is prone to break the semantic consistency (see the 3rd row, \"excited\" is changed to \"mad\", leading to large diversity score but small quality score). However, CEM tends to make the augmented samples keep semantic consistency, i.e., has large quality score but small diversity score (see the 4th row, \"comes\" is deleted). By jointly considering REM and CEM, satisfactory samples with balanced diversity and quality can be found (see the 5th row).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_65",
            "content": "Besides, the calculation of s div and s qua requires the feedback of the classifier, so we first pre-train the classifier using the original samples, then with EPiDA we can generate samples of high diversity and quality for the classifier continuously.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_66",
            "content": "Performance Evaluation",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "380-ARR_v2_67",
            "content": "In this section, we conduct extensive experiments to evaluate EPiDA, including performance comparison with SOTA methods, performance evaluation when working with different DA algorithms and classification models, ablation study, and qualitative visualization of samples augmented by EPiDA.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_68",
            "content": "Datasets and Settings",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "380-ARR_v2_69",
            "content": "Datasets for five different tasks are used in our experiments: Question Classification (Li and Roth, 2002) (TREC, N =5,452), News Classification (Zhang et al., 2015) (AGNews, N =120,000), Tweets Sentiment Analysis (Rosenthal et al., 2017) Macro-F1 (F1 for binary tasks) is used as performance metric, and all the experiments are repeated five times. The amplification factor K is set to 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_70",
            "content": "Comparing with SOTA Methods",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "380-ARR_v2_71",
            "content": "Here we carry out performance comparison with major SOTA methods to show the superiority of EPiDA on three datasets: Sentiment, Irony and Offense. For a fair comparison, we strictly follow the experimental setting of DataBoost (Liu et al., 2020): we do only one round of augmentation to ensure that the number of samples of our method is consistent with that of the other methods, and use BERT as the classifier. We use the widely used EDA as the DA algorithm of EPiDA. We do not use DataBoost because it is not yet open-sourced.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_72",
            "content": "The experimental results are presented in Tab. 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_73",
            "content": "From Tab. 3, we can see that 1) with the help of EPiDA, the performance of EDA is greatly improved. In particular, comparing with the original EDA, EPiDA gets performance improvement of 14.1%, 8.39%, 22.83%, 33.40%, 6.75% and 9.22% in six task settings, respectively. 2) Our method outperforms DataBoost in four settings. In particular, EPiDA+EDA gets performance improvement of 8.12%, 2.65%, 10.15% and 6.99% in various settings of the Sentiment and Irony tasks. 3) The variants of EPiDA that utilize only REM or CEM to enhance diversity or quality are inferior to using both, which demonstrates the effectiveness of joint enhancement. 4) DataBoost performs better in the Offense task, the reason lies in that Data-Boost can create novel sentences from Offense (a relatively huge corpus) via GPT-2, while EDA only conducts word-level augmentation, which limits EPiDA's performance. 5) We also present PPL as an auxiliary metric to measure the generation perplexity. Our method outperforms the others due to the high quality of data generation. We also provide experimental comparisons with other DA approaches (SUB 2 and VDA) and generation speed results in the supplementary file. In conclusion, EPiDA is a powerful and efficient technique.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_74",
            "content": "Performance with Different DA Algorithms and Classifiers",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "380-ARR_v2_75",
            "content": "EPiDA is a plug-in component that can work with different DA algorithms and classifiers. Here, to check how EPiDA performs with different DA algorithms and classifiers, we consider three frequentlyused DA algorithms: rule-based EDA (Wei and Zou, 2019), model-based CWE (Kobayashi, 2018) and Attack-based TextAttack (Morris et al., 2020), and three different classifiers: CNN (Kim, 2014), BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) cases. For the three different classification models: CNN, BERT and XLNet, with the help of EPiDA, they all but XLNet on Sentiment get classification performance improvement, which shows that EP-iDA is insensitive to classification models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_76",
            "content": "Ablation Study",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "380-ARR_v2_77",
            "content": "Here we conduct ablation study to check the effectiveness of different EPiDA configurations. We take CNN as the classifier, EDA as the DA algorithm and report the Macro-F1 score over five repeated experiments on TREC 1% and Irony 1%. Tab. 5 shows the experimental results.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_78",
            "content": "Effect of REM and CEM. The 4th and 5th rows show the results with only REM and CEM, respectively. Both of them perform better than the baseline (1st row), but not as good as the combined case (the 8th row). On TREC (relatively simple task), REM outperforms CEM (0.729 vs. 0.723), while on Irony (relatively hard task), CEM outperforms REM (0.559 vs. 0.557). Using only REM can limitedly boost performance since REM promotes the generation of high diversity samples, which may have wrong labels. And using only CEM is also not enough to fully tap the performance as CEM tends to generate redundant samples.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_79",
            "content": "We also compare our 'REM + CEM' with 'PPL + cosine similarity' used in LDA (Zuo et al., 2021). Our method achieves the performance of 0.740 and 0.576 on TREC 1% and Irony 1%, while the latter achieves 0.730 and 0.562. This shows that our 'REM + CEM' is more effective.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_80",
            "content": "Effect of online augmentation. Comparing the results of the 2nd and the 3rd rows, the 6th and the 7th rows, the 8th and the 9th rows, we can see that generally online augmentation can boost performance, as online augmentation can generate sufficient qualified samples to train the model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_81",
            "content": "Effect of pre-training. As REM and CEM use the feedback of the classifier, a pre-trained classification model should be beneficial to REM and CEM. By comparing the results of the 6th and the 8th rows, the 7th and the 9th rows, it is obvious that pre-training can improve performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_82",
            "content": "Effect of normalization. In Alg. 1, we normalize s div and s qua . Here, we check the effect of normalization. With the same experimental settings, the performance results on TREC 1% and Irony 1% without normalization are 0.732 and 0.568, lower than the normalized results 0.740 and 0.576. This shows that normalization is effective.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_83",
            "content": "How to combine REM and CEM? How to combine REM and CEM is actually how to combine the values of s div and s qua . We consider three simple schemes: addition (s tot = s div + s qua ), multiplication (s tot = s div * s qua ) and weighted addition (s tot = \u03b1s div + (1 \u2212 \u03b1)s qua , \u03b1 is a hyperparameter to tradeoff REM and CEM). Note that for multiplication, there is possibly an extreme situation: after normalization, s div or s qua may be very small and even approaches 0, then the multiplication result is very small or even zero, which means that REM and CEM do not take effect in sample generation. In our experiments, the multiplication scheme achieves performance of 0.725 and 0.572 on TREC 1% and Irony 1%, lower than the addition scheme 0.740 and 0.576. As for weighted addition, we find that setting \u03b1 = 0.5 can achieve satisfactory results (see the supplementary file). This is actually equal to the addition scheme. Therefore, in our experiments, we use only the addition scheme.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_84",
            "content": "Quality and diversity metrics. Here, we provide another two metrics to verify EPiDA from the perspective of quality and diversity. For quality, we use the augmentation error rate. As for diversity, we calculate the average distance of samples before and after augmentation (ignoring wrong samples). From the perspective of quality and diversity, a good DA should has a small error rate but a large distance. Experimental results are given in Tab. 6. We can see that EPiDA gets better trade-off between error rate and distance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_85",
            "content": "Effect of the amplification factor K. The amplification factor K determines the size Km of candidate samples from which m samples are chosen. On the one hand, with a large K, we have more choices, which seems beneficial to diversity. On the other hand, more candidate samples make the selected samples more homogenous, not good for diversity. By grid search, we set K to 3 in our experiments, the experimental results are shown in the supplementary file.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_86",
            "content": "Visualization Effect of EPiDA",
            "ntype": "title",
            "meta": {
                "section": "4.5"
            }
        },
        {
            "ix": "380-ARR_v2_87",
            "content": "Above we give comprehensive quantitative performance evaluation of EPiDA, here to intuitively illustrate the effectiveness of EPiDA, we visualize some augmented samples of EPiDA, and compare them with that of EDA. Specifically, we utilize BERT as the classifier and visualize its hidden state on the sentiment analysis task via t-SNE (Van der Maaten and Hinton, 2008). Fig. 2 shows the results. In terms of data quality, we find that two negative samples generated by EDA are located in Neural and Positive classes, while samples generated by EPiDA are generally properly located. And in the point of view of diversity, samples generated by EPiDA extend the distributed areas of the original data, while samples generated by EDA are mainly located in the areas of the original samples. This shows that samples generated by EPiDA are more diverse than those generated by EDA.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_88",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "380-ARR_v2_89",
            "content": "In this paper, we present an easy plug-in data augmentation technique EPiDA to control augmented data diversity and quality via two mechanisms: relative entropy maximization and conditional entropy minimization. Through extensive experiments, we show that EPiDA outperforms existing methods, and can work well with different DA algorithms and classification models. EPiDA is general, effective, efficient, and easy-to-deploy. In the future, more verification of our method is expected to be conducted on other classification tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "380-ARR_v2_90",
            "content": "UNKNOWN, None, , Naama Tepper, and Naama Zwerdling. 2020. Do not have enough data? deep learning to the rescue! In Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Naama Tepper, and Naama Zwerdling. 2020. Do not have enough data? deep learning to the rescue! In Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_91",
            "content": "UNKNOWN, None, 2019, Deep batch active learning by diverse, uncertain gradient lower bounds, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Deep batch active learning by diverse, uncertain gradient lower bounds",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_92",
            "content": "UNKNOWN, None, 2014, Return of the devil in the details: Delving deep into convolutional nets, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2014",
                "pub_title": "Return of the devil in the details: Delving deep into convolutional nets",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_93",
            "content": "UNKNOWN, None, 2020, Mixtext: Linguistically-informed interpolation of hidden space for semi-supervised text classification, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Mixtext: Linguistically-informed interpolation of hidden space for semi-supervised text classification",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_94",
            "content": "UNKNOWN, None, 2018, Text data augmentation made simple by leveraging nlp cloud apis, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Text data augmentation made simple by leveraging nlp cloud apis",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_95",
            "content": "D Ekin, Barret Cubuk, Dandelion Zoph, Vijay Mane, Quoc V Vasudevan,  Le, Autoaugment: Learning augmentation strategies from data, 2019, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "D Ekin",
                    "Barret Cubuk",
                    "Dandelion Zoph",
                    "Vijay Mane",
                    "Quoc V Vasudevan",
                    " Le"
                ],
                "title": "Autoaugment: Learning augmentation strategies from data",
                "pub_date": "2019",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_96",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_97",
            "content": "Yaxin Xin Luna Dong, Zuohui Zhu, Dongkuan Fu, Gerard Xu,  De Melo, Data augmentation with adversarial training for cross-lingual nli, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Yaxin Xin Luna Dong",
                    "Zuohui Zhu",
                    "Dongkuan Fu",
                    "Gerard Xu",
                    " De Melo"
                ],
                "title": "Data augmentation with adversarial training for cross-lingual nli",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "380-ARR_v2_98",
            "content": "UNKNOWN, None, , Teruko Mitamura, and Eduard Hovy. 2021. A survey of data augmentation approaches for nlp, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Teruko Mitamura, and Eduard Hovy. 2021. A survey of data augmentation approaches for nlp",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_99",
            "content": "Constantinos Antigoni Maria Founta, Despoina Djouvas, Ilias Chatzakou, Jeremy Leontiadis, Gianluca Blackburn, Athena Stringhini, Michael Vakali, Nicolas Sirivianos,  Kourtellis, Large scale crowdsourcing and characterization of twitter abusive behavior, 2018, Twelfth International AAAI Conference on Web and Social Media, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Constantinos Antigoni Maria Founta",
                    "Despoina Djouvas",
                    "Ilias Chatzakou",
                    "Jeremy Leontiadis",
                    "Gianluca Blackburn",
                    "Athena Stringhini",
                    "Michael Vakali",
                    "Nicolas Sirivianos",
                    " Kourtellis"
                ],
                "title": "Large scale crowdsourcing and characterization of twitter abusive behavior",
                "pub_date": "2018",
                "pub_title": "Twelfth International AAAI Conference on Web and Social Media",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_100",
            "content": "Fei Gao, Jinhua Zhu, Lijun Wu, Yingce Xia, Tao Qin, Xueqi Cheng, Wengang Zhou, Tie-Yan Liu, Soft contextual data augmentation for neural machine translation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Fei Gao",
                    "Jinhua Zhu",
                    "Lijun Wu",
                    "Yingce Xia",
                    "Tao Qin",
                    "Xueqi Cheng",
                    "Wengang Zhou",
                    "Tie-Yan Liu"
                ],
                "title": "Soft contextual data augmentation for neural machine translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_101",
            "content": "Akbar Karimi, Leonardo Rossi, Andrea Prati, Aeda: An easier data augmentation technique for text classification, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Akbar Karimi",
                    "Leonardo Rossi",
                    "Andrea Prati"
                ],
                "title": "Aeda: An easier data augmentation technique for text classification",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_102",
            "content": "Yoon Kim, Convolutional neural networks for sentence classification, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Yoon Kim"
                ],
                "title": "Convolutional neural networks for sentence classification",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_103",
            "content": "Sosuke Kobayashi, Contextual augmentation: Data augmentation by words with paradigmatic relations, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Sosuke Kobayashi"
                ],
                "title": "Contextual augmentation: Data augmentation by words with paradigmatic relations",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_104",
            "content": "Oleksandr Kolomiyets, Steven Bethard, Marie Moens, Model-portability experiments for textual temporal analysis, 2011, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Oleksandr Kolomiyets",
                    "Steven Bethard",
                    "Marie Moens"
                ],
                "title": "Model-portability experiments for textual temporal analysis",
                "pub_date": "2011",
                "pub_title": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_105",
            "content": "Alex Krizhevsky, Ilya Sutskever, Geoffrey Hin, Imagenet classification with deep convolutional neural networks, 2012, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Alex Krizhevsky",
                    "Ilya Sutskever",
                    "Geoffrey Hin"
                ],
                "title": "Imagenet classification with deep convolutional neural networks",
                "pub_date": "2012",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_106",
            "content": "Ashutosh Kumar, Satwik Bhattamishra, Manik Bhandari, Partha Talukdar, Submodular optimization-based diverse paraphrasing and its effectiveness in data augmentation, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Ashutosh Kumar",
                    "Satwik Bhattamishra",
                    "Manik Bhandari",
                    "Partha Talukdar"
                ],
                "title": "Submodular optimization-based diverse paraphrasing and its effectiveness in data augmentation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_107",
            "content": "Xin Li, Dan Roth, Learning question classifiers, 2002, COLING 2002: The 19th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Xin Li",
                    "Dan Roth"
                ],
                "title": "Learning question classifiers",
                "pub_date": "2002",
                "pub_title": "COLING 2002: The 19th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_108",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_109",
            "content": "Ruibo Liu, Guangxuan Xu, Chenyan Jia, Weicheng Ma, Lili Wang, Soroush Vosoughi, Data boost: Text data augmentation through reinforcement learning guided conditional generation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Ruibo Liu",
                    "Guangxuan Xu",
                    "Chenyan Jia",
                    "Weicheng Ma",
                    "Lili Wang",
                    "Soroush Vosoughi"
                ],
                "title": "Data boost: Text data augmentation through reinforcement learning guided conditional generation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_110",
            "content": "Canjie Luo, Yuanzhi Zhu, Lianwen Jin, Yongpan Wang, Learn to augment: Joint data augmentation and network optimization for text recognition, 2020, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Canjie Luo",
                    "Yuanzhi Zhu",
                    "Lianwen Jin",
                    "Yongpan Wang"
                ],
                "title": "Learn to augment: Joint data augmentation and network optimization for text recognition",
                "pub_date": "2020",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_111",
            "content": "X John, Eli Morris, Jin Lifland, Jake Yoo, Di Grigsby, Yanjun Jin,  Qi, Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp, 2020, Proceedings of the 2020 EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "X John",
                    "Eli Morris",
                    "Jin Lifland",
                    "Jake Yoo",
                    "Di Grigsby",
                    "Yanjun Jin",
                    " Qi"
                ],
                "title": "Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 EMNLP",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_112",
            "content": "Nathan Ng, Kyunghyun Cho, Marzyeh Ghassemi, Ssmba: Self-supervised manifold based data augmentation for improving out-of-domain robustness, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Nathan Ng",
                    "Kyunghyun Cho",
                    "Marzyeh Ghassemi"
                ],
                "title": "Ssmba: Self-supervised manifold based data augmentation for improving out-of-domain robustness",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_113",
            "content": "Yuyang Nie, Yuanhe Tian, Xiang Wan, Yan Song, Bo Dai, Named entity recognition for social media texts with semantic augmentation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Yuyang Nie",
                    "Yuanhe Tian",
                    "Xiang Wan",
                    "Yan Song",
                    "Bo Dai"
                ],
                "title": "Named entity recognition for social media texts with semantic augmentation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_114",
            "content": "Tong Niu, Mohit Bansal, Adversarial oversensitivity and over-stability strategies for dialogue models, 2018, Proceedings of the 22nd Conference on Computational Natural Language Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Tong Niu",
                    "Mohit Bansal"
                ],
                "title": "Adversarial oversensitivity and over-stability strategies for dialogue models",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 22nd Conference on Computational Natural Language Learning",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_115",
            "content": "Husam Quteineh, Spyridon Samothrakis, Richard Sutcliffe, Textual data augmentation for efficient active learning on tiny datasets, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Husam Quteineh",
                    "Spyridon Samothrakis",
                    "Richard Sutcliffe"
                ],
                "title": "Textual data augmentation for efficient active learning on tiny datasets",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_116",
            "content": "UNKNOWN, None, 2019, Language models are unsupervised multitask learners, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Language models are unsupervised multitask learners",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_117",
            "content": "Sara Rosenthal, Noura Farra, Preslav Nakov, Semeval-2017 task 4: Sentiment analysis in twitter, 2017, Proceedings of the 11th international workshop on semantic evaluation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Sara Rosenthal",
                    "Noura Farra",
                    "Preslav Nakov"
                ],
                "title": "Semeval-2017 task 4: Sentiment analysis in twitter",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 11th international workshop on semantic evaluation",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_118",
            "content": "G\u00f6zde G\u00fcl \u015eahin, Mark Steedman, Data augmentation via dependency tree morphing for lowresource languages, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "G\u00f6zde G\u00fcl \u015eahin",
                    "Mark Steedman"
                ],
                "title": "Data augmentation via dependency tree morphing for lowresource languages",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_119",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Improving neural machine translation models with monolingual data, 2016, Proceedings of the 54th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Rico Sennrich",
                    "Barry Haddow",
                    "Alexandra Birch"
                ],
                "title": "Improving neural machine translation models with monolingual data",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 54th",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_120",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "380-ARR_v2_121",
            "content": "UNKNOWN, None, 2021, Substructure substitution: Structured data augmentation for nlp, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Substructure substitution: Structured data augmentation for nlp",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_122",
            "content": "Connor Shorten, M Taghi,  Khoshgoftaar, A survey on image data augmentation for deep learning, 2019, Journal of Big Data, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Connor Shorten",
                    "M Taghi",
                    " Khoshgoftaar"
                ],
                "title": "A survey on image data augmentation for deep learning",
                "pub_date": "2019",
                "pub_title": "Journal of Big Data",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_123",
            "content": "Chenglei Si, Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Qun Liu, Maosong Sun, Better robustness by more coverage: Adversarial and mixup data augmentation for robust finetuning, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Chenglei Si",
                    "Zhengyan Zhang",
                    "Fanchao Qi",
                    "Zhiyuan Liu",
                    "Yasheng Wang",
                    "Qun Liu",
                    "Maosong Sun"
                ],
                "title": "Better robustness by more coverage: Adversarial and mixup data augmentation for robust finetuning",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_124",
            "content": "Walter Simoncini, Gerasimos Spanakis, Seqattack: On adversarial attacks for named entity recognition, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Walter Simoncini",
                    "Gerasimos Spanakis"
                ],
                "title": "Seqattack: On adversarial attacks for named entity recognition",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_125",
            "content": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, Going deeper with convolutions, 2015, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Christian Szegedy",
                    "Wei Liu",
                    "Yangqing Jia",
                    "Pierre Sermanet",
                    "Scott Reed",
                    "Dragomir Anguelov",
                    "Dumitru Erhan",
                    "Vincent Vanhoucke",
                    "Andrew Rabinovich"
                ],
                "title": "Going deeper with convolutions",
                "pub_date": "2015",
                "pub_title": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_126",
            "content": "Laurens Van Der Maaten, Geoffrey Hinton, Visualizing data using t-sne, 2008, Journal of machine learning research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Laurens Van Der Maaten",
                    "Geoffrey Hinton"
                ],
                "title": "Visualizing data using t-sne",
                "pub_date": "2008",
                "pub_title": "Journal of machine learning research",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_127",
            "content": "Cynthia Van Hee, Els Lefever, V\u00e9ronique Hoste, Semeval-2018 task 3: Irony detection in english tweets, 2018, Proceedings of The 12th International Workshop on Semantic Evaluation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Cynthia Van Hee",
                    "Els Lefever",
                    "V\u00e9ronique Hoste"
                ],
                "title": "Semeval-2018 task 3: Irony detection in english tweets",
                "pub_date": "2018",
                "pub_title": "Proceedings of The 12th International Workshop on Semantic Evaluation",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_128",
            "content": "Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, Yoshua Bengio, Manifold mixup: Better representations by interpolating hidden states, 2019, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Vikas Verma",
                    "Alex Lamb",
                    "Christopher Beckham",
                    "Amir Najafi",
                    "Ioannis Mitliagkas",
                    "David Lopez-Paz",
                    "Yoshua Bengio"
                ],
                "title": "Manifold mixup: Better representations by interpolating hidden states",
                "pub_date": "2019",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "380-ARR_v2_129",
            "content": "Yang William, Diyi Wang,  Yang, That's so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using# petpeeve tweets, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Yang William",
                    "Diyi Wang",
                    " Yang"
                ],
                "title": "That's so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using# petpeeve tweets",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_130",
            "content": "UNKNOWN, None, 2021, Text augmentation in a multi-task view, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Text augmentation in a multi-task view",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_131",
            "content": "UNKNOWN, None, 2019, Eda: Easy data augmentation techniques for boosting performance on text classification tasks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Eda: Easy data augmentation techniques for boosting performance on text classification tasks",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_132",
            "content": "UNKNOWN, None, 2017, Data noising as smoothing in neural network language models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Data noising as smoothing in neural network language models",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_133",
            "content": "Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ji-Ping Ronan Le Bras, Chandra Wang, Yejin Bhagavatula, Doug Choi,  Downey, G-daug: Generative data augmentation for commonsense reasoning, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Yiben Yang",
                    "Chaitanya Malaviya",
                    "Jared Fernandez",
                    "Swabha Swayamdipta",
                    "Ji-Ping Ronan Le Bras",
                    "Chandra Wang",
                    "Yejin Bhagavatula",
                    "Doug Choi",
                    " Downey"
                ],
                "title": "G-daug: Generative data augmentation for commonsense reasoning",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_134",
            "content": "UNKNOWN, None, 2019, Xlnet: Generalized autoregressive pretraining for language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Xlnet: Generalized autoregressive pretraining for language understanding",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_135",
            "content": "UNKNOWN, None, 2018, Qanet: Combining local convolution with global self-attention for reading comprehension, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Qanet: Combining local convolution with global self-attention for reading comprehension",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_136",
            "content": "Sangdoo Yun, Dongyoon Han, Sanghyuk Seong Joon Oh, Junsuk Chun, Youngjoon Choe,  Yoo, Cutmix: Regularization strategy to train strong classifiers with localizable features, 2019, Proceedings of the IEEE/CVF International Conference on Computer Vision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Sangdoo Yun",
                    "Dongyoon Han",
                    "Sanghyuk Seong Joon Oh",
                    "Junsuk Chun",
                    "Youngjoon Choe",
                    " Yoo"
                ],
                "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
                "pub_date": "2019",
                "pub_title": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_137",
            "content": "UNKNOWN, None, 2017, mixup: Beyond empirical risk minimization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "mixup: Beyond empirical risk minimization",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_138",
            "content": "Lu Zhang, Jiandong Ding, Yi Xu, Yingyao Liu, Shuigeng Zhou, Weakly-supervised text classification based on keyword graph, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Lu Zhang",
                    "Jiandong Ding",
                    "Yi Xu",
                    "Yingyao Liu",
                    "Shuigeng Zhou"
                ],
                "title": "Weakly-supervised text classification based on keyword graph",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_139",
            "content": "Xiang Zhang, Junbo Zhao, Yann Lecun, Character-level convolutional networks for text classification, 2015, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Xiang Zhang",
                    "Junbo Zhao",
                    "Yann Lecun"
                ],
                "title": "Character-level convolutional networks for text classification",
                "pub_date": "2015",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_140",
            "content": "Kun Zhou, Wayne Zhao, Sirui Wang, Fuzheng Zhang, Wei Wu, Ji-Rong Wen, Virtual data augmentation: A robust and general framework for fine-tuning pre-trained models, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Kun Zhou",
                    "Wayne Zhao",
                    "Sirui Wang",
                    "Fuzheng Zhang",
                    "Wei Wu",
                    "Ji-Rong Wen"
                ],
                "title": "Virtual data augmentation: A robust and general framework for fine-tuning pre-trained models",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "380-ARR_v2_141",
            "content": "UNKNOWN, None, 2021, Learnda: Learnable knowledge-guided data augmentation for event causality identification, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Learnda: Learnable knowledge-guided data augmentation for event causality identification",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "380-ARR_v2_0@0",
            "content": "EPiDA: An Easy Plug-in Data Augmentation Framework for High Performance Text Classification",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_0",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_2@0",
            "content": "Recent works have empirically shown the effectiveness of data augmentation (DA) for NLP tasks, especially for those suffering from data scarcity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_2",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_2@1",
            "content": "Intuitively, given the size of generated data, their diversity and quality are crucial to the performance of targeted tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_2",
            "start": 146,
            "end": 269,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_2@2",
            "content": "However, to the best of our knowledge, most existing methods consider only either the diversity or the quality of augmented data, thus cannot fully tap the potential of DA for NLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_2",
            "start": 271,
            "end": 450,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_2@3",
            "content": "In this paper, we present an easy and plug-in data augmentation framework EPiDA to support effective text classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_2",
            "start": 452,
            "end": 572,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_2@4",
            "content": "EPiDA employs two mechanisms: relative entropy maximization (REM) and conditional entropy minimization (CEM) to control data generation, where REM is designed to enhance the diversity of augmented data while CEM is exploited to ensure their semantic consistency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_2",
            "start": 574,
            "end": 835,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_2@5",
            "content": "EPiDA can support efficient and continuous data generation for effective classifier training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_2",
            "start": 837,
            "end": 929,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_2@6",
            "content": "Extensive experiments show that EP-iDA outperforms existing SOTA methods in most cases, though not using any agent network or pre-trained generation network, and it works well with various DA algorithms and classification models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_2",
            "start": 931,
            "end": 1159,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_2@7",
            "content": "Code is available at https: //github.com/zhaominyiz/EPiDA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_2",
            "start": 1161,
            "end": 1218,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_4@0",
            "content": "Data augmentation (DA) is widely-used in classification tasks (Shorten and Khoshgoftaar, 2019;Feng et al., 2021;. In computer vision (CV), (Krizhevsky et al., 2012;Chatfield et al., 2014;Szegedy et al., 2015) adopt strategies like flipping, cropping, tilting to perform DA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_4",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_4@1",
            "content": "In natural language processing (NLP), (Xie et al., 2017;Coulombe, 2018;Niu and Bansal, 2018;Wei and Zou, 2019) find that native augmentation skills such as spelling errors, synonym replacement, deleting and swapping, can bring considerable performance improvement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_4",
            "start": 274,
            "end": 537,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_4@2",
            "content": "All these methods use * Corresponding author.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_4",
            "start": 539,
            "end": 583,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_5@0",
            "content": "various transformations for data augmentation, but they do not achieve equal success in different NLP tasks .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_5",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_5@1",
            "content": "Sometimes, they fail to guarantee semantic consistency, and may even bring semantic errors that are harmful to classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_5",
            "start": 110,
            "end": 235,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_5@2",
            "content": "The reason lies in that data augmentation for NLP is in discrete space, so it can easily incur large deviation of semantics (e.g. in sentiment classification task, deleting emotional words from a sentence will make its meaning completely different).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_5",
            "start": 237,
            "end": 485,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_6@0",
            "content": "Generally, given the size of generated data, their diversity and quality are crucial to the performance of targeted tasks (Ash et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_6",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_6@1",
            "content": "Recent works have begun to emphasize the diversity or quality of augmented data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_6",
            "start": 142,
            "end": 221,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_6@2",
            "content": "For example, in CV, AA (Cubuk et al., 2019), Fast-AA (Lim et al., 2019) and LTA (Luo et al., 2020) employ agent networks to learn how to enhance diversity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_6",
            "start": 223,
            "end": 377,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_6@3",
            "content": "In NLP, language models are widely used to control generation quality, including Back-translation (Sennrich et al., 2016;Yu et al., 2018), Seq2seq models (Kobayashi, 2018;Kumar et al., 2019;, GPT-2 (Radford et al., 2019;Anaby-Tavor et al., 2020;Quteineh et al., 2020;Liu et al., 2020) and T5 (Dong et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_6",
            "start": 379,
            "end": 690,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_6@4",
            "content": "In addition, some works (Morris et al., 2020) in NLP utilize adversarial augmentation to enrich the diversity of the samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_6",
            "start": 692,
            "end": 816,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_6@5",
            "content": "However, to the best of our knowledge, most existing works consider only either the quality or the diversity of augmented data, so cannot fully exploit the potential of data augmentation for NLP tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_6",
            "start": 818,
            "end": 1018,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_6@6",
            "content": "Besides, recent existing DA methods for NLP tasks usually resort to pre-trained language models, are extremely inefficient due to huge model complexity and tedious finetuning, which limits the scope of their applications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_6",
            "start": 1020,
            "end": 1240,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_7@0",
            "content": "In this paper, we propose a new data augmentation framework for text classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_7",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_7@1",
            "content": "This framework is called EPiDA (the abbreviation of Easy Plug-in Data Augmentation), which employs two mechanisms to control the diversity and quality of augmented data: relative entropy maximization (REM) and conditional entropy minimization (CEM), where the former is for boosting diversity while the latter for ensuring quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_7",
            "start": 85,
            "end": 415,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_7@2",
            "content": "Fig. 1 shows the pipeline of EPiDA The main contributions of this paper are as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_7",
            "start": 417,
            "end": 503,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_8@0",
            "content": "1. We propose an easy plug-in data augmentation framework EPiDA for text classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_8",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_9@0",
            "content": "EPiDA can work with various existing DA algorithms and classification models, it is general, efficient, and easy-to-deploy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_9",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_10@0",
            "content": "2. We design two mechanisms relative entropy maximization (REM) and conditional entropy minimization (CEM) to boost the diversity and quality of augmented data simultaneously in an explicit and controllable way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_10",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_11@0",
            "content": "3. We conduct extensive experiments to evaluate EPiDA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_11",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_11@1",
            "content": "Experimental results show that EP-iDA outperforms existing DA methods, and works well with different DA algorithms and classification models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_11",
            "start": 55,
            "end": 195,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_12@0",
            "content": "The rest of this paper is organized as follows: Sec. 2 reviews related work and highlights the differences between our work and major existing methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_12",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_12@1",
            "content": "Sec. 3 introduce our method in details.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_12",
            "start": 152,
            "end": 190,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_12@2",
            "content": "Sec. 4 presents the results of performance evaluation, and Sec. 5 concludes the paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_12",
            "start": 192,
            "end": 277,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_13@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_13",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_14@0",
            "content": "In this section, we first review the related work of DA for NLP, then expound the differences between our method and the major existing ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_14",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_14@1",
            "content": "According to the methodology of data generation, existing methods can be categorized into three types: rule-based, interpolation-based, and model-based, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_14",
            "start": 142,
            "end": 307,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_15@0",
            "content": "Rule-Based Methods",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_15",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_16@0",
            "content": "These works use easy and predetermined transformations without model components.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_16",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_16@1",
            "content": "(Kolomiyets et al., 2011;Zhang et al., 2015;Wang and Yang, 2015) use synonyms to replace words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_16",
            "start": 81,
            "end": 175,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_16@2",
            "content": "EDA (Wei and Zou, 2019) and AEDA (Karimi et al., 2021) introduce random insertions, swaps, and deletions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_16",
            "start": 177,
            "end": 281,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_16@3",
            "content": "Xie et al. (2017) employed spelling errors to augment sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_16",
            "start": 283,
            "end": 346,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_16@4",
            "content": "\u015eahin and Steedman (2018) conducted sentence rotating via dependency tree morphing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_16",
            "start": 348,
            "end": 430,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_16@5",
            "content": "proposed a multi-task view of DA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_16",
            "start": 432,
            "end": 464,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_16@6",
            "content": "SUB 2 (Shi et al., 2021) generates new examples by substituting substructures via constituency parse trees.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_16",
            "start": 466,
            "end": 572,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_16@7",
            "content": "Although these methods are easy to implement, they do not consider controlling data quality and diversity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_16",
            "start": 574,
            "end": 679,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_17@0",
            "content": "Interpolation-Based Methods",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_17",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_18@0",
            "content": "MIXUP (Zhang et al., 2017) pioneers this type of works by interpolating the input and labels of two or more real examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_18",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_18@1",
            "content": "Recently, many MIXUP strategies (Verma et al., 2019;Yun et al., 2019) were proposed in CV.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_18",
            "start": 123,
            "end": 212,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_18@2",
            "content": "Due to the discrete nature of inputs of NLP tasks, such methods can be applied to NLP tasks only via padding and mixing embeddings or higher hidden layers (Chen et al., 2020;Si et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_18",
            "start": 214,
            "end": 404,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_19@0",
            "content": "Model-Based Methods",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_19",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_20@0",
            "content": "Seq2seq and language models have been used to generate high quality samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_20",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_20@1",
            "content": "Among these approaches, Back-translation (Sennrich et al., 2016;Yu et al., 2018) translates sentences into another language and then translates it back to the original language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_20",
            "start": 77,
            "end": 253,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_20@2",
            "content": "RNNs and transformers are used to reconstruct sub-parts of real data with contextual information (Kobayashi, 2018;Gao et al., 2019;. Recently, we have witnessed the great success of large-scale pre-trained language models (PLMs) such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), GPT-2 (Radford et al., 2019) are also widely used to augment samples (Ng et al., 2020;Nie et al., 2020;Anaby-Tavor et al., 2020;Quteineh et al., 2020;Liu et al., 2020;Dong et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_20",
            "start": 255,
            "end": 732,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_20@3",
            "content": "For example, DataBoost (Liu et al., 2020) develops a reinforcement learning strategy to guide the conditional generation without changing the architecture of GPT-2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_20",
            "start": 734,
            "end": 897,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_20@4",
            "content": "Besides, adversarial augmentation (i.e., attack, GANs) are also used to enrich the diversity of the generated samples (Morris et al., 2020;Simoncini and Spanakis, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_20",
            "start": 899,
            "end": 1067,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_20@5",
            "content": "Although model-based methods can control generation quality well via PLMs, they are computationally inefficient, which limits their applications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_20",
            "start": 1069,
            "end": 1213,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_21@0",
            "content": "Differences between EPiDA and Existing Methods",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_21",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_22@0",
            "content": "To expound the differences between EPiDA and typical existing methods, in Tab. 1 we present a qualitative comparison from four dimensions: whether controlling the diversity and quality of the augmented data, whether using pre-trained model (language model or agent network), and whether using the feedback from the classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_22",
            "start": 0,
            "end": 325,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_23@0",
            "content": "As shown in Tab. 1, among the existing methods, most control only either diversity (e.g. AA and EDA) or quality (e.g. DataBoost) of augmented data, thus cannot completely leverage the potential of data augmentation. And most use language model or agent network, which is beneficial to data quality but also inefficient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_23",
            "start": 0,
            "end": 318,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_23@1",
            "content": "Only the recent LearnDA (Zuo et al., 2021) and VDA consider both diversity and quality, and only AA uses feedback of the classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_23",
            "start": 320,
            "end": 451,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_23@2",
            "content": "Our EPiDA addresses both diversity and quality of augmented data via the feedback of the classifier in an explicit and controllable way, without the help of any additional model components, which makes it not only more effective but also more efficient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_23",
            "start": 453,
            "end": 705,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_24@0",
            "content": "Note that in addition to the differences listed in Tab. 1, our method EPiDA differs from LearnDA in at least three other aspects: 1) LearnDA employs perplexity score (PPL) and cosine similarity to measure diversity and quality respectively, while EPiDA adopts two mechanisms relative entropy maximization (REM) and conditional entropy minimization (CEM) to control diversity and quality, which is theoretically more rational and solid.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_24",
            "start": 0,
            "end": 434,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_24@1",
            "content": "2) LearnDA is for event causality identification, while EPiDA is mainly for text classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_24",
            "start": 436,
            "end": 531,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_24@2",
            "content": "3) LearnDA needs knowledge guidance, while EP-iDA does not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_24",
            "start": 533,
            "end": 591,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_24@3",
            "content": "These make it difficult to evaluate LearnDA in our experimental settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_24",
            "start": 593,
            "end": 665,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_24@4",
            "content": "Thus, we do not conduct performance comparison between EPiDA and LearnDA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_24",
            "start": 667,
            "end": 739,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_24@5",
            "content": "Nevertheless, in our ablation study, we replace REM and CEM with PPL and cosine similarity in EPiDA, and our experimental results show that EPiDA with REM and CEM performs better than that with PPL and cosine similarity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_24",
            "start": 741,
            "end": 960,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_24@6",
            "content": "Besides, comparing with VDA that requires PLM to provide substitution probability, EPiDA is free of PLMs, and is more effective, efficient and practical.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_24",
            "start": 962,
            "end": 1114,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_25@0",
            "content": "Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_25",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_26@0",
            "content": "As shown in Fig. 1, EPiDA consists of three components: a DA algorithm T , a classifier or classification model C, and a Sample Evaluation and Selection (SEAS) module that is the core component of EPiDA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_26",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_26@1",
            "content": "Generally, the DA algorithm and the classifier can be any of existing DA algorithms and classifiers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_26",
            "start": 204,
            "end": 303,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_26@2",
            "content": "With the feedback of the classifier, SEAS evaluates candidate samples generated by the DA algorithm in terms of diversity and quality via the Relative Entropy Maximization (REM) mechanism and the Conditional Entropy Minimization (CEM) mechanism, and outputs the qualified samples to further train the classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_26",
            "start": 305,
            "end": 616,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_26@3",
            "content": "So EPiDA can serve as a plug-in component to boost existing DA algorithms for training better target models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_26",
            "start": 618,
            "end": 725,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_27@0",
            "content": "The Rationale to Control DA",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_27",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_28@0",
            "content": "Consider a classification task with a dataset X of n samples: X={(x 1 , y 1 ),(x 2 , y 2 ),...,(x n , y n )}.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_28",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_28@1",
            "content": "Here, x i is a sample, y i is its label.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_28",
            "start": 110,
            "end": 149,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_28@2",
            "content": "The loss function is",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_28",
            "start": 151,
            "end": 170,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_29@0",
            "content": "L o (\u03c9) = 1 n n i=1 l(\u03c9 \u22a4 \u03d5(x i ); y i ).(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_29",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_30@0",
            "content": "where \u03d5 : R d \u2192 R D is a finite-dimensional feature map, \u03c9 \u2208 R D means learnable parameters, and l can be a common loss function like cross-entropy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_30",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_30@1",
            "content": "Now we employ a DA algorithm T to conduct augmentation for each sample in X. Let t j i be the j-th sample generated by T with x i as input, and m samples are generated from x i , the loss function for the generated samples can be written as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_30",
            "start": 149,
            "end": 388,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_31@0",
            "content": "L g (\u03c9) = 1 n n i=1 1 m m j=1 l(\u03c9 \u22a4 \u03d5(t j i ); y i ).(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_31",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_32@0",
            "content": "Here, we assume 1) t j i and x i have the same label y i , so we can use y i to optimize the new loss function; 2) Data augmentation does not significantly change the feature map \u03d5, that is, augmentation can maintain semantic consistency of the sample space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_32",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_32@1",
            "content": "Now we combine the augmented samples into the original samples, thus the total loss function of EPiDA can be written as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_32",
            "start": 259,
            "end": 386,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_33@0",
            "content": "L(\u03c9) = L o (\u03c9) + L g (\u03c9).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_33",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_34@0",
            "content": "(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_34",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_35@0",
            "content": "Recall that we use the feedback of the classifier C to select samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_35",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_35@1",
            "content": "Specifically, we use the original training samples X to pre-train the classifier C, and for each generated sample t j i , the feedback signal about t j i from the classifier is used for evaluating t j i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_35",
            "start": 71,
            "end": 274,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_35@2",
            "content": "When the generation process is over, all generated samples {t j i } are used to train C again.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_35",
            "start": 276,
            "end": 369,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_35@3",
            "content": "First, we consider how to generate samples of high diversity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_35",
            "start": 371,
            "end": 431,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_35@4",
            "content": "Intuitively, generated samples should be different from the original samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_35",
            "start": 433,
            "end": 509,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_35@5",
            "content": "Recalling that the classifier C is pretrained by X, so for generated sample t j i , its loss l(\u03c9 \u22a4 \u03d5(t j i ); y i ) should be large.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_35",
            "start": 511,
            "end": 642,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_35@6",
            "content": "In this sense, given the classifier C (\u03c9 is fixed), we select samples that meet the following objective function:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_35",
            "start": 644,
            "end": 756,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_36@0",
            "content": "max t j i L g (\u03c9, \u03d5(t j i )),(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_36",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_37@0",
            "content": "which means that we are to generate \"hard\" samples for the classifier to cope with.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_37",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_37@1",
            "content": "Second, we consider how to control the quality of augmented data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_37",
            "start": 84,
            "end": 148,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_37@2",
            "content": "Recall that we assume for each augmented sample t j i , its label y i keeps unchanged, so we can use the original label to evaluate the loss function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_37",
            "start": 150,
            "end": 299,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_37@3",
            "content": "However, due to the discrete nature of language, it is nontrivial for augmented samples to meet this assumption.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_37",
            "start": 301,
            "end": 412,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_37@4",
            "content": "Taking the sentiment analysis task for example, suppose we use EDA (Wei and Zou, 2019) to augment x i =\"you'll probably love it\", EDA may delete the word \"love\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_37",
            "start": 414,
            "end": 574,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_37@5",
            "content": "Obviously, the resulting sentence breaks the semantic consistency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_37",
            "start": 576,
            "end": 641,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_37@6",
            "content": "To guarantee semantic consistency, we limit the semantic deviation of \u03d5(t j i ) from \u03d5(x i ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_37",
            "start": 643,
            "end": 735,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_37@7",
            "content": "Let M and \u03c1 be a metric function to measure semantic difference between samples and a threshold respectively, we impose the following constraint on \u03d5(t j i ):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_37",
            "start": 737,
            "end": 894,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_38@0",
            "content": "|M (\u03c9 \u22a4 \u03d5(t j i ), \u03c9 \u22a4 \u03d5(x i ))| \u2264 \u03c1.(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_38",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_39@0",
            "content": "Thus, we can enhance data diversity by optimizing Eq. ( 4), and improve data quality using Eq. ( 5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_39",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_39@1",
            "content": "The problem turns to solve Eq. ( 4) and Eq. ( 5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_39",
            "start": 101,
            "end": 149,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_40@0",
            "content": "Relative Entropy Maximization",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_40",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_41@0",
            "content": "We rewrite the objective function in Eq. ( 4) via:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_41",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_42@0",
            "content": "Lg(\u03c9, \u03d5(t j i )) = 1 nm n i=1 m j=1 l(\u03c9 \u22a4 \u03d5(t j i ); yi) = 1 nm n i=1 m j=1 D(p(\u03c9 \u22a4 \u03d5(t j i )), p(yi)) + H(p(yi)),(6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_42",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_43@0",
            "content": "where p, H, D indicate probability distribution, Shannon entropy, and relative entropy respectively, and actually H(p(y i )) = 0 since p(y i ) is a onehot vector.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_43",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_43@1",
            "content": "According to Eq. ( 6), we try to augment samples with large relative entropy under the given labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_43",
            "start": 163,
            "end": 262,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_43@2",
            "content": "Thus, we call this method relative entropy maximization (REM) mechanism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_43",
            "start": 264,
            "end": 335,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_43@3",
            "content": "As relative entropy measures the difference between the two distributions p(\u03c9 \u22a4 \u03d5(t j i )) and p(y i ), the larger the difference is, the more diverse the augmented sample is.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_43",
            "start": 337,
            "end": 511,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_43@4",
            "content": "Therefore, we define the diversity score s ij div of augmented sample t j i as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_43",
            "start": 513,
            "end": 599,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_44@0",
            "content": "s ij div = D(p(\u03c9 \u22a4 \u03d5(t j i )), p(y i )).(7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_44",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_45@0",
            "content": "Conditional Entropy Minimization",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_45",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_46@0",
            "content": "We use conditional entropy as the metric function in Eq. ( 5) to constrain the semantic deviation of",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_46",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_47@0",
            "content": "\u03d5(t j i ) from \u03d5(x i ), i.e., M (\u2022, \u2022) := H(\u2022|\u2022).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_47",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_48@0",
            "content": "Then, Eq. ( 5) can be rewritten to",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_48",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_49@0",
            "content": "H(p(\u03c9 \u22a4 \u03d5(t j i ))|p(\u03c9 \u22a4 \u03d5(x i ))) \u2264 \u03c1. (8",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_49",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_50@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_50",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_51@0",
            "content": "where H(\u2022, \u2022) is conditional entropy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_51",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_51@1",
            "content": "Furthermore, to meet Eq. ( 8), we select samples {t j i } by solving the following optimization problem:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_51",
            "start": 38,
            "end": 141,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_52@0",
            "content": "min t j i H(p(\u03c9 \u22a4 \u03d5(t j i ))|p(\u03c9 \u22a4 \u03d5(x i ))).(9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_52",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_53@0",
            "content": "We call this conditional entropy minimization (CEM) mechanism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_53",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_54@0",
            "content": "The smallest value of",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_54",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_55@0",
            "content": "H(p(\u03c9 \u22a4 \u03d5(t j i ))|p(\u03c9 \u22a4 \u03d5(x i ))) is 0, indicating that given p(\u03c9 \u22a4 \u03d5(x i )), p(\u03c9 \u22a4 \u03d5(t j i )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_55",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_56@0",
            "content": ") is exactly predictable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_56",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_56@1",
            "content": "Eq. ( 9) can also be expanded to the difference between Shannon entropy H and mutual information I, i.e., H(X|Y )=H(X)-I(X, Y ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_56",
            "start": 26,
            "end": 153,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_56@2",
            "content": "In other words, CEM minimizes the entropy of the selected sample t j i and maximizes the mutual information between t j i and the original sample x i , which means that CEM tries to augment samples of high prediction probability and high similarity with the original sample.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_56",
            "start": 155,
            "end": 428,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_56@3",
            "content": "As in REM, we define the quality score s ij qua of augmented sample t j i as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_56",
            "start": 430,
            "end": 505,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_57@0",
            "content": "s ij qua = \u2212H(p(\u03c9 \u22a4 \u03d5(t j i ))|p(\u03c9 \u22a4 \u03d5(x i ))). (10",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_57",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_58@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_58",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_59@0",
            "content": "Algorithm 1: EPiDA Data Augmentation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_59",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_60@0",
            "content": "Input: Classification model \u03c9, input sample x i and its label y i , DA algorithm T , augmentation number m and amplification factor K. Output: m augmented samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_60",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_60@1",
            "content": "// assign the set T (xi) of mK candidates to array ti ti \u2190 T (x i );",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_60",
            "start": 164,
            "end": 231,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_61@0",
            "content": "s div , s qua , s tot = R K * m , R K * m , R K * m ; for j = 1, 2, . . . , K * m do",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_61",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_62@0",
            "content": "Calculate s ij div via Eq. (7) ; Calculate s ij qua via Eq. ( 10) ; end Take Min_Max_Norm for s div and s qua ; s tot = s div + s qua ; // find the subscripts of the top m small elements id = argtopm(\u2212s tot ) ; Return ti [id] ;",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_62",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_63@0",
            "content": "Algorithm and Implementation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_63",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_64@0",
            "content": "The procedure of EPiDA is presented in Alg.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_64",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_64@1",
            "content": "1. For each input sample x i , EPiDA outputs m augmented samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_64",
            "start": 44,
            "end": 108,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_64@2",
            "content": "First, we employ T to generate K * m candidate augmented samples for x i , where K is a hyperparameter to amplify the number of candidate samples, which is called amplification factor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_64",
            "start": 110,
            "end": 293,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_64@3",
            "content": "Then, for each augmented sample, we use REM and CEM to evaluate its diversity score (s div ) and quality score (s qua ), respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_64",
            "start": 295,
            "end": 428,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_64@4",
            "content": "Next, we adopt Min_Max_Norm to make s div and s qua fall in [0,1].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_64",
            "start": 430,
            "end": 495,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_64@5",
            "content": "After that, we add them together as the overall score of the sample, and sort all the augmented samples in descending order according to their scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_64",
            "start": 497,
            "end": 646,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_64@6",
            "content": "Finally, we take the top m samples from all the K * m candidate samples as the output, and utilize them to train the classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_64",
            "start": 648,
            "end": 775,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_64@7",
            "content": "By nature, the goals of REM and CEM are conflicting, i.e., a sample of high diversity is more probably of low quality, and vice versa.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_64",
            "start": 777,
            "end": 910,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_64@8",
            "content": "We give an example in Tab.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_64",
            "start": 912,
            "end": 937,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_64@9",
            "content": "2 to demonstrate this point.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_64",
            "start": 939,
            "end": 966,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_64@10",
            "content": "REM encourages to change salient words, which is prone to break the semantic consistency (see the 3rd row, \"excited\" is changed to \"mad\", leading to large diversity score but small quality score).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_64",
            "start": 968,
            "end": 1163,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_64@11",
            "content": "However, CEM tends to make the augmented samples keep semantic consistency, i.e., has large quality score but small diversity score (see the 4th row, \"comes\" is deleted).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_64",
            "start": 1165,
            "end": 1334,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_64@12",
            "content": "By jointly considering REM and CEM, satisfactory samples with balanced diversity and quality can be found (see the 5th row).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_64",
            "start": 1336,
            "end": 1459,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_65@0",
            "content": "Besides, the calculation of s div and s qua requires the feedback of the classifier, so we first pre-train the classifier using the original samples, then with EPiDA we can generate samples of high diversity and quality for the classifier continuously.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_65",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_66@0",
            "content": "Performance Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_66",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_67@0",
            "content": "In this section, we conduct extensive experiments to evaluate EPiDA, including performance comparison with SOTA methods, performance evaluation when working with different DA algorithms and classification models, ablation study, and qualitative visualization of samples augmented by EPiDA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_67",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_68@0",
            "content": "Datasets and Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_68",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_69@0",
            "content": "Datasets for five different tasks are used in our experiments: Question Classification (Li and Roth, 2002) (TREC, N =5,452), News Classification (Zhang et al., 2015) (AGNews, N =120,000), Tweets Sentiment Analysis (Rosenthal et al., 2017) Macro-F1 (F1 for binary tasks) is used as performance metric, and all the experiments are repeated five times.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_69",
            "start": 0,
            "end": 348,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_69@1",
            "content": "The amplification factor K is set to 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_69",
            "start": 350,
            "end": 388,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_70@0",
            "content": "Comparing with SOTA Methods",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_70",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_71@0",
            "content": "Here we carry out performance comparison with major SOTA methods to show the superiority of EPiDA on three datasets: Sentiment, Irony and Offense.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_71",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_71@1",
            "content": "For a fair comparison, we strictly follow the experimental setting of DataBoost (Liu et al., 2020): we do only one round of augmentation to ensure that the number of samples of our method is consistent with that of the other methods, and use BERT as the classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_71",
            "start": 147,
            "end": 411,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_71@2",
            "content": "We use the widely used EDA as the DA algorithm of EPiDA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_71",
            "start": 413,
            "end": 468,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_71@3",
            "content": "We do not use DataBoost because it is not yet open-sourced.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_71",
            "start": 470,
            "end": 528,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_72@0",
            "content": "The experimental results are presented in Tab.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_72",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_72@1",
            "content": "3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_72",
            "start": 47,
            "end": 48,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_73@0",
            "content": "From Tab. 3, we can see that 1) with the help of EPiDA, the performance of EDA is greatly improved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_73",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_73@1",
            "content": "In particular, comparing with the original EDA, EPiDA gets performance improvement of 14.1%, 8.39%, 22.83%, 33.40%, 6.75% and 9.22% in six task settings, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_73",
            "start": 100,
            "end": 266,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_73@2",
            "content": "2) Our method outperforms DataBoost in four settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_73",
            "start": 268,
            "end": 320,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_73@3",
            "content": "In particular, EPiDA+EDA gets performance improvement of 8.12%, 2.65%, 10.15% and 6.99% in various settings of the Sentiment and Irony tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_73",
            "start": 322,
            "end": 462,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_73@4",
            "content": "3) The variants of EPiDA that utilize only REM or CEM to enhance diversity or quality are inferior to using both, which demonstrates the effectiveness of joint enhancement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_73",
            "start": 464,
            "end": 635,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_73@5",
            "content": "4) DataBoost performs better in the Offense task, the reason lies in that Data-Boost can create novel sentences from Offense (a relatively huge corpus) via GPT-2, while EDA only conducts word-level augmentation, which limits EPiDA's performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_73",
            "start": 637,
            "end": 881,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_73@6",
            "content": "5) We also present PPL as an auxiliary metric to measure the generation perplexity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_73",
            "start": 883,
            "end": 965,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_73@7",
            "content": "Our method outperforms the others due to the high quality of data generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_73",
            "start": 967,
            "end": 1043,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_73@8",
            "content": "We also provide experimental comparisons with other DA approaches (SUB 2 and VDA) and generation speed results in the supplementary file.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_73",
            "start": 1045,
            "end": 1181,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_73@9",
            "content": "In conclusion, EPiDA is a powerful and efficient technique.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_73",
            "start": 1183,
            "end": 1241,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_74@0",
            "content": "Performance with Different DA Algorithms and Classifiers",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_74",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_75@0",
            "content": "EPiDA is a plug-in component that can work with different DA algorithms and classifiers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_75",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_75@1",
            "content": "Here, to check how EPiDA performs with different DA algorithms and classifiers, we consider three frequentlyused DA algorithms: rule-based EDA (Wei and Zou, 2019), model-based CWE (Kobayashi, 2018) and Attack-based TextAttack (Morris et al., 2020), and three different classifiers: CNN (Kim, 2014), BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) cases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_75",
            "start": 89,
            "end": 450,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_75@2",
            "content": "For the three different classification models: CNN, BERT and XLNet, with the help of EPiDA, they all but XLNet on Sentiment get classification performance improvement, which shows that EP-iDA is insensitive to classification models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_75",
            "start": 452,
            "end": 683,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_76@0",
            "content": "Ablation Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_76",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_77@0",
            "content": "Here we conduct ablation study to check the effectiveness of different EPiDA configurations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_77",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_77@1",
            "content": "We take CNN as the classifier, EDA as the DA algorithm and report the Macro-F1 score over five repeated experiments on TREC 1% and Irony 1%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_77",
            "start": 93,
            "end": 232,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_77@2",
            "content": "Tab. 5 shows the experimental results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_77",
            "start": 234,
            "end": 271,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_78@0",
            "content": "Effect of REM and CEM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_78",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_78@1",
            "content": "The 4th and 5th rows show the results with only REM and CEM, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_78",
            "start": 23,
            "end": 96,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_78@2",
            "content": "Both of them perform better than the baseline (1st row), but not as good as the combined case (the 8th row).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_78",
            "start": 98,
            "end": 205,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_78@3",
            "content": "On TREC (relatively simple task), REM outperforms CEM (0.729 vs. 0.723), while on Irony (relatively hard task), CEM outperforms REM (0.559 vs. 0.557).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_78",
            "start": 207,
            "end": 356,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_78@4",
            "content": "Using only REM can limitedly boost performance since REM promotes the generation of high diversity samples, which may have wrong labels. And using only CEM is also not enough to fully tap the performance as CEM tends to generate redundant samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_78",
            "start": 358,
            "end": 604,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_79@0",
            "content": "We also compare our 'REM + CEM' with 'PPL + cosine similarity' used in LDA (Zuo et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_79",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_79@1",
            "content": "Our method achieves the performance of 0.740 and 0.576 on TREC 1% and Irony 1%, while the latter achieves 0.730 and 0.562.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_79",
            "start": 95,
            "end": 216,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_79@2",
            "content": "This shows that our 'REM + CEM' is more effective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_79",
            "start": 218,
            "end": 267,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_80@0",
            "content": "Effect of online augmentation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_80",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_80@1",
            "content": "Comparing the results of the 2nd and the 3rd rows, the 6th and the 7th rows, the 8th and the 9th rows, we can see that generally online augmentation can boost performance, as online augmentation can generate sufficient qualified samples to train the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_80",
            "start": 31,
            "end": 286,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_81@0",
            "content": "Effect of pre-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_81",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_81@1",
            "content": "As REM and CEM use the feedback of the classifier, a pre-trained classification model should be beneficial to REM and CEM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_81",
            "start": 24,
            "end": 145,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_81@2",
            "content": "By comparing the results of the 6th and the 8th rows, the 7th and the 9th rows, it is obvious that pre-training can improve performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_81",
            "start": 147,
            "end": 282,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_82@0",
            "content": "Effect of normalization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_82",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_82@1",
            "content": "In Alg. 1, we normalize s div and s qua .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_82",
            "start": 25,
            "end": 65,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_82@2",
            "content": "Here, we check the effect of normalization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_82",
            "start": 67,
            "end": 109,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_82@3",
            "content": "With the same experimental settings, the performance results on TREC 1% and Irony 1% without normalization are 0.732 and 0.568, lower than the normalized results 0.740 and 0.576.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_82",
            "start": 111,
            "end": 288,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_82@4",
            "content": "This shows that normalization is effective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_82",
            "start": 290,
            "end": 332,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_83@0",
            "content": "How to combine REM and CEM? How to combine REM and CEM is actually how to combine the values of s div and s qua .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_83",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_83@1",
            "content": "We consider three simple schemes: addition (s tot = s div + s qua ), multiplication (s tot = s div * s qua ) and weighted addition (s tot = \u03b1s div + (1 \u2212 \u03b1)s qua , \u03b1 is a hyperparameter to tradeoff REM and CEM).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_83",
            "start": 114,
            "end": 324,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_83@2",
            "content": "Note that for multiplication, there is possibly an extreme situation: after normalization, s div or s qua may be very small and even approaches 0, then the multiplication result is very small or even zero, which means that REM and CEM do not take effect in sample generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_83",
            "start": 326,
            "end": 600,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_83@3",
            "content": "In our experiments, the multiplication scheme achieves performance of 0.725 and 0.572 on TREC 1% and Irony 1%, lower than the addition scheme 0.740 and 0.576.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_83",
            "start": 602,
            "end": 759,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_83@4",
            "content": "As for weighted addition, we find that setting \u03b1 = 0.5 can achieve satisfactory results (see the supplementary file).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_83",
            "start": 761,
            "end": 877,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_83@5",
            "content": "This is actually equal to the addition scheme.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_83",
            "start": 879,
            "end": 924,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_83@6",
            "content": "Therefore, in our experiments, we use only the addition scheme.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_83",
            "start": 926,
            "end": 988,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_84@0",
            "content": "Quality and diversity metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_84",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_84@1",
            "content": "Here, we provide another two metrics to verify EPiDA from the perspective of quality and diversity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_84",
            "start": 31,
            "end": 129,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_84@2",
            "content": "For quality, we use the augmentation error rate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_84",
            "start": 131,
            "end": 178,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_84@3",
            "content": "As for diversity, we calculate the average distance of samples before and after augmentation (ignoring wrong samples).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_84",
            "start": 180,
            "end": 297,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_84@4",
            "content": "From the perspective of quality and diversity, a good DA should has a small error rate but a large distance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_84",
            "start": 299,
            "end": 406,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_84@5",
            "content": "Experimental results are given in Tab.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_84",
            "start": 408,
            "end": 445,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_84@6",
            "content": "6. We can see that EPiDA gets better trade-off between error rate and distance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_84",
            "start": 447,
            "end": 525,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_85@0",
            "content": "Effect of the amplification factor K. The amplification factor K determines the size Km of candidate samples from which m samples are chosen.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_85",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_85@1",
            "content": "On the one hand, with a large K, we have more choices, which seems beneficial to diversity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_85",
            "start": 142,
            "end": 232,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_85@2",
            "content": "On the other hand, more candidate samples make the selected samples more homogenous, not good for diversity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_85",
            "start": 234,
            "end": 341,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_85@3",
            "content": "By grid search, we set K to 3 in our experiments, the experimental results are shown in the supplementary file.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_85",
            "start": 343,
            "end": 453,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_86@0",
            "content": "Visualization Effect of EPiDA",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_86",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_87@0",
            "content": "Above we give comprehensive quantitative performance evaluation of EPiDA, here to intuitively illustrate the effectiveness of EPiDA, we visualize some augmented samples of EPiDA, and compare them with that of EDA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_87",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_87@1",
            "content": "Specifically, we utilize BERT as the classifier and visualize its hidden state on the sentiment analysis task via t-SNE (Van der Maaten and Hinton, 2008).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_87",
            "start": 214,
            "end": 367,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_87@2",
            "content": "Fig. 2 shows the results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_87",
            "start": 369,
            "end": 393,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_87@3",
            "content": "In terms of data quality, we find that two negative samples generated by EDA are located in Neural and Positive classes, while samples generated by EPiDA are generally properly located.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_87",
            "start": 395,
            "end": 579,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_87@4",
            "content": "And in the point of view of diversity, samples generated by EPiDA extend the distributed areas of the original data, while samples generated by EDA are mainly located in the areas of the original samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_87",
            "start": 581,
            "end": 784,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_87@5",
            "content": "This shows that samples generated by EPiDA are more diverse than those generated by EDA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_87",
            "start": 786,
            "end": 873,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_88@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_88",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_89@0",
            "content": "In this paper, we present an easy plug-in data augmentation technique EPiDA to control augmented data diversity and quality via two mechanisms: relative entropy maximization and conditional entropy minimization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_89",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_89@1",
            "content": "Through extensive experiments, we show that EPiDA outperforms existing methods, and can work well with different DA algorithms and classification models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_89",
            "start": 212,
            "end": 364,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_89@2",
            "content": "EPiDA is general, effective, efficient, and easy-to-deploy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_89",
            "start": 366,
            "end": 424,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_89@3",
            "content": "In the future, more verification of our method is expected to be conducted on other classification tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_89",
            "start": 426,
            "end": 530,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_90@0",
            "content": "UNKNOWN, None, , Naama Tepper, and Naama Zwerdling. 2020. Do not have enough data? deep learning to the rescue! In Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_90",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_91@0",
            "content": "UNKNOWN, None, 2019, Deep batch active learning by diverse, uncertain gradient lower bounds, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_91",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_92@0",
            "content": "UNKNOWN, None, 2014, Return of the devil in the details: Delving deep into convolutional nets, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_92",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_93@0",
            "content": "UNKNOWN, None, 2020, Mixtext: Linguistically-informed interpolation of hidden space for semi-supervised text classification, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_93",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_94@0",
            "content": "UNKNOWN, None, 2018, Text data augmentation made simple by leveraging nlp cloud apis, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_94",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_95@0",
            "content": "D Ekin, Barret Cubuk, Dandelion Zoph, Vijay Mane, Quoc V Vasudevan,  Le, Autoaugment: Learning augmentation strategies from data, 2019, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_95",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_96@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_96",
            "start": 0,
            "end": 294,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_97@0",
            "content": "Yaxin Xin Luna Dong, Zuohui Zhu, Dongkuan Fu, Gerard Xu,  De Melo, Data augmentation with adversarial training for cross-lingual nli, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_97",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_98@0",
            "content": "UNKNOWN, None, , Teruko Mitamura, and Eduard Hovy. 2021. A survey of data augmentation approaches for nlp, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_98",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_99@0",
            "content": "Constantinos Antigoni Maria Founta, Despoina Djouvas, Ilias Chatzakou, Jeremy Leontiadis, Gianluca Blackburn, Athena Stringhini, Michael Vakali, Nicolas Sirivianos,  Kourtellis, Large scale crowdsourcing and characterization of twitter abusive behavior, 2018, Twelfth International AAAI Conference on Web and Social Media, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_99",
            "start": 0,
            "end": 323,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_100@0",
            "content": "Fei Gao, Jinhua Zhu, Lijun Wu, Yingce Xia, Tao Qin, Xueqi Cheng, Wengang Zhou, Tie-Yan Liu, Soft contextual data augmentation for neural machine translation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_100",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_101@0",
            "content": "Akbar Karimi, Leonardo Rossi, Andrea Prati, Aeda: An easier data augmentation technique for text classification, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_101",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_102@0",
            "content": "Yoon Kim, Convolutional neural networks for sentence classification, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_102",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_103@0",
            "content": "Sosuke Kobayashi, Contextual augmentation: Data augmentation by words with paradigmatic relations, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_103",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_104@0",
            "content": "Oleksandr Kolomiyets, Steven Bethard, Marie Moens, Model-portability experiments for textual temporal analysis, 2011, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_104",
            "start": 0,
            "end": 236,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_105@0",
            "content": "Alex Krizhevsky, Ilya Sutskever, Geoffrey Hin, Imagenet classification with deep convolutional neural networks, 2012, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_105",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_106@0",
            "content": "Ashutosh Kumar, Satwik Bhattamishra, Manik Bhandari, Partha Talukdar, Submodular optimization-based diverse paraphrasing and its effectiveness in data augmentation, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_106",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_107@0",
            "content": "Xin Li, Dan Roth, Learning question classifiers, 2002, COLING 2002: The 19th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_107",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_108@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_108",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_109@0",
            "content": "Ruibo Liu, Guangxuan Xu, Chenyan Jia, Weicheng Ma, Lili Wang, Soroush Vosoughi, Data boost: Text data augmentation through reinforcement learning guided conditional generation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_109",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_110@0",
            "content": "Canjie Luo, Yuanzhi Zhu, Lianwen Jin, Yongpan Wang, Learn to augment: Joint data augmentation and network optimization for text recognition, 2020, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_110",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_111@0",
            "content": "X John, Eli Morris, Jin Lifland, Jake Yoo, Di Grigsby, Yanjun Jin,  Qi, Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp, 2020, Proceedings of the 2020 EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_111",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_112@0",
            "content": "Nathan Ng, Kyunghyun Cho, Marzyeh Ghassemi, Ssmba: Self-supervised manifold based data augmentation for improving out-of-domain robustness, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_112",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_113@0",
            "content": "Yuyang Nie, Yuanhe Tian, Xiang Wan, Yan Song, Bo Dai, Named entity recognition for social media texts with semantic augmentation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_113",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_114@0",
            "content": "Tong Niu, Mohit Bansal, Adversarial oversensitivity and over-stability strategies for dialogue models, 2018, Proceedings of the 22nd Conference on Computational Natural Language Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_114",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_115@0",
            "content": "Husam Quteineh, Spyridon Samothrakis, Richard Sutcliffe, Textual data augmentation for efficient active learning on tiny datasets, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_115",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_116@0",
            "content": "UNKNOWN, None, 2019, Language models are unsupervised multitask learners, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_116",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_117@0",
            "content": "Sara Rosenthal, Noura Farra, Preslav Nakov, Semeval-2017 task 4: Sentiment analysis in twitter, 2017, Proceedings of the 11th international workshop on semantic evaluation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_117",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_118@0",
            "content": "G\u00f6zde G\u00fcl \u015eahin, Mark Steedman, Data augmentation via dependency tree morphing for lowresource languages, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_118",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_119@0",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Improving neural machine translation models with monolingual data, 2016, Proceedings of the 54th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_119",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_120@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_120",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_121@0",
            "content": "UNKNOWN, None, 2021, Substructure substitution: Structured data augmentation for nlp, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_121",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_122@0",
            "content": "Connor Shorten, M Taghi,  Khoshgoftaar, A survey on image data augmentation for deep learning, 2019, Journal of Big Data, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_122",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_123@0",
            "content": "Chenglei Si, Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Qun Liu, Maosong Sun, Better robustness by more coverage: Adversarial and mixup data augmentation for robust finetuning, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_123",
            "start": 0,
            "end": 271,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_124@0",
            "content": "Walter Simoncini, Gerasimos Spanakis, Seqattack: On adversarial attacks for named entity recognition, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_124",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_125@0",
            "content": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, Going deeper with convolutions, 2015, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_125",
            "start": 0,
            "end": 260,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_126@0",
            "content": "Laurens Van Der Maaten, Geoffrey Hinton, Visualizing data using t-sne, 2008, Journal of machine learning research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_126",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_127@0",
            "content": "Cynthia Van Hee, Els Lefever, V\u00e9ronique Hoste, Semeval-2018 task 3: Irony detection in english tweets, 2018, Proceedings of The 12th International Workshop on Semantic Evaluation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_127",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_128@0",
            "content": "Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, Yoshua Bengio, Manifold mixup: Better representations by interpolating hidden states, 2019, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_128",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_129@0",
            "content": "Yang William, Diyi Wang,  Yang, That's so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using# petpeeve tweets, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_129",
            "start": 0,
            "end": 295,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_130@0",
            "content": "UNKNOWN, None, 2021, Text augmentation in a multi-task view, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_130",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_131@0",
            "content": "UNKNOWN, None, 2019, Eda: Easy data augmentation techniques for boosting performance on text classification tasks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_131",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_132@0",
            "content": "UNKNOWN, None, 2017, Data noising as smoothing in neural network language models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_132",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_133@0",
            "content": "Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ji-Ping Ronan Le Bras, Chandra Wang, Yejin Bhagavatula, Doug Choi,  Downey, G-daug: Generative data augmentation for commonsense reasoning, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_133",
            "start": 0,
            "end": 313,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_134@0",
            "content": "UNKNOWN, None, 2019, Xlnet: Generalized autoregressive pretraining for language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_134",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_135@0",
            "content": "UNKNOWN, None, 2018, Qanet: Combining local convolution with global self-attention for reading comprehension, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_135",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_136@0",
            "content": "Sangdoo Yun, Dongyoon Han, Sanghyuk Seong Joon Oh, Junsuk Chun, Youngjoon Choe,  Yoo, Cutmix: Regularization strategy to train strong classifiers with localizable features, 2019, Proceedings of the IEEE/CVF International Conference on Computer Vision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_136",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_137@0",
            "content": "UNKNOWN, None, 2017, mixup: Beyond empirical risk minimization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_137",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_138@0",
            "content": "Lu Zhang, Jiandong Ding, Yi Xu, Yingyao Liu, Shuigeng Zhou, Weakly-supervised text classification based on keyword graph, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_138",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_139@0",
            "content": "Xiang Zhang, Junbo Zhao, Yann Lecun, Character-level convolutional networks for text classification, 2015, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_139",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_140@0",
            "content": "Kun Zhou, Wayne Zhao, Sirui Wang, Fuzheng Zhang, Wei Wu, Ji-Rong Wen, Virtual data augmentation: A robust and general framework for fine-tuning pre-trained models, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_140",
            "start": 0,
            "end": 258,
            "label": {}
        },
        {
            "ix": "380-ARR_v2_141@0",
            "content": "UNKNOWN, None, 2021, Learnda: Learnable knowledge-guided data augmentation for event causality identification, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "380-ARR_v2_141",
            "start": 0,
            "end": 111,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "380-ARR_v2_0",
            "tgt_ix": "380-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_0",
            "tgt_ix": "380-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_1",
            "tgt_ix": "380-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_1",
            "tgt_ix": "380-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_0",
            "tgt_ix": "380-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_2",
            "tgt_ix": "380-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_4",
            "tgt_ix": "380-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_5",
            "tgt_ix": "380-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_6",
            "tgt_ix": "380-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_7",
            "tgt_ix": "380-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_8",
            "tgt_ix": "380-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_9",
            "tgt_ix": "380-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_10",
            "tgt_ix": "380-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_11",
            "tgt_ix": "380-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_3",
            "tgt_ix": "380-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_3",
            "tgt_ix": "380-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_3",
            "tgt_ix": "380-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_3",
            "tgt_ix": "380-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_3",
            "tgt_ix": "380-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_3",
            "tgt_ix": "380-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_3",
            "tgt_ix": "380-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_3",
            "tgt_ix": "380-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_3",
            "tgt_ix": "380-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_3",
            "tgt_ix": "380-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_0",
            "tgt_ix": "380-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_12",
            "tgt_ix": "380-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_13",
            "tgt_ix": "380-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_13",
            "tgt_ix": "380-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_13",
            "tgt_ix": "380-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_14",
            "tgt_ix": "380-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_15",
            "tgt_ix": "380-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_15",
            "tgt_ix": "380-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_13",
            "tgt_ix": "380-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_16",
            "tgt_ix": "380-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_17",
            "tgt_ix": "380-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_17",
            "tgt_ix": "380-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_13",
            "tgt_ix": "380-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_18",
            "tgt_ix": "380-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_19",
            "tgt_ix": "380-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_19",
            "tgt_ix": "380-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_13",
            "tgt_ix": "380-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_20",
            "tgt_ix": "380-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_22",
            "tgt_ix": "380-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_23",
            "tgt_ix": "380-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_21",
            "tgt_ix": "380-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_21",
            "tgt_ix": "380-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_21",
            "tgt_ix": "380-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_21",
            "tgt_ix": "380-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_0",
            "tgt_ix": "380-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_24",
            "tgt_ix": "380-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_25",
            "tgt_ix": "380-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_25",
            "tgt_ix": "380-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_25",
            "tgt_ix": "380-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_26",
            "tgt_ix": "380-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_28",
            "tgt_ix": "380-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_29",
            "tgt_ix": "380-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_30",
            "tgt_ix": "380-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_31",
            "tgt_ix": "380-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_32",
            "tgt_ix": "380-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_33",
            "tgt_ix": "380-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_34",
            "tgt_ix": "380-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_35",
            "tgt_ix": "380-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_36",
            "tgt_ix": "380-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_37",
            "tgt_ix": "380-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_38",
            "tgt_ix": "380-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_27",
            "tgt_ix": "380-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_27",
            "tgt_ix": "380-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_27",
            "tgt_ix": "380-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_27",
            "tgt_ix": "380-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_27",
            "tgt_ix": "380-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_27",
            "tgt_ix": "380-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_27",
            "tgt_ix": "380-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_27",
            "tgt_ix": "380-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_27",
            "tgt_ix": "380-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_27",
            "tgt_ix": "380-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_27",
            "tgt_ix": "380-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_27",
            "tgt_ix": "380-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_27",
            "tgt_ix": "380-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_25",
            "tgt_ix": "380-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_39",
            "tgt_ix": "380-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_41",
            "tgt_ix": "380-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_42",
            "tgt_ix": "380-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_43",
            "tgt_ix": "380-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_40",
            "tgt_ix": "380-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_40",
            "tgt_ix": "380-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_40",
            "tgt_ix": "380-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_40",
            "tgt_ix": "380-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_40",
            "tgt_ix": "380-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_25",
            "tgt_ix": "380-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_44",
            "tgt_ix": "380-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_46",
            "tgt_ix": "380-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_47",
            "tgt_ix": "380-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_48",
            "tgt_ix": "380-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_49",
            "tgt_ix": "380-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_50",
            "tgt_ix": "380-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_51",
            "tgt_ix": "380-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_52",
            "tgt_ix": "380-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_53",
            "tgt_ix": "380-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_54",
            "tgt_ix": "380-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_55",
            "tgt_ix": "380-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_56",
            "tgt_ix": "380-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_57",
            "tgt_ix": "380-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_58",
            "tgt_ix": "380-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_59",
            "tgt_ix": "380-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_60",
            "tgt_ix": "380-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_61",
            "tgt_ix": "380-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_25",
            "tgt_ix": "380-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_62",
            "tgt_ix": "380-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_64",
            "tgt_ix": "380-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_63",
            "tgt_ix": "380-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_63",
            "tgt_ix": "380-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_63",
            "tgt_ix": "380-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_0",
            "tgt_ix": "380-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_65",
            "tgt_ix": "380-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_66",
            "tgt_ix": "380-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_66",
            "tgt_ix": "380-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_66",
            "tgt_ix": "380-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_67",
            "tgt_ix": "380-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_68",
            "tgt_ix": "380-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_68",
            "tgt_ix": "380-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_66",
            "tgt_ix": "380-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_69",
            "tgt_ix": "380-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_71",
            "tgt_ix": "380-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_72",
            "tgt_ix": "380-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_70",
            "tgt_ix": "380-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_70",
            "tgt_ix": "380-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_70",
            "tgt_ix": "380-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_70",
            "tgt_ix": "380-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_66",
            "tgt_ix": "380-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_73",
            "tgt_ix": "380-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_74",
            "tgt_ix": "380-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_74",
            "tgt_ix": "380-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_66",
            "tgt_ix": "380-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_75",
            "tgt_ix": "380-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_77",
            "tgt_ix": "380-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_78",
            "tgt_ix": "380-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_79",
            "tgt_ix": "380-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_80",
            "tgt_ix": "380-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_81",
            "tgt_ix": "380-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_82",
            "tgt_ix": "380-ARR_v2_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_83",
            "tgt_ix": "380-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_84",
            "tgt_ix": "380-ARR_v2_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_76",
            "tgt_ix": "380-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_76",
            "tgt_ix": "380-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_76",
            "tgt_ix": "380-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_76",
            "tgt_ix": "380-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_76",
            "tgt_ix": "380-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_76",
            "tgt_ix": "380-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_76",
            "tgt_ix": "380-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_76",
            "tgt_ix": "380-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_76",
            "tgt_ix": "380-ARR_v2_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_76",
            "tgt_ix": "380-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_66",
            "tgt_ix": "380-ARR_v2_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_85",
            "tgt_ix": "380-ARR_v2_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_86",
            "tgt_ix": "380-ARR_v2_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_86",
            "tgt_ix": "380-ARR_v2_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_0",
            "tgt_ix": "380-ARR_v2_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_87",
            "tgt_ix": "380-ARR_v2_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_88",
            "tgt_ix": "380-ARR_v2_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_88",
            "tgt_ix": "380-ARR_v2_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "380-ARR_v2_0",
            "tgt_ix": "380-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_1",
            "tgt_ix": "380-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_2",
            "tgt_ix": "380-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_2",
            "tgt_ix": "380-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_2",
            "tgt_ix": "380-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_2",
            "tgt_ix": "380-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_2",
            "tgt_ix": "380-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_2",
            "tgt_ix": "380-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_2",
            "tgt_ix": "380-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_2",
            "tgt_ix": "380-ARR_v2_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_3",
            "tgt_ix": "380-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_4",
            "tgt_ix": "380-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_4",
            "tgt_ix": "380-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_4",
            "tgt_ix": "380-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_5",
            "tgt_ix": "380-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_5",
            "tgt_ix": "380-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_5",
            "tgt_ix": "380-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_6",
            "tgt_ix": "380-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_6",
            "tgt_ix": "380-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_6",
            "tgt_ix": "380-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_6",
            "tgt_ix": "380-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_6",
            "tgt_ix": "380-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_6",
            "tgt_ix": "380-ARR_v2_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_6",
            "tgt_ix": "380-ARR_v2_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_7",
            "tgt_ix": "380-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_7",
            "tgt_ix": "380-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_7",
            "tgt_ix": "380-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_8",
            "tgt_ix": "380-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_9",
            "tgt_ix": "380-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_10",
            "tgt_ix": "380-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_11",
            "tgt_ix": "380-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_11",
            "tgt_ix": "380-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_12",
            "tgt_ix": "380-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_12",
            "tgt_ix": "380-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_12",
            "tgt_ix": "380-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_13",
            "tgt_ix": "380-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_14",
            "tgt_ix": "380-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_14",
            "tgt_ix": "380-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_15",
            "tgt_ix": "380-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_16",
            "tgt_ix": "380-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_16",
            "tgt_ix": "380-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_16",
            "tgt_ix": "380-ARR_v2_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_16",
            "tgt_ix": "380-ARR_v2_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_16",
            "tgt_ix": "380-ARR_v2_16@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_16",
            "tgt_ix": "380-ARR_v2_16@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_16",
            "tgt_ix": "380-ARR_v2_16@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_16",
            "tgt_ix": "380-ARR_v2_16@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_17",
            "tgt_ix": "380-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_18",
            "tgt_ix": "380-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_18",
            "tgt_ix": "380-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_18",
            "tgt_ix": "380-ARR_v2_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_19",
            "tgt_ix": "380-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_20",
            "tgt_ix": "380-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_20",
            "tgt_ix": "380-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_20",
            "tgt_ix": "380-ARR_v2_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_20",
            "tgt_ix": "380-ARR_v2_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_20",
            "tgt_ix": "380-ARR_v2_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_20",
            "tgt_ix": "380-ARR_v2_20@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_21",
            "tgt_ix": "380-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_22",
            "tgt_ix": "380-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_23",
            "tgt_ix": "380-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_23",
            "tgt_ix": "380-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_23",
            "tgt_ix": "380-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_24",
            "tgt_ix": "380-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_24",
            "tgt_ix": "380-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_24",
            "tgt_ix": "380-ARR_v2_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_24",
            "tgt_ix": "380-ARR_v2_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_24",
            "tgt_ix": "380-ARR_v2_24@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_24",
            "tgt_ix": "380-ARR_v2_24@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_24",
            "tgt_ix": "380-ARR_v2_24@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_25",
            "tgt_ix": "380-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_26",
            "tgt_ix": "380-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_26",
            "tgt_ix": "380-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_26",
            "tgt_ix": "380-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_26",
            "tgt_ix": "380-ARR_v2_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_27",
            "tgt_ix": "380-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_28",
            "tgt_ix": "380-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_28",
            "tgt_ix": "380-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_28",
            "tgt_ix": "380-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_29",
            "tgt_ix": "380-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_30",
            "tgt_ix": "380-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_30",
            "tgt_ix": "380-ARR_v2_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_31",
            "tgt_ix": "380-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_32",
            "tgt_ix": "380-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_32",
            "tgt_ix": "380-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_33",
            "tgt_ix": "380-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_34",
            "tgt_ix": "380-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_35",
            "tgt_ix": "380-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_35",
            "tgt_ix": "380-ARR_v2_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_35",
            "tgt_ix": "380-ARR_v2_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_35",
            "tgt_ix": "380-ARR_v2_35@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_35",
            "tgt_ix": "380-ARR_v2_35@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_35",
            "tgt_ix": "380-ARR_v2_35@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_35",
            "tgt_ix": "380-ARR_v2_35@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_36",
            "tgt_ix": "380-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_37",
            "tgt_ix": "380-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_37",
            "tgt_ix": "380-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_37",
            "tgt_ix": "380-ARR_v2_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_37",
            "tgt_ix": "380-ARR_v2_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_37",
            "tgt_ix": "380-ARR_v2_37@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_37",
            "tgt_ix": "380-ARR_v2_37@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_37",
            "tgt_ix": "380-ARR_v2_37@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_37",
            "tgt_ix": "380-ARR_v2_37@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_38",
            "tgt_ix": "380-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_39",
            "tgt_ix": "380-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_39",
            "tgt_ix": "380-ARR_v2_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_40",
            "tgt_ix": "380-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_41",
            "tgt_ix": "380-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_42",
            "tgt_ix": "380-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_43",
            "tgt_ix": "380-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_43",
            "tgt_ix": "380-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_43",
            "tgt_ix": "380-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_43",
            "tgt_ix": "380-ARR_v2_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_43",
            "tgt_ix": "380-ARR_v2_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_44",
            "tgt_ix": "380-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_45",
            "tgt_ix": "380-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_46",
            "tgt_ix": "380-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_47",
            "tgt_ix": "380-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_48",
            "tgt_ix": "380-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_49",
            "tgt_ix": "380-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_50",
            "tgt_ix": "380-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_51",
            "tgt_ix": "380-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_51",
            "tgt_ix": "380-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_52",
            "tgt_ix": "380-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_53",
            "tgt_ix": "380-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_54",
            "tgt_ix": "380-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_55",
            "tgt_ix": "380-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_56",
            "tgt_ix": "380-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_56",
            "tgt_ix": "380-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_56",
            "tgt_ix": "380-ARR_v2_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_56",
            "tgt_ix": "380-ARR_v2_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_57",
            "tgt_ix": "380-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_58",
            "tgt_ix": "380-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_59",
            "tgt_ix": "380-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_60",
            "tgt_ix": "380-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_60",
            "tgt_ix": "380-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_61",
            "tgt_ix": "380-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_62",
            "tgt_ix": "380-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_63",
            "tgt_ix": "380-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_64",
            "tgt_ix": "380-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_64",
            "tgt_ix": "380-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_64",
            "tgt_ix": "380-ARR_v2_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_64",
            "tgt_ix": "380-ARR_v2_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_64",
            "tgt_ix": "380-ARR_v2_64@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_64",
            "tgt_ix": "380-ARR_v2_64@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_64",
            "tgt_ix": "380-ARR_v2_64@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_64",
            "tgt_ix": "380-ARR_v2_64@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_64",
            "tgt_ix": "380-ARR_v2_64@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_64",
            "tgt_ix": "380-ARR_v2_64@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_64",
            "tgt_ix": "380-ARR_v2_64@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_64",
            "tgt_ix": "380-ARR_v2_64@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_64",
            "tgt_ix": "380-ARR_v2_64@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_65",
            "tgt_ix": "380-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_66",
            "tgt_ix": "380-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_67",
            "tgt_ix": "380-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_68",
            "tgt_ix": "380-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_69",
            "tgt_ix": "380-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_69",
            "tgt_ix": "380-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_70",
            "tgt_ix": "380-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_71",
            "tgt_ix": "380-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_71",
            "tgt_ix": "380-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_71",
            "tgt_ix": "380-ARR_v2_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_71",
            "tgt_ix": "380-ARR_v2_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_72",
            "tgt_ix": "380-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_72",
            "tgt_ix": "380-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_73",
            "tgt_ix": "380-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_73",
            "tgt_ix": "380-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_73",
            "tgt_ix": "380-ARR_v2_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_73",
            "tgt_ix": "380-ARR_v2_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_73",
            "tgt_ix": "380-ARR_v2_73@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_73",
            "tgt_ix": "380-ARR_v2_73@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_73",
            "tgt_ix": "380-ARR_v2_73@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_73",
            "tgt_ix": "380-ARR_v2_73@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_73",
            "tgt_ix": "380-ARR_v2_73@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_73",
            "tgt_ix": "380-ARR_v2_73@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_74",
            "tgt_ix": "380-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_75",
            "tgt_ix": "380-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_75",
            "tgt_ix": "380-ARR_v2_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_75",
            "tgt_ix": "380-ARR_v2_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_76",
            "tgt_ix": "380-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_77",
            "tgt_ix": "380-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_77",
            "tgt_ix": "380-ARR_v2_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_77",
            "tgt_ix": "380-ARR_v2_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_78",
            "tgt_ix": "380-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_78",
            "tgt_ix": "380-ARR_v2_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_78",
            "tgt_ix": "380-ARR_v2_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_78",
            "tgt_ix": "380-ARR_v2_78@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_78",
            "tgt_ix": "380-ARR_v2_78@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_79",
            "tgt_ix": "380-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_79",
            "tgt_ix": "380-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_79",
            "tgt_ix": "380-ARR_v2_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_80",
            "tgt_ix": "380-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_80",
            "tgt_ix": "380-ARR_v2_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_81",
            "tgt_ix": "380-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_81",
            "tgt_ix": "380-ARR_v2_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_81",
            "tgt_ix": "380-ARR_v2_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_82",
            "tgt_ix": "380-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_82",
            "tgt_ix": "380-ARR_v2_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_82",
            "tgt_ix": "380-ARR_v2_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_82",
            "tgt_ix": "380-ARR_v2_82@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_82",
            "tgt_ix": "380-ARR_v2_82@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_83",
            "tgt_ix": "380-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_83",
            "tgt_ix": "380-ARR_v2_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_83",
            "tgt_ix": "380-ARR_v2_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_83",
            "tgt_ix": "380-ARR_v2_83@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_83",
            "tgt_ix": "380-ARR_v2_83@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_83",
            "tgt_ix": "380-ARR_v2_83@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_83",
            "tgt_ix": "380-ARR_v2_83@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_84",
            "tgt_ix": "380-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_84",
            "tgt_ix": "380-ARR_v2_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_84",
            "tgt_ix": "380-ARR_v2_84@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_84",
            "tgt_ix": "380-ARR_v2_84@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_84",
            "tgt_ix": "380-ARR_v2_84@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_84",
            "tgt_ix": "380-ARR_v2_84@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_84",
            "tgt_ix": "380-ARR_v2_84@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_85",
            "tgt_ix": "380-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_85",
            "tgt_ix": "380-ARR_v2_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_85",
            "tgt_ix": "380-ARR_v2_85@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_85",
            "tgt_ix": "380-ARR_v2_85@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_86",
            "tgt_ix": "380-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_87",
            "tgt_ix": "380-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_87",
            "tgt_ix": "380-ARR_v2_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_87",
            "tgt_ix": "380-ARR_v2_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_87",
            "tgt_ix": "380-ARR_v2_87@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_87",
            "tgt_ix": "380-ARR_v2_87@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_87",
            "tgt_ix": "380-ARR_v2_87@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_88",
            "tgt_ix": "380-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_89",
            "tgt_ix": "380-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_89",
            "tgt_ix": "380-ARR_v2_89@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_89",
            "tgt_ix": "380-ARR_v2_89@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_89",
            "tgt_ix": "380-ARR_v2_89@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_90",
            "tgt_ix": "380-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_91",
            "tgt_ix": "380-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_92",
            "tgt_ix": "380-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_93",
            "tgt_ix": "380-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_94",
            "tgt_ix": "380-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_95",
            "tgt_ix": "380-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_96",
            "tgt_ix": "380-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_97",
            "tgt_ix": "380-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_98",
            "tgt_ix": "380-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_99",
            "tgt_ix": "380-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_100",
            "tgt_ix": "380-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_101",
            "tgt_ix": "380-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_102",
            "tgt_ix": "380-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_103",
            "tgt_ix": "380-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_104",
            "tgt_ix": "380-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_105",
            "tgt_ix": "380-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_106",
            "tgt_ix": "380-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_107",
            "tgt_ix": "380-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_108",
            "tgt_ix": "380-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_109",
            "tgt_ix": "380-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_110",
            "tgt_ix": "380-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_111",
            "tgt_ix": "380-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_112",
            "tgt_ix": "380-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_113",
            "tgt_ix": "380-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_114",
            "tgt_ix": "380-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_115",
            "tgt_ix": "380-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_116",
            "tgt_ix": "380-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_117",
            "tgt_ix": "380-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_118",
            "tgt_ix": "380-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_119",
            "tgt_ix": "380-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_120",
            "tgt_ix": "380-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_121",
            "tgt_ix": "380-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_122",
            "tgt_ix": "380-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_123",
            "tgt_ix": "380-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_124",
            "tgt_ix": "380-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_125",
            "tgt_ix": "380-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_126",
            "tgt_ix": "380-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_127",
            "tgt_ix": "380-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_128",
            "tgt_ix": "380-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_129",
            "tgt_ix": "380-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_130",
            "tgt_ix": "380-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_131",
            "tgt_ix": "380-ARR_v2_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_132",
            "tgt_ix": "380-ARR_v2_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_133",
            "tgt_ix": "380-ARR_v2_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_134",
            "tgt_ix": "380-ARR_v2_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_135",
            "tgt_ix": "380-ARR_v2_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_136",
            "tgt_ix": "380-ARR_v2_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_137",
            "tgt_ix": "380-ARR_v2_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_138",
            "tgt_ix": "380-ARR_v2_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_139",
            "tgt_ix": "380-ARR_v2_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_140",
            "tgt_ix": "380-ARR_v2_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "380-ARR_v2_141",
            "tgt_ix": "380-ARR_v2_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 891,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "380-ARR",
        "version": 2
    }
}