{
    "nodes": [
        {
            "ix": "370-ARR_v1_0",
            "content": "Non-Autoregressive Machine Translation: It's Not as Fast as it Seems",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_2",
            "content": "Efficient machine translation models are commercially important as they can increase inference speeds, and reduce costs and carbon emissions. Recently, there has been much interest in non-autoregressive (NAR) models, which promise faster translation. In parallel to the research on NAR models, there have been successful attempts to create optimized autoregressive models as part of the WMT shared task on efficient translation. In this paper, we point out flaws in the evaluation methodology present in the literature on NAR models and we provide fair comparison between a state-of-the-art NAR model and the autoregressive submissions to the shared task. We make the case for consistent evaluation of NAR models, and also for the importance of comparing NAR models with other widely used efficiency approaches. We run experiments with a connectionist-temporalclassification-based (CTC) NAR model implemented in C++ and compare it with AR models using wall clock times. Our results show that, although NAR models are faster on GPUs, with small batch sizes, they are nearly always slower under more realistic usage conditions. We call for more realistic and extensive evaluation of NAR models in future work.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "370-ARR_v1_4",
            "content": "Non-autoregressive neural machine translation (NAR NMT, or NAT; Gu et al., 2018;Lee et al., 2018) is an emerging subfield of NMT which focuses on lowering the decoding time complexity by changing the model architecture.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_5",
            "content": "The defining feature of non-autoregressive models is the conditional independence assumption on the output probability distributions; this is in contrast to autoregressive models, where the output distributions are conditioned on the previous outputs. This conditional independence allows one to decode the target tokens in parallel. This can substantially reduce the decoding time, especially for longer target sentences.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_6",
            "content": "The speed of the decoding is assessed by translating a test set and measuring the overall time the process takes. This may sound simple, but there are various aspects to be considered that can affect decoding speed, such as batching, number of hypotheses in beam search or hardware used (i.e., using CPU or GPU). Decoding speed evaluation is a challenging task, especially when it comes to comparability across different approaches. Unlike the translation quality, decoding speed can be measured exactly. However, also unlike the translation quality, different results are obtained under different evaluation environments. The WMT Efficient Translation Shared Task aims to evaluate efficiency research and encourages the reporting of a range of speed and translation quality values to better understand the trade-off across different model configurations . In this paper, we follow the emerging best practices developed in the Efficiency Shared Task and directly compare with the submitted systems.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_7",
            "content": "Over the course of research on NAR models, modeling error and its subsequent negative effect on translation quality remains the biggest issue. Therefore, the goal of contemporary research is to close the performance gap between the AR models and their NAR counterparts, while maintaining high decoding speed. Considering these stated research goals, the evaluation should comprise of assessing translation quality as well as decoding speed.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_8",
            "content": "Translation quality is usually evaluated by scoring translations of an unseen test set either using automatic metrics, such as BLEU (Papineni et al., 2002), ChrF (Popovi\u0107, 2015) or COMET (Rei et al., 2020), or using human evaluation. To prevent the methods from eventually overfitting a single test set, new test sets are published each year as part of the WMT News Translation Shared Task. In contrast, translation quality evaluation in NAR research is often measured using BLEU scores only, measured almost exclusively on the WMT 14 English-German test set, which is highly problematic. Automatic evaluation of translation quality however remains an open research problem (Mathur et al., 2020;Kocmi et al., 2021). In our experiments, we follow the recent best practices by using multiple metrics and recent test sets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_9",
            "content": "In this paper, we examine the evaluation methodology generally accepted in literature on NAR methods, and we identify a number of flaws. First, the results are reported on different hardware architectures, which makes them incomparable, even when comparing only relative speedups. Second, most of the methods only report latency (decoding with a single sentence per batch) using a GPU; we show that this is the only setup favors NAR models. Third, the reported baseline performance is usually questionable, both in terms of speed and translation quality. Finally, despite the fact that the main motivation for using NAR models is the lower time complexity, the findings of the efficiency task are ignored in most of the NAR papers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_10",
            "content": "We try to connect the separate worlds of NAR and efficient translation research. We train nonautoregressive models based on connectionist temporal classification (CTC), an approach previously shown to be effective (Libovick\u00fd and Helcl, 2018;Gu and Kong, 2021;Ghazvininejad et al., 2020). We employ a number of techniques for improving the translation quality, including data cleaning and sequence-level knowledge distillation (Kim and Rush, 2016). We evaluate our models following a unified evaluation methodology: In order to compare the translation quality with the rest of the NAR literature, we report BLEU scores measured on the WMT 14 test set, on which we achieve state-ofthe-art performance among (both single-step and iterative) NAR methods; we evaluate the translation quality and decoding speed of our models in the same conditions as the efficiency task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_11",
            "content": "We find that despite achieving very good results among the NAT models on the WMT 14 test set, our models fall behind in translation quality when measured on the recent WMT 21 test set using three different automatic evaluation metrics. Moreover, we show that GPU decoding latency is the only scenario in which non-autoregressive models outperform autoregressive models. This paper contributes to the research community in the following aspects: First, we point out to weaknesses in standard evaluation methodology of non-autoregressive models. Second, we link the worlds of non-autoregressive translation and model optimization to provide a better understanding of the results achieved in the related work.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_12",
            "content": "Non-Autoregressive NMT",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "370-ARR_v1_13",
            "content": "The current state-of-the-art NMT models are autoregressive -the output distributions are conditioned on the previously generated tokens (Bahdanau et al., 2016;Vaswani et al., 2017). The decoding process is sequential in its nature, limiting the opportunities for parallelization.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_14",
            "content": "Non-autoregressive models use output distributions which are conditionally independent on each other, which opens up the possibility of parallelization. Formally, the probability of a sequence y given the input x in a non-autoregressive model with parameters \u03b8 is modeled as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_15",
            "content": "p \u03b8 (y|x) = y i \u2208y p(y i |x, \u03b8).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_16",
            "content": "(1)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_17",
            "content": "Unsurprisingly, the independence assumption in NAR models has a negative impact on the translation quality. The culprit for this behavior is the multimodality problem -the inability of the model to differentiate between different modes of the joint probability distribution over output sequences inside the distributions corresponding to individual time steps. A classic example of this issue is the sentence \"Thank you\" with its two equally probable German translations \"Danke sch\u00f6n\" and \"Vielen Dank\" (Gu et al., 2018). Because of the independence assumption, a non-autoregressive model cannot assign high probabilities to these two translations without also allowing for the incorrect sentences \"Vielen sch\u00f6n\" and \"Danke Dank\".",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_18",
            "content": "Knowledge distillation (Kim and Rush, 2016) has been successfully employed to reduce the negative influence of the multimodality problem in NAR models (Gu et al., 2018;Saharia et al., 2020). Synthetic data tends to be less diverse than authentic texts, therefore the number of equally likely translation candidates gets smaller (Zhou et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_19",
            "content": "A number of techniques have been proposed for training NAR models, including iterative methods (Lee et al., 2018;Ghazvininejad et al., 2019), auxiliary training objectives Qian et al., 2021), or latent variables (Gu et al., 2018;Lee et al., 2018;Kaiser et al., 2018). In some form, all of the aforementioned approaches use explicit target length estimation, and rely on one-to-one correspondence between the output distributions and the reference sentence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_20",
            "content": "A group of methods that relax the requirement of the strict one-to-one alignment between the model outputs and the ground-truth target sequence include aligned cross-entropy (Ghazvininejad et al., 2020) and connectionist temporal classification (Libovick\u00fd and Helcl, 2018).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_21",
            "content": "The schema CTC-based model, as proposed by Libovick\u00fd and Helcl (2018), is shown in Figure 1. The model extends the Transformer architecture (Vaswani et al., 2017). It consists of an encoder, a state-splitting layer, and a non-autoregressive decoder. The encoder has the same architecture as in the Transformer model. The state-splitting layer, applied on the encoder output, linearly projects and splits each state into k states with the same dimension. The decoder consists of a stack of Transformer layers. Unlike the Transformer model, the self-attention in the non-autoregressive decoder does not use the causal mask, so the model is not prevented from attending to future states. Since the output length is fixed to k-times the length of the source sentence, the model is permitted to output blank tokens. Different positions of the blank tokens in the output sequence represent different alignments between the outputs and the groundtruth sequence. Connectionist temporal classification (Graves et al., 2006) is a dynamic algorithm that efficiently computes the standard cross-entropy loss summed over all possible alignments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_22",
            "content": "We choose the CTC-based architecture for our models because it has been previously shown to be effective for NAR NMT (Gu and Kong, 2021;Saharia et al., 2020) and performs well in the context of the non-autoregressive research. It is also one of the fastest NAR architectures since it is not iterative.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_23",
            "content": "Evaluation Methodology",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "370-ARR_v1_24",
            "content": "The research goal of the non-autoregressive methods is to improve the translation quality while maintaining the speedup brought by the conditional independence assumption. This means that careful thought should be given to both quantifying the speed gains and the translation quality evaluation. The speed-vs-quality trade-off can be characterized by the Pareto frontier. In this section we discuss the evaluation from both perspectives.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_25",
            "content": "Translation Quality. In the world of nonautoregressive NMT, the experimental settings are not very diverse. The primary language pair for translation experiments is English-German, sometimes accompanied by English-Romanian to simulate the low-resource scenario. These language pairs, along with the widely used test sets -WMT 14 (Bojar et al., 2014) for En-De and WMT 16 (Bojar et al., 2016) for En-Ro -became the de facto standard benchmark for NAR model evaluation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_26",
            "content": "A common weakness seen in the literature is the use of weak baseline models. The base variant of the Transformer model is used almost exclusively (Gu et al., 2018;Gu and Kong, 2021;Lee et al., 2018;Ghazvininejad et al., 2020;Qian et al., 2021). We argue that using weaker baselines might lead to overrating the positive effects brought by proposed improvements. Since the baseline autoregressive models are used to generate the synthetic parallel data for knowledge distillation, the weakness is potentially further amplified in this step.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_27",
            "content": "Evaluation is normally with automatic metrics only, and often only BLEU is reported. In light of recent research casting further doubt on the reliability of BLEU as a measure of translation quality (Kocmi et al., 2021), we argue that this is insufficient.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_28",
            "content": "Decoding Speed. The current standard in evaluation of NAR models is to measure translation latency using a GPU, i.e., the average time to translate a single sentence without batching. Since the time depends on the hardware, relative speedup is usually reported along with latency. This is a reasonable approach but we need to keep in mind the associated difficulties. First, the results achieved on different hardware architectures are not easily comparable even when considering the relative speedups. We also note that the relative speedup values should always be accompanied by the corresponding decoding times in absolute numbers. Sometimes, this information is missing from the published results (Qian et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_29",
            "content": "We argue that measuring only GPU latency disregards other use-cases. In the WMT Efficiency Shared Task, the decoding speed is measured in five scenarios. The speed is reported using a GPU with and without batching, using all 36 CPU cores (also, with and without batching), and using a single CPU core without batching. In batched decoding, the shared task participants could choose the optimal batch size. Our results in Section 5 show that measuring latency is the only one that favors NAR models, and as the batch size increases, AR models quickly reach higher translation speeds.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_30",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "370-ARR_v1_31",
            "content": "We experiment with non-autoregressive models for English-German translation. We used the data provided by the WMT 21 News Translation Shared Task organizers (Akhbardeh et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_32",
            "content": "As our baseline model, we use the CTC-based NAR model as described by Libovick\u00fd and Helcl (2018). We use stack of 6 encoder and 6 decoder layers, separated by the state splitting layer which extends the state sequence 3 times.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_33",
            "content": "We implement our models in the Marian toolkit . For the CTC loss computation, we use the warp-ctc library (Amodei et al., 2016).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_34",
            "content": "Teacher Models",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "370-ARR_v1_35",
            "content": "For training our baseline autoregressive models, we closely follow the approach of . The preparation of the baseline models consists of three phases -data cleaning, backtranslation, and the training of the final models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_36",
            "content": "We train the teacher models on cleaned parallel corpora and backtranslated monolingual data. For the parallel data, we used Europarl (Koehn, 2005), the RAPID corpus (Rozis and Skadin , \u0161, 2017), and the News Commentary corpus from OPUS (Tiedemann, 2012). We consider these three parallel dataset clean. We also use noisier parallel datasets, namely Paracrawl (Ba\u00f1\u00f3n et al., 2020), Common Crawl 1 , WikiMatrix (Schwenk et al., 2019), and Wikititles 2 . For backtranslation, we used the monolingual datasets from the News Crawl from the years 2018-2020, in both English and German.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_37",
            "content": "We clean the parallel corpus (i.e. both clean and noisy portions) using rule-based cleaning 3 . Additionally, we exclude sentence pairs with nonlatin characters. Additionally, we apply dual crossentropy filtering on the noisy part of the parallel data (Junczys-Dowmunt, 2018). We train Transformer base models in both directions on the clean portion of the parallel data. Then, we select 75% of the best-scoring sentence pairs into the final parallel portion of the training dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_38",
            "content": "For backtranslation (Sennrich et al., 2016), we train four Transformer big models on the cleaned parallel data in both directions. We then use them in an ensemble to create the synthetic source side for the monolingual corpora. We add a special symbol to the generated sentences to help the models differentiate between synthetic and authentic source language data (Caswell et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_39",
            "content": "We use hyperparameters of the Transformer big model, i.e. model dimension 1,024, feed-forward hidden dimension of 4,096, and 16 attention heads.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_40",
            "content": "For training, we use the Adam optimizer (Kingma and Ba, 2014) with \u03b2 1 , \u03b2 2 and \u03f5 set to 0.9, 0.998 and 10 -9 respectively. We used the inverted squareroot learning rate decay with 8,000 of linear warmup and initial learning rate of 10 -4 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_41",
            "content": "The teacher models follow the same hyperparameter settings as the models for backtranslation, but are trained with the tagged backtranslations included in the data. As in the previous case, we train four teacher models with different random seeds for ensembling.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_42",
            "content": "Similar to creating the backtranslations, we use the four teacher models in an ensemble to create the knowledge-distilled data (Kim and Rush, 2016). We translate the source side of the parallel data, as well as the source-language monolingual data. We do not translate back-translated data. Thus, the source side data for the student models is authentic, and the target side is synthetic, created by the teacher models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_43",
            "content": "Student Models",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "370-ARR_v1_44",
            "content": "We train five variants of the student models with different hyperparameter settings. The \"Large\" model is our baseline model -the same number of layers as the teacher models, 6 in the encoder, followed by the state splitting layer, and another 6 layers in the decoder. The \"Base\" model has the same number of layers with reduced dimension of the embeddings and the feed-forward Transformer sublayer, to match the Transformer base settings. We also try reducing the numbers of encoder and decoder layers. We shrink the base model to 3-3 (\"Small\"), 2-2 (\"Micro\"), and 1-1 (\"Tiny\") architectures.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_45",
            "content": "We run the training of each model for three weeks on four Nvidia Pascal P100 GPUs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_46",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "370-ARR_v1_47",
            "content": "In this section, we try and view the results of the NAR and efficiency research in a shared perspective. We evaluate our models and present results in terms of translation quality and decoding speed. We compare the results to the related work on both non-autoregressive translation and model optimization.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_48",
            "content": "Translation Quality. The research on nonautoregressive models uses the BLEU score (Papineni et al., 2002) measured on the WMT 14 test set (Bojar et al., 2014) as a standard benchmark for evaluating translation quality. We use Sacrebleu (Post, 2018) as the implementation of the BLEU score metric. 4 Using a single test set for the whole volume of research on this topic may however produce misleading results. up to date with the current state-of-the-art translation systems, we also evaluate our models using COMET (Rei et al., 2020) 5 and BLEU scores on the recent WMT 21 test set. The same test set was used in the WMT 21 Efficiency Task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_49",
            "content": "Table 2 shows the BLEU scores of our NAR models on the WMT 14 test set. We show the results of the five variants of the NAR models and we include three of the best-performing NAR approaches from the related work. We see from the table that using BLEU, the \"Large\" model scores among the best NAR models on the WMT 14 test set. As the NAR model size decreases, so does the translation quality, with the notable exception of the En\u2192De \"Micro\" model, which outperforms the \"Base\" model consistently on different test sets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_50",
            "content": "In Table 3, we report the automatic evaluation results of our AR and NAR models on the multireference WMT 21 test set (Akhbardeh et al., 2021). We compare our NAR models to the AR large teacher models from Section 4.1, an AR base model trained on the original clean data, and an AR base student model trained on the distilled data. Following , we use references A, C, and D for English-German translation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_51",
            "content": "We see that there is a considerable difference in the translation quality between the NAR models and the AR large teacher model. This difference grows with beam search and ensembling applied on the AR decoding, techniques not usually used with NAR models because of the speed cost. We also note that when we train an AR base model on the distilled data, it outperforms the NAR large model by a considerable margin.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_52",
            "content": "Another thing we notice is the enormous difference in the COMET scores between the AR and NAR models. The AR base models achieve comparable BLEU scores to the NAR large models, but differ substantially in the COMET score. From a look at the system outputs, we hypothesize that the NAR systems produce unusual errors which BLEU does not penalise as heavily as COMET. This might suggest that NAR models would rank poorly in human evaluation relative to their autoregressive counterparts, despite the reasonable BLEU score values.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_53",
            "content": "Decoding speed. We follow the decoding time evaluation methodology of the WMT 21 Efficient Translation Shared Task . We recreate the hardware conditions that were used in the task. For the GPU decoding measurements, we use a single Nvidia Ampere A100 GPU. The CPU evaluation was performed on a 36-core CPU Intel Xeon Gold 6354 server from Oracle cloud.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_54",
            "content": "To amortize for the various computation overheads, the models submitted to the shared task are evaluated on a million sentence benchmark dataset. We measure the overall wall time to translate the shared task dataset with different batching settings on both the GPU and the 36-core CPU. The decoding times are shown in Figures 2 and 4 for the GPU and CPU times, respectively. We do not report the single-core CPU latencies as the decoding speed of the NAR models falls far behind the efficient AR models in this setup and the translation of the Model Latency (ms) Gu et al. (2018) 39 22 37",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_55",
            "content": "Ours -Large 14 dataset takes too long.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_56",
            "content": "We can see that in case of GPU decoding that all models benefit from having larger batch sizes. However, the non-autoregressive models are much faster when the batch size is small. We also ran the evaluation on an Nvidia Pascal P100 GPU, which showed that when the batch size is large enough, autoregressive models eventually match the speed of non-autoregressive models. We show the decoding times on the Pascal GPU in Figure 3. In Table 4, we compare the latencies measured on the Pascal GPU to some of the related NAR approaches that report results on this GPU type. Due to implementation reasons, the maximum batch size for our NAR models is around 220 sentences.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_57",
            "content": "Comparison with Efficient AR Models. In Table 5, we present a comparison on the million sentence test set with \"Edinburgh base\", one of the leading submissions in the WMT 21 efficiency task (Behnke et al., 2021). First, we see that using three different evaluation metrics (ChrF, COMET, and BLEU), our models lag behind the Edinburgh base model. In line with our previous observation, we see a considerable drop in the COMET score values. In terms of decoding speed, the only scenario in which the non-autoregressive model is better is on GPU with batch size 1. This is in line with our intuition that the parallelization potential brought by the GPU is utilized more efficiently by the NAR model. On one hand, larger batches open up the parallelization possibilities to AR models. On the other hand, limited parallelization potential (in form of CPU decoding) reduces the differences between AR and NAR models. Table 5: A comparison of our AR and NAR models with one of the submissions to the WMT 21 efficiency task. We show the results of automatic translation quality evaluation using three different metrics, and the decoding time to translate the test set using a GPU and 36-core CPU with either latency (b=1) or batched (b>1) decoding.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_58",
            "content": "Conclusions",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "370-ARR_v1_59",
            "content": "In this paper, we challenge the evaluation methodology adopted by the research on non-autoregressive models for NMT. We argue that in terms of translation quality, the evaluation should include newer test sets and metrics other than BLEU (particularly COMET and ChrF). This will provide more insight and put the results into the context of the recent research.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_60",
            "content": "From the decoding speed perspective, we should bear in mind various use-cases for the model deployment, such as the hardware environment or batching conditions. Preferably, the research should evaluate the speed gains across a range of scenarios. Finally, given that the latency condition -translation of one sentence at a time on a GPU -already translates too fast to be perceived by human users of MT, there is currently no compelling scenario that warrants the deployment of NAR models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "370-ARR_v1_61",
            "content": "Farhad Akhbardeh, Arkady Arkhangorodsky, Magdalena Biesialska, Ond\u0159ej Bojar, Rajen Chatterjee, Vishrav Chaudhary, Marta Costa-Jussa, Cristina Espa\u00f1a-Bonet, Angela Fan, Christian Federmann, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Leonie Harter, Kenneth Heafield, Christopher Homan, Matthias Huck, Kwabena Amponsah-Kaakyire, Jungo Kasai, Daniel Khashabi, Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Allahsera Auguste Tapo, Marco Turchi, Valentin Vydrin, and Marcos Zampieri. 2021. Findings of the 2021 conference on machine translation (WMT21), , Proceedings of the Sixth Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Farhad Akhbardeh",
                    "Arkady Arkhangorodsky",
                    "Magdalena Biesialska",
                    "Ond\u0159ej Bojar",
                    "Rajen Chatterjee",
                    "Vishrav Chaudhary",
                    "Marta Costa-Jussa",
                    "Cristina Espa\u00f1a-Bonet",
                    "Angela Fan",
                    "Christian Federmann",
                    "Markus Freitag",
                    "Yvette Graham",
                    "Roman Grundkiewicz",
                    "Barry Haddow",
                    "Leonie Harter",
                    "Kenneth Heafield",
                    "Christopher Homan",
                    "Matthias Huck",
                    "Kwabena Amponsah-Kaakyire",
                    "Jungo Kasai",
                    "Daniel Khashabi"
                ],
                "title": "Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Allahsera Auguste Tapo, Marco Turchi, Valentin Vydrin, and Marcos Zampieri. 2021. Findings of the 2021 conference on machine translation (WMT21)",
                "pub_date": null,
                "pub_title": "Proceedings of the Sixth Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_62",
            "content": "Dario Amodei, Rishita Sundaram Ananthanarayanan, Jingliang Anubhai, Eric Bai, Carl Battenberg, Jared Case, Bryan Casper, Qiang Catanzaro, Guoliang Cheng,  Chen, Deep speech 2: End-to-end speech recognition in english and mandarin, 2016, International conference on machine learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Dario Amodei",
                    "Rishita Sundaram Ananthanarayanan",
                    "Jingliang Anubhai",
                    "Eric Bai",
                    "Carl Battenberg",
                    "Jared Case",
                    "Bryan Casper",
                    "Qiang Catanzaro",
                    "Guoliang Cheng",
                    " Chen"
                ],
                "title": "Deep speech 2: End-to-end speech recognition in english and mandarin",
                "pub_date": "2016",
                "pub_title": "International conference on machine learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "370-ARR_v1_63",
            "content": "UNKNOWN, None, 2016, Neural machine translation by jointly learning to align and translate, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Neural machine translation by jointly learning to align and translate",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_64",
            "content": "Marta Ba\u00f1\u00f3n, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Espl\u00e0-Gomis, Mikel Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Rojas, Leopoldo Sempere, Gema Ram\u00edrez-S\u00e1nchez, ParaCrawl: Web-scale acquisition of parallel corpora, 2020, Proceedings of the 58th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Marta Ba\u00f1\u00f3n",
                    "Pinzhen Chen",
                    "Barry Haddow",
                    "Kenneth Heafield",
                    "Hieu Hoang",
                    "Miquel Espl\u00e0-Gomis",
                    "Mikel Forcada",
                    "Amir Kamran",
                    "Faheem Kirefu",
                    "Philipp Koehn",
                    "Sergio Rojas",
                    "Leopoldo Sempere",
                    "Gema Ram\u00edrez-S\u00e1nchez"
                ],
                "title": "ParaCrawl: Web-scale acquisition of parallel corpora",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_65",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "370-ARR_v1_66",
            "content": "Maximiliana Behnke, Nikolay Bogoychev, Alham Fikri Aji, Kenneth Heafield, Graeme Nail, Qianqian Zhu, Svetlana Tchistiakova, Jelmer Van Der Linde, Pinzhen Chen, Sidharth Kashyap, Roman Grundkiewicz, Efficient machine translation with model pruning and quantization, 2021, Proceedings of the Conference on Machine Translation at the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Maximiliana Behnke",
                    "Nikolay Bogoychev",
                    "Alham Fikri Aji",
                    "Kenneth Heafield",
                    "Graeme Nail",
                    "Qianqian Zhu",
                    "Svetlana Tchistiakova",
                    "Jelmer Van Der Linde",
                    "Pinzhen Chen",
                    "Sidharth Kashyap",
                    "Roman Grundkiewicz"
                ],
                "title": "Efficient machine translation with model pruning and quantization",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Conference on Machine Translation at the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_67",
            "content": "Ond\u0159ej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, Ale\u0161 Tamchyna, Findings of the 2014 workshop on statistical machine translation, 2014, Proceedings of the Ninth Workshop on Statistical Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Ond\u0159ej Bojar",
                    "Christian Buck",
                    "Christian Federmann",
                    "Barry Haddow",
                    "Philipp Koehn",
                    "Johannes Leveling",
                    "Christof Monz",
                    "Pavel Pecina",
                    "Matt Post",
                    "Herve Saint-Amand",
                    "Radu Soricut",
                    "Lucia Specia",
                    "Ale\u0161 Tamchyna"
                ],
                "title": "Findings of the 2014 workshop on statistical machine translation",
                "pub_date": "2014",
                "pub_title": "Proceedings of the Ninth Workshop on Statistical Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_68",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_69",
            "content": "Mariana N\u00e9v\u00e9ol, Martin Neves, Matt Popel, Raphael Post, Carolina Rubino, Lucia Scarton, Marco Specia, Karin Turchi, Marcos Verspoor,  Zampieri, Findings of the 2016 conference on machine translation, 2016, Proceedings of the First Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Mariana N\u00e9v\u00e9ol",
                    "Martin Neves",
                    "Matt Popel",
                    "Raphael Post",
                    "Carolina Rubino",
                    "Lucia Scarton",
                    "Marco Specia",
                    "Karin Turchi",
                    "Marcos Verspoor",
                    " Zampieri"
                ],
                "title": "Findings of the 2016 conference on machine translation",
                "pub_date": "2016",
                "pub_title": "Proceedings of the First Conference on Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "370-ARR_v1_70",
            "content": "Isaac Caswell, Ciprian Chelba, David Grangier, Tagged back-translation, 2019, Proceedings of the Fourth Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Isaac Caswell",
                    "Ciprian Chelba",
                    "David Grangier"
                ],
                "title": "Tagged back-translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fourth Conference on Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "370-ARR_v1_71",
            "content": "Pinzhen Chen, Jind\u0159ich Helcl, Ulrich Germann, Laurie Burchell, Nikolay Bogoychev, Antonio Valerio Miceli, Jonas Barone, Alexandra Waldendorf, Kenneth Birch,  Heafield, The University of Edinburgh's English-German and English-Hausa submissions to the WMT21 news translation task, 2021, Proceedings of the Conference on Machine Translation at the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Pinzhen Chen",
                    "Jind\u0159ich Helcl",
                    "Ulrich Germann",
                    "Laurie Burchell",
                    "Nikolay Bogoychev",
                    "Antonio Valerio Miceli",
                    "Jonas Barone",
                    "Alexandra Waldendorf",
                    "Kenneth Birch",
                    " Heafield"
                ],
                "title": "The University of Edinburgh's English-German and English-Hausa submissions to the WMT21 news translation task",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Conference on Machine Translation at the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_72",
            "content": "Marjan Ghazvininejad, Vladimir Karpukhin, Luke Zettlemoyer, Omer Levy, Aligned cross entropy for non-autoregressive machine translation, 2020, Proceedings of the 37th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Marjan Ghazvininejad",
                    "Vladimir Karpukhin",
                    "Luke Zettlemoyer",
                    "Omer Levy"
                ],
                "title": "Aligned cross entropy for non-autoregressive machine translation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 37th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "370-ARR_v1_73",
            "content": "Marjan Ghazvininejad, Omer Levy, Yinhan Liu, Luke Zettlemoyer, Mask-predict: Parallel decoding of conditional masked language models, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Marjan Ghazvininejad",
                    "Omer Levy",
                    "Yinhan Liu",
                    "Luke Zettlemoyer"
                ],
                "title": "Mask-predict: Parallel decoding of conditional masked language models",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_74",
            "content": "Alex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, J\u00fcrgen Schmidhuber, Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks, 2006, Proceedings of the 23rd International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Alex Graves",
                    "Santiago Fern\u00e1ndez",
                    "Faustino Gomez",
                    "J\u00fcrgen Schmidhuber"
                ],
                "title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks",
                "pub_date": "2006",
                "pub_title": "Proceedings of the 23rd International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_75",
            "content": "Jiatao Gu, James Bradbury, Caiming Xiong, O Victor, Richard Li,  Socher, Non-autoregressive neural machine translation, 2018, 6th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Jiatao Gu",
                    "James Bradbury",
                    "Caiming Xiong",
                    "O Victor",
                    "Richard Li",
                    " Socher"
                ],
                "title": "Non-autoregressive neural machine translation",
                "pub_date": "2018",
                "pub_title": "6th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_76",
            "content": "Jiatao Gu, Xiang Kong, Fully nonautoregressive neural machine translation: Tricks of the trade, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Jiatao Gu",
                    "Xiang Kong"
                ],
                "title": "Fully nonautoregressive neural machine translation: Tricks of the trade",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_77",
            "content": "Kenneth Heafield, Qianqian Zhu, Roman Grundkiewicz, Findings of the WMT 2021 shared task on efficient translation, 2021, Proceedings of the Conference on Machine Translation at the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Kenneth Heafield",
                    "Qianqian Zhu",
                    "Roman Grundkiewicz"
                ],
                "title": "Findings of the WMT 2021 shared task on efficient translation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Conference on Machine Translation at the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_78",
            "content": "Marcin Junczys-Dowmunt, Dual conditional cross-entropy filtering of noisy parallel corpora, 2018, Proceedings of the Third Conference on Machine Translation: Shared Task Papers, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Marcin Junczys-Dowmunt"
                ],
                "title": "Dual conditional cross-entropy filtering of noisy parallel corpora",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "370-ARR_v1_79",
            "content": "Marcin Junczys-Dowmunt, Roman Grundkiewicz, Tomasz Dwojak, Hieu Hoang, Kenneth Heafield, Tom Neckermann, Frank Seide, Ulrich Germann, Alham Fikri Aji, Nikolay Bogoychev, F Andr\u00e9, Alexandra Martins,  Birch, Marian: Fast neural machine translation in C++, 2018, Proceedings of ACL 2018, System Demonstrations, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Marcin Junczys-Dowmunt",
                    "Roman Grundkiewicz",
                    "Tomasz Dwojak",
                    "Hieu Hoang",
                    "Kenneth Heafield",
                    "Tom Neckermann",
                    "Frank Seide",
                    "Ulrich Germann",
                    "Alham Fikri Aji",
                    "Nikolay Bogoychev",
                    "F Andr\u00e9",
                    "Alexandra Martins",
                    " Birch"
                ],
                "title": "Marian: Fast neural machine translation in C++",
                "pub_date": "2018",
                "pub_title": "Proceedings of ACL 2018, System Demonstrations",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "370-ARR_v1_80",
            "content": "Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish Vaswani, Niki Parmar, Jakob Uszkoreit, Noam Shazeer, Fast decoding in sequence models using discrete latent variables, 2018, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Lukasz Kaiser",
                    "Samy Bengio",
                    "Aurko Roy",
                    "Ashish Vaswani",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Noam Shazeer"
                ],
                "title": "Fast decoding in sequence models using discrete latent variables",
                "pub_date": "2018",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "370-ARR_v1_81",
            "content": "Yoon Kim, Alexander Rush, Sequencelevel knowledge distillation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Yoon Kim",
                    "Alexander Rush"
                ],
                "title": "Sequencelevel knowledge distillation",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "370-ARR_v1_82",
            "content": "UNKNOWN, None, 2014, Adam: A method for stochastic optimization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": null,
                "title": null,
                "pub_date": "2014",
                "pub_title": "Adam: A method for stochastic optimization",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_83",
            "content": "UNKNOWN, None, , Hitokazu Matsushita, and Arul Menezes. 2021. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Hitokazu Matsushita, and Arul Menezes. 2021. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_84",
            "content": "Philipp Koehn, Europarl: A parallel corpus for statistical machine translation, 2005, Proceedings of Machine Translation Summit X: Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Philipp Koehn"
                ],
                "title": "Europarl: A parallel corpus for statistical machine translation",
                "pub_date": "2005",
                "pub_title": "Proceedings of Machine Translation Summit X: Papers",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_85",
            "content": "Jason Lee, Elman Mansimov, Kyunghyun Cho, Deterministic non-autoregressive neural sequence modeling by iterative refinement, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Jason Lee",
                    "Elman Mansimov",
                    "Kyunghyun Cho"
                ],
                "title": "Deterministic non-autoregressive neural sequence modeling by iterative refinement",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "370-ARR_v1_86",
            "content": "Jind\u0159ich Libovick\u00fd, Jind\u0159ich Helcl, End-toend non-autoregressive neural machine translation with connectionist temporal classification, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Jind\u0159ich Libovick\u00fd",
                    "Jind\u0159ich Helcl"
                ],
                "title": "End-toend non-autoregressive neural machine translation with connectionist temporal classification",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_87",
            "content": "Nitika Mathur, Timothy Baldwin, Trevor Cohn, Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Nitika Mathur",
                    "Timothy Baldwin",
                    "Trevor Cohn"
                ],
                "title": "Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_88",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "pub_date": "2002",
                "pub_title": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "370-ARR_v1_89",
            "content": "Maja Popovi\u0107, chrF: character n-gram F-score for automatic MT evaluation, 2015, Proceedings of the Tenth Workshop on Statistical Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Maja Popovi\u0107"
                ],
                "title": "chrF: character n-gram F-score for automatic MT evaluation",
                "pub_date": "2015",
                "pub_title": "Proceedings of the Tenth Workshop on Statistical Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "370-ARR_v1_90",
            "content": "Matt Post, A call for clarity in reporting BLEU scores, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Matt Post"
                ],
                "title": "A call for clarity in reporting BLEU scores",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Third Conference on Machine Translation: Research Papers",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_91",
            "content": "Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, Lei Li, Glancing transformer for non-autoregressive neural machine translation, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Lihua Qian",
                    "Hao Zhou",
                    "Yu Bao",
                    "Mingxuan Wang",
                    "Lin Qiu",
                    "Weinan Zhang",
                    "Yong Yu",
                    "Lei Li"
                ],
                "title": "Glancing transformer for non-autoregressive neural machine translation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "370-ARR_v1_92",
            "content": "Ricardo Rei, Craig Stewart, Ana Farinha, Alon Lavie, COMET: A neural framework for MT evaluation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Ricardo Rei",
                    "Craig Stewart",
                    "Ana Farinha",
                    "Alon Lavie"
                ],
                "title": "COMET: A neural framework for MT evaluation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "370-ARR_v1_93",
            "content": "Roberts Rozis, Raivis Skadin, Tilde MODEL -multilingual open data for EU languages, 2017, Proceedings of the 21st Nordic Conference on Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Roberts Rozis",
                    "Raivis Skadin"
                ],
                "title": "Tilde MODEL -multilingual open data for EU languages",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 21st Nordic Conference on Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "370-ARR_v1_94",
            "content": "Chitwan Saharia, William Chan, Saurabh Saxena, Mohammad Norouzi, Non-autoregressive machine translation with latent alignments, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Chitwan Saharia",
                    "William Chan",
                    "Saurabh Saxena",
                    "Mohammad Norouzi"
                ],
                "title": "Non-autoregressive machine translation with latent alignments",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "370-ARR_v1_95",
            "content": "UNKNOWN, None, 2019, Wikimatrix: Mining 135M parallel sentences in 1620 language pairs from wikipedia, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Wikimatrix: Mining 135M parallel sentences in 1620 language pairs from wikipedia",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_96",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Improving neural machine translation models with monolingual data, 2016, Proceedings of the 54th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Rico Sennrich",
                    "Barry Haddow",
                    "Alexandra Birch"
                ],
                "title": "Improving neural machine translation models with monolingual data",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 54th",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_97",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_98",
            "content": "Zhiqing Sun, Zhuohan Li, Haoqing Wang, Di He, Zi Lin, Zhihong Deng, Fast structured decoding for sequence models, 2019, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Zhiqing Sun",
                    "Zhuohan Li",
                    "Haoqing Wang",
                    "Di He",
                    "Zi Lin",
                    "Zhihong Deng"
                ],
                "title": "Fast structured decoding for sequence models",
                "pub_date": "2019",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "370-ARR_v1_99",
            "content": "J\u00f6rg Tiedemann, Parallel data, tools and interfaces in OPUS, 2012, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "J\u00f6rg Tiedemann"
                ],
                "title": "Parallel data, tools and interfaces in OPUS",
                "pub_date": "2012",
                "pub_title": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_100",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Illia Kaiser,  Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "Illia Kaiser",
                    " Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "370-ARR_v1_101",
            "content": "Yiren Wang, Fei Tian, Di He, Tao Qin, Chengxiang Zhai, Tie-Yan Liu, Non-autoregressive machine translation with auxiliary regularization, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Yiren Wang",
                    "Fei Tian",
                    "Di He",
                    "Tao Qin",
                    "Chengxiang Zhai",
                    "Tie-Yan Liu"
                ],
                "title": "Non-autoregressive machine translation with auxiliary regularization",
                "pub_date": "2019",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "370-ARR_v1_102",
            "content": "Chunting Zhou, Jiatao Gu, Graham Neubig, Understanding knowledge distillation in nonautoregressive machine translation, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Chunting Zhou",
                    "Jiatao Gu",
                    "Graham Neubig"
                ],
                "title": "Understanding knowledge distillation in nonautoregressive machine translation",
                "pub_date": "2020-04-26",
                "pub_title": "8th International Conference on Learning Representations",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "370-ARR_v1_0@0",
            "content": "Non-Autoregressive Machine Translation: It's Not as Fast as it Seems",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_0",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_2@0",
            "content": "Efficient machine translation models are commercially important as they can increase inference speeds, and reduce costs and carbon emissions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_2",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_2@1",
            "content": "Recently, there has been much interest in non-autoregressive (NAR) models, which promise faster translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_2",
            "start": 142,
            "end": 249,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_2@2",
            "content": "In parallel to the research on NAR models, there have been successful attempts to create optimized autoregressive models as part of the WMT shared task on efficient translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_2",
            "start": 251,
            "end": 427,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_2@3",
            "content": "In this paper, we point out flaws in the evaluation methodology present in the literature on NAR models and we provide fair comparison between a state-of-the-art NAR model and the autoregressive submissions to the shared task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_2",
            "start": 429,
            "end": 654,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_2@4",
            "content": "We make the case for consistent evaluation of NAR models, and also for the importance of comparing NAR models with other widely used efficiency approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_2",
            "start": 656,
            "end": 810,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_2@5",
            "content": "We run experiments with a connectionist-temporalclassification-based (CTC) NAR model implemented in C++ and compare it with AR models using wall clock times.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_2",
            "start": 812,
            "end": 968,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_2@6",
            "content": "Our results show that, although NAR models are faster on GPUs, with small batch sizes, they are nearly always slower under more realistic usage conditions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_2",
            "start": 970,
            "end": 1124,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_2@7",
            "content": "We call for more realistic and extensive evaluation of NAR models in future work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_2",
            "start": 1126,
            "end": 1206,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_4@0",
            "content": "Non-autoregressive neural machine translation (NAR NMT, or NAT; Gu et al., 2018;Lee et al., 2018) is an emerging subfield of NMT which focuses on lowering the decoding time complexity by changing the model architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_4",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_5@0",
            "content": "The defining feature of non-autoregressive models is the conditional independence assumption on the output probability distributions; this is in contrast to autoregressive models, where the output distributions are conditioned on the previous outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_5",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_5@1",
            "content": "This conditional independence allows one to decode the target tokens in parallel.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_5",
            "start": 252,
            "end": 332,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_5@2",
            "content": "This can substantially reduce the decoding time, especially for longer target sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_5",
            "start": 334,
            "end": 421,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_6@0",
            "content": "The speed of the decoding is assessed by translating a test set and measuring the overall time the process takes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_6",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_6@1",
            "content": "This may sound simple, but there are various aspects to be considered that can affect decoding speed, such as batching, number of hypotheses in beam search or hardware used (i.e., using CPU or GPU).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_6",
            "start": 114,
            "end": 311,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_6@2",
            "content": "Decoding speed evaluation is a challenging task, especially when it comes to comparability across different approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_6",
            "start": 313,
            "end": 431,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_6@3",
            "content": "Unlike the translation quality, decoding speed can be measured exactly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_6",
            "start": 433,
            "end": 503,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_6@4",
            "content": "However, also unlike the translation quality, different results are obtained under different evaluation environments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_6",
            "start": 505,
            "end": 621,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_6@5",
            "content": "The WMT Efficient Translation Shared Task aims to evaluate efficiency research and encourages the reporting of a range of speed and translation quality values to better understand the trade-off across different model configurations .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_6",
            "start": 623,
            "end": 855,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_6@6",
            "content": "In this paper, we follow the emerging best practices developed in the Efficiency Shared Task and directly compare with the submitted systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_6",
            "start": 857,
            "end": 997,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_7@0",
            "content": "Over the course of research on NAR models, modeling error and its subsequent negative effect on translation quality remains the biggest issue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_7",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_7@1",
            "content": "Therefore, the goal of contemporary research is to close the performance gap between the AR models and their NAR counterparts, while maintaining high decoding speed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_7",
            "start": 143,
            "end": 307,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_7@2",
            "content": "Considering these stated research goals, the evaluation should comprise of assessing translation quality as well as decoding speed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_7",
            "start": 309,
            "end": 439,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_8@0",
            "content": "Translation quality is usually evaluated by scoring translations of an unseen test set either using automatic metrics, such as BLEU (Papineni et al., 2002), ChrF (Popovi\u0107, 2015) or COMET (Rei et al., 2020), or using human evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_8",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_8@1",
            "content": "To prevent the methods from eventually overfitting a single test set, new test sets are published each year as part of the WMT News Translation Shared Task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_8",
            "start": 234,
            "end": 389,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_8@2",
            "content": "In contrast, translation quality evaluation in NAR research is often measured using BLEU scores only, measured almost exclusively on the WMT 14 English-German test set, which is highly problematic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_8",
            "start": 391,
            "end": 587,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_8@3",
            "content": "Automatic evaluation of translation quality however remains an open research problem (Mathur et al., 2020;Kocmi et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_8",
            "start": 589,
            "end": 714,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_8@4",
            "content": "In our experiments, we follow the recent best practices by using multiple metrics and recent test sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_8",
            "start": 716,
            "end": 818,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_9@0",
            "content": "In this paper, we examine the evaluation methodology generally accepted in literature on NAR methods, and we identify a number of flaws.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_9",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_9@1",
            "content": "First, the results are reported on different hardware architectures, which makes them incomparable, even when comparing only relative speedups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_9",
            "start": 137,
            "end": 279,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_9@2",
            "content": "Second, most of the methods only report latency (decoding with a single sentence per batch) using a GPU; we show that this is the only setup favors NAR models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_9",
            "start": 281,
            "end": 439,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_9@3",
            "content": "Third, the reported baseline performance is usually questionable, both in terms of speed and translation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_9",
            "start": 441,
            "end": 553,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_9@4",
            "content": "Finally, despite the fact that the main motivation for using NAR models is the lower time complexity, the findings of the efficiency task are ignored in most of the NAR papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_9",
            "start": 555,
            "end": 730,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_10@0",
            "content": "We try to connect the separate worlds of NAR and efficient translation research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_10",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_10@1",
            "content": "We train nonautoregressive models based on connectionist temporal classification (CTC), an approach previously shown to be effective (Libovick\u00fd and Helcl, 2018;Gu and Kong, 2021;Ghazvininejad et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_10",
            "start": 81,
            "end": 286,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_10@2",
            "content": "We employ a number of techniques for improving the translation quality, including data cleaning and sequence-level knowledge distillation (Kim and Rush, 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_10",
            "start": 288,
            "end": 446,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_10@3",
            "content": "We evaluate our models following a unified evaluation methodology: In order to compare the translation quality with the rest of the NAR literature, we report BLEU scores measured on the WMT 14 test set, on which we achieve state-ofthe-art performance among (both single-step and iterative) NAR methods; we evaluate the translation quality and decoding speed of our models in the same conditions as the efficiency task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_10",
            "start": 448,
            "end": 865,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_11@0",
            "content": "We find that despite achieving very good results among the NAT models on the WMT 14 test set, our models fall behind in translation quality when measured on the recent WMT 21 test set using three different automatic evaluation metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_11",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_11@1",
            "content": "Moreover, we show that GPU decoding latency is the only scenario in which non-autoregressive models outperform autoregressive models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_11",
            "start": 236,
            "end": 368,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_11@2",
            "content": "This paper contributes to the research community in the following aspects: First, we point out to weaknesses in standard evaluation methodology of non-autoregressive models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_11",
            "start": 370,
            "end": 542,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_11@3",
            "content": "Second, we link the worlds of non-autoregressive translation and model optimization to provide a better understanding of the results achieved in the related work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_11",
            "start": 544,
            "end": 705,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_12@0",
            "content": "Non-Autoregressive NMT",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_12",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_13@0",
            "content": "The current state-of-the-art NMT models are autoregressive -the output distributions are conditioned on the previously generated tokens (Bahdanau et al., 2016;Vaswani et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_13",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_13@1",
            "content": "The decoding process is sequential in its nature, limiting the opportunities for parallelization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_13",
            "start": 182,
            "end": 278,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_14@0",
            "content": "Non-autoregressive models use output distributions which are conditionally independent on each other, which opens up the possibility of parallelization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_14",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_14@1",
            "content": "Formally, the probability of a sequence y given the input x in a non-autoregressive model with parameters \u03b8 is modeled as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_14",
            "start": 153,
            "end": 273,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_15@0",
            "content": "p \u03b8 (y|x) = y i \u2208y p(y i |x, \u03b8).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_15",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_16@0",
            "content": "(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_16",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_17@0",
            "content": "Unsurprisingly, the independence assumption in NAR models has a negative impact on the translation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_17",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_17@1",
            "content": "The culprit for this behavior is the multimodality problem -the inability of the model to differentiate between different modes of the joint probability distribution over output sequences inside the distributions corresponding to individual time steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_17",
            "start": 108,
            "end": 359,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_17@2",
            "content": "A classic example of this issue is the sentence \"Thank you\" with its two equally probable German translations \"Danke sch\u00f6n\" and \"Vielen Dank\" (Gu et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_17",
            "start": 361,
            "end": 520,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_17@3",
            "content": "Because of the independence assumption, a non-autoregressive model cannot assign high probabilities to these two translations without also allowing for the incorrect sentences \"Vielen sch\u00f6n\" and \"Danke Dank\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_17",
            "start": 522,
            "end": 729,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_18@0",
            "content": "Knowledge distillation (Kim and Rush, 2016) has been successfully employed to reduce the negative influence of the multimodality problem in NAR models (Gu et al., 2018;Saharia et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_18",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_18@1",
            "content": "Synthetic data tends to be less diverse than authentic texts, therefore the number of equally likely translation candidates gets smaller (Zhou et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_18",
            "start": 191,
            "end": 347,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_19@0",
            "content": "A number of techniques have been proposed for training NAR models, including iterative methods (Lee et al., 2018;Ghazvininejad et al., 2019), auxiliary training objectives Qian et al., 2021), or latent variables (Gu et al., 2018;Lee et al., 2018;Kaiser et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_19",
            "start": 0,
            "end": 266,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_19@1",
            "content": "In some form, all of the aforementioned approaches use explicit target length estimation, and rely on one-to-one correspondence between the output distributions and the reference sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_19",
            "start": 268,
            "end": 455,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_20@0",
            "content": "A group of methods that relax the requirement of the strict one-to-one alignment between the model outputs and the ground-truth target sequence include aligned cross-entropy (Ghazvininejad et al., 2020) and connectionist temporal classification (Libovick\u00fd and Helcl, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_20",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_21@0",
            "content": "The schema CTC-based model, as proposed by Libovick\u00fd and Helcl (2018), is shown in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_21",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_21@1",
            "content": "The model extends the Transformer architecture (Vaswani et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_21",
            "start": 93,
            "end": 162,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_21@2",
            "content": "It consists of an encoder, a state-splitting layer, and a non-autoregressive decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_21",
            "start": 164,
            "end": 248,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_21@3",
            "content": "The encoder has the same architecture as in the Transformer model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_21",
            "start": 250,
            "end": 315,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_21@4",
            "content": "The state-splitting layer, applied on the encoder output, linearly projects and splits each state into k states with the same dimension.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_21",
            "start": 317,
            "end": 452,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_21@5",
            "content": "The decoder consists of a stack of Transformer layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_21",
            "start": 454,
            "end": 507,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_21@6",
            "content": "Unlike the Transformer model, the self-attention in the non-autoregressive decoder does not use the causal mask, so the model is not prevented from attending to future states.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_21",
            "start": 509,
            "end": 683,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_21@7",
            "content": "Since the output length is fixed to k-times the length of the source sentence, the model is permitted to output blank tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_21",
            "start": 685,
            "end": 809,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_21@8",
            "content": "Different positions of the blank tokens in the output sequence represent different alignments between the outputs and the groundtruth sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_21",
            "start": 811,
            "end": 953,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_21@9",
            "content": "Connectionist temporal classification (Graves et al., 2006) is a dynamic algorithm that efficiently computes the standard cross-entropy loss summed over all possible alignments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_21",
            "start": 955,
            "end": 1131,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_22@0",
            "content": "We choose the CTC-based architecture for our models because it has been previously shown to be effective for NAR NMT (Gu and Kong, 2021;Saharia et al., 2020) and performs well in the context of the non-autoregressive research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_22",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_22@1",
            "content": "It is also one of the fastest NAR architectures since it is not iterative.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_22",
            "start": 227,
            "end": 300,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_23@0",
            "content": "Evaluation Methodology",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_23",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_24@0",
            "content": "The research goal of the non-autoregressive methods is to improve the translation quality while maintaining the speedup brought by the conditional independence assumption.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_24",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_24@1",
            "content": "This means that careful thought should be given to both quantifying the speed gains and the translation quality evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_24",
            "start": 172,
            "end": 294,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_24@2",
            "content": "The speed-vs-quality trade-off can be characterized by the Pareto frontier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_24",
            "start": 296,
            "end": 370,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_24@3",
            "content": "In this section we discuss the evaluation from both perspectives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_24",
            "start": 372,
            "end": 436,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_25@0",
            "content": "Translation Quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_25",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_25@1",
            "content": "In the world of nonautoregressive NMT, the experimental settings are not very diverse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_25",
            "start": 21,
            "end": 106,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_25@2",
            "content": "The primary language pair for translation experiments is English-German, sometimes accompanied by English-Romanian to simulate the low-resource scenario.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_25",
            "start": 108,
            "end": 260,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_25@3",
            "content": "These language pairs, along with the widely used test sets -WMT 14 (Bojar et al., 2014) for En-De and WMT 16 (Bojar et al., 2016) for En-Ro -became the de facto standard benchmark for NAR model evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_25",
            "start": 262,
            "end": 466,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_26@0",
            "content": "A common weakness seen in the literature is the use of weak baseline models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_26",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_26@1",
            "content": "The base variant of the Transformer model is used almost exclusively (Gu et al., 2018;Gu and Kong, 2021;Lee et al., 2018;Ghazvininejad et al., 2020;Qian et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_26",
            "start": 77,
            "end": 243,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_26@2",
            "content": "We argue that using weaker baselines might lead to overrating the positive effects brought by proposed improvements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_26",
            "start": 245,
            "end": 360,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_26@3",
            "content": "Since the baseline autoregressive models are used to generate the synthetic parallel data for knowledge distillation, the weakness is potentially further amplified in this step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_26",
            "start": 362,
            "end": 538,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_27@0",
            "content": "Evaluation is normally with automatic metrics only, and often only BLEU is reported.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_27",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_27@1",
            "content": "In light of recent research casting further doubt on the reliability of BLEU as a measure of translation quality (Kocmi et al., 2021), we argue that this is insufficient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_27",
            "start": 85,
            "end": 254,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_28@0",
            "content": "Decoding Speed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_28",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_28@1",
            "content": "The current standard in evaluation of NAR models is to measure translation latency using a GPU, i.e., the average time to translate a single sentence without batching.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_28",
            "start": 16,
            "end": 182,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_28@2",
            "content": "Since the time depends on the hardware, relative speedup is usually reported along with latency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_28",
            "start": 184,
            "end": 279,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_28@3",
            "content": "This is a reasonable approach but we need to keep in mind the associated difficulties.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_28",
            "start": 281,
            "end": 366,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_28@4",
            "content": "First, the results achieved on different hardware architectures are not easily comparable even when considering the relative speedups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_28",
            "start": 368,
            "end": 501,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_28@5",
            "content": "We also note that the relative speedup values should always be accompanied by the corresponding decoding times in absolute numbers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_28",
            "start": 503,
            "end": 633,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_28@6",
            "content": "Sometimes, this information is missing from the published results (Qian et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_28",
            "start": 635,
            "end": 720,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_29@0",
            "content": "We argue that measuring only GPU latency disregards other use-cases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_29",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_29@1",
            "content": "In the WMT Efficiency Shared Task, the decoding speed is measured in five scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_29",
            "start": 69,
            "end": 152,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_29@2",
            "content": "The speed is reported using a GPU with and without batching, using all 36 CPU cores (also, with and without batching), and using a single CPU core without batching.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_29",
            "start": 154,
            "end": 317,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_29@3",
            "content": "In batched decoding, the shared task participants could choose the optimal batch size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_29",
            "start": 319,
            "end": 404,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_29@4",
            "content": "Our results in Section 5 show that measuring latency is the only one that favors NAR models, and as the batch size increases, AR models quickly reach higher translation speeds.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_29",
            "start": 406,
            "end": 581,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_30@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_30",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_31@0",
            "content": "We experiment with non-autoregressive models for English-German translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_31",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_31@1",
            "content": "We used the data provided by the WMT 21 News Translation Shared Task organizers (Akhbardeh et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_31",
            "start": 77,
            "end": 181,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_32@0",
            "content": "As our baseline model, we use the CTC-based NAR model as described by Libovick\u00fd and Helcl (2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_32",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_32@1",
            "content": "We use stack of 6 encoder and 6 decoder layers, separated by the state splitting layer which extends the state sequence 3 times.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_32",
            "start": 98,
            "end": 225,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_33@0",
            "content": "We implement our models in the Marian toolkit .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_33",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_33@1",
            "content": "For the CTC loss computation, we use the warp-ctc library (Amodei et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_33",
            "start": 48,
            "end": 127,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_34@0",
            "content": "Teacher Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_34",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_35@0",
            "content": "For training our baseline autoregressive models, we closely follow the approach of .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_35",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_35@1",
            "content": "The preparation of the baseline models consists of three phases -data cleaning, backtranslation, and the training of the final models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_35",
            "start": 85,
            "end": 218,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_36@0",
            "content": "We train the teacher models on cleaned parallel corpora and backtranslated monolingual data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_36",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_36@1",
            "content": "For the parallel data, we used Europarl (Koehn, 2005), the RAPID corpus (Rozis and Skadin , \u0161, 2017), and the News Commentary corpus from OPUS (Tiedemann, 2012).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_36",
            "start": 93,
            "end": 253,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_36@2",
            "content": "We consider these three parallel dataset clean.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_36",
            "start": 255,
            "end": 301,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_36@3",
            "content": "We also use noisier parallel datasets, namely Paracrawl (Ba\u00f1\u00f3n et al., 2020), Common Crawl 1 , WikiMatrix (Schwenk et al., 2019), and Wikititles 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_36",
            "start": 303,
            "end": 450,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_36@4",
            "content": "For backtranslation, we used the monolingual datasets from the News Crawl from the years 2018-2020, in both English and German.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_36",
            "start": 452,
            "end": 578,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_37@0",
            "content": "We clean the parallel corpus (i.e. both clean and noisy portions) using rule-based cleaning 3 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_37",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_37@1",
            "content": "Additionally, we exclude sentence pairs with nonlatin characters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_37",
            "start": 96,
            "end": 160,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_37@2",
            "content": "Additionally, we apply dual crossentropy filtering on the noisy part of the parallel data (Junczys-Dowmunt, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_37",
            "start": 162,
            "end": 275,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_37@3",
            "content": "We train Transformer base models in both directions on the clean portion of the parallel data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_37",
            "start": 277,
            "end": 370,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_37@4",
            "content": "Then, we select 75% of the best-scoring sentence pairs into the final parallel portion of the training dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_37",
            "start": 372,
            "end": 482,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_38@0",
            "content": "For backtranslation (Sennrich et al., 2016), we train four Transformer big models on the cleaned parallel data in both directions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_38",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_38@1",
            "content": "We then use them in an ensemble to create the synthetic source side for the monolingual corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_38",
            "start": 131,
            "end": 226,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_38@2",
            "content": "We add a special symbol to the generated sentences to help the models differentiate between synthetic and authentic source language data (Caswell et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_38",
            "start": 228,
            "end": 387,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_39@0",
            "content": "We use hyperparameters of the Transformer big model, i.e. model dimension 1,024, feed-forward hidden dimension of 4,096, and 16 attention heads.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_39",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_40@0",
            "content": "For training, we use the Adam optimizer (Kingma and Ba, 2014) with \u03b2 1 , \u03b2 2 and \u03f5 set to 0.9, 0.998 and 10 -9 respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_40",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_40@1",
            "content": "We used the inverted squareroot learning rate decay with 8,000 of linear warmup and initial learning rate of 10 -4 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_40",
            "start": 125,
            "end": 240,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_41@0",
            "content": "The teacher models follow the same hyperparameter settings as the models for backtranslation, but are trained with the tagged backtranslations included in the data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_41",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_41@1",
            "content": "As in the previous case, we train four teacher models with different random seeds for ensembling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_41",
            "start": 165,
            "end": 261,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_42@0",
            "content": "Similar to creating the backtranslations, we use the four teacher models in an ensemble to create the knowledge-distilled data (Kim and Rush, 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_42",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_42@1",
            "content": "We translate the source side of the parallel data, as well as the source-language monolingual data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_42",
            "start": 149,
            "end": 247,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_42@2",
            "content": "We do not translate back-translated data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_42",
            "start": 249,
            "end": 289,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_42@3",
            "content": "Thus, the source side data for the student models is authentic, and the target side is synthetic, created by the teacher models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_42",
            "start": 291,
            "end": 418,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_43@0",
            "content": "Student Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_43",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_44@0",
            "content": "We train five variants of the student models with different hyperparameter settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_44",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_44@1",
            "content": "The \"Large\" model is our baseline model -the same number of layers as the teacher models, 6 in the encoder, followed by the state splitting layer, and another 6 layers in the decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_44",
            "start": 85,
            "end": 267,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_44@2",
            "content": "The \"Base\" model has the same number of layers with reduced dimension of the embeddings and the feed-forward Transformer sublayer, to match the Transformer base settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_44",
            "start": 269,
            "end": 438,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_44@3",
            "content": "We also try reducing the numbers of encoder and decoder layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_44",
            "start": 440,
            "end": 502,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_44@4",
            "content": "We shrink the base model to 3-3 (\"Small\"), 2-2 (\"Micro\"), and 1-1 (\"Tiny\") architectures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_44",
            "start": 504,
            "end": 592,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_45@0",
            "content": "We run the training of each model for three weeks on four Nvidia Pascal P100 GPUs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_45",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_46@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_46",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_47@0",
            "content": "In this section, we try and view the results of the NAR and efficiency research in a shared perspective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_47",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_47@1",
            "content": "We evaluate our models and present results in terms of translation quality and decoding speed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_47",
            "start": 105,
            "end": 198,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_47@2",
            "content": "We compare the results to the related work on both non-autoregressive translation and model optimization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_47",
            "start": 200,
            "end": 304,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_48@0",
            "content": "Translation Quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_48",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_48@1",
            "content": "The research on nonautoregressive models uses the BLEU score (Papineni et al., 2002) measured on the WMT 14 test set (Bojar et al., 2014) as a standard benchmark for evaluating translation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_48",
            "start": 21,
            "end": 217,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_48@2",
            "content": "We use Sacrebleu (Post, 2018) as the implementation of the BLEU score metric.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_48",
            "start": 219,
            "end": 295,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_48@3",
            "content": "4 Using a single test set for the whole volume of research on this topic may however produce misleading results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_48",
            "start": 297,
            "end": 408,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_48@4",
            "content": "up to date with the current state-of-the-art translation systems, we also evaluate our models using COMET (Rei et al., 2020) 5 and BLEU scores on the recent WMT 21 test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_48",
            "start": 410,
            "end": 582,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_48@5",
            "content": "The same test set was used in the WMT 21 Efficiency Task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_48",
            "start": 584,
            "end": 640,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_49@0",
            "content": "Table 2 shows the BLEU scores of our NAR models on the WMT 14 test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_49",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_49@1",
            "content": "We show the results of the five variants of the NAR models and we include three of the best-performing NAR approaches from the related work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_49",
            "start": 72,
            "end": 211,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_49@2",
            "content": "We see from the table that using BLEU, the \"Large\" model scores among the best NAR models on the WMT 14 test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_49",
            "start": 213,
            "end": 325,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_49@3",
            "content": "As the NAR model size decreases, so does the translation quality, with the notable exception of the En\u2192De \"Micro\" model, which outperforms the \"Base\" model consistently on different test sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_49",
            "start": 327,
            "end": 518,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_50@0",
            "content": "In Table 3, we report the automatic evaluation results of our AR and NAR models on the multireference WMT 21 test set (Akhbardeh et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_50",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_50@1",
            "content": "We compare our NAR models to the AR large teacher models from Section 4.1, an AR base model trained on the original clean data, and an AR base student model trained on the distilled data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_50",
            "start": 144,
            "end": 330,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_50@2",
            "content": "Following , we use references A, C, and D for English-German translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_50",
            "start": 332,
            "end": 404,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_51@0",
            "content": "We see that there is a considerable difference in the translation quality between the NAR models and the AR large teacher model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_51",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_51@1",
            "content": "This difference grows with beam search and ensembling applied on the AR decoding, techniques not usually used with NAR models because of the speed cost.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_51",
            "start": 129,
            "end": 280,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_51@2",
            "content": "We also note that when we train an AR base model on the distilled data, it outperforms the NAR large model by a considerable margin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_51",
            "start": 282,
            "end": 413,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_52@0",
            "content": "Another thing we notice is the enormous difference in the COMET scores between the AR and NAR models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_52",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_52@1",
            "content": "The AR base models achieve comparable BLEU scores to the NAR large models, but differ substantially in the COMET score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_52",
            "start": 102,
            "end": 220,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_52@2",
            "content": "From a look at the system outputs, we hypothesize that the NAR systems produce unusual errors which BLEU does not penalise as heavily as COMET.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_52",
            "start": 222,
            "end": 364,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_52@3",
            "content": "This might suggest that NAR models would rank poorly in human evaluation relative to their autoregressive counterparts, despite the reasonable BLEU score values.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_52",
            "start": 366,
            "end": 526,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_53@0",
            "content": "Decoding speed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_53",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_53@1",
            "content": "We follow the decoding time evaluation methodology of the WMT 21 Efficient Translation Shared Task .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_53",
            "start": 16,
            "end": 115,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_53@2",
            "content": "We recreate the hardware conditions that were used in the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_53",
            "start": 117,
            "end": 179,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_53@3",
            "content": "For the GPU decoding measurements, we use a single Nvidia Ampere A100 GPU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_53",
            "start": 181,
            "end": 254,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_53@4",
            "content": "The CPU evaluation was performed on a 36-core CPU Intel Xeon Gold 6354 server from Oracle cloud.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_53",
            "start": 256,
            "end": 351,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_54@0",
            "content": "To amortize for the various computation overheads, the models submitted to the shared task are evaluated on a million sentence benchmark dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_54",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_54@1",
            "content": "We measure the overall wall time to translate the shared task dataset with different batching settings on both the GPU and the 36-core CPU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_54",
            "start": 146,
            "end": 284,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_54@2",
            "content": "The decoding times are shown in Figures 2 and 4 for the GPU and CPU times, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_54",
            "start": 286,
            "end": 373,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_54@3",
            "content": "We do not report the single-core CPU latencies as the decoding speed of the NAR models falls far behind the efficient AR models in this setup and the translation of the Model Latency (ms) Gu et al. (2018) 39 22 37",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_54",
            "start": 375,
            "end": 587,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_55@0",
            "content": "Ours -Large 14 dataset takes too long.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_55",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_56@0",
            "content": "We can see that in case of GPU decoding that all models benefit from having larger batch sizes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_56",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_56@1",
            "content": "However, the non-autoregressive models are much faster when the batch size is small.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_56",
            "start": 96,
            "end": 179,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_56@2",
            "content": "We also ran the evaluation on an Nvidia Pascal P100 GPU, which showed that when the batch size is large enough, autoregressive models eventually match the speed of non-autoregressive models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_56",
            "start": 181,
            "end": 370,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_56@3",
            "content": "We show the decoding times on the Pascal GPU in Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_56",
            "start": 372,
            "end": 428,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_56@4",
            "content": "In Table 4, we compare the latencies measured on the Pascal GPU to some of the related NAR approaches that report results on this GPU type.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_56",
            "start": 430,
            "end": 568,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_56@5",
            "content": "Due to implementation reasons, the maximum batch size for our NAR models is around 220 sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_56",
            "start": 570,
            "end": 666,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_57@0",
            "content": "Comparison with Efficient AR Models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_57",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_57@1",
            "content": "In Table 5, we present a comparison on the million sentence test set with \"Edinburgh base\", one of the leading submissions in the WMT 21 efficiency task (Behnke et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_57",
            "start": 37,
            "end": 211,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_57@2",
            "content": "First, we see that using three different evaluation metrics (ChrF, COMET, and BLEU), our models lag behind the Edinburgh base model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_57",
            "start": 213,
            "end": 344,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_57@3",
            "content": "In line with our previous observation, we see a considerable drop in the COMET score values.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_57",
            "start": 346,
            "end": 437,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_57@4",
            "content": "In terms of decoding speed, the only scenario in which the non-autoregressive model is better is on GPU with batch size 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_57",
            "start": 439,
            "end": 560,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_57@5",
            "content": "This is in line with our intuition that the parallelization potential brought by the GPU is utilized more efficiently by the NAR model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_57",
            "start": 562,
            "end": 696,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_57@6",
            "content": "On one hand, larger batches open up the parallelization possibilities to AR models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_57",
            "start": 698,
            "end": 780,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_57@7",
            "content": "On the other hand, limited parallelization potential (in form of CPU decoding) reduces the differences between AR and NAR models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_57",
            "start": 782,
            "end": 910,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_57@8",
            "content": "Table 5: A comparison of our AR and NAR models with one of the submissions to the WMT 21 efficiency task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_57",
            "start": 912,
            "end": 1016,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_57@9",
            "content": "We show the results of automatic translation quality evaluation using three different metrics, and the decoding time to translate the test set using a GPU and 36-core CPU with either latency (b=1) or batched (b>1) decoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_57",
            "start": 1018,
            "end": 1240,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_58@0",
            "content": "Conclusions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_58",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_59@0",
            "content": "In this paper, we challenge the evaluation methodology adopted by the research on non-autoregressive models for NMT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_59",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_59@1",
            "content": "We argue that in terms of translation quality, the evaluation should include newer test sets and metrics other than BLEU (particularly COMET and ChrF).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_59",
            "start": 117,
            "end": 267,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_59@2",
            "content": "This will provide more insight and put the results into the context of the recent research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_59",
            "start": 269,
            "end": 359,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_60@0",
            "content": "From the decoding speed perspective, we should bear in mind various use-cases for the model deployment, such as the hardware environment or batching conditions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_60",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_60@1",
            "content": "Preferably, the research should evaluate the speed gains across a range of scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_60",
            "start": 161,
            "end": 245,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_60@2",
            "content": "Finally, given that the latency condition -translation of one sentence at a time on a GPU -already translates too fast to be perceived by human users of MT, there is currently no compelling scenario that warrants the deployment of NAR models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_60",
            "start": 247,
            "end": 488,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_61@0",
            "content": "Farhad Akhbardeh, Arkady Arkhangorodsky, Magdalena Biesialska, Ond\u0159ej Bojar, Rajen Chatterjee, Vishrav Chaudhary, Marta Costa-Jussa, Cristina Espa\u00f1a-Bonet, Angela Fan, Christian Federmann, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Leonie Harter, Kenneth Heafield, Christopher Homan, Matthias Huck, Kwabena Amponsah-Kaakyire, Jungo Kasai, Daniel Khashabi, Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Allahsera Auguste Tapo, Marco Turchi, Valentin Vydrin, and Marcos Zampieri. 2021. Findings of the 2021 conference on machine translation (WMT21), , Proceedings of the Sixth Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_61",
            "start": 0,
            "end": 661,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_62@0",
            "content": "Dario Amodei, Rishita Sundaram Ananthanarayanan, Jingliang Anubhai, Eric Bai, Carl Battenberg, Jared Case, Bryan Casper, Qiang Catanzaro, Guoliang Cheng,  Chen, Deep speech 2: End-to-end speech recognition in english and mandarin, 2016, International conference on machine learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_62",
            "start": 0,
            "end": 287,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_63@0",
            "content": "UNKNOWN, None, 2016, Neural machine translation by jointly learning to align and translate, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_63",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_64@0",
            "content": "Marta Ba\u00f1\u00f3n, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Espl\u00e0-Gomis, Mikel Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Rojas, Leopoldo Sempere, Gema Ram\u00edrez-S\u00e1nchez, ParaCrawl: Web-scale acquisition of parallel corpora, 2020, Proceedings of the 58th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_64",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_65@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_65",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_66@0",
            "content": "Maximiliana Behnke, Nikolay Bogoychev, Alham Fikri Aji, Kenneth Heafield, Graeme Nail, Qianqian Zhu, Svetlana Tchistiakova, Jelmer Van Der Linde, Pinzhen Chen, Sidharth Kashyap, Roman Grundkiewicz, Efficient machine translation with model pruning and quantization, 2021, Proceedings of the Conference on Machine Translation at the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_66",
            "start": 0,
            "end": 400,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_67@0",
            "content": "Ond\u0159ej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, Ale\u0161 Tamchyna, Findings of the 2014 workshop on statistical machine translation, 2014, Proceedings of the Ninth Workshop on Statistical Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_67",
            "start": 0,
            "end": 343,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_68@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_68",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_69@0",
            "content": "Mariana N\u00e9v\u00e9ol, Martin Neves, Matt Popel, Raphael Post, Carolina Rubino, Lucia Scarton, Marco Specia, Karin Turchi, Marcos Verspoor,  Zampieri, Findings of the 2016 conference on machine translation, 2016, Proceedings of the First Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_69",
            "start": 0,
            "end": 307,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_70@0",
            "content": "Isaac Caswell, Ciprian Chelba, David Grangier, Tagged back-translation, 2019, Proceedings of the Fourth Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_70",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_71@0",
            "content": "Pinzhen Chen, Jind\u0159ich Helcl, Ulrich Germann, Laurie Burchell, Nikolay Bogoychev, Antonio Valerio Miceli, Jonas Barone, Alexandra Waldendorf, Kenneth Birch,  Heafield, The University of Edinburgh's English-German and English-Hausa submissions to the WMT21 news translation task, 2021, Proceedings of the Conference on Machine Translation at the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_71",
            "start": 0,
            "end": 414,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_72@0",
            "content": "Marjan Ghazvininejad, Vladimir Karpukhin, Luke Zettlemoyer, Omer Levy, Aligned cross entropy for non-autoregressive machine translation, 2020, Proceedings of the 37th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_72",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_73@0",
            "content": "Marjan Ghazvininejad, Omer Levy, Yinhan Liu, Luke Zettlemoyer, Mask-predict: Parallel decoding of conditional masked language models, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_73",
            "start": 0,
            "end": 317,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_74@0",
            "content": "Alex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, J\u00fcrgen Schmidhuber, Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks, 2006, Proceedings of the 23rd International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_74",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_75@0",
            "content": "Jiatao Gu, James Bradbury, Caiming Xiong, O Victor, Richard Li,  Socher, Non-autoregressive neural machine translation, 2018, 6th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_75",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_76@0",
            "content": "Jiatao Gu, Xiang Kong, Fully nonautoregressive neural machine translation: Tricks of the trade, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_76",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_77@0",
            "content": "Kenneth Heafield, Qianqian Zhu, Roman Grundkiewicz, Findings of the WMT 2021 shared task on efficient translation, 2021, Proceedings of the Conference on Machine Translation at the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_77",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_78@0",
            "content": "Marcin Junczys-Dowmunt, Dual conditional cross-entropy filtering of noisy parallel corpora, 2018, Proceedings of the Third Conference on Machine Translation: Shared Task Papers, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_78",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_79@0",
            "content": "Marcin Junczys-Dowmunt, Roman Grundkiewicz, Tomasz Dwojak, Hieu Hoang, Kenneth Heafield, Tom Neckermann, Frank Seide, Ulrich Germann, Alham Fikri Aji, Nikolay Bogoychev, F Andr\u00e9, Alexandra Martins,  Birch, Marian: Fast neural machine translation in C++, 2018, Proceedings of ACL 2018, System Demonstrations, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_79",
            "start": 0,
            "end": 349,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_80@0",
            "content": "Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish Vaswani, Niki Parmar, Jakob Uszkoreit, Noam Shazeer, Fast decoding in sequence models using discrete latent variables, 2018, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_80",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_81@0",
            "content": "Yoon Kim, Alexander Rush, Sequencelevel knowledge distillation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_81",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_82@0",
            "content": "UNKNOWN, None, 2014, Adam: A method for stochastic optimization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_82",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_83@0",
            "content": "UNKNOWN, None, , Hitokazu Matsushita, and Arul Menezes. 2021. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_83",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_84@0",
            "content": "Philipp Koehn, Europarl: A parallel corpus for statistical machine translation, 2005, Proceedings of Machine Translation Summit X: Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_84",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_85@0",
            "content": "Jason Lee, Elman Mansimov, Kyunghyun Cho, Deterministic non-autoregressive neural sequence modeling by iterative refinement, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_85",
            "start": 0,
            "end": 260,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_86@0",
            "content": "Jind\u0159ich Libovick\u00fd, Jind\u0159ich Helcl, End-toend non-autoregressive neural machine translation with connectionist temporal classification, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_86",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_87@0",
            "content": "Nitika Mathur, Timothy Baldwin, Trevor Cohn, Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_87",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_88@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_88",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_89@0",
            "content": "Maja Popovi\u0107, chrF: character n-gram F-score for automatic MT evaluation, 2015, Proceedings of the Tenth Workshop on Statistical Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_89",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_90@0",
            "content": "Matt Post, A call for clarity in reporting BLEU scores, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_90",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_91@0",
            "content": "Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, Lei Li, Glancing transformer for non-autoregressive neural machine translation, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_91",
            "start": 0,
            "end": 338,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_92@0",
            "content": "Ricardo Rei, Craig Stewart, Ana Farinha, Alon Lavie, COMET: A neural framework for MT evaluation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_92",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_93@0",
            "content": "Roberts Rozis, Raivis Skadin, Tilde MODEL -multilingual open data for EU languages, 2017, Proceedings of the 21st Nordic Conference on Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_93",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_94@0",
            "content": "Chitwan Saharia, William Chan, Saurabh Saxena, Mohammad Norouzi, Non-autoregressive machine translation with latent alignments, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_94",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_95@0",
            "content": "UNKNOWN, None, 2019, Wikimatrix: Mining 135M parallel sentences in 1620 language pairs from wikipedia, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_95",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_96@0",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Improving neural machine translation models with monolingual data, 2016, Proceedings of the 54th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_96",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_97@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_97",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_98@0",
            "content": "Zhiqing Sun, Zhuohan Li, Haoqing Wang, Di He, Zi Lin, Zhihong Deng, Fast structured decoding for sequence models, 2019, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_98",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_99@0",
            "content": "J\u00f6rg Tiedemann, Parallel data, tools and interfaces in OPUS, 2012, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_99",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_100@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Illia Kaiser,  Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_100",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_101@0",
            "content": "Yiren Wang, Fei Tian, Di He, Tao Qin, Chengxiang Zhai, Tie-Yan Liu, Non-autoregressive machine translation with auxiliary regularization, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_101",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "370-ARR_v1_102@0",
            "content": "Chunting Zhou, Jiatao Gu, Graham Neubig, Understanding knowledge distillation in nonautoregressive machine translation, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "370-ARR_v1_102",
            "start": 0,
            "end": 190,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "370-ARR_v1_0",
            "tgt_ix": "370-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_0",
            "tgt_ix": "370-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_1",
            "tgt_ix": "370-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_1",
            "tgt_ix": "370-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_0",
            "tgt_ix": "370-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_2",
            "tgt_ix": "370-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_4",
            "tgt_ix": "370-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_5",
            "tgt_ix": "370-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_6",
            "tgt_ix": "370-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_7",
            "tgt_ix": "370-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_8",
            "tgt_ix": "370-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_9",
            "tgt_ix": "370-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_10",
            "tgt_ix": "370-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_3",
            "tgt_ix": "370-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_3",
            "tgt_ix": "370-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_3",
            "tgt_ix": "370-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_3",
            "tgt_ix": "370-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_3",
            "tgt_ix": "370-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_3",
            "tgt_ix": "370-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_3",
            "tgt_ix": "370-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_3",
            "tgt_ix": "370-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_3",
            "tgt_ix": "370-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_0",
            "tgt_ix": "370-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_11",
            "tgt_ix": "370-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_13",
            "tgt_ix": "370-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_14",
            "tgt_ix": "370-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_15",
            "tgt_ix": "370-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_16",
            "tgt_ix": "370-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_17",
            "tgt_ix": "370-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_18",
            "tgt_ix": "370-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_19",
            "tgt_ix": "370-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_20",
            "tgt_ix": "370-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_21",
            "tgt_ix": "370-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_12",
            "tgt_ix": "370-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_12",
            "tgt_ix": "370-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_12",
            "tgt_ix": "370-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_12",
            "tgt_ix": "370-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_12",
            "tgt_ix": "370-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_12",
            "tgt_ix": "370-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_12",
            "tgt_ix": "370-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_12",
            "tgt_ix": "370-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_12",
            "tgt_ix": "370-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_12",
            "tgt_ix": "370-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_12",
            "tgt_ix": "370-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_0",
            "tgt_ix": "370-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_22",
            "tgt_ix": "370-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_24",
            "tgt_ix": "370-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_25",
            "tgt_ix": "370-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_26",
            "tgt_ix": "370-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_27",
            "tgt_ix": "370-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_28",
            "tgt_ix": "370-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_23",
            "tgt_ix": "370-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_23",
            "tgt_ix": "370-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_23",
            "tgt_ix": "370-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_23",
            "tgt_ix": "370-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_23",
            "tgt_ix": "370-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_23",
            "tgt_ix": "370-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_23",
            "tgt_ix": "370-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_0",
            "tgt_ix": "370-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_29",
            "tgt_ix": "370-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_31",
            "tgt_ix": "370-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_32",
            "tgt_ix": "370-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_30",
            "tgt_ix": "370-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_30",
            "tgt_ix": "370-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_30",
            "tgt_ix": "370-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_30",
            "tgt_ix": "370-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_30",
            "tgt_ix": "370-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_33",
            "tgt_ix": "370-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_35",
            "tgt_ix": "370-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_36",
            "tgt_ix": "370-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_37",
            "tgt_ix": "370-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_38",
            "tgt_ix": "370-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_39",
            "tgt_ix": "370-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_40",
            "tgt_ix": "370-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_41",
            "tgt_ix": "370-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_34",
            "tgt_ix": "370-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_34",
            "tgt_ix": "370-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_34",
            "tgt_ix": "370-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_34",
            "tgt_ix": "370-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_34",
            "tgt_ix": "370-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_34",
            "tgt_ix": "370-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_34",
            "tgt_ix": "370-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_34",
            "tgt_ix": "370-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_34",
            "tgt_ix": "370-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_30",
            "tgt_ix": "370-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_42",
            "tgt_ix": "370-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_44",
            "tgt_ix": "370-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_43",
            "tgt_ix": "370-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_43",
            "tgt_ix": "370-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_43",
            "tgt_ix": "370-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_0",
            "tgt_ix": "370-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_45",
            "tgt_ix": "370-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_47",
            "tgt_ix": "370-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_48",
            "tgt_ix": "370-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_49",
            "tgt_ix": "370-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_50",
            "tgt_ix": "370-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_51",
            "tgt_ix": "370-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_52",
            "tgt_ix": "370-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_53",
            "tgt_ix": "370-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_54",
            "tgt_ix": "370-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_55",
            "tgt_ix": "370-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_56",
            "tgt_ix": "370-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_46",
            "tgt_ix": "370-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_46",
            "tgt_ix": "370-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_46",
            "tgt_ix": "370-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_46",
            "tgt_ix": "370-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_46",
            "tgt_ix": "370-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_46",
            "tgt_ix": "370-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_46",
            "tgt_ix": "370-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_46",
            "tgt_ix": "370-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_46",
            "tgt_ix": "370-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_46",
            "tgt_ix": "370-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_46",
            "tgt_ix": "370-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_46",
            "tgt_ix": "370-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_0",
            "tgt_ix": "370-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_57",
            "tgt_ix": "370-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_59",
            "tgt_ix": "370-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_58",
            "tgt_ix": "370-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_58",
            "tgt_ix": "370-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_58",
            "tgt_ix": "370-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "370-ARR_v1_0",
            "tgt_ix": "370-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_1",
            "tgt_ix": "370-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_2",
            "tgt_ix": "370-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_2",
            "tgt_ix": "370-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_2",
            "tgt_ix": "370-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_2",
            "tgt_ix": "370-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_2",
            "tgt_ix": "370-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_2",
            "tgt_ix": "370-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_2",
            "tgt_ix": "370-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_2",
            "tgt_ix": "370-ARR_v1_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_3",
            "tgt_ix": "370-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_4",
            "tgt_ix": "370-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_5",
            "tgt_ix": "370-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_5",
            "tgt_ix": "370-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_5",
            "tgt_ix": "370-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_6",
            "tgt_ix": "370-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_6",
            "tgt_ix": "370-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_6",
            "tgt_ix": "370-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_6",
            "tgt_ix": "370-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_6",
            "tgt_ix": "370-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_6",
            "tgt_ix": "370-ARR_v1_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_6",
            "tgt_ix": "370-ARR_v1_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_7",
            "tgt_ix": "370-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_7",
            "tgt_ix": "370-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_7",
            "tgt_ix": "370-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_8",
            "tgt_ix": "370-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_8",
            "tgt_ix": "370-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_8",
            "tgt_ix": "370-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_8",
            "tgt_ix": "370-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_8",
            "tgt_ix": "370-ARR_v1_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_9",
            "tgt_ix": "370-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_9",
            "tgt_ix": "370-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_9",
            "tgt_ix": "370-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_9",
            "tgt_ix": "370-ARR_v1_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_9",
            "tgt_ix": "370-ARR_v1_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_10",
            "tgt_ix": "370-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_10",
            "tgt_ix": "370-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_10",
            "tgt_ix": "370-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_10",
            "tgt_ix": "370-ARR_v1_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_11",
            "tgt_ix": "370-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_11",
            "tgt_ix": "370-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_11",
            "tgt_ix": "370-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_11",
            "tgt_ix": "370-ARR_v1_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_12",
            "tgt_ix": "370-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_13",
            "tgt_ix": "370-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_13",
            "tgt_ix": "370-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_14",
            "tgt_ix": "370-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_14",
            "tgt_ix": "370-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_15",
            "tgt_ix": "370-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_16",
            "tgt_ix": "370-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_17",
            "tgt_ix": "370-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_17",
            "tgt_ix": "370-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_17",
            "tgt_ix": "370-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_17",
            "tgt_ix": "370-ARR_v1_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_18",
            "tgt_ix": "370-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_18",
            "tgt_ix": "370-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_19",
            "tgt_ix": "370-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_19",
            "tgt_ix": "370-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_20",
            "tgt_ix": "370-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_21",
            "tgt_ix": "370-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_21",
            "tgt_ix": "370-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_21",
            "tgt_ix": "370-ARR_v1_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_21",
            "tgt_ix": "370-ARR_v1_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_21",
            "tgt_ix": "370-ARR_v1_21@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_21",
            "tgt_ix": "370-ARR_v1_21@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_21",
            "tgt_ix": "370-ARR_v1_21@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_21",
            "tgt_ix": "370-ARR_v1_21@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_21",
            "tgt_ix": "370-ARR_v1_21@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_21",
            "tgt_ix": "370-ARR_v1_21@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_22",
            "tgt_ix": "370-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_22",
            "tgt_ix": "370-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_23",
            "tgt_ix": "370-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_24",
            "tgt_ix": "370-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_24",
            "tgt_ix": "370-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_24",
            "tgt_ix": "370-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_24",
            "tgt_ix": "370-ARR_v1_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_25",
            "tgt_ix": "370-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_25",
            "tgt_ix": "370-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_25",
            "tgt_ix": "370-ARR_v1_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_25",
            "tgt_ix": "370-ARR_v1_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_26",
            "tgt_ix": "370-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_26",
            "tgt_ix": "370-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_26",
            "tgt_ix": "370-ARR_v1_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_26",
            "tgt_ix": "370-ARR_v1_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_27",
            "tgt_ix": "370-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_27",
            "tgt_ix": "370-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_28",
            "tgt_ix": "370-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_28",
            "tgt_ix": "370-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_28",
            "tgt_ix": "370-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_28",
            "tgt_ix": "370-ARR_v1_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_28",
            "tgt_ix": "370-ARR_v1_28@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_28",
            "tgt_ix": "370-ARR_v1_28@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_28",
            "tgt_ix": "370-ARR_v1_28@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_29",
            "tgt_ix": "370-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_29",
            "tgt_ix": "370-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_29",
            "tgt_ix": "370-ARR_v1_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_29",
            "tgt_ix": "370-ARR_v1_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_29",
            "tgt_ix": "370-ARR_v1_29@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_30",
            "tgt_ix": "370-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_31",
            "tgt_ix": "370-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_31",
            "tgt_ix": "370-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_32",
            "tgt_ix": "370-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_32",
            "tgt_ix": "370-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_33",
            "tgt_ix": "370-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_33",
            "tgt_ix": "370-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_34",
            "tgt_ix": "370-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_35",
            "tgt_ix": "370-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_35",
            "tgt_ix": "370-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_36",
            "tgt_ix": "370-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_36",
            "tgt_ix": "370-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_36",
            "tgt_ix": "370-ARR_v1_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_36",
            "tgt_ix": "370-ARR_v1_36@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_36",
            "tgt_ix": "370-ARR_v1_36@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_37",
            "tgt_ix": "370-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_37",
            "tgt_ix": "370-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_37",
            "tgt_ix": "370-ARR_v1_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_37",
            "tgt_ix": "370-ARR_v1_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_37",
            "tgt_ix": "370-ARR_v1_37@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_38",
            "tgt_ix": "370-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_38",
            "tgt_ix": "370-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_38",
            "tgt_ix": "370-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_39",
            "tgt_ix": "370-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_40",
            "tgt_ix": "370-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_40",
            "tgt_ix": "370-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_41",
            "tgt_ix": "370-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_41",
            "tgt_ix": "370-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_42",
            "tgt_ix": "370-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_42",
            "tgt_ix": "370-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_42",
            "tgt_ix": "370-ARR_v1_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_42",
            "tgt_ix": "370-ARR_v1_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_43",
            "tgt_ix": "370-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_44",
            "tgt_ix": "370-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_44",
            "tgt_ix": "370-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_44",
            "tgt_ix": "370-ARR_v1_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_44",
            "tgt_ix": "370-ARR_v1_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_44",
            "tgt_ix": "370-ARR_v1_44@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_45",
            "tgt_ix": "370-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_46",
            "tgt_ix": "370-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_47",
            "tgt_ix": "370-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_47",
            "tgt_ix": "370-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_47",
            "tgt_ix": "370-ARR_v1_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_48",
            "tgt_ix": "370-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_48",
            "tgt_ix": "370-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_48",
            "tgt_ix": "370-ARR_v1_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_48",
            "tgt_ix": "370-ARR_v1_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_48",
            "tgt_ix": "370-ARR_v1_48@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_48",
            "tgt_ix": "370-ARR_v1_48@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_49",
            "tgt_ix": "370-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_49",
            "tgt_ix": "370-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_49",
            "tgt_ix": "370-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_49",
            "tgt_ix": "370-ARR_v1_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_50",
            "tgt_ix": "370-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_50",
            "tgt_ix": "370-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_50",
            "tgt_ix": "370-ARR_v1_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_51",
            "tgt_ix": "370-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_51",
            "tgt_ix": "370-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_51",
            "tgt_ix": "370-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_52",
            "tgt_ix": "370-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_52",
            "tgt_ix": "370-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_52",
            "tgt_ix": "370-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_52",
            "tgt_ix": "370-ARR_v1_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_53",
            "tgt_ix": "370-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_53",
            "tgt_ix": "370-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_53",
            "tgt_ix": "370-ARR_v1_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_53",
            "tgt_ix": "370-ARR_v1_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_53",
            "tgt_ix": "370-ARR_v1_53@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_54",
            "tgt_ix": "370-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_54",
            "tgt_ix": "370-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_54",
            "tgt_ix": "370-ARR_v1_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_54",
            "tgt_ix": "370-ARR_v1_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_55",
            "tgt_ix": "370-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_56",
            "tgt_ix": "370-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_56",
            "tgt_ix": "370-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_56",
            "tgt_ix": "370-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_56",
            "tgt_ix": "370-ARR_v1_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_56",
            "tgt_ix": "370-ARR_v1_56@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_56",
            "tgt_ix": "370-ARR_v1_56@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_57",
            "tgt_ix": "370-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_57",
            "tgt_ix": "370-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_57",
            "tgt_ix": "370-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_57",
            "tgt_ix": "370-ARR_v1_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_57",
            "tgt_ix": "370-ARR_v1_57@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_57",
            "tgt_ix": "370-ARR_v1_57@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_57",
            "tgt_ix": "370-ARR_v1_57@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_57",
            "tgt_ix": "370-ARR_v1_57@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_57",
            "tgt_ix": "370-ARR_v1_57@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_57",
            "tgt_ix": "370-ARR_v1_57@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_58",
            "tgt_ix": "370-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_59",
            "tgt_ix": "370-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_59",
            "tgt_ix": "370-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_59",
            "tgt_ix": "370-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_60",
            "tgt_ix": "370-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_60",
            "tgt_ix": "370-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_60",
            "tgt_ix": "370-ARR_v1_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_61",
            "tgt_ix": "370-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_62",
            "tgt_ix": "370-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_63",
            "tgt_ix": "370-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_64",
            "tgt_ix": "370-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_65",
            "tgt_ix": "370-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_66",
            "tgt_ix": "370-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_67",
            "tgt_ix": "370-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_68",
            "tgt_ix": "370-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_69",
            "tgt_ix": "370-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_70",
            "tgt_ix": "370-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_71",
            "tgt_ix": "370-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_72",
            "tgt_ix": "370-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_73",
            "tgt_ix": "370-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_74",
            "tgt_ix": "370-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_75",
            "tgt_ix": "370-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_76",
            "tgt_ix": "370-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_77",
            "tgt_ix": "370-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_78",
            "tgt_ix": "370-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_79",
            "tgt_ix": "370-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_80",
            "tgt_ix": "370-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_81",
            "tgt_ix": "370-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_82",
            "tgt_ix": "370-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_83",
            "tgt_ix": "370-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_84",
            "tgt_ix": "370-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_85",
            "tgt_ix": "370-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_86",
            "tgt_ix": "370-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_87",
            "tgt_ix": "370-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_88",
            "tgt_ix": "370-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_89",
            "tgt_ix": "370-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_90",
            "tgt_ix": "370-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_91",
            "tgt_ix": "370-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_92",
            "tgt_ix": "370-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_93",
            "tgt_ix": "370-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_94",
            "tgt_ix": "370-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_95",
            "tgt_ix": "370-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_96",
            "tgt_ix": "370-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_97",
            "tgt_ix": "370-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_98",
            "tgt_ix": "370-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_99",
            "tgt_ix": "370-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_100",
            "tgt_ix": "370-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_101",
            "tgt_ix": "370-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "370-ARR_v1_102",
            "tgt_ix": "370-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1166,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "370-ARR",
        "version": 1
    }
}