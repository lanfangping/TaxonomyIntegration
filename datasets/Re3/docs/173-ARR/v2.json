{
    "nodes": [
        {
            "ix": "173-ARR_v2_0",
            "content": "A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_2",
            "content": "Large pre-trained vision-language (VL) models can learn a new task with a handful of examples and generalize to a new task without fine-tuning. However, these VL models are hard to deploy for real-world applications due to their impractically huge sizes and slow inference speed. To solve this limitation, we study prompt-based low-resource learning of VL tasks with our proposed method, FEWVLM, relatively smaller than recent fewshot learners. For FEWVLM, we pre-train a sequence-to-sequence transformer model with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM). Furthermore, we analyze the effect of diverse prompts for few-shot tasks. Experimental results on VQA show that FEWVLM with prompt-based learning outperforms Frozen (Tsimpoukelli et al., 2021) which is 31\u00d7 larger than FEWVLM by 18.2% point and achieves comparable results to a 246\u00d7 larger model, PICa . In our analysis, we observe that (1) prompts significantly affect zero-shot performance but marginally affect few-shot performance, (2) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts captioning performance. Our code is publicly available at https://github. com/woojeongjin/FewVLM",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "173-ARR_v2_4",
            "content": "Fine-tuning large pre-trained language models (PLMs) have led to strong results in various domains including vision-language tasks (Devlin et al., 2019;Raffel et al., 2020;Brown et al., 2020;Radford et al., 2021) In our setup, we convert the tasks into generative tasks in which models need to generate target text given input text and an image.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_5",
            "content": "ing (Brown et al., 2020;Radford et al., 2021;Tsimpoukelli et al., 2021). Few-shot learning overcomes the challenges of data-hungry supervised learning, where collecting human-labeled data is costly and slow. However, recent few-shot models such as GPT3 (Brown et al., 2020), Frozen (Tsimpoukelli et al., 2021), and PICa are too large to deploy in small or moderate computing machines due to their gigantic model sizes In this paper, we study low-resource learning of VL tasks with our proposed method, FEWVLM, a moderate-sized vision-language model, in which we fine-tune the model with no or a handful of training examples. For FEWVLM, we pre-train a sequence-to-sequence transformer model Raffel et al., 2020) with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM). This setup is more practical in that training and inference can be run economically using standard computing hardware and it is expensive to obtain a large number of quality training examples in the real world. In such a few-shot setting, task-specific prompts or task descriptions are important and have shown effectiveness in few-shot NLP tasks Radford et al., 2021;Schick and Sch\u00fctze, 2021a,b;Brown et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_6",
            "content": "To extend the success to VL tasks, we aim to answer the following questions for prompt-based low-resource VL learning. Q1) How does prompt design affect zero/few-shot learning on new tasks? Q2) Does prompt design still matter given larger training? Q3) How do different pre-training objectives affect zero/few-shot learning? To answer these questions, we explore various prompt formats including hand-crafted and noisy prompts on zero/few-shot VL learning datasets. In addition, we study pre-training objectives on few-shot tasks inspired by Raffel et al. (2020): prefix language modeling (PrefixLM) inspired by Raffel et al. (2020) and masked language modeling (MaskedLM). To this end, we investigate the model's performance on few-shot VL tasks including visual question answering (Goyal et al., 2017;Marino et al., 2019;Hudson and Manning, 2019), captioning (Agrawal et al., 2019;Young et al., 2014) (Fig. 1), and mini-ImageNet (Vinyals et al., 2016).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_7",
            "content": "In our empirical analysis, our FEWVLM with prompt-based learning outperforms Frozen (Tsimpoukelli et al., 2021) which is 31\u00d7 larger than FEWVLM by 18.2% point on zero-shot VQAv2 and achieves comparable results to a 246\u00d7 larger model, PICa . Furthermore, we observe that (1) prompts significantly affect zero-shot performance but marginally affect fewshot performance on new tasks ( \u00a76.2 and \u00a76.3), (2) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data ( \u00a76.5), and (3) MaskedLM helps few-shot VQA tasks while PrefixLM boosts captioning performance ( \u00a76.6).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_8",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "173-ARR_v2_9",
            "content": "Vision-language few-shot learning. Recently, several few-shot learners on vision-language tasks were proposed including GPT (Radford et al., 2019;Brown et al., 2020), Frozen (Tsimpoukelli et al., 2021), PICa , and SimVLM . Frozen (Tsimpoukelli et al., 2021) is a large language model based on GPT-2 (Radford et al., 2019), and is transformed into a multimodal few-shot learner by extending the soft prompting to incorporate a set of images and text. Their approach shows the fewshot capability on visual question answering and image classification tasks. Similarly, PICa uses GPT-3 (Brown et al., 2020) to solve VQA tasks in a few-shot manner by providing a few in-context VQA examples. It converts images into textual descriptions so that GPT-3 can understand the images. SimVLM is trained with prefix language modeling on weakly-supervised datasets. It demonstrates its effectiveness on a zero-shot captioning task. While these models achieve improvement on few-shot tasks, they are impractical to use in real-world applications due to their model sizes.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_10",
            "content": "Language model prompting. Providing prompts or task descriptions play an vital role in improving pre-trained language models in many tasks Radford et al., 2021;Schick and Sch\u00fctze, 2021a,b;Brown et al., 2020). Among them, GPT models (Radford et al., 2019;Brown et al., 2020) achieved great success in prompting We pretrain FEWVLM with masked language modeling (MaskedLM) and prefix language modeling (Pre-fixLM).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_11",
            "content": "or task demonstrations in NLP tasks. In light of this direction, prompt-based approaches improve small pre-trained models in few-shot text classification tasks Schick and Sch\u00fctze, 2021a,b). CLIP (Radford et al., 2021) also explores prompt templates for image classification which affect zero-shot performance. We follow these core ideas so we aim to improve zero-shot and few-shot performance using prompts in visionlanguage tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_12",
            "content": "Analysis Setup",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "173-ARR_v2_13",
            "content": "In this work, we study the zero-shot and few-shot performance of vision-language models L. We introduce our analysis setup: problem formulation, analysis questions, downstream tasks and datasets, evaluation metrics, and baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_14",
            "content": "Problem Formulation",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "173-ARR_v2_15",
            "content": "For zero-shot tasks, a pre-trained VL model L have no access to training set D train and development set D dev , and directly makes inference on the test instances D test . For few-shot tasks, we compose a dev set D dev from training data and ensure that |D train | = |D dev | following Perez et al. (2021); to tune the hyper-parameters and select the model. We limit the sizes of training and development sets to meet the goal of learning from limited data. The size of D train and D dev are small -i.e., we set the size of both to 16 in our study.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_16",
            "content": "Analysis Questions",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "173-ARR_v2_17",
            "content": "We aim to answer the following questions in this study through experiments on multiple VL datasets. Q1) How does prompt design affect zero/fewshot learning on new tasks? Providing a pretrained language model with task-specific prompts or significantly improves zero-shot and few-shot performance on NLP domains Schick and Sch\u00fctze, 2021a,b;Brown et al., 2020). For this question, we test several ad-hoc prompts on vision-language tasks and analyze how large zero-shot and few-shot performance is affected by different prompts, hand-crafted and noisy prompts, in Sec. 6.5. Q2) Does prompt design still matter given larger training data? As we will see in our experiments, prompts affect the zero/few-shot performance. However, prompts may have different effects when models are given different sizes of training data. To answer this question, we train models with different sizes of training data and various prompts, and compare the performance between different prompts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_18",
            "content": "Q3) How do different pre-training objectives affect zero/few-shot performance? We study two different pre-training objectives on few-shot performance: prefix language modeling (PrefixLM) inspired by Raffel et al. (2020) and masked language modeling (MaskedLM). In this setup, we pre-train our model with different objectives and test the model on zero-shot and few-shot tasks in Sec. 6.6.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_19",
            "content": "Downstream Tasks and Datasets",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "173-ARR_v2_20",
            "content": "In this work, we mainly focus on three tasks: visual question answering, captioning, and categorical learning. The visual question answering task requires models to answer a question to a given context image. We convert the visual question answering task into a generation task so that the model can generate answers in the zero-shot setting. The captioning task requires a model to generate descriptions for a given context image. The categorical learning requires a model to choose the correct category or class. We evaluate our model in an open-ended fashion to quantify fast learning of categories, in which it must generate correct labels unlike other classification methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_21",
            "content": "We include VQAv2 (Goyal et al., 2017), OK-VQA (Marino et al., 2019), and GQA (Hudson",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_22",
            "content": "Evaluation Metrics",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "173-ARR_v2_23",
            "content": "To evaluate few-shot performance, we randomly sample 5 different training and dev splits and measure average performance on the 5 splits. We finetune the vision-language models with 200 epochs for the few-shot setup and choose the best checkpoint on the dev set. et al., 2015) and SPICE (Anderson et al., 2016) as evaluation metrics for captioning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_24",
            "content": "Baselines",
            "ntype": "title",
            "meta": {
                "section": "3.5"
            }
        },
        {
            "ix": "173-ARR_v2_25",
            "content": "We evaluate strong zero/few-shot vision-language learners for comparison: Frozen (Tsimpoukelli et al., 2021), PICa for VQA datasets and SimVLM for captioning datasets. We include Unified VLP for few-shot VQAv2 and Flickr30k. Also, we compare them with fully fine-tuned models L f ull as upper bounds of few-shot models for each task; these models are fine-tuned on the entire datasets while few-shot models can access a small amount of data. For fully fine-tuned models L f ull , we borrow numbers from Uniter large for VQAv2, Oscar (Li et al., 2020b) for GQA, SimVLM and VinVL (Zhang et al., 2021) for NoCaps CIDER and SPICE respectively, and Unified VLP for Flickr30k captioning. We include VL-T5 no-vqa as a baseline which is pre-trained without visual question answering datasets . For miniImageNet, we include Frozen and AFHN (Li et al., 2020a). Frozen is designed for few-shot learning while AFHN is for meta learning, which is smaller and faster.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_26",
            "content": "Method",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "173-ARR_v2_27",
            "content": "Before diving into the analysis, we introduce our model, FEWVLM, to do zero/few-shot learning on VL tasks and answer the analysis questions we raised. We introduce FEWVLM architecture and pre-training objectives.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_28",
            "content": "Encoder-decoder Vision-language Model",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "173-ARR_v2_29",
            "content": "We adopt an encoder-decoder architecture Vaswani et al., 2017), to encode visual and text inputs and generate target text. We represent an input image with 36 object regions from a Faster R-CNN (Ren et al., 2015) trained on Visual Genome (Krishna et al., 2017). The sets of region representations are fed into the encoder by appending them to the text . We train the model parameters \u03b8 by minimizing the negative log-likelihood of target text y tokens given input text x and image v:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_30",
            "content": "L \u03b8 = \u2212 |y| i=1 log P \u03b8 (y i |y <i , x, v).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_31",
            "content": "(1)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_32",
            "content": "The model is not task-specific, so it is a good option for zero/few-shot settings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_33",
            "content": "Pre-training Objectives",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "173-ARR_v2_34",
            "content": "We pre-train the models with both prefix language modeling (PrefixLM) and masked language modeling (MaskedLM). Fig. 3 illustrates the PrefixLM and MaskedLM. Prefix language modeling. We include prefix language modeling (PrefixLM) following Raffel et al. (2020). Given an image and a span of text, this objective randomly splits the text into two separate components; the former component with the given image is used as inputs to the encoder and the latter component is used as target text to be generated by the decoder. Masked language modeling. We follow to do masked language modeling. This objective is to replace random spans with numbered sentinel tokens, e.g., <text_1>, and then the masked text is fed into the encoder. Then the decoder generates the masked spans as target text.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_35",
            "content": "We randomly mask 15% of input text tokens and replace them with sentinel tokens.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_36",
            "content": "Pre-training data. To pre-train FEWVLM, we collect image-caption data from MS COCO (Lin et al., 2014;Chen et al., 2015) and Visual Genome (VG) (Krishna et al., 2017). The pre-training datasets contains 9.18M image-text pairs and 180K distinct images.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_37",
            "content": "Low-resource Adaptation",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "173-ARR_v2_38",
            "content": "In downstream tasks, we train our model with few-shot examples. Fig. 2 shows an illustration of FEWVLM in inference time. Given a prompt template P, we first get input text and target text using the template x, y = P(input, label). Then we train model parameters by minimizing the negative log-likelihood in Eq. (1). In inference, we use the same prompt and the model generates the label text. Here we obtain the final label by removing the target prompt template.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_39",
            "content": "Prompt Design",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "173-ARR_v2_40",
            "content": "Prompts affect the performance of the visionlanguage model ; we study the effect of different prompts on the zero-shot and fewshot performance on downstream tasks. Tables 1 and 11 show prompts we used in our experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_41",
            "content": "Visual Question Answering",
            "ntype": "title",
            "meta": {
                "section": "5.1.1"
            }
        },
        {
            "ix": "173-ARR_v2_42",
            "content": "The visual question answering tasks (VQA, OK-VQA, and GQA) require models to answer a question to a given context image. Recent approaches Tan and Bansal, 2019;Su et al., 2020;Li et al., 2019Li et al., , 2020b tackle visual question answering tasks as multi-label classification over a predefined set of answer candidates. Instead, we approach the visual question answering tasks as a generation task so that the model can produce the answers without introducing any task-specific heads. In this setup, prompts act as constraints to guide the models to generate proper formats of answers; models might generate a sentence for VQA, which is not the correct format, without prompts. Therefore, we study several prompts for input and output as shown in Tables 1 and 11; we explore hand-crafted prompts (Table 1) and noisy prompts for ablation study (Table 11). Hand-crafted prompts. For input prompts, we explore three different templates: \"question: [Q] answer:\" and with the <text_1> sentinel token at the end. Similarly to masked language modeling, we expect models to generate words thanks to the sentinel token. For target prompts, we explore two different templates: \"[A]\" (an answer) and \"<text_1> [A]\" (an answer with a sentinel token). Here, we aim to mimic MaskedLM's target text format, so the similar format helps the model quickly adapt to the new task. We call each prompt ID as in Table 1. Noisy prompts. To understand the effect of noisy prompts in zero/few-shot learning, we include irrelevant prompts, noisy tokens, and random sentences as in Table 11. Irrelevant prompts are random questions or instructions that mislead models to answer wrong questions or follow irrelevant instructions. Noisy tokens are randomly selected from T5's vocabulary, so we test how robust our model is to random tokens. Finally, random sentences are captions from MS COCO and this gives false information to models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_43",
            "content": "Captioning",
            "ntype": "title",
            "meta": {
                "section": "5.1.2"
            }
        },
        {
            "ix": "173-ARR_v2_44",
            "content": "In NoCaps and Flickr30k, we explore three handcrafted input prompts: \"a picture of \", \"a photo of \", and \"an image of \". We study the effect of different word choices in this captioning task. While the three different words have similar meanings, they show different performance in zero-shot and fewshot tasks as we will see in our experiments.. For target prompts, we just train the model with the original caption without any additional prompts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_45",
            "content": "MiniImageNet",
            "ntype": "title",
            "meta": {
                "section": "5.1.3"
            }
        },
        {
            "ix": "173-ARR_v2_46",
            "content": "In miniImageNet, we train our model with a handcrafted input prompt, \"This is <text_1>,\" and target prompt, \"<text_1> [A].\" We compare our model with and without prompts in this dataset to study whether prompts are helpful in categorical learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_47",
            "content": "Results and Discussion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "173-ARR_v2_48",
            "content": "In this section, we first discuss our main results on zero-shot and few-shot tasks and then answer the questions we raised: does prompt design matter in zero/few-shot learning?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_49",
            "content": "Experiment Details",
            "ntype": "title",
            "meta": {
                "section": "6.1"
            }
        },
        {
            "ix": "173-ARR_v2_50",
            "content": "For pre-training, we set batch size 1,280 and 800 for FEWVLM base and FEWVLM large , respectively and pre-train them with 30 epochs. We use learning rate 1e-4 with 5% linear warmup.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_51",
            "content": "For few-shot learning, we train models with 200 epochs, learning rate 5e-5 and 5% linear warmup and choose the best checkpoint on the dev set. For FEWVLM, we use \"question:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_52",
            "content": "[Q] answer <text_1>\" (P3) as an input prompt and \"<text_1> [A]\" as a target prompt for visual question answering, and \"an image of\" (Q3) as an input prompt for captioning, which show the best performance. We will study the effect of different prompts in Sec. 6.5. The sizes of of D train and D dev are 16 on VQA and captioning tasks. For miniImageNet, we use 'This is <text_1>,\" and \"<text_1> [A]\" as input and target prompts. In this data, we test with {1, 3, 5}-shots per class.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_53",
            "content": "Performance on Zero-shot Learning",
            "ntype": "title",
            "meta": {
                "section": "6.2"
            }
        },
        {
            "ix": "173-ARR_v2_54",
            "content": "We evaluate the existing models in a zero-shot manner, in which models do not have access to any training data. Tables 2 and 4 show the results on VQA and captioning datasets, respectively. First, FEWVLM with the hand-crafted prompt (P3) achieves better performance than other baselines on VQA datasets. In particular, our FEWVLM base significantly outperforms Frozen which is about 31\u00d7 larger than ours. Also, PICa based on GPT3 (Brown et al., 2020) shows the best performance on OK-VQA. It is noticeable that our FEWVLM large , the 246\u00d7 smaller model, achieves the comparable result to PICa. Compared to VL-T5 no-vqa which is the same architecture as ours, FEWVLM base improves VQAv2 performance by about 30% point. As we will see in the later section, our pre-training objectives and the prompts boost the VQA performance. On NoCaps, SimVLM huge shows the best performance. Our FEWVLM base significantly improves the performance compared to VL-T5 no-vqa . As we will see in the later section, our pre-training objectives and the prompts boost the VQA and captioning performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_55",
            "content": "Performance on Few-shot Learning",
            "ntype": "title",
            "meta": {
                "section": "6.3"
            }
        },
        {
            "ix": "173-ARR_v2_56",
            "content": "Tables 3 and 5 show the few-shot performance on VQA and captioning datasets. of training and validation sets are 16 for FEWVLM, VL-T5 no-vqa , and Unified VLP; and Frozen and PICa use 4 and 16 in-context demonstration examples, respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_57",
            "content": "On VQAv2 and OK-VQA, PICa shows the best performance while our FEWVLM large achieves the comparable result on VQAv2. OK-VQA requires external knowledge to answer unlike other VQA datasets, so larger models and large pre-training data (prior knowledge) are necessary to improve. Interestingly, FEWVLM * base , which is trained with 4 training examples, outperforms Frozen. On captioning data, FEWVLM base notably outperforms VL-T5 no-vqa by 31.1% point on NoCaps CIDEr.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_58",
            "content": "Unified VLP slightly underperforms FEWVLM on Flickr30k captioning task. We conjecture that their architecture is based on a encoder-decoder transfomer and it is pre-trained with a captioning task .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_59",
            "content": "MiniImageNet",
            "ntype": "title",
            "meta": {
                "section": "6.4"
            }
        },
        {
            "ix": "173-ARR_v2_60",
            "content": "Table 6 shows results on miniImageNet, where models must choose the correct class for each image. We train and evaluate FEWVLM in an generative manner; the model must generate correct label text to get the credit. FEWVLM significantly outperforms Frozen in all shots. Note that we train FEWVLM with a few training samples while Frozen uses them as in-context demonstration. Interestingly, FEWVLM with a hand-crafted prompt improves performance a lot on the 1-shot case, while it marginally improves on the 5-shot case.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_61",
            "content": "Study of Prompt Design",
            "ntype": "title",
            "meta": {
                "section": "6.5"
            }
        },
        {
            "ix": "173-ARR_v2_62",
            "content": "Here we examine the effect of different prompts on FEWVLM base in Table 7 and Figs . 6, 5, and 4. We test the model on VQAv2 and Flickr30k datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_63",
            "content": "Zero-shot Predictions",
            "ntype": "title",
            "meta": {
                "section": "6.5.1"
            }
        },
        {
            "ix": "173-ARR_v2_64",
            "content": "Table 7 shows the zero-shot performance on VQAv2 and Flickr30k. We observe that zero-shot results are remarkably affected by input prompts on both datasets. For input prompts, <text_1> in P1 and P3 helps the zero-shot predictions significantly compared to \"no prompt\" and P2. We conjecture that <text_1> guides the model to predict masked spans similarly to MaskedLM, so it improves the performance. On Flickr30k, we examine different word choices of prompts: \"a picture of\" (Q1), \"a photo of\" (Q2), and \"an image of\" (Q3). For instance, using \"an image of\" outperforms using no prompt by 21.4 point. It is noticeable that different word choices significantly affect the zero-shot results.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_65",
            "content": "Few-shot Predictions",
            "ntype": "title",
            "meta": {
                "section": "6.5.2"
            }
        },
        {
            "ix": "173-ARR_v2_66",
            "content": "We study various input prompts including irrelevant prompts, noisy tokens, and random sentences on VQAv2 (Fig. 4). First, noisy prompts and no prompt achieve near 0 accuracy on the zero-shot setting. In few-shot predictions, FEWVLM with noisy prompts learns as quickly as hand-crafted prompts given larger data. For example, our model with noisy prompts achieves comparable results to the best hand-crafted prompt. Among all different types of noisy prompts, random sentences deteriorate performance the most. This is because the random sentences come from captions in MS COCO, so the model might choose the answer from wrong captions not from images. Interestingly, no prompt outperforms the other noisy prompts and even shows similar to or better than the handcrafted prompt with larger training data. We also observe a similar phenomenon on Flickr30k; no prompt performs similar to hand-crafted prompts in Fig. 5. In addition, we explore two different target prompts, \"<text_1> [A]\" and \" [A].\" We try to mimic the MaskedLM's target text format, so we add \"<text_1>\" to target prompt on VQA. This might help the model's fast adaptation to a new task since they share the same target prompt. In Fig. 6, we notice an interesting phenomenon; the target prompt \"[A]\" shows a larger variance than the other suggesting that introducing \"<text_1>\" helps the model quickly adapt to a new task. However, both prompts show similar results given larger training data, e.g., 300.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_67",
            "content": "Pre-training Objectives",
            "ntype": "title",
            "meta": {
                "section": "6.6"
            }
        },
        {
            "ix": "173-ARR_v2_68",
            "content": "We investigate how pre-training objectives affect different tasks. We pre-train FEWVLM with different pre-training objectives: masked language modeling (MaskedLM) and prefix language modeling (PrefixLM).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_69",
            "content": "In Table 8, we observe that MaskedLM helps VQA tasks while PrefixLM helps captioning tasks in zero-shot and few-shot settings. We conjecture that MaskedLM is to predict spans, which is analogous to predict correct answers to questions, and PrefixLM is to generate the rest of the given prefix, which is similar to captioning tasks. In other words, if the pre-training task is similar to the downstream tasks, then it will help performance further. When pre-training with both objectives, they create a synergetic effect and thus improve cross-task generalization.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_70",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "173-ARR_v2_71",
            "content": "In this work, we present FEWVLM, a few-shot prompt-based learner on vision-language tasks. On diverse datasets, FEWVLM outperforms baselines and shows comparable results to PICa which is 246\u00d7 larger than ours. We observe that prompts are vital in zero-shot and few-shot tasks and each pre-training objective helps different few-shot tasks. Also, we find out that models with larger training data are not significantly affected by noisy prompts. Future work includes exploring automatic prompt generation and diverse formats of few-shot tasks such as multiple-choice VQA. Finding optimal prompts require exhaustive engineering to achieve the best performance and leads to impressive results. We leave the exploration of these directions to future investigations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_72",
            "content": "Table 9 shows model parameters in our model, FEWVLM. FEWVLM base and FEWVLM large is based on VL-T5 and T5 (Raffel et al., 2020), respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_73",
            "content": "We evaluate our model with COCO captioning data. We use Karpathy split (Karpathy and Li, 2015) for MS COCO captioning, which re-splits train and val images into 113,287 / 5000 / 5000 for train / validation / test. Table 10 shows the results on COCO. sizes.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_74",
            "content": "We pre-train our model with different datasets: MS COCO and Visual Genome (VG), and Conceptual Captions (CC). We investigate which pre-training dataset helps the downstream tasks in a few-shot manner. In Table 12, we observe that MS COCO and VG datasets are more helpful to the downstream tasks than CC.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "173-ARR_v2_75",
            "content": "Harsh Agrawal, Peter Anderson, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, nocaps: novel object captioning at scale, 2019-10-27, 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Harsh Agrawal",
                    "Peter Anderson",
                    "Karan Desai",
                    "Yufei Wang",
                    "Xinlei Chen",
                    "Rishabh Jain",
                    "Mark Johnson",
                    "Dhruv Batra",
                    "Devi Parikh",
                    "Stefan Lee"
                ],
                "title": "nocaps: novel object captioning at scale",
                "pub_date": "2019-10-27",
                "pub_title": "2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019",
                "pub": "IEEE"
            }
        },
        {
            "ix": "173-ARR_v2_76",
            "content": "Peter Anderson, Basura Fernando, Mark Johnson, Stephen Gould, Spice: Semantic propositional image caption evaluation, 2016, European conference on computer vision, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Peter Anderson",
                    "Basura Fernando",
                    "Mark Johnson",
                    "Stephen Gould"
                ],
                "title": "Spice: Semantic propositional image caption evaluation",
                "pub_date": "2016",
                "pub_title": "European conference on computer vision",
                "pub": "Springer"
            }
        },
        {
            "ix": "173-ARR_v2_77",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Mc-Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners, , Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Tom Brown",
                    "Benjamin Mann",
                    "Nick Ryder",
                    "Melanie Subbiah",
                    "Jared Kaplan",
                    "Prafulla Dhariwal",
                    "Arvind Neelakantan",
                    "Pranav Shyam",
                    "Girish Sastry",
                    "Amanda Askell",
                    "Sandhini Agarwal",
                    "Ariel Herbert-Voss",
                    "Gretchen Krueger",
                    "Tom Henighan",
                    "Rewon Child",
                    "Aditya Ramesh",
                    "Daniel Ziegler",
                    "Jeffrey Wu",
                    "Clemens Winter",
                    "Christopher Hesse",
                    "Mark Chen",
                    "Eric Sigler",
                    "Mateusz Litwin"
                ],
                "title": "Mc-Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners",
                "pub_date": null,
                "pub_title": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_78",
            "content": "Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, C Lawrence Zitnick, Microsoft coco captions: Data collection and evaluation server, 2015, ArXiv preprint, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Xinlei Chen",
                    "Hao Fang",
                    "Tsung-Yi Lin",
                    "Ramakrishna Vedantam",
                    "Saurabh Gupta",
                    "Piotr Doll\u00e1r",
                    "C Lawrence Zitnick"
                ],
                "title": "Microsoft coco captions: Data collection and evaluation server",
                "pub_date": "2015",
                "pub_title": "ArXiv preprint",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_79",
            "content": "UNKNOWN, None, 2019, Uniter: Learning universal image-text representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Uniter: Learning universal image-text representations",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_80",
            "content": "Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal, Unifying vision-and-language tasks via text generation, 2021-07-24, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Jaemin Cho",
                    "Jie Lei",
                    "Hao Tan",
                    "Mohit Bansal"
                ],
                "title": "Unifying vision-and-language tasks via text generation",
                "pub_date": "2021-07-24",
                "pub_title": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021",
                "pub": "PMLR"
            }
        },
        {
            "ix": "173-ARR_v2_81",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "173-ARR_v2_82",
            "content": "Tianyu Gao, Adam Fisch, Danqi Chen, Making pre-trained language models better few-shot learners, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Tianyu Gao",
                    "Adam Fisch",
                    "Danqi Chen"
                ],
                "title": "Making pre-trained language models better few-shot learners",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "173-ARR_v2_83",
            "content": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Making the V in VQA matter: Elevating the role of image understanding in visual question answering, 2017-07-21, 2017 IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Yash Goyal",
                    "Tejas Khot",
                    "Douglas Summers-Stay",
                    "Dhruv Batra",
                    "Devi Parikh"
                ],
                "title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
                "pub_date": "2017-07-21",
                "pub_title": "2017 IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_84",
            "content": "A Drew,  Hudson, D Christopher,  Manning, GQA: A new dataset for real-world visual reasoning and compositional question answering, 2019-06-16, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "A Drew",
                    " Hudson",
                    "D Christopher",
                    " Manning"
                ],
                "title": "GQA: A new dataset for real-world visual reasoning and compositional question answering",
                "pub_date": "2019-06-16",
                "pub_title": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_85",
            "content": "Andrej Karpathy, Fei-Fei Li, Deep visualsemantic alignments for generating image descriptions, 2015-06-07, IEEE Conference on Computer Vision and Pattern Recognition, IEEE Computer Society.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Andrej Karpathy",
                    "Fei-Fei Li"
                ],
                "title": "Deep visualsemantic alignments for generating image descriptions",
                "pub_date": "2015-06-07",
                "pub_title": "IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": "IEEE Computer Society"
            }
        },
        {
            "ix": "173-ARR_v2_86",
            "content": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, Visual genome: Connecting language and vision using crowdsourced dense image annotations, 2017, International journal of computer vision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Ranjay Krishna",
                    "Yuke Zhu",
                    "Oliver Groth",
                    "Justin Johnson",
                    "Kenji Hata",
                    "Joshua Kravitz",
                    "Stephanie Chen",
                    "Yannis Kalantidis",
                    "Li-Jia Li",
                    "David Shamma"
                ],
                "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
                "pub_date": "2017",
                "pub_title": "International journal of computer vision",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_87",
            "content": "Kai Li, Yulun Zhang, Kunpeng Li, Yun Fu, Adversarial feature hallucination networks for fewshot learning, 2020-06-13, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Kai Li",
                    "Yulun Zhang",
                    "Kunpeng Li",
                    "Yun Fu"
                ],
                "title": "Adversarial feature hallucination networks for fewshot learning",
                "pub_date": "2020-06-13",
                "pub_title": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "pub": "IEEE"
            }
        },
        {
            "ix": "173-ARR_v2_88",
            "content": "UNKNOWN, None, 2019, Visualbert: A simple and performant baseline for vision and language, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Visualbert: A simple and performant baseline for vision and language",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_89",
            "content": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Oscar: Objectsemantics aligned pre-training for vision-language tasks, 2020, European Conference on Computer Vision, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Xiujun Li",
                    "Xi Yin",
                    "Chunyuan Li",
                    "Pengchuan Zhang",
                    "Xiaowei Hu",
                    "Lei Zhang",
                    "Lijuan Wang",
                    "Houdong Hu",
                    "Li Dong",
                    "Furu Wei"
                ],
                "title": "Oscar: Objectsemantics aligned pre-training for vision-language tasks",
                "pub_date": "2020",
                "pub_title": "European Conference on Computer Vision",
                "pub": "Springer"
            }
        },
        {
            "ix": "173-ARR_v2_90",
            "content": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, Microsoft coco: Common objects in context, 2014, European conference on computer vision, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Tsung-Yi Lin",
                    "Michael Maire",
                    "Serge Belongie",
                    "James Hays",
                    "Pietro Perona",
                    "Deva Ramanan",
                    "Piotr Doll\u00e1r",
                    "C Lawrence Zitnick"
                ],
                "title": "Microsoft coco: Common objects in context",
                "pub_date": "2014",
                "pub_title": "European conference on computer vision",
                "pub": "Springer"
            }
        },
        {
            "ix": "173-ARR_v2_91",
            "content": "Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi, OK-VQA: A visual question answering benchmark requiring external knowledge, 2019-06-16, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Computer Vision Foundation / IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Kenneth Marino",
                    "Mohammad Rastegari",
                    "Ali Farhadi",
                    "Roozbeh Mottaghi"
                ],
                "title": "OK-VQA: A visual question answering benchmark requiring external knowledge",
                "pub_date": "2019-06-16",
                "pub_title": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019",
                "pub": "Computer Vision Foundation / IEEE"
            }
        },
        {
            "ix": "173-ARR_v2_92",
            "content": "UNKNOWN, None, 2021, True few-shot learning with language models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "True few-shot learning with language models",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_93",
            "content": "Alec Radford, Jong Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Learning transferable visual models from natural language supervision, 2021-07, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Alec Radford",
                    "Jong Kim",
                    "Chris Hallacy",
                    "Aditya Ramesh",
                    "Gabriel Goh",
                    "Sandhini Agarwal",
                    "Girish Sastry",
                    "Amanda Askell",
                    "Pamela Mishkin",
                    "Jack Clark",
                    "Gretchen Krueger",
                    "Ilya Sutskever"
                ],
                "title": "Learning transferable visual models from natural language supervision",
                "pub_date": "2021-07",
                "pub_title": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021",
                "pub": "PMLR"
            }
        },
        {
            "ix": "173-ARR_v2_94",
            "content": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Language models are unsupervised multitask learners, 2019, OpenAI blog, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Alec Radford",
                    "Jeffrey Wu",
                    "Rewon Child",
                    "David Luan",
                    "Dario Amodei",
                    "Ilya Sutskever"
                ],
                "title": "Language models are unsupervised multitask learners",
                "pub_date": "2019",
                "pub_title": "OpenAI blog",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_95",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, J. Mach. Learn. Res, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Colin Raffel",
                    "Noam Shazeer",
                    "Adam Roberts",
                    "Katherine Lee",
                    "Sharan Narang",
                    "Michael Matena",
                    "Yanqi Zhou",
                    "Wei Li",
                    "Peter Liu"
                ],
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "pub_date": "2020",
                "pub_title": "J. Mach. Learn. Res",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_96",
            "content": "Kaiming Shaoqing Ren, Ross He, Jian Girshick,  Sun, Faster R-CNN: towards real-time object detection with region proposal networks, 2015-12-07, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Kaiming Shaoqing Ren",
                    "Ross He",
                    "Jian Girshick",
                    " Sun"
                ],
                "title": "Faster R-CNN: towards real-time object detection with region proposal networks",
                "pub_date": "2015-12-07",
                "pub_title": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_97",
            "content": "Timo Schick, Hinrich Sch\u00fctze, Exploiting cloze-questions for few-shot text classification and natural language inference, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Timo Schick",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "Exploiting cloze-questions for few-shot text classification and natural language inference",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_98",
            "content": "Timo Schick, Hinrich Sch\u00fctze, It's not just size that matters: Small language models are also few-shot learners, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Timo Schick",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "It's not just size that matters: Small language models are also few-shot learners",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_99",
            "content": "Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai, VL-BERT: pretraining of generic visual-linguistic representations, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Weijie Su",
                    "Xizhou Zhu",
                    "Yue Cao",
                    "Bin Li",
                    "Lewei Lu",
                    "Furu Wei",
                    "Jifeng Dai"
                ],
                "title": "VL-BERT: pretraining of generic visual-linguistic representations",
                "pub_date": "2020-04-26",
                "pub_title": "8th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_100",
            "content": "Hao Tan, Mohit Bansal, LXMERT: Learning cross-modality encoder representations from transformers, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Hao Tan",
                    "Mohit Bansal"
                ],
                "title": "LXMERT: Learning cross-modality encoder representations from transformers",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "173-ARR_v2_101",
            "content": "UNKNOWN, None, 2021, Multimodal few-shot learning with frozen language models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Multimodal few-shot learning with frozen language models",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_102",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "Lukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017-12-04",
                "pub_title": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_103",
            "content": "C Ramakrishna Vedantam, Devi Zitnick,  Parikh, Cider: Consensus-based image description evaluation, 2015-06-07, IEEE Conference on Computer Vision and Pattern Recognition, IEEE Computer Society.",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "C Ramakrishna Vedantam",
                    "Devi Zitnick",
                    " Parikh"
                ],
                "title": "Cider: Consensus-based image description evaluation",
                "pub_date": "2015-06-07",
                "pub_title": "IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": "IEEE Computer Society"
            }
        },
        {
            "ix": "173-ARR_v2_104",
            "content": "Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, Daan Wierstra, Matching networks for one shot learning, 2016-12-05, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Oriol Vinyals",
                    "Charles Blundell",
                    "Tim Lillicrap",
                    "Koray Kavukcuoglu",
                    "Daan Wierstra"
                ],
                "title": "Matching networks for one shot learning",
                "pub_date": "2016-12-05",
                "pub_title": "Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_105",
            "content": "UNKNOWN, None, 2021, Simvlm: Simple visual language model pretraining with weak supervision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Simvlm: Simple visual language model pretraining with weak supervision",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_106",
            "content": "UNKNOWN, None, 2021, An empirical study of gpt-3 for few-shot knowledge-based vqa, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "An empirical study of gpt-3 for few-shot knowledge-based vqa",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_107",
            "content": "Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier, From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions, 2014, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Peter Young",
                    "Alice Lai",
                    "Micah Hodosh",
                    "Julia Hockenmaier"
                ],
                "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
                "pub_date": "2014",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_108",
            "content": "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao, Vinvl: Revisiting visual representations in vision-language models, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Pengchuan Zhang",
                    "Xiujun Li",
                    "Xiaowei Hu",
                    "Jianwei Yang",
                    "Lei Zhang",
                    "Lijuan Wang",
                    "Yejin Choi",
                    "Jianfeng Gao"
                ],
                "title": "Vinvl: Revisiting visual representations in vision-language models",
                "pub_date": "2021",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "173-ARR_v2_109",
            "content": "Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, Jianfeng Gao, Unified vision-language pre-training for image captioning and VQA, 2020-02-07, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Luowei Zhou",
                    "Hamid Palangi",
                    "Lei Zhang",
                    "Houdong Hu",
                    "Jason Corso",
                    "Jianfeng Gao"
                ],
                "title": "Unified vision-language pre-training for image captioning and VQA",
                "pub_date": "2020-02-07",
                "pub_title": "The Thirty-Second Innovative Applications of Artificial Intelligence Conference",
                "pub": "AAAI Press"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "173-ARR_v2_0@0",
            "content": "A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_0",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_2@0",
            "content": "Large pre-trained vision-language (VL) models can learn a new task with a handful of examples and generalize to a new task without fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_2",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_2@1",
            "content": "However, these VL models are hard to deploy for real-world applications due to their impractically huge sizes and slow inference speed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_2",
            "start": 144,
            "end": 278,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_2@2",
            "content": "To solve this limitation, we study prompt-based low-resource learning of VL tasks with our proposed method, FEWVLM, relatively smaller than recent fewshot learners.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_2",
            "start": 280,
            "end": 443,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_2@3",
            "content": "For FEWVLM, we pre-train a sequence-to-sequence transformer model with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_2",
            "start": 445,
            "end": 591,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_2@4",
            "content": "Furthermore, we analyze the effect of diverse prompts for few-shot tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_2",
            "start": 593,
            "end": 665,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_2@5",
            "content": "Experimental results on VQA show that FEWVLM with prompt-based learning outperforms Frozen (Tsimpoukelli et al., 2021) which is 31\u00d7 larger than FEWVLM by 18.2% point and achieves comparable results to a 246\u00d7 larger model, PICa .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_2",
            "start": 667,
            "end": 894,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_2@6",
            "content": "In our analysis, we observe that (1) prompts significantly affect zero-shot performance but marginally affect few-shot performance, (2) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts captioning performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_2",
            "start": 896,
            "end": 1204,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_2@7",
            "content": "Our code is publicly available at https://github.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_2",
            "start": 1206,
            "end": 1254,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_2@8",
            "content": "com/woojeongjin/FewVLM",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_2",
            "start": 1256,
            "end": 1277,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_4@0",
            "content": "Fine-tuning large pre-trained language models (PLMs) have led to strong results in various domains including vision-language tasks (Devlin et al., 2019;Raffel et al., 2020;Brown et al., 2020;Radford et al., 2021) In our setup, we convert the tasks into generative tasks in which models need to generate target text given input text and an image.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_4",
            "start": 0,
            "end": 344,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_5@0",
            "content": "ing (Brown et al., 2020;Radford et al., 2021;Tsimpoukelli et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_5",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_5@1",
            "content": "Few-shot learning overcomes the challenges of data-hungry supervised learning, where collecting human-labeled data is costly and slow.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_5",
            "start": 73,
            "end": 206,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_5@2",
            "content": "However, recent few-shot models such as GPT3 (Brown et al., 2020), Frozen (Tsimpoukelli et al., 2021), and PICa are too large to deploy in small or moderate computing machines due to their gigantic model sizes In this paper, we study low-resource learning of VL tasks with our proposed method, FEWVLM, a moderate-sized vision-language model, in which we fine-tune the model with no or a handful of training examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_5",
            "start": 208,
            "end": 623,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_5@3",
            "content": "For FEWVLM, we pre-train a sequence-to-sequence transformer model Raffel et al., 2020) with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_5",
            "start": 625,
            "end": 792,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_5@4",
            "content": "This setup is more practical in that training and inference can be run economically using standard computing hardware and it is expensive to obtain a large number of quality training examples in the real world.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_5",
            "start": 794,
            "end": 1003,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_5@5",
            "content": "In such a few-shot setting, task-specific prompts or task descriptions are important and have shown effectiveness in few-shot NLP tasks Radford et al., 2021;Schick and Sch\u00fctze, 2021a,b;Brown et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_5",
            "start": 1005,
            "end": 1209,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_6@0",
            "content": "To extend the success to VL tasks, we aim to answer the following questions for prompt-based low-resource VL learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_6",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_6@1",
            "content": "Q1) How does prompt design affect zero/few-shot learning on new tasks? Q2) Does prompt design still matter given larger training?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_6",
            "start": 119,
            "end": 247,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_6@2",
            "content": "Q3) How do different pre-training objectives affect zero/few-shot learning? To answer these questions, we explore various prompt formats including hand-crafted and noisy prompts on zero/few-shot VL learning datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_6",
            "start": 249,
            "end": 464,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_6@3",
            "content": "In addition, we study pre-training objectives on few-shot tasks inspired by Raffel et al. (2020): prefix language modeling (PrefixLM) inspired by Raffel et al. (2020) and masked language modeling (MaskedLM).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_6",
            "start": 466,
            "end": 672,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_6@4",
            "content": "To this end, we investigate the model's performance on few-shot VL tasks including visual question answering (Goyal et al., 2017;Marino et al., 2019;Hudson and Manning, 2019), captioning (Agrawal et al., 2019;Young et al., 2014) (Fig. 1), and mini-ImageNet (Vinyals et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_6",
            "start": 674,
            "end": 953,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_7@0",
            "content": "In our empirical analysis, our FEWVLM with prompt-based learning outperforms Frozen (Tsimpoukelli et al., 2021) which is 31\u00d7 larger than FEWVLM by 18.2% point on zero-shot VQAv2 and achieves comparable results to a 246\u00d7 larger model, PICa .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_7",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_7@1",
            "content": "Furthermore, we observe that (1) prompts significantly affect zero-shot performance but marginally affect fewshot performance on new tasks ( \u00a76.2 and \u00a76.3), (2) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data ( \u00a76.5), and (3) MaskedLM helps few-shot VQA tasks while PrefixLM boosts captioning performance ( \u00a76.6).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_7",
            "start": 241,
            "end": 599,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_8@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_8",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_9@0",
            "content": "Vision-language few-shot learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_9",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_9@1",
            "content": "Recently, several few-shot learners on vision-language tasks were proposed including GPT (Radford et al., 2019;Brown et al., 2020), Frozen (Tsimpoukelli et al., 2021), PICa , and SimVLM .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_9",
            "start": 35,
            "end": 221,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_9@2",
            "content": "Frozen (Tsimpoukelli et al., 2021) is a large language model based on GPT-2 (Radford et al., 2019), and is transformed into a multimodal few-shot learner by extending the soft prompting to incorporate a set of images and text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_9",
            "start": 223,
            "end": 448,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_9@3",
            "content": "Their approach shows the fewshot capability on visual question answering and image classification tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_9",
            "start": 450,
            "end": 553,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_9@4",
            "content": "Similarly, PICa uses GPT-3 (Brown et al., 2020) to solve VQA tasks in a few-shot manner by providing a few in-context VQA examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_9",
            "start": 555,
            "end": 685,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_9@5",
            "content": "It converts images into textual descriptions so that GPT-3 can understand the images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_9",
            "start": 687,
            "end": 771,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_9@6",
            "content": "SimVLM is trained with prefix language modeling on weakly-supervised datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_9",
            "start": 773,
            "end": 850,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_9@7",
            "content": "It demonstrates its effectiveness on a zero-shot captioning task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_9",
            "start": 852,
            "end": 916,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_9@8",
            "content": "While these models achieve improvement on few-shot tasks, they are impractical to use in real-world applications due to their model sizes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_9",
            "start": 918,
            "end": 1055,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_10@0",
            "content": "Language model prompting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_10",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_10@1",
            "content": "Providing prompts or task descriptions play an vital role in improving pre-trained language models in many tasks Radford et al., 2021;Schick and Sch\u00fctze, 2021a,b;Brown et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_10",
            "start": 26,
            "end": 207,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_10@2",
            "content": "Among them, GPT models (Radford et al., 2019;Brown et al., 2020) achieved great success in prompting We pretrain FEWVLM with masked language modeling (MaskedLM) and prefix language modeling (Pre-fixLM).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_10",
            "start": 209,
            "end": 410,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_11@0",
            "content": "or task demonstrations in NLP tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_11",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_11@1",
            "content": "In light of this direction, prompt-based approaches improve small pre-trained models in few-shot text classification tasks Schick and Sch\u00fctze, 2021a,b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_11",
            "start": 37,
            "end": 188,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_11@2",
            "content": "CLIP (Radford et al., 2021) also explores prompt templates for image classification which affect zero-shot performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_11",
            "start": 190,
            "end": 308,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_11@3",
            "content": "We follow these core ideas so we aim to improve zero-shot and few-shot performance using prompts in visionlanguage tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_11",
            "start": 310,
            "end": 430,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_12@0",
            "content": "Analysis Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_12",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_13@0",
            "content": "In this work, we study the zero-shot and few-shot performance of vision-language models L. We introduce our analysis setup: problem formulation, analysis questions, downstream tasks and datasets, evaluation metrics, and baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_13",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_14@0",
            "content": "Problem Formulation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_14",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_15@0",
            "content": "For zero-shot tasks, a pre-trained VL model L have no access to training set D train and development set D dev , and directly makes inference on the test instances D test .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_15",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_15@1",
            "content": "For few-shot tasks, we compose a dev set D dev from training data and ensure that |D train | = |D dev | following Perez et al. (2021); to tune the hyper-parameters and select the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_15",
            "start": 173,
            "end": 357,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_15@2",
            "content": "We limit the sizes of training and development sets to meet the goal of learning from limited data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_15",
            "start": 359,
            "end": 457,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_15@3",
            "content": "The size of D train and D dev are small -i.e., we set the size of both to 16 in our study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_15",
            "start": 459,
            "end": 548,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_16@0",
            "content": "Analysis Questions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_16",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_17@0",
            "content": "We aim to answer the following questions in this study through experiments on multiple VL datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_17",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_17@1",
            "content": "Q1) How does prompt design affect zero/fewshot learning on new tasks?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_17",
            "start": 100,
            "end": 168,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_17@2",
            "content": "Providing a pretrained language model with task-specific prompts or significantly improves zero-shot and few-shot performance on NLP domains Schick and Sch\u00fctze, 2021a,b;Brown et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_17",
            "start": 170,
            "end": 358,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_17@3",
            "content": "For this question, we test several ad-hoc prompts on vision-language tasks and analyze how large zero-shot and few-shot performance is affected by different prompts, hand-crafted and noisy prompts, in Sec. 6.5. Q2) Does prompt design still matter given larger training data?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_17",
            "start": 360,
            "end": 633,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_17@4",
            "content": "As we will see in our experiments, prompts affect the zero/few-shot performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_17",
            "start": 635,
            "end": 714,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_17@5",
            "content": "However, prompts may have different effects when models are given different sizes of training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_17",
            "start": 716,
            "end": 814,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_17@6",
            "content": "To answer this question, we train models with different sizes of training data and various prompts, and compare the performance between different prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_17",
            "start": 816,
            "end": 969,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_18@0",
            "content": "Q3) How do different pre-training objectives affect zero/few-shot performance?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_18",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_18@1",
            "content": "We study two different pre-training objectives on few-shot performance: prefix language modeling (PrefixLM) inspired by Raffel et al. (2020) and masked language modeling (MaskedLM).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_18",
            "start": 79,
            "end": 259,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_18@2",
            "content": "In this setup, we pre-train our model with different objectives and test the model on zero-shot and few-shot tasks in Sec. 6.6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_18",
            "start": 261,
            "end": 387,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_19@0",
            "content": "Downstream Tasks and Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_19",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_20@0",
            "content": "In this work, we mainly focus on three tasks: visual question answering, captioning, and categorical learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_20",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_20@1",
            "content": "The visual question answering task requires models to answer a question to a given context image.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_20",
            "start": 111,
            "end": 207,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_20@2",
            "content": "We convert the visual question answering task into a generation task so that the model can generate answers in the zero-shot setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_20",
            "start": 209,
            "end": 341,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_20@3",
            "content": "The captioning task requires a model to generate descriptions for a given context image.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_20",
            "start": 343,
            "end": 430,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_20@4",
            "content": "The categorical learning requires a model to choose the correct category or class.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_20",
            "start": 432,
            "end": 513,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_20@5",
            "content": "We evaluate our model in an open-ended fashion to quantify fast learning of categories, in which it must generate correct labels unlike other classification methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_20",
            "start": 515,
            "end": 679,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_21@0",
            "content": "We include VQAv2 (Goyal et al., 2017), OK-VQA (Marino et al., 2019), and GQA (Hudson",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_21",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_22@0",
            "content": "Evaluation Metrics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_22",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_23@0",
            "content": "To evaluate few-shot performance, we randomly sample 5 different training and dev splits and measure average performance on the 5 splits.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_23",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_23@1",
            "content": "We finetune the vision-language models with 200 epochs for the few-shot setup and choose the best checkpoint on the dev set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_23",
            "start": 138,
            "end": 261,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_23@2",
            "content": "et al., 2015) and SPICE (Anderson et al., 2016) as evaluation metrics for captioning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_23",
            "start": 263,
            "end": 347,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_24@0",
            "content": "Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_24",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_25@0",
            "content": "We evaluate strong zero/few-shot vision-language learners for comparison: Frozen (Tsimpoukelli et al., 2021), PICa for VQA datasets and SimVLM for captioning datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_25",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_25@1",
            "content": "We include Unified VLP for few-shot VQAv2 and Flickr30k.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_25",
            "start": 168,
            "end": 223,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_25@2",
            "content": "Also, we compare them with fully fine-tuned models L f ull as upper bounds of few-shot models for each task; these models are fine-tuned on the entire datasets while few-shot models can access a small amount of data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_25",
            "start": 225,
            "end": 440,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_25@3",
            "content": "For fully fine-tuned models L f ull , we borrow numbers from Uniter large for VQAv2, Oscar (Li et al., 2020b) for GQA, SimVLM and VinVL (Zhang et al., 2021) for NoCaps CIDER and SPICE respectively, and Unified VLP for Flickr30k captioning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_25",
            "start": 442,
            "end": 680,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_25@4",
            "content": "We include VL-T5 no-vqa as a baseline which is pre-trained without visual question answering datasets .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_25",
            "start": 682,
            "end": 784,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_25@5",
            "content": "For miniImageNet, we include Frozen and AFHN (Li et al., 2020a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_25",
            "start": 786,
            "end": 849,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_25@6",
            "content": "Frozen is designed for few-shot learning while AFHN is for meta learning, which is smaller and faster.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_25",
            "start": 851,
            "end": 952,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_26@0",
            "content": "Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_26",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_27@0",
            "content": "Before diving into the analysis, we introduce our model, FEWVLM, to do zero/few-shot learning on VL tasks and answer the analysis questions we raised.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_27",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_27@1",
            "content": "We introduce FEWVLM architecture and pre-training objectives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_27",
            "start": 151,
            "end": 211,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_28@0",
            "content": "Encoder-decoder Vision-language Model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_28",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_29@0",
            "content": "We adopt an encoder-decoder architecture Vaswani et al., 2017), to encode visual and text inputs and generate target text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_29",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_29@1",
            "content": "We represent an input image with 36 object regions from a Faster R-CNN (Ren et al., 2015) trained on Visual Genome (Krishna et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_29",
            "start": 123,
            "end": 260,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_29@2",
            "content": "The sets of region representations are fed into the encoder by appending them to the text .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_29",
            "start": 262,
            "end": 352,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_29@3",
            "content": "We train the model parameters \u03b8 by minimizing the negative log-likelihood of target text y tokens given input text x and image v:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_29",
            "start": 354,
            "end": 482,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_30@0",
            "content": "L \u03b8 = \u2212 |y| i=1 log P \u03b8 (y i |y <i , x, v).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_30",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_31@0",
            "content": "(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_31",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_32@0",
            "content": "The model is not task-specific, so it is a good option for zero/few-shot settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_32",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_33@0",
            "content": "Pre-training Objectives",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_33",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_34@0",
            "content": "We pre-train the models with both prefix language modeling (PrefixLM) and masked language modeling (MaskedLM).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_34",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_34@1",
            "content": "Fig. 3 illustrates the PrefixLM and MaskedLM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_34",
            "start": 111,
            "end": 155,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_34@2",
            "content": "Prefix language modeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_34",
            "start": 157,
            "end": 181,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_34@3",
            "content": "We include prefix language modeling (PrefixLM) following Raffel et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_34",
            "start": 183,
            "end": 260,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_34@4",
            "content": "Given an image and a span of text, this objective randomly splits the text into two separate components; the former component with the given image is used as inputs to the encoder and the latter component is used as target text to be generated by the decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_34",
            "start": 262,
            "end": 520,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_34@5",
            "content": "Masked language modeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_34",
            "start": 522,
            "end": 546,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_34@6",
            "content": "We follow to do masked language modeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_34",
            "start": 548,
            "end": 588,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_34@7",
            "content": "This objective is to replace random spans with numbered sentinel tokens, e.g., <text_1>, and then the masked text is fed into the encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_34",
            "start": 590,
            "end": 727,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_34@8",
            "content": "Then the decoder generates the masked spans as target text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_34",
            "start": 729,
            "end": 787,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_35@0",
            "content": "We randomly mask 15% of input text tokens and replace them with sentinel tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_35",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_36@0",
            "content": "Pre-training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_36",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_36@1",
            "content": "To pre-train FEWVLM, we collect image-caption data from MS COCO (Lin et al., 2014;Chen et al., 2015) and Visual Genome (VG) (Krishna et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_36",
            "start": 19,
            "end": 165,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_36@2",
            "content": "The pre-training datasets contains 9.18M image-text pairs and 180K distinct images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_36",
            "start": 167,
            "end": 249,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_37@0",
            "content": "Low-resource Adaptation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_37",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_38@0",
            "content": "In downstream tasks, we train our model with few-shot examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_38",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_38@1",
            "content": "Fig. 2 shows an illustration of FEWVLM in inference time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_38",
            "start": 64,
            "end": 120,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_38@2",
            "content": "Given a prompt template P, we first get input text and target text using the template x, y = P(input, label).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_38",
            "start": 122,
            "end": 230,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_38@3",
            "content": "Then we train model parameters by minimizing the negative log-likelihood in Eq. (1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_38",
            "start": 232,
            "end": 315,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_38@4",
            "content": "In inference, we use the same prompt and the model generates the label text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_38",
            "start": 317,
            "end": 392,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_38@5",
            "content": "Here we obtain the final label by removing the target prompt template.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_38",
            "start": 394,
            "end": 463,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_39@0",
            "content": "Prompt Design",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_39",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_40@0",
            "content": "Prompts affect the performance of the visionlanguage model ; we study the effect of different prompts on the zero-shot and fewshot performance on downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_40",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_40@1",
            "content": "Tables 1 and 11 show prompts we used in our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_40",
            "start": 164,
            "end": 219,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_41@0",
            "content": "Visual Question Answering",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_41",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_42@0",
            "content": "The visual question answering tasks (VQA, OK-VQA, and GQA) require models to answer a question to a given context image.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_42",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_42@1",
            "content": "Recent approaches Tan and Bansal, 2019;Su et al., 2020;Li et al., 2019Li et al., , 2020b tackle visual question answering tasks as multi-label classification over a predefined set of answer candidates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_42",
            "start": 121,
            "end": 321,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_42@2",
            "content": "Instead, we approach the visual question answering tasks as a generation task so that the model can produce the answers without introducing any task-specific heads.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_42",
            "start": 323,
            "end": 486,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_42@3",
            "content": "In this setup, prompts act as constraints to guide the models to generate proper formats of answers; models might generate a sentence for VQA, which is not the correct format, without prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_42",
            "start": 488,
            "end": 679,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_42@4",
            "content": "Therefore, we study several prompts for input and output as shown in Tables 1 and 11; we explore hand-crafted prompts (Table 1) and noisy prompts for ablation study (Table 11).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_42",
            "start": 681,
            "end": 856,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_42@5",
            "content": "Hand-crafted prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_42",
            "start": 858,
            "end": 878,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_42@6",
            "content": "For input prompts, we explore three different templates: \"question: [Q] answer:\" and with the <text_1> sentinel token at the end.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_42",
            "start": 880,
            "end": 1008,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_42@7",
            "content": "Similarly to masked language modeling, we expect models to generate words thanks to the sentinel token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_42",
            "start": 1010,
            "end": 1112,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_42@8",
            "content": "For target prompts, we explore two different templates: \"[A]\" (an answer) and \"<text_1> [A]\" (an answer with a sentinel token).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_42",
            "start": 1114,
            "end": 1240,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_42@9",
            "content": "Here, we aim to mimic MaskedLM's target text format, so the similar format helps the model quickly adapt to the new task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_42",
            "start": 1242,
            "end": 1362,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_42@10",
            "content": "We call each prompt ID as in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_42",
            "start": 1364,
            "end": 1400,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_42@11",
            "content": "Noisy prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_42",
            "start": 1402,
            "end": 1415,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_42@12",
            "content": "To understand the effect of noisy prompts in zero/few-shot learning, we include irrelevant prompts, noisy tokens, and random sentences as in Table 11.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_42",
            "start": 1417,
            "end": 1566,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_42@13",
            "content": "Irrelevant prompts are random questions or instructions that mislead models to answer wrong questions or follow irrelevant instructions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_42",
            "start": 1568,
            "end": 1703,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_42@14",
            "content": "Noisy tokens are randomly selected from T5's vocabulary, so we test how robust our model is to random tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_42",
            "start": 1705,
            "end": 1813,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_42@15",
            "content": "Finally, random sentences are captions from MS COCO and this gives false information to models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_42",
            "start": 1815,
            "end": 1909,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_43@0",
            "content": "Captioning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_43",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_44@0",
            "content": "In NoCaps and Flickr30k, we explore three handcrafted input prompts: \"a picture of \", \"a photo of \", and \"an image of \".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_44",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_44@1",
            "content": "We study the effect of different word choices in this captioning task. While the three different words have similar meanings, they show different performance in zero-shot and fewshot tasks as we will see in our experiments..",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_44",
            "start": 121,
            "end": 344,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_44@2",
            "content": "For target prompts, we just train the model with the original caption without any additional prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_44",
            "start": 346,
            "end": 446,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_45@0",
            "content": "MiniImageNet",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_45",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_46@0",
            "content": "In miniImageNet, we train our model with a handcrafted input prompt, \"This is <text_1>,\" and target prompt, \"<text_1> [A].\" We compare our model with and without prompts in this dataset to study whether prompts are helpful in categorical learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_46",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_47@0",
            "content": "Results and Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_47",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_48@0",
            "content": "In this section, we first discuss our main results on zero-shot and few-shot tasks and then answer the questions we raised: does prompt design matter in zero/few-shot learning?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_48",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_49@0",
            "content": "Experiment Details",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_49",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_50@0",
            "content": "For pre-training, we set batch size 1,280 and 800 for FEWVLM base and FEWVLM large , respectively and pre-train them with 30 epochs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_50",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_50@1",
            "content": "We use learning rate 1e-4 with 5% linear warmup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_50",
            "start": 133,
            "end": 180,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_51@0",
            "content": "For few-shot learning, we train models with 200 epochs, learning rate 5e-5 and 5% linear warmup and choose the best checkpoint on the dev set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_51",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_51@1",
            "content": "For FEWVLM, we use \"question:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_51",
            "start": 143,
            "end": 171,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_52@0",
            "content": "[Q] answer <text_1>\" (P3) as an input prompt and \"<text_1> [A]\" as a target prompt for visual question answering, and \"an image of\" (Q3) as an input prompt for captioning, which show the best performance. We will study the effect of different prompts in Sec. 6.5. The sizes of of D train and D dev are 16 on VQA and captioning tasks. For miniImageNet, we use 'This is <text_1>,\" and \"<text_1> [A]\" as input and target prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_52",
            "start": 0,
            "end": 425,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_52@1",
            "content": "In this data, we test with {1, 3, 5}-shots per class.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_52",
            "start": 427,
            "end": 479,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_53@0",
            "content": "Performance on Zero-shot Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_53",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_54@0",
            "content": "We evaluate the existing models in a zero-shot manner, in which models do not have access to any training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_54",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_54@1",
            "content": "Tables 2 and 4 show the results on VQA and captioning datasets, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_54",
            "start": 112,
            "end": 188,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_54@2",
            "content": "First, FEWVLM with the hand-crafted prompt (P3) achieves better performance than other baselines on VQA datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_54",
            "start": 190,
            "end": 302,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_54@3",
            "content": "In particular, our FEWVLM base significantly outperforms Frozen which is about 31\u00d7 larger than ours.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_54",
            "start": 304,
            "end": 403,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_54@4",
            "content": "Also, PICa based on GPT3 (Brown et al., 2020) shows the best performance on OK-VQA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_54",
            "start": 405,
            "end": 487,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_54@5",
            "content": "It is noticeable that our FEWVLM large , the 246\u00d7 smaller model, achieves the comparable result to PICa.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_54",
            "start": 489,
            "end": 592,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_54@6",
            "content": "Compared to VL-T5 no-vqa which is the same architecture as ours, FEWVLM base improves VQAv2 performance by about 30% point.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_54",
            "start": 594,
            "end": 716,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_54@7",
            "content": "As we will see in the later section, our pre-training objectives and the prompts boost the VQA performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_54",
            "start": 718,
            "end": 824,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_54@8",
            "content": "On NoCaps, SimVLM huge shows the best performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_54",
            "start": 826,
            "end": 875,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_54@9",
            "content": "Our FEWVLM base significantly improves the performance compared to VL-T5 no-vqa .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_54",
            "start": 877,
            "end": 957,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_54@10",
            "content": "As we will see in the later section, our pre-training objectives and the prompts boost the VQA and captioning performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_54",
            "start": 959,
            "end": 1080,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_55@0",
            "content": "Performance on Few-shot Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_55",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_56@0",
            "content": "Tables 3 and 5 show the few-shot performance on VQA and captioning datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_56",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_56@1",
            "content": "of training and validation sets are 16 for FEWVLM, VL-T5 no-vqa , and Unified VLP; and Frozen and PICa use 4 and 16 in-context demonstration examples, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_56",
            "start": 77,
            "end": 240,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_57@0",
            "content": "On VQAv2 and OK-VQA, PICa shows the best performance while our FEWVLM large achieves the comparable result on VQAv2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_57",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_57@1",
            "content": "OK-VQA requires external knowledge to answer unlike other VQA datasets, so larger models and large pre-training data (prior knowledge) are necessary to improve.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_57",
            "start": 117,
            "end": 276,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_57@2",
            "content": "Interestingly, FEWVLM * base , which is trained with 4 training examples, outperforms Frozen.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_57",
            "start": 278,
            "end": 370,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_57@3",
            "content": "On captioning data, FEWVLM base notably outperforms VL-T5 no-vqa by 31.1% point on NoCaps CIDEr.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_57",
            "start": 372,
            "end": 467,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_58@0",
            "content": "Unified VLP slightly underperforms FEWVLM on Flickr30k captioning task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_58",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_58@1",
            "content": "We conjecture that their architecture is based on a encoder-decoder transfomer and it is pre-trained with a captioning task .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_58",
            "start": 72,
            "end": 196,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_59@0",
            "content": "MiniImageNet",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_59",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_60@0",
            "content": "Table 6 shows results on miniImageNet, where models must choose the correct class for each image.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_60",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_60@1",
            "content": "We train and evaluate FEWVLM in an generative manner; the model must generate correct label text to get the credit.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_60",
            "start": 98,
            "end": 212,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_60@2",
            "content": "FEWVLM significantly outperforms Frozen in all shots.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_60",
            "start": 214,
            "end": 266,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_60@3",
            "content": "Note that we train FEWVLM with a few training samples while Frozen uses them as in-context demonstration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_60",
            "start": 268,
            "end": 372,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_60@4",
            "content": "Interestingly, FEWVLM with a hand-crafted prompt improves performance a lot on the 1-shot case, while it marginally improves on the 5-shot case.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_60",
            "start": 374,
            "end": 517,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_61@0",
            "content": "Study of Prompt Design",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_61",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_62@0",
            "content": "Here we examine the effect of different prompts on FEWVLM base in Table 7 and Figs .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_62",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_62@1",
            "content": "6, 5, and 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_62",
            "start": 85,
            "end": 96,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_62@2",
            "content": "We test the model on VQAv2 and Flickr30k datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_62",
            "start": 98,
            "end": 147,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_63@0",
            "content": "Zero-shot Predictions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_63",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_64@0",
            "content": "Table 7 shows the zero-shot performance on VQAv2 and Flickr30k.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_64",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_64@1",
            "content": "We observe that zero-shot results are remarkably affected by input prompts on both datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_64",
            "start": 64,
            "end": 155,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_64@2",
            "content": "For input prompts, <text_1> in P1 and P3 helps the zero-shot predictions significantly compared to \"no prompt\" and P2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_64",
            "start": 157,
            "end": 274,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_64@3",
            "content": "We conjecture that <text_1> guides the model to predict masked spans similarly to MaskedLM, so it improves the performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_64",
            "start": 276,
            "end": 398,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_64@4",
            "content": "On Flickr30k, we examine different word choices of prompts: \"a picture of\" (Q1), \"a photo of\" (Q2), and \"an image of\" (Q3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_64",
            "start": 400,
            "end": 522,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_64@5",
            "content": "For instance, using \"an image of\" outperforms using no prompt by 21.4 point.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_64",
            "start": 524,
            "end": 599,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_64@6",
            "content": "It is noticeable that different word choices significantly affect the zero-shot results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_64",
            "start": 601,
            "end": 688,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_65@0",
            "content": "Few-shot Predictions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_65",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_66@0",
            "content": "We study various input prompts including irrelevant prompts, noisy tokens, and random sentences on VQAv2 (Fig. 4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_66",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_66@1",
            "content": "First, noisy prompts and no prompt achieve near 0 accuracy on the zero-shot setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_66",
            "start": 115,
            "end": 198,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_66@2",
            "content": "In few-shot predictions, FEWVLM with noisy prompts learns as quickly as hand-crafted prompts given larger data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_66",
            "start": 200,
            "end": 310,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_66@3",
            "content": "For example, our model with noisy prompts achieves comparable results to the best hand-crafted prompt.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_66",
            "start": 312,
            "end": 413,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_66@4",
            "content": "Among all different types of noisy prompts, random sentences deteriorate performance the most.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_66",
            "start": 415,
            "end": 508,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_66@5",
            "content": "This is because the random sentences come from captions in MS COCO, so the model might choose the answer from wrong captions not from images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_66",
            "start": 510,
            "end": 650,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_66@6",
            "content": "Interestingly, no prompt outperforms the other noisy prompts and even shows similar to or better than the handcrafted prompt with larger training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_66",
            "start": 652,
            "end": 802,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_66@7",
            "content": "We also observe a similar phenomenon on Flickr30k; no prompt performs similar to hand-crafted prompts in Fig. 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_66",
            "start": 804,
            "end": 915,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_66@8",
            "content": "In addition, we explore two different target prompts, \"<text_1> [A]\" and \" [A].\"",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_66",
            "start": 917,
            "end": 996,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_66@9",
            "content": "We try to mimic the MaskedLM's target text format, so we add \"<text_1>\" to target prompt on VQA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_66",
            "start": 998,
            "end": 1093,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_66@10",
            "content": "This might help the model's fast adaptation to a new task since they share the same target prompt.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_66",
            "start": 1095,
            "end": 1192,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_66@11",
            "content": "In Fig. 6, we notice an interesting phenomenon; the target prompt \"[A]\" shows a larger variance than the other suggesting that introducing \"<text_1>\" helps the model quickly adapt to a new task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_66",
            "start": 1194,
            "end": 1387,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_66@12",
            "content": "However, both prompts show similar results given larger training data, e.g., 300.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_66",
            "start": 1389,
            "end": 1469,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_67@0",
            "content": "Pre-training Objectives",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_67",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_68@0",
            "content": "We investigate how pre-training objectives affect different tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_68",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_68@1",
            "content": "We pre-train FEWVLM with different pre-training objectives: masked language modeling (MaskedLM) and prefix language modeling (PrefixLM).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_68",
            "start": 67,
            "end": 202,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_69@0",
            "content": "In Table 8, we observe that MaskedLM helps VQA tasks while PrefixLM helps captioning tasks in zero-shot and few-shot settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_69",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_69@1",
            "content": "We conjecture that MaskedLM is to predict spans, which is analogous to predict correct answers to questions, and PrefixLM is to generate the rest of the given prefix, which is similar to captioning tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_69",
            "start": 127,
            "end": 330,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_69@2",
            "content": "In other words, if the pre-training task is similar to the downstream tasks, then it will help performance further.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_69",
            "start": 332,
            "end": 446,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_69@3",
            "content": "When pre-training with both objectives, they create a synergetic effect and thus improve cross-task generalization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_69",
            "start": 448,
            "end": 562,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_70@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_70",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_71@0",
            "content": "In this work, we present FEWVLM, a few-shot prompt-based learner on vision-language tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_71",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_71@1",
            "content": "On diverse datasets, FEWVLM outperforms baselines and shows comparable results to PICa which is 246\u00d7 larger than ours.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_71",
            "start": 91,
            "end": 208,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_71@2",
            "content": "We observe that prompts are vital in zero-shot and few-shot tasks and each pre-training objective helps different few-shot tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_71",
            "start": 210,
            "end": 338,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_71@3",
            "content": "Also, we find out that models with larger training data are not significantly affected by noisy prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_71",
            "start": 340,
            "end": 443,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_71@4",
            "content": "Future work includes exploring automatic prompt generation and diverse formats of few-shot tasks such as multiple-choice VQA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_71",
            "start": 445,
            "end": 569,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_71@5",
            "content": "Finding optimal prompts require exhaustive engineering to achieve the best performance and leads to impressive results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_71",
            "start": 571,
            "end": 689,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_71@6",
            "content": "We leave the exploration of these directions to future investigations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_71",
            "start": 691,
            "end": 760,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_72@0",
            "content": "Table 9 shows model parameters in our model, FEWVLM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_72",
            "start": 0,
            "end": 51,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_72@1",
            "content": "FEWVLM base and FEWVLM large is based on VL-T5 and T5 (Raffel et al., 2020), respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_72",
            "start": 53,
            "end": 142,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_73@0",
            "content": "We evaluate our model with COCO captioning data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_73",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_73@1",
            "content": "We use Karpathy split (Karpathy and Li, 2015) for MS COCO captioning, which re-splits train and val images into 113,287 / 5000 / 5000 for train / validation / test.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_73",
            "start": 49,
            "end": 212,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_73@2",
            "content": "Table 10 shows the results on COCO.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_73",
            "start": 214,
            "end": 248,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_73@3",
            "content": "sizes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_73",
            "start": 250,
            "end": 255,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_74@0",
            "content": "We pre-train our model with different datasets: MS COCO and Visual Genome (VG), and Conceptual Captions (CC).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_74",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_74@1",
            "content": "We investigate which pre-training dataset helps the downstream tasks in a few-shot manner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_74",
            "start": 110,
            "end": 199,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_74@2",
            "content": "In Table 12, we observe that MS COCO and VG datasets are more helpful to the downstream tasks than CC.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_74",
            "start": 201,
            "end": 302,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_75@0",
            "content": "Harsh Agrawal, Peter Anderson, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, nocaps: novel object captioning at scale, 2019-10-27, 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_75",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_76@0",
            "content": "Peter Anderson, Basura Fernando, Mark Johnson, Stephen Gould, Spice: Semantic propositional image caption evaluation, 2016, European conference on computer vision, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_76",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_77@0",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Mc-Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners, , Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_77",
            "start": 0,
            "end": 580,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_78@0",
            "content": "Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, C Lawrence Zitnick, Microsoft coco captions: Data collection and evaluation server, 2015, ArXiv preprint, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_78",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_79@0",
            "content": "UNKNOWN, None, 2019, Uniter: Learning universal image-text representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_79",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_80@0",
            "content": "Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal, Unifying vision-and-language tasks via text generation, 2021-07-24, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_80",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_81@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_81",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_82@0",
            "content": "Tianyu Gao, Adam Fisch, Danqi Chen, Making pre-trained language models better few-shot learners, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_82",
            "start": 0,
            "end": 278,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_83@0",
            "content": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Making the V in VQA matter: Elevating the role of image understanding in visual question answering, 2017-07-21, 2017 IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_83",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_84@0",
            "content": "A Drew,  Hudson, D Christopher,  Manning, GQA: A new dataset for real-world visual reasoning and compositional question answering, 2019-06-16, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_84",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_85@0",
            "content": "Andrej Karpathy, Fei-Fei Li, Deep visualsemantic alignments for generating image descriptions, 2015-06-07, IEEE Conference on Computer Vision and Pattern Recognition, IEEE Computer Society.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_85",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_86@0",
            "content": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, Visual genome: Connecting language and vision using crowdsourced dense image annotations, 2017, International journal of computer vision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_86",
            "start": 0,
            "end": 282,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_87@0",
            "content": "Kai Li, Yulun Zhang, Kunpeng Li, Yun Fu, Adversarial feature hallucination networks for fewshot learning, 2020-06-13, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_87",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_88@0",
            "content": "UNKNOWN, None, 2019, Visualbert: A simple and performant baseline for vision and language, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_88",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_89@0",
            "content": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Oscar: Objectsemantics aligned pre-training for vision-language tasks, 2020, European Conference on Computer Vision, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_89",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_90@0",
            "content": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, Microsoft coco: Common objects in context, 2014, European conference on computer vision, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_90",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_91@0",
            "content": "Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi, OK-VQA: A visual question answering benchmark requiring external knowledge, 2019-06-16, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Computer Vision Foundation / IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_91",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_92@0",
            "content": "UNKNOWN, None, 2021, True few-shot learning with language models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_92",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_93@0",
            "content": "Alec Radford, Jong Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Learning transferable visual models from natural language supervision, 2021-07, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_93",
            "start": 0,
            "end": 342,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_94@0",
            "content": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Language models are unsupervised multitask learners, 2019, OpenAI blog, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_94",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_95@0",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, J. Mach. Learn. Res, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_95",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_96@0",
            "content": "Kaiming Shaoqing Ren, Ross He, Jian Girshick,  Sun, Faster R-CNN: towards real-time object detection with region proposal networks, 2015-12-07, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_96",
            "start": 0,
            "end": 258,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_97@0",
            "content": "Timo Schick, Hinrich Sch\u00fctze, Exploiting cloze-questions for few-shot text classification and natural language inference, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_97",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_98@0",
            "content": "Timo Schick, Hinrich Sch\u00fctze, It's not just size that matters: Small language models are also few-shot learners, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_98",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_99@0",
            "content": "Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai, VL-BERT: pretraining of generic visual-linguistic representations, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_99",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_100@0",
            "content": "Hao Tan, Mohit Bansal, LXMERT: Learning cross-modality encoder representations from transformers, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_100",
            "start": 0,
            "end": 322,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_101@0",
            "content": "UNKNOWN, None, 2021, Multimodal few-shot learning with frozen language models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_101",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_102@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_102",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_103@0",
            "content": "C Ramakrishna Vedantam, Devi Zitnick,  Parikh, Cider: Consensus-based image description evaluation, 2015-06-07, IEEE Conference on Computer Vision and Pattern Recognition, IEEE Computer Society.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_103",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_104@0",
            "content": "Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, Daan Wierstra, Matching networks for one shot learning, 2016-12-05, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_104",
            "start": 0,
            "end": 254,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_105@0",
            "content": "UNKNOWN, None, 2021, Simvlm: Simple visual language model pretraining with weak supervision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_105",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_106@0",
            "content": "UNKNOWN, None, 2021, An empirical study of gpt-3 for few-shot knowledge-based vqa, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_106",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_107@0",
            "content": "Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier, From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions, 2014, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_107",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_108@0",
            "content": "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao, Vinvl: Revisiting visual representations in vision-language models, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_108",
            "start": 0,
            "end": 261,
            "label": {}
        },
        {
            "ix": "173-ARR_v2_109@0",
            "content": "Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, Jianfeng Gao, Unified vision-language pre-training for image captioning and VQA, 2020-02-07, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "173-ARR_v2_109",
            "start": 0,
            "end": 248,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "173-ARR_v2_0",
            "tgt_ix": "173-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_0",
            "tgt_ix": "173-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_1",
            "tgt_ix": "173-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_1",
            "tgt_ix": "173-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_0",
            "tgt_ix": "173-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_2",
            "tgt_ix": "173-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_4",
            "tgt_ix": "173-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_5",
            "tgt_ix": "173-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_6",
            "tgt_ix": "173-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_3",
            "tgt_ix": "173-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_3",
            "tgt_ix": "173-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_3",
            "tgt_ix": "173-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_3",
            "tgt_ix": "173-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_3",
            "tgt_ix": "173-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_0",
            "tgt_ix": "173-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_7",
            "tgt_ix": "173-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_9",
            "tgt_ix": "173-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_10",
            "tgt_ix": "173-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_8",
            "tgt_ix": "173-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_8",
            "tgt_ix": "173-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_8",
            "tgt_ix": "173-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_8",
            "tgt_ix": "173-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_0",
            "tgt_ix": "173-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_11",
            "tgt_ix": "173-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_12",
            "tgt_ix": "173-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_12",
            "tgt_ix": "173-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_12",
            "tgt_ix": "173-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_13",
            "tgt_ix": "173-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_14",
            "tgt_ix": "173-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_14",
            "tgt_ix": "173-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_12",
            "tgt_ix": "173-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_15",
            "tgt_ix": "173-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_17",
            "tgt_ix": "173-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_16",
            "tgt_ix": "173-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_16",
            "tgt_ix": "173-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_16",
            "tgt_ix": "173-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_12",
            "tgt_ix": "173-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_18",
            "tgt_ix": "173-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_20",
            "tgt_ix": "173-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_19",
            "tgt_ix": "173-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_19",
            "tgt_ix": "173-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_19",
            "tgt_ix": "173-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_12",
            "tgt_ix": "173-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_21",
            "tgt_ix": "173-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_22",
            "tgt_ix": "173-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_22",
            "tgt_ix": "173-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_12",
            "tgt_ix": "173-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_23",
            "tgt_ix": "173-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_24",
            "tgt_ix": "173-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_24",
            "tgt_ix": "173-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_0",
            "tgt_ix": "173-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_25",
            "tgt_ix": "173-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_26",
            "tgt_ix": "173-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_26",
            "tgt_ix": "173-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_26",
            "tgt_ix": "173-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_27",
            "tgt_ix": "173-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_29",
            "tgt_ix": "173-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_30",
            "tgt_ix": "173-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_31",
            "tgt_ix": "173-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_28",
            "tgt_ix": "173-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_28",
            "tgt_ix": "173-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_28",
            "tgt_ix": "173-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_28",
            "tgt_ix": "173-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_28",
            "tgt_ix": "173-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_26",
            "tgt_ix": "173-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_32",
            "tgt_ix": "173-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_34",
            "tgt_ix": "173-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_35",
            "tgt_ix": "173-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_33",
            "tgt_ix": "173-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_33",
            "tgt_ix": "173-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_33",
            "tgt_ix": "173-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_33",
            "tgt_ix": "173-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_0",
            "tgt_ix": "173-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_36",
            "tgt_ix": "173-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_37",
            "tgt_ix": "173-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_37",
            "tgt_ix": "173-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_37",
            "tgt_ix": "173-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_38",
            "tgt_ix": "173-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_39",
            "tgt_ix": "173-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_39",
            "tgt_ix": "173-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_37",
            "tgt_ix": "173-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_40",
            "tgt_ix": "173-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_41",
            "tgt_ix": "173-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_41",
            "tgt_ix": "173-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_37",
            "tgt_ix": "173-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_42",
            "tgt_ix": "173-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_43",
            "tgt_ix": "173-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_43",
            "tgt_ix": "173-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_37",
            "tgt_ix": "173-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_44",
            "tgt_ix": "173-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_45",
            "tgt_ix": "173-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_45",
            "tgt_ix": "173-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_0",
            "tgt_ix": "173-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_46",
            "tgt_ix": "173-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_47",
            "tgt_ix": "173-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_47",
            "tgt_ix": "173-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_47",
            "tgt_ix": "173-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_48",
            "tgt_ix": "173-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_50",
            "tgt_ix": "173-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_51",
            "tgt_ix": "173-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_49",
            "tgt_ix": "173-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_49",
            "tgt_ix": "173-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_49",
            "tgt_ix": "173-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_49",
            "tgt_ix": "173-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_47",
            "tgt_ix": "173-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_52",
            "tgt_ix": "173-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_53",
            "tgt_ix": "173-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_53",
            "tgt_ix": "173-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_47",
            "tgt_ix": "173-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_54",
            "tgt_ix": "173-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_56",
            "tgt_ix": "173-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_57",
            "tgt_ix": "173-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_55",
            "tgt_ix": "173-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_55",
            "tgt_ix": "173-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_55",
            "tgt_ix": "173-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_55",
            "tgt_ix": "173-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_47",
            "tgt_ix": "173-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_58",
            "tgt_ix": "173-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_59",
            "tgt_ix": "173-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_59",
            "tgt_ix": "173-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_47",
            "tgt_ix": "173-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_60",
            "tgt_ix": "173-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_61",
            "tgt_ix": "173-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_61",
            "tgt_ix": "173-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_47",
            "tgt_ix": "173-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_62",
            "tgt_ix": "173-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_63",
            "tgt_ix": "173-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_63",
            "tgt_ix": "173-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_47",
            "tgt_ix": "173-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_64",
            "tgt_ix": "173-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_65",
            "tgt_ix": "173-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_65",
            "tgt_ix": "173-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_47",
            "tgt_ix": "173-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_66",
            "tgt_ix": "173-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_68",
            "tgt_ix": "173-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_67",
            "tgt_ix": "173-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_67",
            "tgt_ix": "173-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_67",
            "tgt_ix": "173-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_0",
            "tgt_ix": "173-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_69",
            "tgt_ix": "173-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_70",
            "tgt_ix": "173-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_70",
            "tgt_ix": "173-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_70",
            "tgt_ix": "173-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_71",
            "tgt_ix": "173-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_70",
            "tgt_ix": "173-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_72",
            "tgt_ix": "173-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_70",
            "tgt_ix": "173-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_73",
            "tgt_ix": "173-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "173-ARR_v2_0",
            "tgt_ix": "173-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_1",
            "tgt_ix": "173-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_2",
            "tgt_ix": "173-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_2",
            "tgt_ix": "173-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_2",
            "tgt_ix": "173-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_2",
            "tgt_ix": "173-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_2",
            "tgt_ix": "173-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_2",
            "tgt_ix": "173-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_2",
            "tgt_ix": "173-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_2",
            "tgt_ix": "173-ARR_v2_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_2",
            "tgt_ix": "173-ARR_v2_2@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_3",
            "tgt_ix": "173-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_4",
            "tgt_ix": "173-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_5",
            "tgt_ix": "173-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_5",
            "tgt_ix": "173-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_5",
            "tgt_ix": "173-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_5",
            "tgt_ix": "173-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_5",
            "tgt_ix": "173-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_5",
            "tgt_ix": "173-ARR_v2_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_6",
            "tgt_ix": "173-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_6",
            "tgt_ix": "173-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_6",
            "tgt_ix": "173-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_6",
            "tgt_ix": "173-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_6",
            "tgt_ix": "173-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_7",
            "tgt_ix": "173-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_7",
            "tgt_ix": "173-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_8",
            "tgt_ix": "173-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_9",
            "tgt_ix": "173-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_9",
            "tgt_ix": "173-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_9",
            "tgt_ix": "173-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_9",
            "tgt_ix": "173-ARR_v2_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_9",
            "tgt_ix": "173-ARR_v2_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_9",
            "tgt_ix": "173-ARR_v2_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_9",
            "tgt_ix": "173-ARR_v2_9@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_9",
            "tgt_ix": "173-ARR_v2_9@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_9",
            "tgt_ix": "173-ARR_v2_9@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_10",
            "tgt_ix": "173-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_10",
            "tgt_ix": "173-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_10",
            "tgt_ix": "173-ARR_v2_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_11",
            "tgt_ix": "173-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_11",
            "tgt_ix": "173-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_11",
            "tgt_ix": "173-ARR_v2_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_11",
            "tgt_ix": "173-ARR_v2_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_12",
            "tgt_ix": "173-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_13",
            "tgt_ix": "173-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_14",
            "tgt_ix": "173-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_15",
            "tgt_ix": "173-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_15",
            "tgt_ix": "173-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_15",
            "tgt_ix": "173-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_15",
            "tgt_ix": "173-ARR_v2_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_16",
            "tgt_ix": "173-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_17",
            "tgt_ix": "173-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_17",
            "tgt_ix": "173-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_17",
            "tgt_ix": "173-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_17",
            "tgt_ix": "173-ARR_v2_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_17",
            "tgt_ix": "173-ARR_v2_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_17",
            "tgt_ix": "173-ARR_v2_17@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_17",
            "tgt_ix": "173-ARR_v2_17@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_18",
            "tgt_ix": "173-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_18",
            "tgt_ix": "173-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_18",
            "tgt_ix": "173-ARR_v2_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_19",
            "tgt_ix": "173-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_20",
            "tgt_ix": "173-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_20",
            "tgt_ix": "173-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_20",
            "tgt_ix": "173-ARR_v2_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_20",
            "tgt_ix": "173-ARR_v2_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_20",
            "tgt_ix": "173-ARR_v2_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_20",
            "tgt_ix": "173-ARR_v2_20@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_21",
            "tgt_ix": "173-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_22",
            "tgt_ix": "173-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_23",
            "tgt_ix": "173-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_23",
            "tgt_ix": "173-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_23",
            "tgt_ix": "173-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_24",
            "tgt_ix": "173-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_25",
            "tgt_ix": "173-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_25",
            "tgt_ix": "173-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_25",
            "tgt_ix": "173-ARR_v2_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_25",
            "tgt_ix": "173-ARR_v2_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_25",
            "tgt_ix": "173-ARR_v2_25@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_25",
            "tgt_ix": "173-ARR_v2_25@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_25",
            "tgt_ix": "173-ARR_v2_25@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_26",
            "tgt_ix": "173-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_27",
            "tgt_ix": "173-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_27",
            "tgt_ix": "173-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_28",
            "tgt_ix": "173-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_29",
            "tgt_ix": "173-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_29",
            "tgt_ix": "173-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_29",
            "tgt_ix": "173-ARR_v2_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_29",
            "tgt_ix": "173-ARR_v2_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_30",
            "tgt_ix": "173-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_31",
            "tgt_ix": "173-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_32",
            "tgt_ix": "173-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_33",
            "tgt_ix": "173-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_34",
            "tgt_ix": "173-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_34",
            "tgt_ix": "173-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_34",
            "tgt_ix": "173-ARR_v2_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_34",
            "tgt_ix": "173-ARR_v2_34@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_34",
            "tgt_ix": "173-ARR_v2_34@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_34",
            "tgt_ix": "173-ARR_v2_34@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_34",
            "tgt_ix": "173-ARR_v2_34@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_34",
            "tgt_ix": "173-ARR_v2_34@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_34",
            "tgt_ix": "173-ARR_v2_34@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_35",
            "tgt_ix": "173-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_36",
            "tgt_ix": "173-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_36",
            "tgt_ix": "173-ARR_v2_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_36",
            "tgt_ix": "173-ARR_v2_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_37",
            "tgt_ix": "173-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_38",
            "tgt_ix": "173-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_38",
            "tgt_ix": "173-ARR_v2_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_38",
            "tgt_ix": "173-ARR_v2_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_38",
            "tgt_ix": "173-ARR_v2_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_38",
            "tgt_ix": "173-ARR_v2_38@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_38",
            "tgt_ix": "173-ARR_v2_38@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_39",
            "tgt_ix": "173-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_40",
            "tgt_ix": "173-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_40",
            "tgt_ix": "173-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_41",
            "tgt_ix": "173-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_42",
            "tgt_ix": "173-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_42",
            "tgt_ix": "173-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_42",
            "tgt_ix": "173-ARR_v2_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_42",
            "tgt_ix": "173-ARR_v2_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_42",
            "tgt_ix": "173-ARR_v2_42@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_42",
            "tgt_ix": "173-ARR_v2_42@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_42",
            "tgt_ix": "173-ARR_v2_42@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_42",
            "tgt_ix": "173-ARR_v2_42@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_42",
            "tgt_ix": "173-ARR_v2_42@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_42",
            "tgt_ix": "173-ARR_v2_42@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_42",
            "tgt_ix": "173-ARR_v2_42@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_42",
            "tgt_ix": "173-ARR_v2_42@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_42",
            "tgt_ix": "173-ARR_v2_42@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_42",
            "tgt_ix": "173-ARR_v2_42@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_42",
            "tgt_ix": "173-ARR_v2_42@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_42",
            "tgt_ix": "173-ARR_v2_42@15",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_43",
            "tgt_ix": "173-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_44",
            "tgt_ix": "173-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_44",
            "tgt_ix": "173-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_44",
            "tgt_ix": "173-ARR_v2_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_45",
            "tgt_ix": "173-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_46",
            "tgt_ix": "173-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_47",
            "tgt_ix": "173-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_48",
            "tgt_ix": "173-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_49",
            "tgt_ix": "173-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_50",
            "tgt_ix": "173-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_50",
            "tgt_ix": "173-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_51",
            "tgt_ix": "173-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_51",
            "tgt_ix": "173-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_52",
            "tgt_ix": "173-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_52",
            "tgt_ix": "173-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_53",
            "tgt_ix": "173-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_54",
            "tgt_ix": "173-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_54",
            "tgt_ix": "173-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_54",
            "tgt_ix": "173-ARR_v2_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_54",
            "tgt_ix": "173-ARR_v2_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_54",
            "tgt_ix": "173-ARR_v2_54@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_54",
            "tgt_ix": "173-ARR_v2_54@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_54",
            "tgt_ix": "173-ARR_v2_54@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_54",
            "tgt_ix": "173-ARR_v2_54@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_54",
            "tgt_ix": "173-ARR_v2_54@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_54",
            "tgt_ix": "173-ARR_v2_54@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_54",
            "tgt_ix": "173-ARR_v2_54@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_55",
            "tgt_ix": "173-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_56",
            "tgt_ix": "173-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_56",
            "tgt_ix": "173-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_57",
            "tgt_ix": "173-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_57",
            "tgt_ix": "173-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_57",
            "tgt_ix": "173-ARR_v2_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_57",
            "tgt_ix": "173-ARR_v2_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_58",
            "tgt_ix": "173-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_58",
            "tgt_ix": "173-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_59",
            "tgt_ix": "173-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_60",
            "tgt_ix": "173-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_60",
            "tgt_ix": "173-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_60",
            "tgt_ix": "173-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_60",
            "tgt_ix": "173-ARR_v2_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_60",
            "tgt_ix": "173-ARR_v2_60@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_61",
            "tgt_ix": "173-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_62",
            "tgt_ix": "173-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_62",
            "tgt_ix": "173-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_62",
            "tgt_ix": "173-ARR_v2_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_63",
            "tgt_ix": "173-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_64",
            "tgt_ix": "173-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_64",
            "tgt_ix": "173-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_64",
            "tgt_ix": "173-ARR_v2_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_64",
            "tgt_ix": "173-ARR_v2_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_64",
            "tgt_ix": "173-ARR_v2_64@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_64",
            "tgt_ix": "173-ARR_v2_64@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_64",
            "tgt_ix": "173-ARR_v2_64@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_65",
            "tgt_ix": "173-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_66",
            "tgt_ix": "173-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_66",
            "tgt_ix": "173-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_66",
            "tgt_ix": "173-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_66",
            "tgt_ix": "173-ARR_v2_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_66",
            "tgt_ix": "173-ARR_v2_66@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_66",
            "tgt_ix": "173-ARR_v2_66@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_66",
            "tgt_ix": "173-ARR_v2_66@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_66",
            "tgt_ix": "173-ARR_v2_66@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_66",
            "tgt_ix": "173-ARR_v2_66@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_66",
            "tgt_ix": "173-ARR_v2_66@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_66",
            "tgt_ix": "173-ARR_v2_66@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_66",
            "tgt_ix": "173-ARR_v2_66@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_66",
            "tgt_ix": "173-ARR_v2_66@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_67",
            "tgt_ix": "173-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_68",
            "tgt_ix": "173-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_68",
            "tgt_ix": "173-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_69",
            "tgt_ix": "173-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_69",
            "tgt_ix": "173-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_69",
            "tgt_ix": "173-ARR_v2_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_69",
            "tgt_ix": "173-ARR_v2_69@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_70",
            "tgt_ix": "173-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_71",
            "tgt_ix": "173-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_71",
            "tgt_ix": "173-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_71",
            "tgt_ix": "173-ARR_v2_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_71",
            "tgt_ix": "173-ARR_v2_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_71",
            "tgt_ix": "173-ARR_v2_71@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_71",
            "tgt_ix": "173-ARR_v2_71@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_71",
            "tgt_ix": "173-ARR_v2_71@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_72",
            "tgt_ix": "173-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_72",
            "tgt_ix": "173-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_73",
            "tgt_ix": "173-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_73",
            "tgt_ix": "173-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_73",
            "tgt_ix": "173-ARR_v2_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_73",
            "tgt_ix": "173-ARR_v2_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_74",
            "tgt_ix": "173-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_74",
            "tgt_ix": "173-ARR_v2_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_74",
            "tgt_ix": "173-ARR_v2_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_75",
            "tgt_ix": "173-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_76",
            "tgt_ix": "173-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_77",
            "tgt_ix": "173-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_78",
            "tgt_ix": "173-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_79",
            "tgt_ix": "173-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_80",
            "tgt_ix": "173-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_81",
            "tgt_ix": "173-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_82",
            "tgt_ix": "173-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_83",
            "tgt_ix": "173-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_84",
            "tgt_ix": "173-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_85",
            "tgt_ix": "173-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_86",
            "tgt_ix": "173-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_87",
            "tgt_ix": "173-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_88",
            "tgt_ix": "173-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_89",
            "tgt_ix": "173-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_90",
            "tgt_ix": "173-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_91",
            "tgt_ix": "173-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_92",
            "tgt_ix": "173-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_93",
            "tgt_ix": "173-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_94",
            "tgt_ix": "173-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_95",
            "tgt_ix": "173-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_96",
            "tgt_ix": "173-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_97",
            "tgt_ix": "173-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_98",
            "tgt_ix": "173-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_99",
            "tgt_ix": "173-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_100",
            "tgt_ix": "173-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_101",
            "tgt_ix": "173-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_102",
            "tgt_ix": "173-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_103",
            "tgt_ix": "173-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_104",
            "tgt_ix": "173-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_105",
            "tgt_ix": "173-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_106",
            "tgt_ix": "173-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_107",
            "tgt_ix": "173-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_108",
            "tgt_ix": "173-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "173-ARR_v2_109",
            "tgt_ix": "173-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 838,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "173-ARR",
        "version": 2
    }
}