{
    "nodes": [
        {
            "ix": "470-ARR_v1_0",
            "content": "Leveraging Uni-Modal Self-Supervised Learning for Multimodal Audio-visual Speech Recognition",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_2",
            "content": "Training Transformer-based models demands a large amount of data, while obtaining parallel aligned and labelled data in multimodality is rather cost-demanding, especially for audiovisual speech recognition (AVSR). Thus it makes a lot of sense to make use of unlabelled uni-modal data. On the other side, although the effectiveness of large-scale self-supervised learning is well established in both audio and visual modalities, how to integrate those pretrained models into a multimodal scenario remains underexplored. In this work, we successfully leverage uni-modal self-supervised learning to promote the multimodal AVSR. In particular, we first train audio and visual encoders on a large-scale uni-modal dataset, then we integrate components of both encoders into a larger multimodal framework which learns to recognize paired audio-visual data into characters through a combination of CTC and seq2seq decoding. We show that both components inherited from uni-modal selfsupervised learning cooperate well, resulting in that the multimodal framework yields competitive results through fine-tuning. Our model is experimentally validated on both word-level and sentence-level AVSR tasks. Especially, even without an external language model, our proposed model raises the state-of-the-art performances on the widely accepted Lip Reading Sentences 2 (LRS2) dataset by a large margin, with a relative improvement of 30%.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "470-ARR_v1_4",
            "content": "Audio-Visual Speech Recognition (AVSR) is a speech recognition task that leverages both an audio input of human voice and an aligned visual input of lip motions. It has been one of the successful application fields that involve multiple modalities in recent years. Due to the limited amount of labeled, multi-modal parallel data and the difficulty of recognition from the visual inputs (i.e., lip reading), it is a challenging task to tackle.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_5",
            "content": "Existing AVSR models tend to use extra data to increase the performance of the system, in a form of inserting an extra supervised learning stage in the training process. For example, many existing methods rely on an extra sequence level classification to bootstrap its learning on visual features. Petridis et al. (2018); Zhang et al. (2019) train their visual front-end on LRW (Chung and Zisserman, 2016) before learning on the AVSR task. Afouras et al. (2018a,b) chunks the MV-LRS data into pieces of words and pre-train the model through classification. VoxCeleb (Chung et al., 2018) are also used in Afouras et al. (2020) for the same purpose. Learning an effective visual front-end could still be notoriously hard, even with these extra supervised learning tasks. Sometimes curriculum learning is required to adapt the learned visual front-end into AVSR task (Afouras et al., 2018a). End-to-end learning of large-scale AVSR data hasn't been successful until recently (Ma et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_6",
            "content": "Although self-supervised learning could enable leveraging unlabelled or even non-parallel data, it hasn't been adequately explored on this task. Shukla et al. (2020) is among the few attempts in this facet, in which it predicts lip motions from audio inputs. Their proposed learning schemes yield strong emotion recognition results but are relatively weak in speech recognition. Moreover, since in AVSR it is the lip shape and motions between frames rather than the objects in a single image that matters for recognizing speech contents, if pre-trained visual models tailored for tasks targeting at single frame images could work for AVSR remains unknown. In another scenario, selfsupervised learning in uni-modality has been well established as a paradigm to learn general representations from unlabelled examples, such as in natural language processing (Brown et al., 2020;Devlin et al., 2018), speech recognition (Baevski et al., 2020), and computer vision (He et al., 2019;Chen et al., 2020a;Grill et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_7",
            "content": "In this work, we rely on a simple but effective approach, which is to utilize unlabelled uni-modal data by using pre-trained models that are trained in single-modality through self-supervised learning. Specifically, we use Baevski et al. (2020) pretrained on the large LibriLight (Kahn et al., 2020) dataset as our audio front-end. For visual front-end, we found that it is not as straight-forward for it to leverage pre-trained models, as we have to substitute the first ResNet block in MoCo v2 (Chen et al., 2020b) by 3-D convolution layer and fine-tune it through LRW. In total, our approach doesn't require a curriculum learning stage, and the overall training time has been decreased.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_8",
            "content": "Experimental results show that our new frontends significantly outperform previous ones by a big margin in both audio-only and visual-only settings, and a new state-of-the-art has been achieved in the final AVSR setting. To our best knowledge, this is the first work that successfully applies uni-modal pre-trained models in the multimodal setting of AVSR. We also ensure this research is reproducible by publishing our codes at anonymized_url.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_9",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "470-ARR_v1_10",
            "content": "Audio-Visual Speech Recognition",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "470-ARR_v1_11",
            "content": "The earliest work on AVSR could be dated back to around two decades ago, when Dupont and Luettin (2000) showed hand-crafted visual feature improves HMM-based ASR systems. The first modern AVSR system is proposed in Afouras et al. (2018a) where deep neural networks are used. The field has been rapidly developing since then. Most of the works are devoted into the architectural improvements, for example, Zhang et al. (2019) proposed temporal focal block and spatio-temporal fusion, and Lee et al. (2020a) explored to use crossmodality attentions with Transformer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_12",
            "content": "The other line of research focuses on a more diversified learning scheme to improve AVSR performance. Li et al. (2019) uses a cross-modal student-teacher training scheme. Paraskevopoulos et al. (2020) proposes a multi-task learning scheme by making the model to predict on both character and subword level. Self-supervised learning has also been explored in Shukla et al. (2020), where the cross-modality setting is utilized by predicting frames of videos from audio inputs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_13",
            "content": "The end-to-end learning of AVSR systems are first seen in Tao and Busso (2020), albeit in a much simpler dataset than LRS2. More recent work (Ma et al., 2021) has made end-to-end learning on LRS2 possible by using a Conformer acoustic model and a hybrid CTC/attention decoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_14",
            "content": "Self-Supervised Learning",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "470-ARR_v1_15",
            "content": "Self-supervised learning has been chased in recent years since its ability to learn general representations of data through simple tasks that don't require labeling. Contrastive learning (Hadsell et al., 2006) has become the most impactful learning scheme in this field. In natural language processing, uni-or bi-directional language modelling (Brown et al., 2020;Devlin et al., 2018) have been used to significantly increase performances on various tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_16",
            "content": "In audio speech processing, contrastive predictive coding (Baevski et al., 2020) has been proven to be powerful in speech recognition. In the visual domain, Earlier works create self-supervised tasks through image processing based methods, such as distortion (Gidaris et al., 2018),colorization (Zhang et al., 2016) and context prediction (Doersch et al., 2015). More recently, contrastive learning emerged as a paradigm of self-supervised learning, which results in a group of more expressive general visual representations, such as MoCo (He et al., 2019;Chen et al., 2020b), SimCLR (Chen et al., 2020a), BYOL (Grill et al., 2020), etc.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_17",
            "content": "Architecture",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "470-ARR_v1_18",
            "content": "The overall architecture of our model is shown in Fig. 1. The audio-visual model is comprised of four components, the front-ends and back-ends for both modalities, the fusion module, and the decoders.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_19",
            "content": "Front-ends",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "470-ARR_v1_20",
            "content": "Visual Front-end: Visual front-end serves as a component to capture the lip motion and reflect the lip position differences in its output representations. A naive way to apply pre-trained models in the visual front-end is to directly feed the RGB channels of each frame as input. However, since frames within a same clip in AVSR are largely similar in their contents while most pre-trained models in vision target at learning general representations reflecting the content of the whole image, this approach will result in similar outputs for all the frames, collapsing the informative lip position differences between frames.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_21",
            "content": "To overcome the aforementioned problem while still being able to utilize the pre-trained model, we truncate the first convolutional layer in MoCo v2 (Chen et al., 2020b), which is pre-trained on Ima-geNet (Deng et al., 2009), and replace it by a layer of 3-D convolution. The outputs of 3-D convolution layer are intentionally made identical to the input of the first ResBlock in MoCo v2 (see Table 1), thus providing a compatible interface to transfer higher layers of MoCo v2 into this task. On the other hand, we also adopt the common practice to convert the RGB input image to gray-scale before feeding it into the model, as it prevents the model from learning chromatic aberration information.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_22",
            "content": "Audio Front-end: The audio front-end is rather straight-forward. We use wav2vec 2.0 (Schneider et al., 2019) pre-trained on Libri-Light (Kahn et al., 2020), like it is normally used for ASR tasks, both the 1-D convolution layers and the stacked Transformer encoder layers are transferred into our audio front-end. The audio front-end takes as input raw audio wave of 16kHz, and produces one vector representation every 20ms. The audio feature dimensions are shown in Table 2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_23",
            "content": "Back-ends",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "470-ARR_v1_24",
            "content": "Since the visual frames are in 25 FPS and the wav2vec 2.0 outputs are around 49 Hz 1 , one should 1 The odds are due to the larger receptive fields of wav2vec 2.0 1-D convolution layers, which we circumvent by properly prefixing and suffixing the audio sequence and truncate the trailing audio vector. Thus a perfect 1:2 ratio of visual frames note that there is 2x difference in the frequency of frame-wise visual and audio representations at the output of their front-ends. In the back-end, we use 1-D convolution layers on the time dimension combined with Transformer encoder layers to provide single modality temporal modeling, as well as adjusting the features to have the same frequency.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_25",
            "content": "Visual Back-end: The incoming MoCo v2 output to the visual back-end has a feature dimension of 2048, at a frequency of 25 vectors per second. In the visual backend, we keep this frequency while reducing the feature size to 512. See Table 1. For positional encodings of the Transformer, we use fixed positional encoding in the form of sinusoidal functions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_26",
            "content": "Modules Image sequence Audio Back-end: In the audio back-end, the incoming wav2vec 2.0 outputs have a feature size of and audio front-end outputs are ensured.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_27",
            "content": "(T f \u00d7 112 2 \u00d7 1) Front-end 3-D convolution (T f \u00d7 28 2 \u00d7 64) MoCo v2 (T f \u00d7 2048) Back-end 1-D convolution (T f \u00d7 512) Transformer Encoder (T f \u00d7 512)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_28",
            "content": "1024, at a frequency of 50 vectors per second. We downscale the frequency by setting the stride of 1-D convolution layer to 2. The Transformer encoder layers have the identical size to that of the visual back-end, while using a separate set of parameters.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_29",
            "content": "Table 2 shows a clearer picture of audio front-and back-end dimensions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_30",
            "content": "Stage Modules Audio waveform",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_31",
            "content": "(Ts \u00d7 1) Front-end wav2vec 2.0 (T f \u00d7 1024) Back-end 1-D convolution ( T f 2 \u00d7 512) Transformer Encoder ( T f 2 \u00d7 512)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_32",
            "content": "Table 2: The feature dimension of audio stream. The dimensions of features are denoted by {temporal size\u00d7 channels}. T s and T f denote the number of sampled audio input and audio frames, respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_33",
            "content": "Fusion Module",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "470-ARR_v1_34",
            "content": "Features from both the audio and visual modalities are fused together in this section, forming vector representation of 1024 dimensions at a relatively low rate of 25 Hz. We use LayerNorm (Ba et al., 2016) separately on each of the modalities before concatenating them on the feature dimension. The LayerNorm is required since it avoids one modality overtaking the whole representation with larger variance. Similar 1-D convolution layers and a subsequent Transformer encoder block of 6 layers take the fused representations as input, and encode them for the two decoders.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_35",
            "content": "Decoder",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "470-ARR_v1_36",
            "content": "Following the setting of Petridis et al. (2018), there are two decoders trained simultaneously based on the same encoder output in the fusion module.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_37",
            "content": "The first is a Transformer seq2seq decoder, a canonical Transformer decoder with 6 layers is used, and we perform teacher forcing at character level by using ground truth characters as input during training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_38",
            "content": "The second one is arguably a decoder since it yields character probabilities for each timestep and relies on the CTC loss in training. 4 extra 1-D convolution layers with ReLU activation are used on top of the last Transformer encoder layer output. We also include LayerNorm between each of the layers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_39",
            "content": "Loss Functions",
            "ntype": "title",
            "meta": {
                "section": "3.5"
            }
        },
        {
            "ix": "470-ARR_v1_40",
            "content": "In this work, we use a so called hybrid CTC/attention loss (Watanabe et al., 2017) for our training process. Let x = [x 1 , \u2022 \u2022 \u2022 , x T ] be the input frame sequence at the input of Transformer encoder in the fusion module and y = [y 1 , \u2022 \u2022 \u2022 , y L ] being the targets, where T and L denote the input and target lengths, respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_41",
            "content": "The CTC loss assumes conditional independence between each output prediction and has a form of",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_42",
            "content": "p CTC (y|x) \u2248 T t=1 p(y t |x) (1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_43",
            "content": "On the other hand, an auto-regressive decoder gets rid of this assumption by directly estimating the posterior on the basis of the chain rule, which has a form of",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_44",
            "content": "p CE (y|x) = L l=1 p(y l |y <l , x)(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_45",
            "content": "The overall objective function is computed as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_46",
            "content": "L = \u03bb log p CTC (y|x) + (1 \u2212 \u03bb) log p CE (y|x) (3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_47",
            "content": "where \u03bb controls the relative weight between CTC loss and seq2seq loss in the hybrid CTC/attention mechanisms. The weight is needed not only when integrating the two losses into one training loss, but also fusing the two predictions during decoding, which we will revisit in the following subsections.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_48",
            "content": "Training Pipeline",
            "ntype": "title",
            "meta": {
                "section": "3.6"
            }
        },
        {
            "ix": "470-ARR_v1_49",
            "content": "The final AVSR model is achieved through a pipeline of training stages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_50",
            "content": "For audio modality, the audio front-end is first pre-trained through self-supervised learning, which is done by wav2vec 2.0. Then the audio backend is trained through the audio-only (AO) setting, together with a dedicated decoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_51",
            "content": "For the visual modality, we first pre-train the 3-D convolution layer and visual back-end through sequence classification at word level video clips in LRW data. After that, the visual front-end are inherited by the visual-only (VO) model, where dedicated visual back-end and decoder are used.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_52",
            "content": "The final AVSR model can be trained after the audio-only and visual-only models have converged.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_53",
            "content": "Due to computational constraints, we pre-compute the audio and visual back-end outputs, and only learn the parameters in the fusion model and decoder part in this final stage. A detailed visualization of our training pipeline is depicted in Figure 2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_54",
            "content": "Decoding",
            "ntype": "title",
            "meta": {
                "section": "3.7"
            }
        },
        {
            "ix": "470-ARR_v1_55",
            "content": "Decoding is performed using joint CTC/attention one-pass decoding (Watanabe et al., 2017) with beam search. We apply shallow fusion to incorporate CTC and seq2seq predictions:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_56",
            "content": "\u0177 = arg max y\u2208 \u0176 {\u03b1 log p CTC (y|x) + (1 \u2212 \u03b1) log p CE (y|x)}(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_57",
            "content": "where \u0176 denotes predictions set of target symbols, while \u03b1 is the relative weight that tuned on validation set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_58",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "470-ARR_v1_59",
            "content": "In this section, we will first introduce the datasets and various settings we used in each component of our model. Then we will present results of audioonly, visual-only and audio-visual settings. We also present a breakdown of the relative contribution of every component through ablation study.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_60",
            "content": "Dataset",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "470-ARR_v1_61",
            "content": "We use the large-scale publicly AVSR dataset, the Lip Reading Sentences 2 (LRS2) as our main testbed. During training, we also use the Lip Reading in the Wild (LRW) (Chung and Zisserman, 2016) as a word-level video classification task to pre-train our visual encoder. LRS2 consists of 224 hours of aligned audio and videos, with a total of 144K clips from BBC videos, the clips are at a length of sentence level. The training data contains over 2M word instances and a vocabulary of over 40K. The dataset is very challenging as there are large variations in head pose, lighting conditions, genres and the number of speakers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_62",
            "content": "LRW is a word-level dataset, consisting of 157 hours of aligned audio and videos, totalling 489K video clips from BBC videos, each containing the utterance of a single word out of a vocabulary of 500. The videos have a fixed length of 29 frames, the target word occurring in the middle of the clip and surrounded by co-articulation. All of the videos are either frontal or near-frontal. In our experiment, we only use the visual modality from this dataset to train our visual front-end.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_63",
            "content": "Experimental Settings",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "470-ARR_v1_64",
            "content": "We use character level prediction with an output size of 40, consisting of the 26 characters in the alphabet, the 10 digits, the apostrophe, and special tokens for [space], [blank] and [EOS/SOS]. Since the transcriptions of the datasets do not contain other punctuations, we do not include them in the vocabulary.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_65",
            "content": "Our implementation is based on the Pytorch library (Paszke et al., 2019) and trained on four NVIDIA A100 GPUs with a total of 160GB memory. The network is trained using the Adam optimiser (Kingma and Ba, 2014) with \u03b2 1 = 0.9, \u03b2 2 = 0.999 and = 10 \u22128 and an initial learning rate of 10 \u22124 . We use label smoothing with a weight set to 0.01, learning rate warm up and reduce on plateau. The relative weight in CTC loss and seq2seq loss \u03bb is set to 0.2. When decoding, we set \u03b1 to 0.1. The samples in the pre-train set are cropped by randomly sampling a continuous range of 1/3 words of the whole utterances, in order to match the length of clips in the train set. Overlength samples are further truncated at 160 frames to reduce memory occupation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_66",
            "content": "Preprocessing: We detected and tracked 68 facial landmarks using dlib (King, 2009) for each video. To remove differences related to face rotation and scale, the faces are aligned to a neural reference frame using a similarity transformation following (Martinez et al., 2020). Interpolation and frame smoothing with a window width of 12 frames are used to deal with the frames that dlib fails to detect. Then a bounding box of 120 \u00d7 120 is used to crop the mouth ROIs. The cropped frame is further converted to gray-scale and normalized with respect to the overall mean and variance of the train set. Each raw audio waveform is normalized to zero mean and unit variance following (Baevski et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_67",
            "content": "Data Augmentation: Following (Ma et al., 2021), random cropping with a size of 112 \u00d7 112 and horizontal flipping with a probability of 0.5 are performed consistently across all frames of a given image sequence when training visual-only and audiovisual models. For each audio waveform, additive noise is performed in the time domain following (Afouras et al., 2018a) Evaluation: For all experiments, word error rate (WER) are reported which is defined as WER = (S + D + I)/N . The S, D and I in the formula denotes the number of substitutions, deletions and insertions respectively from the reference to the hypothesis, and N is the number of words in the inference. The babble noise added to the audio waveform during evaluation is generated using the same manner as training, while we set a different seed to avoid model fit to a specific generated noise. Decoding is performed using joint CTC/attention one-pass decoding (Watanabe et al., 2017) with beam width 5 (the values were determined on the held-out validation set of LRS2). We don't use an external language model in our experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_68",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "470-ARR_v1_69",
            "content": "We present results for all experiments in Table 3, reporting WERs on audio-only, visual-only and audio-visual models. Note that many of the models listed here are also using extra training data in different stages of training pipeline, such as MV-LRS , LRS3 (Afouras et al., 2018b), LibriSpeech (Panayotov et al., 2015) and LRW.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_70",
            "content": "Audio-visual Setting: In the main audio-visual (AV) setting, the pre-train and train sets in LRS2 are used as train set in the final training stage. Our proposed audio-visual model achieves a WER of 2.6% without the help of an external language model, which improves by 1.1% over the current state-ofthe-art (Ma et al., 2021). This is rather a big improvement, with a relative improvement of around 30%.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_71",
            "content": "The training data used for training audio-only model consists of 224 hours labelled data from LRS2, as well as the 60K hours unlabelled data from LibriLight (Kahn et al., 2020) that are indirectly used through inheriting wav2vec 2.0 parameters. Our model also achieves a WER of 2.7%, which reduces the WER of the current stateof-the-art (Ma et al., 2021) by 1.2%, indicating a relative improvement of 30%.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_72",
            "content": "The visual-only model uses labelled LRS2 data in its pre-train and train sets, the LRW for supervised pre-training, and indirectly using the 1.28M unlabelled images from ImageNet through MoCo v2. The visual-only model achieves a WER of 43.8%, lagging behind the current stateof-the-art (E2E Conformer) with 5.3%. Compared to E2E Conformer, the main difference is that a big Transformer language model is used during decoding, which itself brings a 4.5% difference compared with a normal RNN language model in their ablation study (Ma et al., 2021). The gap between our visual-only model and the E2E Conformer model with a RNN language model is 0.8%, which resides in a quite reasonable range. Additionally, we use a 6-layers Transformer encoder for temporal modelling instead of a 12-layers conformer encoder, which resulted in a smaller model size.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_73",
            "content": "If we consider a fairer comparison by only looking at benchmarks without using an external language model, the best-reported benchmark is Ren et al. (2021), which achieved a WER of 49.2%, lagging behind our model by 6.0%.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_74",
            "content": "Ablation Studies",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "470-ARR_v1_75",
            "content": "In this section, we investigate the impact of every individual building block by testing them in LRW, visual-only and audio-only settings. 4. We first train a model by replacing the ResNet-18 front-end in (Stafylakis and Tzimiropoulos, 2017) with a ResNet-50 frontend, matching the size of MoCo v2 but with fresh weights. This results in an absolute improvement of 2.1%. Then we initialize the ResNet-50 frontend with MoCo v2 weights and a further absolute improvement of 2.3% is observed, which implies that self-supervised learning is actually functioning in better represent the lip movement. Additionally, When Using 6 layers of Transformer encoder instead of TCN as back-end, we can observe another absolute improvement of 5.0%. We also noticed that using MoCo v2 front-end could significantly reduce the training time. Performance Breakdown in Audio-only Setting: Results of audio-only model on LRS2 are shown in Table 5. Starting from (Afouras et al., 2018a), we first train a model by replacing the STFT audio feature with a wav2vec 2.0 front-end pre-trained on LibriSpeech, resulting in an absolute improvement of 11.1%. Then we use another pre-trained model learned on an even larger unlabelled single modality dataset Libri-Light, and a further absolute improvement of 0.6% is observed. We further train the model with hybrid CTC/attention decoder during the training stage, which results in another absolute improvement of 0.9%. Performance Breakdown in Visual-only Setting: Results of the visual-only model on LRS2 are shown in Table 6. Starting from (Afouras et al., 2018a), we first introduce end-to-end training by Method WER Baseline (Afouras et al., 2018a) 15.3% + wav2vec 2.0 (LibriSpeech) encoder 4.2% + wav2vec 2.0 (LibriLight) encoder 3.6% + Hybrid CTC/attention 2.7%",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_76",
            "content": "To evaluate the model's tolerance to audio noise, we tested the performance of our model under babble noise with different SNR levels. Our audio-only and audiovisual models reach WERs of 32.5% and 24.5% when the SNR level is 0dB, respectively, which reduce the reported result in (Afouras et al., 2018a) by 25.5% and 9% 2 . When the SNR level rises to 5dB, our audio-only and audio-visual model obtain WERs of 6.8% and 6.3%. Besides achieving significant improvement over the baseline model under babble noise environment, we further investigate the model performance under human noise environment. The human noise is extremely challenging cause the noise itself contains some words, while the model cannot easily distinguish which audio signal is the one to be recognized. We synthesize the human noise by randomly crop many 1 second signals from different audio samples in the LRS2 dataset. As shown in Fig. 3, we conduct experiments varying different levels of human noise, the models are trained using babble noise augmented audio. The WER increases greatly after the SNR level drops down under 0db.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_77",
            "content": "It is because the model may not be able to distinguish the two overlapped spoken words at a low SNR level.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_78",
            "content": "And the overall performance under each SNR level is worse than babble noise, indicating that noise with specific information is harder than disorganized babble noise. under Low Resource: A significant benefit of using self-supervised pre-trained models is that only a small amount of labelled data is needed for training a model. To further investigate the models' performance in low resource environment, we use the 28 hours train set of LRS2 to train an audio-only and a visual-only model. The results are shown in Table 8. The audio-only model trained with 28 hours data achieves a WER of 3.4%, which is a little bit worse than the one trained with 224 hours data. The result indicates that for the audio-only model, the self-supervised model pretrained on a large-scale single modality dataset can significantly reduce the demands of data. While the visual-only model trained with 28 hours data has a great gap with the one trained with 224 hours data, the reason can be that the visual-only model is harder to train and demands a larger amount of data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_79",
            "content": "Discussion and Conclusion",
            "ntype": "title",
            "meta": {
                "section": "4.5"
            }
        },
        {
            "ix": "470-ARR_v1_80",
            "content": "In this work, we propose to utilize self-supervised learning for AVSR by simply incorporating the pretrained model trained in massive unlabelled single modality data. Although the visual pre-trained models are not straight-forward to be transplanted into visual front-end, we still manage to integrate pre-trained models in both modalities for the AVSR task. Experimental results are impressive, resulting in a 30% relative improvement. It's interesting to observe that self-supervised model in audio modality has an even larger improvement than that of the visual counterpart. We believe the reasons can be listed as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_81",
            "content": "\u2022 The training data scale of audio modality is significantly larger than that of visual modality, with the Libri-Light dataset used for pretraining wav2vec 2.0 consists of 60K hours audio signals, the ImageNet dataset, on the contrary, has only 1.28M images, roughly equivalent to 14 hours silent video under 25 FPS. \u2022 The MoCo v2 model is pre-trained on images to better represent frame-level contents, while there are no pre-training steps to model the temporal correlation between frames. In contrast, the wav2vec 2.0 model is pre-trained on consistent audios, thus having a better temporal modelling ability.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_82",
            "content": "As there has not emerged a dominating crossmodality self-supervised learning approach in the field of AVSR, in future work, we are going to explore two more directions in the self-supervised learning scenario based on this work. The first is utilizing the temporal correlations within the visual domain, while the other is the cross-modal correlations between the audio and visual modality. We hope this work could pave the way towards multimodality self-supervised learning, especially for various aspects in audio-visual speech recognition.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_83",
            "content": "Algorithm 1 Hybrid CTC/attention one-pass decoding adapted from (Watanabe et al., 2017). Notation: X is the speech input; L max is the maximum length of the hypotheses to be searched, we set it to T ; C is the decoded symbol sequence;",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_84",
            "content": "[b] denotes [blank]. Input: X, Lmax Output: C 1: \u21260 = {[SOS]} 2: \u03a9 = \u2205 3: \u03b3 (b) 0 ([SOS]) = 1 4: for t = 1, \u2022 \u2022 \u2022 , T do 5: \u03b3 (n) t ([SOS]) = 0 6: \u03b3 (b) t ([SOS]) = t \u03c4 =1 \u03b3 (b) \u03c4 \u22121 ([SOS])\u2022p(z\u03c4 = [b]|X) 7: end for 8: for l = 1 \u2022 \u2022 \u2022 Lmax do 9: \u2126 l = \u2205 10: while \u2126 l\u22121 = \u2205 do 11: g = HEAD(\u2126 l\u22121 ) 12: DEQUEUE(\u2126 l\u22121 ) 13:",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_85",
            "content": "for each c U do 14: Algorithm 1 describes the hybrid CTC/attention decoding procedure. The CTC prefix probability is defined as the cumulative probability of all label sequences that have h as their prefix:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_86",
            "content": "h = g \u2022 c 15: if c = [EOS] then 16: log pctc(h|X) = log{\u03b3 (n) T (g) + \u03b3 (b) T (g)} 17: else 18: if g = [SOS] then 19: \u03b3 (n) 1 (h) = p(z1 = c|X) 20: else 21: \u03b3 (n) 1 (h) = 0 22: end if 23: \u03b3 (b) 1 (h) = 0 24: \u03a8 = \u03b3 (n) 1 (h) 25: for t = 2 \u2022 \u2022 \u2022 T do 26: if last(g) = c then 27: \u03a6 = \u03b3 (b) t\u22121 (g) 28: else 29: \u03a6 = \u03b3 (b) t\u22121 (g) + \u03b3 (n) t\u22121 (g) 30: end if 31: \u03b3 (n) t (h) = (\u03b3 (n) t\u22121 (h) + \u03a6)p(zt = c|X) 32: \u03b3 (b) t (h) = (\u03b3(",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_87",
            "content": "p ctc (h|X) = v\u2208(U ) + p ctc (h \u2022 v|X) (5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_88",
            "content": "where v denotes all possible symbol sequences except the empty. The CTC probability can be computed by keeping the forward hypothesis prob\u03b3 The decoding algorithm is also a beam search with width W and hyperparameter \u03b1 control the relative weight given to CTC and attention decoding. U is a set of symbols excluding [blank], and a same token is used to represent [SOS] and [EOS] in our implementation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_89",
            "content": "The input images are sampled at 25 FPS and resized to 224 \u00d7 224 pixels. We crop a 120 \u00d7 120 mouth ROI from each frame. Fig. 4 shows the process to generate.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "470-ARR_v1_90",
            "content": "T Afouras, J Chung, A Senior, O Vinyals, A Zisserman, Deep audio-visual speech recognition, 2018, IEEE Transactions on Pattern Analysis & Machine Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "T Afouras",
                    "J Chung",
                    "A Senior",
                    "O Vinyals",
                    "A Zisserman"
                ],
                "title": "Deep audio-visual speech recognition",
                "pub_date": "2018",
                "pub_title": "IEEE Transactions on Pattern Analysis & Machine Intelligence",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_91",
            "content": "Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman, 2018, Lrs3-ted: a large-scale dataset for visual speech recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Triantafyllos Afouras"
                ],
                "title": "Joon Son Chung, and Andrew Zisserman",
                "pub_date": "2018",
                "pub_title": "Lrs3-ted: a large-scale dataset for visual speech recognition",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_92",
            "content": "Triantafyllos Afouras, Joon Chung, Andrew Zisserman, Asr is all you need: Cross-modal distillation for lip reading, 2020, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Triantafyllos Afouras",
                    "Joon Chung",
                    "Andrew Zisserman"
                ],
                "title": "Asr is all you need: Cross-modal distillation for lip reading",
                "pub_date": "2020",
                "pub_title": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "470-ARR_v1_93",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_94",
            "content": "UNKNOWN, None, , Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_95",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_96",
            "content": "Ting Chen, Mohammad Kornblith, Geoffrey Norouzi, A simple framework for contrastive learning of visual representations, 2020, International conference on machine learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Ting Chen",
                    "Mohammad Kornblith",
                    "Geoffrey Norouzi"
                ],
                "title": "A simple framework for contrastive learning of visual representations",
                "pub_date": "2020",
                "pub_title": "International conference on machine learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "470-ARR_v1_97",
            "content": "UNKNOWN, None, 2020, Improved baselines with momentum contrastive learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Improved baselines with momentum contrastive learning",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_98",
            "content": "Chung Joon Son, Arsha Nagrani, and Andrew Zisserman, 2018, Voxceleb2: Deep speaker recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Chung Joon Son"
                ],
                "title": "Arsha Nagrani, and Andrew Zisserman",
                "pub_date": "2018",
                "pub_title": "Voxceleb2: Deep speaker recognition",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_99",
            "content": "Joon Son Chung, Andrew Senior, Oriol Vinyals, Andrew Zisserman, Lip reading sentences in the wild, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Joon Son Chung",
                    "Andrew Senior",
                    "Oriol Vinyals",
                    "Andrew Zisserman"
                ],
                "title": "Lip reading sentences in the wild",
                "pub_date": "2017",
                "pub_title": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "470-ARR_v1_100",
            "content": "Son Joon, Andrew Chung,  Zisserman, Lip reading in the wild, 2016, Asian conference on computer vision, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Son Joon",
                    "Andrew Chung",
                    " Zisserman"
                ],
                "title": "Lip reading in the wild",
                "pub_date": "2016",
                "pub_title": "Asian conference on computer vision",
                "pub": "Springer"
            }
        },
        {
            "ix": "470-ARR_v1_101",
            "content": "UNKNOWN, None, 2017, Lip reading in profile, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Lip reading in profile",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_102",
            "content": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, Imagenet: A large-scale hierarchical image database, 2009, 2009 IEEE conference on computer vision and pattern recognition, Ieee.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Jia Deng",
                    "Wei Dong",
                    "Richard Socher",
                    "Li-Jia Li",
                    "Kai Li",
                    "Li Fei-Fei"
                ],
                "title": "Imagenet: A large-scale hierarchical image database",
                "pub_date": "2009",
                "pub_title": "2009 IEEE conference on computer vision and pattern recognition",
                "pub": "Ieee"
            }
        },
        {
            "ix": "470-ARR_v1_103",
            "content": "UNKNOWN, None, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_104",
            "content": "Carl Doersch, Abhinav Gupta, Alexei Efros, Unsupervised visual representation learning by context prediction, 2015, Proceedings of the IEEE international conference on computer vision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Carl Doersch",
                    "Abhinav Gupta",
                    "Alexei Efros"
                ],
                "title": "Unsupervised visual representation learning by context prediction",
                "pub_date": "2015",
                "pub_title": "Proceedings of the IEEE international conference on computer vision",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_105",
            "content": "S Dupont, J Luettin, Audio-visual speech modeling for continuous speech recognition, 2000, IEEE Transactions on Multimedia, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "S Dupont",
                    "J Luettin"
                ],
                "title": "Audio-visual speech modeling for continuous speech recognition",
                "pub_date": "2000",
                "pub_title": "IEEE Transactions on Multimedia",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_106",
            "content": "UNKNOWN, None, 2018, Unsupervised representation learning by predicting image rotations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Unsupervised representation learning by predicting image rotations",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_107",
            "content": "UNKNOWN, None, 2020, Bootstrap your own latent: A new approach to self-supervised learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Bootstrap your own latent: A new approach to self-supervised learning",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_108",
            "content": "Raia Hadsell, Sumit Chopra, Yann Lecun, Dimensionality reduction by learning an invariant mapping, 2006, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Raia Hadsell",
                    "Sumit Chopra",
                    "Yann Lecun"
                ],
                "title": "Dimensionality reduction by learning an invariant mapping",
                "pub_date": "2006",
                "pub_title": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "470-ARR_v1_109",
            "content": "UNKNOWN, None, 2019, Momentum contrast for unsupervised visual representation learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Momentum contrast for unsupervised visual representation learning",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_110",
            "content": "Jacob Kahn, Morgane Rivi\u00e8re, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazar\u00e9, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, Libri-light: A benchmark for asr with limited or no supervision, 2020, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Jacob Kahn",
                    "Morgane Rivi\u00e8re",
                    "Weiyi Zheng",
                    "Evgeny Kharitonov",
                    "Qiantong Xu",
                    "Pierre-Emmanuel Mazar\u00e9",
                    "Julien Karadayi",
                    "Vitaliy Liptchinsky",
                    "Ronan Collobert",
                    "Christian Fuegen"
                ],
                "title": "Libri-light: A benchmark for asr with limited or no supervision",
                "pub_date": "2020",
                "pub_title": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "470-ARR_v1_111",
            "content": "E Davis,  King, Dlib-ml: A machine learning toolkit, 2009, The Journal of Machine Learning Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "E Davis",
                    " King"
                ],
                "title": "Dlib-ml: A machine learning toolkit",
                "pub_date": "2009",
                "pub_title": "The Journal of Machine Learning Research",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_112",
            "content": "UNKNOWN, None, 2014, Adam: A method for stochastic optimization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": "2014",
                "pub_title": "Adam: A method for stochastic optimization",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_113",
            "content": "Yong-Hyeok Lee, Dong-Won Jang, Jae-Bin Kim, Rae-Hong Park, Hyung-Min Park, Audiovisual speech recognition based on dual crossmodality attentions with the transformer model, 2020, Applied Sciences, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Yong-Hyeok Lee",
                    "Dong-Won Jang",
                    "Jae-Bin Kim",
                    "Rae-Hong Park",
                    "Hyung-Min Park"
                ],
                "title": "Audiovisual speech recognition based on dual crossmodality attentions with the transformer model",
                "pub_date": "2020",
                "pub_title": "Applied Sciences",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_114",
            "content": "Yong-Hyeok Lee, Dong-Won Jang, Jae-Bin Kim, Rae-Hong Park, Hyung-Min Park, Audiovisual speech recognition based on dual crossmodality attentions with the transformer model, 2020, Applied Sciences, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Yong-Hyeok Lee",
                    "Dong-Won Jang",
                    "Jae-Bin Kim",
                    "Rae-Hong Park",
                    "Hyung-Min Park"
                ],
                "title": "Audiovisual speech recognition based on dual crossmodality attentions with the transformer model",
                "pub_date": "2020",
                "pub_title": "Applied Sciences",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_115",
            "content": "Wei Li, Sicheng Wang, Ming Lei, Marco Sabato, Chin-Hui Siniscalchi,  Lee, Improving audio-visual speech recognition performance with cross-modal student-teacher training, 2019, ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Wei Li",
                    "Sicheng Wang",
                    "Ming Lei",
                    "Marco Sabato",
                    "Chin-Hui Siniscalchi",
                    " Lee"
                ],
                "title": "Improving audio-visual speech recognition performance with cross-modal student-teacher training",
                "pub_date": "2019",
                "pub_title": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "470-ARR_v1_116",
            "content": "Pingchuan Ma, Stavros Petridis, Maja Pantic, End-to-end audio-visual speech recognition with conformers, 2021, ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Processing (ICASSP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Pingchuan Ma",
                    "Stavros Petridis",
                    "Maja Pantic"
                ],
                "title": "End-to-end audio-visual speech recognition with conformers",
                "pub_date": "2021",
                "pub_title": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Processing (ICASSP)",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_117",
            "content": ", None, , IEEE, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [],
                "title": null,
                "pub_date": null,
                "pub_title": "IEEE",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_118",
            "content": "Brais Martinez, Pingchuan Ma, Stavros Petridis, Maja Pantic, Lipreading using temporal convolutional networks, 2020, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Brais Martinez",
                    "Pingchuan Ma",
                    "Stavros Petridis",
                    "Maja Pantic"
                ],
                "title": "Lipreading using temporal convolutional networks",
                "pub_date": "2020",
                "pub_title": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "470-ARR_v1_119",
            "content": "Vassil Panayotov, Guoguo Chen, Daniel Povey, Sanjeev Khudanpur, Librispeech: an asr corpus based on public domain audio books, 2015, 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Vassil Panayotov",
                    "Guoguo Chen",
                    "Daniel Povey",
                    "Sanjeev Khudanpur"
                ],
                "title": "Librispeech: an asr corpus based on public domain audio books",
                "pub_date": "2015",
                "pub_title": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "470-ARR_v1_120",
            "content": "UNKNOWN, None, 2020, Multiresolution and multimodal speech recognition with transformers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Multiresolution and multimodal speech recognition with transformers",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_121",
            "content": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Pytorch: An imperative style, high-performance deep learning library, 2019, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Adam Paszke",
                    "Sam Gross",
                    "Francisco Massa",
                    "Adam Lerer",
                    "James Bradbury",
                    "Gregory Chanan",
                    "Trevor Killeen",
                    "Zeming Lin",
                    "Natalia Gimelshein",
                    "Luca Antiga"
                ],
                "title": "Pytorch: An imperative style, high-performance deep learning library",
                "pub_date": "2019",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_122",
            "content": "Stavros Petridis, Themos Stafylakis, Pingchuan Ma, Audio-visual speech recognition with a hybrid ctc/attention architecture, 2018, 2018 IEEE Spoken Language Technology Workshop (SLT), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Stavros Petridis",
                    "Themos Stafylakis",
                    "Pingchuan Ma"
                ],
                "title": "Audio-visual speech recognition with a hybrid ctc/attention architecture",
                "pub_date": "2018",
                "pub_title": "2018 IEEE Spoken Language Technology Workshop (SLT)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "470-ARR_v1_123",
            "content": "Sucheng Ren, Yong Du, Jianming Lv, Guoqiang Han, Shengfeng He, Learning from the master: Distilling cross-modal advanced knowledge for lip reading, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Sucheng Ren",
                    "Yong Du",
                    "Jianming Lv",
                    "Guoqiang Han",
                    "Shengfeng He"
                ],
                "title": "Learning from the master: Distilling cross-modal advanced knowledge for lip reading",
                "pub_date": "2021",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_124",
            "content": "UNKNOWN, None, 2019, wav2vec: Unsupervised pre-training for speech recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "wav2vec: Unsupervised pre-training for speech recognition",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_125",
            "content": "Abhinav Shukla, Konstantinos Vougioukas, Pingchuan Ma, Stavros Petridis, and Maja Pantic. 2020. Visually guided self supervised learning of speech representations, , ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Abhinav Shukla",
                    "Konstantinos Vougioukas",
                    "Pingchuan Ma"
                ],
                "title": "Stavros Petridis, and Maja Pantic. 2020. Visually guided self supervised learning of speech representations",
                "pub_date": null,
                "pub_title": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "470-ARR_v1_126",
            "content": "UNKNOWN, None, , Themos Stafylakis and Georgios Tzimiropoulos. 2017. Combining residual networks with lstms for lipreading, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Themos Stafylakis and Georgios Tzimiropoulos. 2017. Combining residual networks with lstms for lipreading",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_127",
            "content": "Fei Tao, Carlos Busso, End-to-end audiovisual speech recognition system with multitask learning, 2020, IEEE Transactions on Multimedia, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Fei Tao",
                    "Carlos Busso"
                ],
                "title": "End-to-end audiovisual speech recognition system with multitask learning",
                "pub_date": "2020",
                "pub_title": "IEEE Transactions on Multimedia",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_128",
            "content": "Shinji Watanabe, Takaaki Hori, Suyoun Kim, R John, Tomoki Hershey,  Hayashi, Hybrid ctc/attention architecture for end-to-end speech recognition, 2017, IEEE Journal of Selected Topics in Signal Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Shinji Watanabe",
                    "Takaaki Hori",
                    "Suyoun Kim",
                    "R John",
                    "Tomoki Hershey",
                    " Hayashi"
                ],
                "title": "Hybrid ctc/attention architecture for end-to-end speech recognition",
                "pub_date": "2017",
                "pub_title": "IEEE Journal of Selected Topics in Signal Processing",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_129",
            "content": "Jianwei Yu,  Shi-Xiong, Jian Zhang, Shahram Wu, Bo Ghorbani, Shiyin Wu, Shansong Kang, Xunying Liu, Helen Liu, Dong Meng,  Yu, Audio-visual recognition of overlapped speech for the lrs2 dataset, 2020, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Jianwei Yu",
                    " Shi-Xiong",
                    "Jian Zhang",
                    "Shahram Wu",
                    "Bo Ghorbani",
                    "Shiyin Wu",
                    "Shansong Kang",
                    "Xunying Liu",
                    "Helen Liu",
                    "Dong Meng",
                    " Yu"
                ],
                "title": "Audio-visual recognition of overlapped speech for the lrs2 dataset",
                "pub_date": "2020",
                "pub_title": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "470-ARR_v1_130",
            "content": "Richard Zhang, Phillip Isola, Alexei Efros, Colorful image colorization, 2016, European conference on computer vision, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Richard Zhang",
                    "Phillip Isola",
                    "Alexei Efros"
                ],
                "title": "Colorful image colorization",
                "pub_date": "2016",
                "pub_title": "European conference on computer vision",
                "pub": "Springer"
            }
        },
        {
            "ix": "470-ARR_v1_131",
            "content": "Xingxuan Zhang, Feng Cheng, Shilin Wang, Spatio-temporal fusion based convolutional sequence learning for lip reading, 2019, Proceedings of the IEEE/CVF International Conference on Computer Vision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Xingxuan Zhang",
                    "Feng Cheng",
                    "Shilin Wang"
                ],
                "title": "Spatio-temporal fusion based convolutional sequence learning for lip reading",
                "pub_date": "2019",
                "pub_title": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
                "pub": null
            }
        },
        {
            "ix": "470-ARR_v1_132",
            "content": "Ya Zhao, Rui Xu, Xinchao Wang, Peng Hou, Hearing lips: Improving lip reading by distilling speech recognizers, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Ya Zhao",
                    "Rui Xu",
                    "Xinchao Wang",
                    "Peng Hou"
                ],
                "title": "Hearing lips: Improving lip reading by distilling speech recognizers",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "470-ARR_v1_0@0",
            "content": "Leveraging Uni-Modal Self-Supervised Learning for Multimodal Audio-visual Speech Recognition",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_0",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_2@0",
            "content": "Training Transformer-based models demands a large amount of data, while obtaining parallel aligned and labelled data in multimodality is rather cost-demanding, especially for audiovisual speech recognition (AVSR).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_2",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_2@1",
            "content": "Thus it makes a lot of sense to make use of unlabelled uni-modal data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_2",
            "start": 214,
            "end": 283,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_2@2",
            "content": "On the other side, although the effectiveness of large-scale self-supervised learning is well established in both audio and visual modalities, how to integrate those pretrained models into a multimodal scenario remains underexplored.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_2",
            "start": 285,
            "end": 517,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_2@3",
            "content": "In this work, we successfully leverage uni-modal self-supervised learning to promote the multimodal AVSR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_2",
            "start": 519,
            "end": 623,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_2@4",
            "content": "In particular, we first train audio and visual encoders on a large-scale uni-modal dataset, then we integrate components of both encoders into a larger multimodal framework which learns to recognize paired audio-visual data into characters through a combination of CTC and seq2seq decoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_2",
            "start": 625,
            "end": 914,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_2@5",
            "content": "We show that both components inherited from uni-modal selfsupervised learning cooperate well, resulting in that the multimodal framework yields competitive results through fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_2",
            "start": 916,
            "end": 1099,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_2@6",
            "content": "Our model is experimentally validated on both word-level and sentence-level AVSR tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_2",
            "start": 1101,
            "end": 1187,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_2@7",
            "content": "Especially, even without an external language model, our proposed model raises the state-of-the-art performances on the widely accepted Lip Reading Sentences 2 (LRS2) dataset by a large margin, with a relative improvement of 30%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_2",
            "start": 1189,
            "end": 1417,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_4@0",
            "content": "Audio-Visual Speech Recognition (AVSR) is a speech recognition task that leverages both an audio input of human voice and an aligned visual input of lip motions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_4",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_4@1",
            "content": "It has been one of the successful application fields that involve multiple modalities in recent years.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_4",
            "start": 162,
            "end": 263,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_4@2",
            "content": "Due to the limited amount of labeled, multi-modal parallel data and the difficulty of recognition from the visual inputs (i.e., lip reading), it is a challenging task to tackle.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_4",
            "start": 265,
            "end": 441,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_5@0",
            "content": "Existing AVSR models tend to use extra data to increase the performance of the system, in a form of inserting an extra supervised learning stage in the training process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_5",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_5@1",
            "content": "For example, many existing methods rely on an extra sequence level classification to bootstrap its learning on visual features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_5",
            "start": 170,
            "end": 296,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_5@2",
            "content": "Petridis et al. (2018); Zhang et al. (2019) train their visual front-end on LRW (Chung and Zisserman, 2016) before learning on the AVSR task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_5",
            "start": 298,
            "end": 438,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_5@3",
            "content": "Afouras et al. (2018a,b) chunks the MV-LRS data into pieces of words and pre-train the model through classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_5",
            "start": 440,
            "end": 555,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_5@4",
            "content": "VoxCeleb (Chung et al., 2018) are also used in Afouras et al. (2020) for the same purpose.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_5",
            "start": 557,
            "end": 646,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_5@5",
            "content": "Learning an effective visual front-end could still be notoriously hard, even with these extra supervised learning tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_5",
            "start": 648,
            "end": 767,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_5@6",
            "content": "Sometimes curriculum learning is required to adapt the learned visual front-end into AVSR task (Afouras et al., 2018a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_5",
            "start": 769,
            "end": 887,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_5@7",
            "content": "End-to-end learning of large-scale AVSR data hasn't been successful until recently (Ma et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_5",
            "start": 889,
            "end": 989,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_6@0",
            "content": "Although self-supervised learning could enable leveraging unlabelled or even non-parallel data, it hasn't been adequately explored on this task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_6",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_6@1",
            "content": "Shukla et al. (2020) is among the few attempts in this facet, in which it predicts lip motions from audio inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_6",
            "start": 145,
            "end": 257,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_6@2",
            "content": "Their proposed learning schemes yield strong emotion recognition results but are relatively weak in speech recognition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_6",
            "start": 259,
            "end": 377,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_6@3",
            "content": "Moreover, since in AVSR it is the lip shape and motions between frames rather than the objects in a single image that matters for recognizing speech contents, if pre-trained visual models tailored for tasks targeting at single frame images could work for AVSR remains unknown.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_6",
            "start": 379,
            "end": 654,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_6@4",
            "content": "In another scenario, selfsupervised learning in uni-modality has been well established as a paradigm to learn general representations from unlabelled examples, such as in natural language processing (Brown et al., 2020;Devlin et al., 2018), speech recognition (Baevski et al., 2020), and computer vision (He et al., 2019;Chen et al., 2020a;Grill et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_6",
            "start": 656,
            "end": 1015,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_7@0",
            "content": "In this work, we rely on a simple but effective approach, which is to utilize unlabelled uni-modal data by using pre-trained models that are trained in single-modality through self-supervised learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_7",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_7@1",
            "content": "Specifically, we use Baevski et al. (2020) pretrained on the large LibriLight (Kahn et al., 2020) dataset as our audio front-end.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_7",
            "start": 202,
            "end": 330,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_7@2",
            "content": "For visual front-end, we found that it is not as straight-forward for it to leverage pre-trained models, as we have to substitute the first ResNet block in MoCo v2 (Chen et al., 2020b) by 3-D convolution layer and fine-tune it through LRW.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_7",
            "start": 332,
            "end": 570,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_7@3",
            "content": "In total, our approach doesn't require a curriculum learning stage, and the overall training time has been decreased.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_7",
            "start": 572,
            "end": 688,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_8@0",
            "content": "Experimental results show that our new frontends significantly outperform previous ones by a big margin in both audio-only and visual-only settings, and a new state-of-the-art has been achieved in the final AVSR setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_8",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_8@1",
            "content": "To our best knowledge, this is the first work that successfully applies uni-modal pre-trained models in the multimodal setting of AVSR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_8",
            "start": 221,
            "end": 355,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_8@2",
            "content": "We also ensure this research is reproducible by publishing our codes at anonymized_url.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_8",
            "start": 357,
            "end": 443,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_9@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_9",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_10@0",
            "content": "Audio-Visual Speech Recognition",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_10",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_11@0",
            "content": "The earliest work on AVSR could be dated back to around two decades ago, when Dupont and Luettin (2000) showed hand-crafted visual feature improves HMM-based ASR systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_11",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_11@1",
            "content": "The first modern AVSR system is proposed in Afouras et al. (2018a) where deep neural networks are used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_11",
            "start": 171,
            "end": 273,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_11@2",
            "content": "The field has been rapidly developing since then.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_11",
            "start": 275,
            "end": 323,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_11@3",
            "content": "Most of the works are devoted into the architectural improvements, for example, Zhang et al. (2019) proposed temporal focal block and spatio-temporal fusion, and Lee et al. (2020a) explored to use crossmodality attentions with Transformer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_11",
            "start": 325,
            "end": 563,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_12@0",
            "content": "The other line of research focuses on a more diversified learning scheme to improve AVSR performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_12",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_12@1",
            "content": "Li et al. (2019) uses a cross-modal student-teacher training scheme.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_12",
            "start": 102,
            "end": 169,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_12@2",
            "content": "Paraskevopoulos et al. (2020) proposes a multi-task learning scheme by making the model to predict on both character and subword level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_12",
            "start": 171,
            "end": 305,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_12@3",
            "content": "Self-supervised learning has also been explored in Shukla et al. (2020), where the cross-modality setting is utilized by predicting frames of videos from audio inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_12",
            "start": 307,
            "end": 473,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_13@0",
            "content": "The end-to-end learning of AVSR systems are first seen in Tao and Busso (2020), albeit in a much simpler dataset than LRS2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_13",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_13@1",
            "content": "More recent work (Ma et al., 2021) has made end-to-end learning on LRS2 possible by using a Conformer acoustic model and a hybrid CTC/attention decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_13",
            "start": 124,
            "end": 275,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_14@0",
            "content": "Self-Supervised Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_14",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_15@0",
            "content": "Self-supervised learning has been chased in recent years since its ability to learn general representations of data through simple tasks that don't require labeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_15",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_15@1",
            "content": "Contrastive learning (Hadsell et al., 2006) has become the most impactful learning scheme in this field.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_15",
            "start": 166,
            "end": 269,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_15@2",
            "content": "In natural language processing, uni-or bi-directional language modelling (Brown et al., 2020;Devlin et al., 2018) have been used to significantly increase performances on various tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_15",
            "start": 271,
            "end": 455,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_16@0",
            "content": "In audio speech processing, contrastive predictive coding (Baevski et al., 2020) has been proven to be powerful in speech recognition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_16",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_16@1",
            "content": "In the visual domain, Earlier works create self-supervised tasks through image processing based methods, such as distortion (Gidaris et al., 2018),colorization (Zhang et al., 2016) and context prediction (Doersch et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_16",
            "start": 135,
            "end": 361,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_16@2",
            "content": "More recently, contrastive learning emerged as a paradigm of self-supervised learning, which results in a group of more expressive general visual representations, such as MoCo (He et al., 2019;Chen et al., 2020b), SimCLR (Chen et al., 2020a), BYOL (Grill et al., 2020), etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_16",
            "start": 363,
            "end": 636,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_17@0",
            "content": "Architecture",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_17",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_18@0",
            "content": "The overall architecture of our model is shown in Fig. 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_18",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_18@1",
            "content": "The audio-visual model is comprised of four components, the front-ends and back-ends for both modalities, the fusion module, and the decoders.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_18",
            "start": 58,
            "end": 199,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_19@0",
            "content": "Front-ends",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_19",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_20@0",
            "content": "Visual Front-end: Visual front-end serves as a component to capture the lip motion and reflect the lip position differences in its output representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_20",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_20@1",
            "content": "A naive way to apply pre-trained models in the visual front-end is to directly feed the RGB channels of each frame as input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_20",
            "start": 155,
            "end": 278,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_20@2",
            "content": "However, since frames within a same clip in AVSR are largely similar in their contents while most pre-trained models in vision target at learning general representations reflecting the content of the whole image, this approach will result in similar outputs for all the frames, collapsing the informative lip position differences between frames.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_20",
            "start": 280,
            "end": 624,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_21@0",
            "content": "To overcome the aforementioned problem while still being able to utilize the pre-trained model, we truncate the first convolutional layer in MoCo v2 (Chen et al., 2020b), which is pre-trained on Ima-geNet (Deng et al., 2009), and replace it by a layer of 3-D convolution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_21",
            "start": 0,
            "end": 270,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_21@1",
            "content": "The outputs of 3-D convolution layer are intentionally made identical to the input of the first ResBlock in MoCo v2 (see Table 1), thus providing a compatible interface to transfer higher layers of MoCo v2 into this task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_21",
            "start": 272,
            "end": 492,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_21@2",
            "content": "On the other hand, we also adopt the common practice to convert the RGB input image to gray-scale before feeding it into the model, as it prevents the model from learning chromatic aberration information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_21",
            "start": 494,
            "end": 697,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_22@0",
            "content": "Audio Front-end: The audio front-end is rather straight-forward.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_22",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_22@1",
            "content": "We use wav2vec 2.0 (Schneider et al., 2019) pre-trained on Libri-Light (Kahn et al., 2020), like it is normally used for ASR tasks, both the 1-D convolution layers and the stacked Transformer encoder layers are transferred into our audio front-end.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_22",
            "start": 65,
            "end": 312,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_22@2",
            "content": "The audio front-end takes as input raw audio wave of 16kHz, and produces one vector representation every 20ms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_22",
            "start": 314,
            "end": 423,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_22@3",
            "content": "The audio feature dimensions are shown in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_22",
            "start": 425,
            "end": 474,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_23@0",
            "content": "Back-ends",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_23",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_24@0",
            "content": "Since the visual frames are in 25 FPS and the wav2vec 2.0 outputs are around 49 Hz 1 , one should 1 The odds are due to the larger receptive fields of wav2vec 2.0 1-D convolution layers, which we circumvent by properly prefixing and suffixing the audio sequence and truncate the trailing audio vector.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_24",
            "start": 0,
            "end": 300,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_24@1",
            "content": "Thus a perfect 1:2 ratio of visual frames note that there is 2x difference in the frequency of frame-wise visual and audio representations at the output of their front-ends.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_24",
            "start": 302,
            "end": 474,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_24@2",
            "content": "In the back-end, we use 1-D convolution layers on the time dimension combined with Transformer encoder layers to provide single modality temporal modeling, as well as adjusting the features to have the same frequency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_24",
            "start": 476,
            "end": 692,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_25@0",
            "content": "Visual Back-end: The incoming MoCo v2 output to the visual back-end has a feature dimension of 2048, at a frequency of 25 vectors per second.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_25",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_25@1",
            "content": "In the visual backend, we keep this frequency while reducing the feature size to 512.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_25",
            "start": 142,
            "end": 226,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_25@2",
            "content": "See Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_25",
            "start": 228,
            "end": 239,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_25@3",
            "content": "For positional encodings of the Transformer, we use fixed positional encoding in the form of sinusoidal functions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_25",
            "start": 241,
            "end": 354,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_26@0",
            "content": "Modules Image sequence Audio Back-end: In the audio back-end, the incoming wav2vec 2.0 outputs have a feature size of and audio front-end outputs are ensured.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_26",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_27@0",
            "content": "(T f \u00d7 112 2 \u00d7 1) Front-end 3-D convolution (T f \u00d7 28 2 \u00d7 64) MoCo v2 (T f \u00d7 2048) Back-end 1-D convolution (T f \u00d7 512) Transformer Encoder (T f \u00d7 512)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_27",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_28@0",
            "content": "1024, at a frequency of 50 vectors per second.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_28",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_28@1",
            "content": "We downscale the frequency by setting the stride of 1-D convolution layer to 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_28",
            "start": 47,
            "end": 125,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_28@2",
            "content": "The Transformer encoder layers have the identical size to that of the visual back-end, while using a separate set of parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_28",
            "start": 127,
            "end": 254,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_29@0",
            "content": "Table 2 shows a clearer picture of audio front-and back-end dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_29",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_30@0",
            "content": "Stage Modules Audio waveform",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_30",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_31@0",
            "content": "(Ts \u00d7 1) Front-end wav2vec 2.0 (T f \u00d7 1024) Back-end 1-D convolution ( T f 2 \u00d7 512) Transformer Encoder ( T f 2 \u00d7 512)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_31",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_32@0",
            "content": "Table 2: The feature dimension of audio stream.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_32",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_32@1",
            "content": "The dimensions of features are denoted by {temporal size\u00d7 channels}. T s and T f denote the number of sampled audio input and audio frames, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_32",
            "start": 48,
            "end": 200,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_33@0",
            "content": "Fusion Module",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_33",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_34@0",
            "content": "Features from both the audio and visual modalities are fused together in this section, forming vector representation of 1024 dimensions at a relatively low rate of 25 Hz.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_34",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_34@1",
            "content": "We use LayerNorm (Ba et al., 2016) separately on each of the modalities before concatenating them on the feature dimension.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_34",
            "start": 171,
            "end": 293,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_34@2",
            "content": "The LayerNorm is required since it avoids one modality overtaking the whole representation with larger variance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_34",
            "start": 295,
            "end": 406,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_34@3",
            "content": "Similar 1-D convolution layers and a subsequent Transformer encoder block of 6 layers take the fused representations as input, and encode them for the two decoders.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_34",
            "start": 408,
            "end": 571,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_35@0",
            "content": "Decoder",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_35",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_36@0",
            "content": "Following the setting of Petridis et al. (2018), there are two decoders trained simultaneously based on the same encoder output in the fusion module.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_36",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_37@0",
            "content": "The first is a Transformer seq2seq decoder, a canonical Transformer decoder with 6 layers is used, and we perform teacher forcing at character level by using ground truth characters as input during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_37",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_38@0",
            "content": "The second one is arguably a decoder since it yields character probabilities for each timestep and relies on the CTC loss in training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_38",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_38@1",
            "content": "4 extra 1-D convolution layers with ReLU activation are used on top of the last Transformer encoder layer output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_38",
            "start": 135,
            "end": 247,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_38@2",
            "content": "We also include LayerNorm between each of the layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_38",
            "start": 249,
            "end": 301,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_39@0",
            "content": "Loss Functions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_39",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_40@0",
            "content": "In this work, we use a so called hybrid CTC/attention loss (Watanabe et al., 2017) for our training process. Let x = [x 1 , \u2022 \u2022 \u2022 , x T ] be the input frame sequence at the input of Transformer encoder in the fusion module and y = [y 1 , \u2022 \u2022 \u2022 , y L ] being the targets, where T and L denote the input and target lengths, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_40",
            "start": 0,
            "end": 334,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_41@0",
            "content": "The CTC loss assumes conditional independence between each output prediction and has a form of",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_41",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_42@0",
            "content": "p CTC (y|x) \u2248 T t=1 p(y t |x) (1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_42",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_43@0",
            "content": "On the other hand, an auto-regressive decoder gets rid of this assumption by directly estimating the posterior on the basis of the chain rule, which has a form of",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_43",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_44@0",
            "content": "p CE (y|x) = L l=1 p(y l |y <l , x)(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_44",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_45@0",
            "content": "The overall objective function is computed as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_45",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_46@0",
            "content": "L = \u03bb log p CTC (y|x) + (1 \u2212 \u03bb) log p CE (y|x) (3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_46",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_47@0",
            "content": "where \u03bb controls the relative weight between CTC loss and seq2seq loss in the hybrid CTC/attention mechanisms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_47",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_47@1",
            "content": "The weight is needed not only when integrating the two losses into one training loss, but also fusing the two predictions during decoding, which we will revisit in the following subsections.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_47",
            "start": 111,
            "end": 300,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_48@0",
            "content": "Training Pipeline",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_48",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_49@0",
            "content": "The final AVSR model is achieved through a pipeline of training stages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_49",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_50@0",
            "content": "For audio modality, the audio front-end is first pre-trained through self-supervised learning, which is done by wav2vec 2.0.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_50",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_50@1",
            "content": "Then the audio backend is trained through the audio-only (AO) setting, together with a dedicated decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_50",
            "start": 125,
            "end": 229,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_51@0",
            "content": "For the visual modality, we first pre-train the 3-D convolution layer and visual back-end through sequence classification at word level video clips in LRW data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_51",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_51@1",
            "content": "After that, the visual front-end are inherited by the visual-only (VO) model, where dedicated visual back-end and decoder are used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_51",
            "start": 161,
            "end": 291,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_52@0",
            "content": "The final AVSR model can be trained after the audio-only and visual-only models have converged.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_52",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_53@0",
            "content": "Due to computational constraints, we pre-compute the audio and visual back-end outputs, and only learn the parameters in the fusion model and decoder part in this final stage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_53",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_53@1",
            "content": "A detailed visualization of our training pipeline is depicted in Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_53",
            "start": 176,
            "end": 249,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_54@0",
            "content": "Decoding",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_54",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_55@0",
            "content": "Decoding is performed using joint CTC/attention one-pass decoding (Watanabe et al., 2017) with beam search.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_55",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_55@1",
            "content": "We apply shallow fusion to incorporate CTC and seq2seq predictions:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_55",
            "start": 108,
            "end": 174,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_56@0",
            "content": "\u0177 = arg max y\u2208 \u0176 {\u03b1 log p CTC (y|x) + (1 \u2212 \u03b1) log p CE (y|x)}(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_56",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_57@0",
            "content": "where \u0176 denotes predictions set of target symbols, while \u03b1 is the relative weight that tuned on validation set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_57",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_58@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_58",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_59@0",
            "content": "In this section, we will first introduce the datasets and various settings we used in each component of our model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_59",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_59@1",
            "content": "Then we will present results of audioonly, visual-only and audio-visual settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_59",
            "start": 115,
            "end": 195,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_59@2",
            "content": "We also present a breakdown of the relative contribution of every component through ablation study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_59",
            "start": 197,
            "end": 295,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_60@0",
            "content": "Dataset",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_60",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_61@0",
            "content": "We use the large-scale publicly AVSR dataset, the Lip Reading Sentences 2 (LRS2) as our main testbed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_61",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_61@1",
            "content": "During training, we also use the Lip Reading in the Wild (LRW) (Chung and Zisserman, 2016) as a word-level video classification task to pre-train our visual encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_61",
            "start": 102,
            "end": 266,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_61@2",
            "content": "LRS2 consists of 224 hours of aligned audio and videos, with a total of 144K clips from BBC videos, the clips are at a length of sentence level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_61",
            "start": 268,
            "end": 411,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_61@3",
            "content": "The training data contains over 2M word instances and a vocabulary of over 40K.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_61",
            "start": 413,
            "end": 491,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_61@4",
            "content": "The dataset is very challenging as there are large variations in head pose, lighting conditions, genres and the number of speakers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_61",
            "start": 493,
            "end": 623,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_62@0",
            "content": "LRW is a word-level dataset, consisting of 157 hours of aligned audio and videos, totalling 489K video clips from BBC videos, each containing the utterance of a single word out of a vocabulary of 500.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_62",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_62@1",
            "content": "The videos have a fixed length of 29 frames, the target word occurring in the middle of the clip and surrounded by co-articulation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_62",
            "start": 201,
            "end": 331,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_62@2",
            "content": "All of the videos are either frontal or near-frontal.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_62",
            "start": 333,
            "end": 385,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_62@3",
            "content": "In our experiment, we only use the visual modality from this dataset to train our visual front-end.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_62",
            "start": 387,
            "end": 485,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_63@0",
            "content": "Experimental Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_63",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_64@0",
            "content": "We use character level prediction with an output size of 40, consisting of the 26 characters in the alphabet, the 10 digits, the apostrophe, and special tokens for [space], [blank] and [EOS/SOS].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_64",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_64@1",
            "content": "Since the transcriptions of the datasets do not contain other punctuations, we do not include them in the vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_64",
            "start": 196,
            "end": 312,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_65@0",
            "content": "Our implementation is based on the Pytorch library (Paszke et al., 2019) and trained on four NVIDIA A100 GPUs with a total of 160GB memory.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_65",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_65@1",
            "content": "The network is trained using the Adam optimiser (Kingma and Ba, 2014) with \u03b2 1 = 0.9, \u03b2 2 = 0.999 and = 10 \u22128 and an initial learning rate of 10 \u22124 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_65",
            "start": 140,
            "end": 288,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_65@2",
            "content": "We use label smoothing with a weight set to 0.01, learning rate warm up and reduce on plateau.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_65",
            "start": 290,
            "end": 383,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_65@3",
            "content": "The relative weight in CTC loss and seq2seq loss \u03bb is set to 0.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_65",
            "start": 385,
            "end": 449,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_65@4",
            "content": "When decoding, we set \u03b1 to 0.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_65",
            "start": 451,
            "end": 481,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_65@5",
            "content": "The samples in the pre-train set are cropped by randomly sampling a continuous range of 1/3 words of the whole utterances, in order to match the length of clips in the train set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_65",
            "start": 483,
            "end": 660,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_65@6",
            "content": "Overlength samples are further truncated at 160 frames to reduce memory occupation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_65",
            "start": 662,
            "end": 744,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_66@0",
            "content": "Preprocessing: We detected and tracked 68 facial landmarks using dlib (King, 2009) for each video.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_66",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_66@1",
            "content": "To remove differences related to face rotation and scale, the faces are aligned to a neural reference frame using a similarity transformation following (Martinez et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_66",
            "start": 99,
            "end": 274,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_66@2",
            "content": "Interpolation and frame smoothing with a window width of 12 frames are used to deal with the frames that dlib fails to detect.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_66",
            "start": 276,
            "end": 401,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_66@3",
            "content": "Then a bounding box of 120 \u00d7 120 is used to crop the mouth ROIs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_66",
            "start": 403,
            "end": 466,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_66@4",
            "content": "The cropped frame is further converted to gray-scale and normalized with respect to the overall mean and variance of the train set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_66",
            "start": 468,
            "end": 598,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_66@5",
            "content": "Each raw audio waveform is normalized to zero mean and unit variance following (Baevski et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_66",
            "start": 600,
            "end": 701,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_67@0",
            "content": "Data Augmentation: Following (Ma et al., 2021), random cropping with a size of 112 \u00d7 112 and horizontal flipping with a probability of 0.5 are performed consistently across all frames of a given image sequence when training visual-only and audiovisual models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_67",
            "start": 0,
            "end": 258,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_67@1",
            "content": "For each audio waveform, additive noise is performed in the time domain following (Afouras et al., 2018a) Evaluation: For all experiments, word error rate (WER) are reported which is defined as WER = (S + D + I)/N .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_67",
            "start": 260,
            "end": 474,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_67@2",
            "content": "The S, D and I in the formula denotes the number of substitutions, deletions and insertions respectively from the reference to the hypothesis, and N is the number of words in the inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_67",
            "start": 476,
            "end": 664,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_67@3",
            "content": "The babble noise added to the audio waveform during evaluation is generated using the same manner as training, while we set a different seed to avoid model fit to a specific generated noise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_67",
            "start": 666,
            "end": 855,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_67@4",
            "content": "Decoding is performed using joint CTC/attention one-pass decoding (Watanabe et al., 2017) with beam width 5 (the values were determined on the held-out validation set of LRS2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_67",
            "start": 857,
            "end": 1032,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_67@5",
            "content": "We don't use an external language model in our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_67",
            "start": 1034,
            "end": 1092,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_68@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_68",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_69@0",
            "content": "We present results for all experiments in Table 3, reporting WERs on audio-only, visual-only and audio-visual models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_69",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_69@1",
            "content": "Note that many of the models listed here are also using extra training data in different stages of training pipeline, such as MV-LRS , LRS3 (Afouras et al., 2018b), LibriSpeech (Panayotov et al., 2015) and LRW.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_69",
            "start": 118,
            "end": 327,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_70@0",
            "content": "Audio-visual Setting: In the main audio-visual (AV) setting, the pre-train and train sets in LRS2 are used as train set in the final training stage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_70",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_70@1",
            "content": "Our proposed audio-visual model achieves a WER of 2.6% without the help of an external language model, which improves by 1.1% over the current state-ofthe-art (Ma et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_70",
            "start": 149,
            "end": 325,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_70@2",
            "content": "This is rather a big improvement, with a relative improvement of around 30%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_70",
            "start": 327,
            "end": 402,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_71@0",
            "content": "The training data used for training audio-only model consists of 224 hours labelled data from LRS2, as well as the 60K hours unlabelled data from LibriLight (Kahn et al., 2020) that are indirectly used through inheriting wav2vec 2.0 parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_71",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_71@1",
            "content": "Our model also achieves a WER of 2.7%, which reduces the WER of the current stateof-the-art (Ma et al., 2021) by 1.2%, indicating a relative improvement of 30%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_71",
            "start": 245,
            "end": 404,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_72@0",
            "content": "The visual-only model uses labelled LRS2 data in its pre-train and train sets, the LRW for supervised pre-training, and indirectly using the 1.28M unlabelled images from ImageNet through MoCo v2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_72",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_72@1",
            "content": "The visual-only model achieves a WER of 43.8%, lagging behind the current stateof-the-art (E2E Conformer) with 5.3%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_72",
            "start": 196,
            "end": 311,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_72@2",
            "content": "Compared to E2E Conformer, the main difference is that a big Transformer language model is used during decoding, which itself brings a 4.5% difference compared with a normal RNN language model in their ablation study (Ma et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_72",
            "start": 313,
            "end": 547,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_72@3",
            "content": "The gap between our visual-only model and the E2E Conformer model with a RNN language model is 0.8%, which resides in a quite reasonable range.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_72",
            "start": 549,
            "end": 691,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_72@4",
            "content": "Additionally, we use a 6-layers Transformer encoder for temporal modelling instead of a 12-layers conformer encoder, which resulted in a smaller model size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_72",
            "start": 693,
            "end": 848,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_73@0",
            "content": "If we consider a fairer comparison by only looking at benchmarks without using an external language model, the best-reported benchmark is Ren et al. (2021), which achieved a WER of 49.2%, lagging behind our model by 6.0%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_73",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_74@0",
            "content": "Ablation Studies",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_74",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_75@0",
            "content": "In this section, we investigate the impact of every individual building block by testing them in LRW, visual-only and audio-only settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_75",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_75@1",
            "content": "4. We first train a model by replacing the ResNet-18 front-end in (Stafylakis and Tzimiropoulos, 2017) with a ResNet-50 frontend, matching the size of MoCo v2 but with fresh weights.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_75",
            "start": 139,
            "end": 320,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_75@2",
            "content": "This results in an absolute improvement of 2.1%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_75",
            "start": 322,
            "end": 369,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_75@3",
            "content": "Then we initialize the ResNet-50 frontend with MoCo v2 weights and a further absolute improvement of 2.3% is observed, which implies that self-supervised learning is actually functioning in better represent the lip movement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_75",
            "start": 371,
            "end": 594,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_75@4",
            "content": "Additionally, When Using 6 layers of Transformer encoder instead of TCN as back-end, we can observe another absolute improvement of 5.0%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_75",
            "start": 596,
            "end": 732,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_75@5",
            "content": "We also noticed that using MoCo v2 front-end could significantly reduce the training time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_75",
            "start": 734,
            "end": 823,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_75@6",
            "content": "Performance Breakdown in Audio-only Setting: Results of audio-only model on LRS2 are shown in Table",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_75",
            "start": 825,
            "end": 923,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_75@7",
            "content": "5. Starting from (Afouras et al., 2018a), we first train a model by replacing the STFT audio feature with a wav2vec 2.0 front-end pre-trained on LibriSpeech, resulting in an absolute improvement of 11.1%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_75",
            "start": 925,
            "end": 1128,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_75@8",
            "content": "Then we use another pre-trained model learned on an even larger unlabelled single modality dataset Libri-Light, and a further absolute improvement of 0.6% is observed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_75",
            "start": 1130,
            "end": 1296,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_75@9",
            "content": "We further train the model with hybrid CTC/attention decoder during the training stage, which results in another absolute improvement of 0.9%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_75",
            "start": 1298,
            "end": 1439,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_75@10",
            "content": "Performance Breakdown in Visual-only Setting: Results of the visual-only model on LRS2 are shown in Table",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_75",
            "start": 1441,
            "end": 1545,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_75@11",
            "content": "6. Starting from (Afouras et al., 2018a), we first introduce end-to-end training by Method WER Baseline (Afouras et al., 2018a) 15.3% + wav2vec 2.0 (LibriSpeech) encoder 4.2% + wav2vec 2.0 (LibriLight) encoder 3.6% + Hybrid CTC/attention 2.7%",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_75",
            "start": 1547,
            "end": 1788,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_76@0",
            "content": "To evaluate the model's tolerance to audio noise, we tested the performance of our model under babble noise with different SNR levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_76",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_76@1",
            "content": "Our audio-only and audiovisual models reach WERs of 32.5% and 24.5% when the SNR level is 0dB, respectively, which reduce the reported result in (Afouras et al., 2018a) by 25.5% and 9% 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_76",
            "start": 135,
            "end": 322,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_76@2",
            "content": "When the SNR level rises to 5dB, our audio-only and audio-visual model obtain WERs of 6.8% and 6.3%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_76",
            "start": 324,
            "end": 423,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_76@3",
            "content": "Besides achieving significant improvement over the baseline model under babble noise environment, we further investigate the model performance under human noise environment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_76",
            "start": 425,
            "end": 597,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_76@4",
            "content": "The human noise is extremely challenging cause the noise itself contains some words, while the model cannot easily distinguish which audio signal is the one to be recognized.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_76",
            "start": 599,
            "end": 772,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_76@5",
            "content": "We synthesize the human noise by randomly crop many 1 second signals from different audio samples in the LRS2 dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_76",
            "start": 774,
            "end": 891,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_76@6",
            "content": "As shown in Fig. 3, we conduct experiments varying different levels of human noise, the models are trained using babble noise augmented audio.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_76",
            "start": 893,
            "end": 1034,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_76@7",
            "content": "The WER increases greatly after the SNR level drops down under 0db.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_76",
            "start": 1036,
            "end": 1102,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_77@0",
            "content": "It is because the model may not be able to distinguish the two overlapped spoken words at a low SNR level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_77",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_78@0",
            "content": "And the overall performance under each SNR level is worse than babble noise, indicating that noise with specific information is harder than disorganized babble noise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_78",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_78@1",
            "content": "under Low Resource: A significant benefit of using self-supervised pre-trained models is that only a small amount of labelled data is needed for training a model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_78",
            "start": 167,
            "end": 328,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_78@2",
            "content": "To further investigate the models' performance in low resource environment, we use the 28 hours train set of LRS2 to train an audio-only and a visual-only model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_78",
            "start": 330,
            "end": 490,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_78@3",
            "content": "The results are shown in Table 8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_78",
            "start": 492,
            "end": 524,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_78@4",
            "content": "The audio-only model trained with 28 hours data achieves a WER of 3.4%, which is a little bit worse than the one trained with 224 hours data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_78",
            "start": 526,
            "end": 666,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_78@5",
            "content": "The result indicates that for the audio-only model, the self-supervised model pretrained on a large-scale single modality dataset can significantly reduce the demands of data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_78",
            "start": 668,
            "end": 842,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_78@6",
            "content": "While the visual-only model trained with 28 hours data has a great gap with the one trained with 224 hours data, the reason can be that the visual-only model is harder to train and demands a larger amount of data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_78",
            "start": 844,
            "end": 1056,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_79@0",
            "content": "Discussion and Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_79",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_80@0",
            "content": "In this work, we propose to utilize self-supervised learning for AVSR by simply incorporating the pretrained model trained in massive unlabelled single modality data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_80",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_80@1",
            "content": "Although the visual pre-trained models are not straight-forward to be transplanted into visual front-end, we still manage to integrate pre-trained models in both modalities for the AVSR task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_80",
            "start": 167,
            "end": 357,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_80@2",
            "content": "Experimental results are impressive, resulting in a 30% relative improvement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_80",
            "start": 359,
            "end": 435,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_80@3",
            "content": "It's interesting to observe that self-supervised model in audio modality has an even larger improvement than that of the visual counterpart.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_80",
            "start": 437,
            "end": 576,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_80@4",
            "content": "We believe the reasons can be listed as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_80",
            "start": 578,
            "end": 625,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_81@0",
            "content": "\u2022 The training data scale of audio modality is significantly larger than that of visual modality, with the Libri-Light dataset used for pretraining wav2vec 2.0 consists of 60K hours audio signals, the ImageNet dataset, on the contrary, has only 1.28M images, roughly equivalent to 14 hours silent video under 25 FPS. \u2022 The MoCo v2 model is pre-trained on images to better represent frame-level contents, while there are no pre-training steps to model the temporal correlation between frames. In contrast, the wav2vec 2.0 model is pre-trained on consistent audios, thus having a better temporal modelling ability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_81",
            "start": 0,
            "end": 611,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_82@0",
            "content": "As there has not emerged a dominating crossmodality self-supervised learning approach in the field of AVSR, in future work, we are going to explore two more directions in the self-supervised learning scenario based on this work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_82",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_82@1",
            "content": "The first is utilizing the temporal correlations within the visual domain, while the other is the cross-modal correlations between the audio and visual modality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_82",
            "start": 229,
            "end": 389,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_82@2",
            "content": "We hope this work could pave the way towards multimodality self-supervised learning, especially for various aspects in audio-visual speech recognition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_82",
            "start": 391,
            "end": 541,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_83@0",
            "content": "Algorithm 1 Hybrid CTC/attention one-pass decoding adapted from (Watanabe et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_83",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_83@1",
            "content": "Notation: X is the speech input; L max is the maximum length of the hypotheses to be searched, we set it to T ; C is the decoded symbol sequence;",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_83",
            "start": 89,
            "end": 233,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_84@0",
            "content": "[b] denotes [blank]. Input: X, Lmax Output: C 1: \u21260 = {[SOS]} 2: \u03a9 = \u2205 3: \u03b3 (b) 0 ([SOS]) = 1 4: for t = 1, \u2022 \u2022 \u2022 , T do 5: \u03b3 (n) t ([SOS]) = 0 6: \u03b3 (b) t ([SOS]) = t \u03c4 =1 \u03b3 (b) \u03c4 \u22121 ([SOS])\u2022p(z\u03c4 = [b]|X) 7: end for 8: for l = 1 \u2022 \u2022 \u2022 Lmax do 9: \u2126 l = \u2205 10: while \u2126 l\u22121 = \u2205 do 11: g = HEAD(\u2126 l\u22121 ) 12: DEQUEUE(\u2126 l\u22121 ) 13:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_84",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_85@0",
            "content": "for each c U do 14: Algorithm 1 describes the hybrid CTC/attention decoding procedure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_85",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_85@1",
            "content": "The CTC prefix probability is defined as the cumulative probability of all label sequences that have h as their prefix:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_85",
            "start": 87,
            "end": 205,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_86@0",
            "content": "h = g \u2022 c 15: if c = [EOS] then 16: log pctc(h|X) = log{\u03b3 (n) T (g) + \u03b3 (b) T (g)} 17: else 18: if g = [SOS] then 19: \u03b3 (n) 1 (h) = p(z1 = c|X) 20: else 21: \u03b3 (n) 1 (h) = 0 22: end if 23: \u03b3 (b) 1 (h) = 0 24: \u03a8 = \u03b3 (n) 1 (h) 25: for t = 2 \u2022 \u2022 \u2022 T do 26: if last(g) = c then 27: \u03a6 = \u03b3 (b) t\u22121 (g) 28: else 29: \u03a6 = \u03b3 (b) t\u22121 (g) + \u03b3 (n) t\u22121 (g) 30: end if 31: \u03b3 (n) t (h) = (\u03b3 (n) t\u22121 (h) + \u03a6)p(zt = c|X) 32: \u03b3 (b) t (h) = (\u03b3(",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_86",
            "start": 0,
            "end": 422,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_87@0",
            "content": "p ctc (h|X) = v\u2208(U ) + p ctc (h \u2022 v|X) (5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_87",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_88@0",
            "content": "where v denotes all possible symbol sequences except the empty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_88",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_88@1",
            "content": "The CTC probability can be computed by keeping the forward hypothesis prob\u03b3 The decoding algorithm is also a beam search with width W and hyperparameter \u03b1 control the relative weight given to CTC and attention decoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_88",
            "start": 64,
            "end": 282,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_88@2",
            "content": "U is a set of symbols excluding [blank], and a same token is used to represent [SOS] and [EOS] in our implementation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_88",
            "start": 284,
            "end": 400,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_89@0",
            "content": "The input images are sampled at 25 FPS and resized to 224 \u00d7 224 pixels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_89",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_89@1",
            "content": "We crop a 120 \u00d7 120 mouth ROI from each frame.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_89",
            "start": 72,
            "end": 117,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_89@2",
            "content": "Fig. 4 shows the process to generate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_89",
            "start": 119,
            "end": 155,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_90@0",
            "content": "T Afouras, J Chung, A Senior, O Vinyals, A Zisserman, Deep audio-visual speech recognition, 2018, IEEE Transactions on Pattern Analysis & Machine Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_90",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_91@0",
            "content": "Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman, 2018, Lrs3-ted: a large-scale dataset for visual speech recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_91",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_92@0",
            "content": "Triantafyllos Afouras, Joon Chung, Andrew Zisserman, Asr is all you need: Cross-modal distillation for lip reading, 2020, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_92",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_93@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_93",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_94@0",
            "content": "UNKNOWN, None, , Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_94",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_95@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_95",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_96@0",
            "content": "Ting Chen, Mohammad Kornblith, Geoffrey Norouzi, A simple framework for contrastive learning of visual representations, 2020, International conference on machine learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_96",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_97@0",
            "content": "UNKNOWN, None, 2020, Improved baselines with momentum contrastive learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_97",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_98@0",
            "content": "Chung Joon Son, Arsha Nagrani, and Andrew Zisserman, 2018, Voxceleb2: Deep speaker recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_98",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_99@0",
            "content": "Joon Son Chung, Andrew Senior, Oriol Vinyals, Andrew Zisserman, Lip reading sentences in the wild, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_99",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_100@0",
            "content": "Son Joon, Andrew Chung,  Zisserman, Lip reading in the wild, 2016, Asian conference on computer vision, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_100",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_101@0",
            "content": "UNKNOWN, None, 2017, Lip reading in profile, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_101",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_102@0",
            "content": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, Imagenet: A large-scale hierarchical image database, 2009, 2009 IEEE conference on computer vision and pattern recognition, Ieee.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_102",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_103@0",
            "content": "UNKNOWN, None, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_103",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_104@0",
            "content": "Carl Doersch, Abhinav Gupta, Alexei Efros, Unsupervised visual representation learning by context prediction, 2015, Proceedings of the IEEE international conference on computer vision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_104",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_105@0",
            "content": "S Dupont, J Luettin, Audio-visual speech modeling for continuous speech recognition, 2000, IEEE Transactions on Multimedia, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_105",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_106@0",
            "content": "UNKNOWN, None, 2018, Unsupervised representation learning by predicting image rotations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_106",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_107@0",
            "content": "UNKNOWN, None, 2020, Bootstrap your own latent: A new approach to self-supervised learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_107",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_108@0",
            "content": "Raia Hadsell, Sumit Chopra, Yann Lecun, Dimensionality reduction by learning an invariant mapping, 2006, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_108",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_109@0",
            "content": "UNKNOWN, None, 2019, Momentum contrast for unsupervised visual representation learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_109",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_110@0",
            "content": "Jacob Kahn, Morgane Rivi\u00e8re, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazar\u00e9, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, Libri-light: A benchmark for asr with limited or no supervision, 2020, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_110",
            "start": 0,
            "end": 346,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_111@0",
            "content": "E Davis,  King, Dlib-ml: A machine learning toolkit, 2009, The Journal of Machine Learning Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_111",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_112@0",
            "content": "UNKNOWN, None, 2014, Adam: A method for stochastic optimization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_112",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_113@0",
            "content": "Yong-Hyeok Lee, Dong-Won Jang, Jae-Bin Kim, Rae-Hong Park, Hyung-Min Park, Audiovisual speech recognition based on dual crossmodality attentions with the transformer model, 2020, Applied Sciences, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_113",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_114@0",
            "content": "Yong-Hyeok Lee, Dong-Won Jang, Jae-Bin Kim, Rae-Hong Park, Hyung-Min Park, Audiovisual speech recognition based on dual crossmodality attentions with the transformer model, 2020, Applied Sciences, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_114",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_115@0",
            "content": "Wei Li, Sicheng Wang, Ming Lei, Marco Sabato, Chin-Hui Siniscalchi,  Lee, Improving audio-visual speech recognition performance with cross-modal student-teacher training, 2019, ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_115",
            "start": 0,
            "end": 281,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_116@0",
            "content": "Pingchuan Ma, Stavros Petridis, Maja Pantic, End-to-end audio-visual speech recognition with conformers, 2021, ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Processing (ICASSP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_116",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_117@0",
            "content": ", None, , IEEE, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_117",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_118@0",
            "content": "Brais Martinez, Pingchuan Ma, Stavros Petridis, Maja Pantic, Lipreading using temporal convolutional networks, 2020, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_118",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_119@0",
            "content": "Vassil Panayotov, Guoguo Chen, Daniel Povey, Sanjeev Khudanpur, Librispeech: an asr corpus based on public domain audio books, 2015, 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_119",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_120@0",
            "content": "UNKNOWN, None, 2020, Multiresolution and multimodal speech recognition with transformers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_120",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_121@0",
            "content": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Pytorch: An imperative style, high-performance deep learning library, 2019, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_121",
            "start": 0,
            "end": 273,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_122@0",
            "content": "Stavros Petridis, Themos Stafylakis, Pingchuan Ma, Audio-visual speech recognition with a hybrid ctc/attention architecture, 2018, 2018 IEEE Spoken Language Technology Workshop (SLT), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_122",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_123@0",
            "content": "Sucheng Ren, Yong Du, Jianming Lv, Guoqiang Han, Shengfeng He, Learning from the master: Distilling cross-modal advanced knowledge for lip reading, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_123",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_124@0",
            "content": "UNKNOWN, None, 2019, wav2vec: Unsupervised pre-training for speech recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_124",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_125@0",
            "content": "Abhinav Shukla, Konstantinos Vougioukas, Pingchuan Ma, Stavros Petridis, and Maja Pantic. 2020. Visually guided self supervised learning of speech representations, , ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_125",
            "start": 0,
            "end": 270,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_126@0",
            "content": "UNKNOWN, None, , Themos Stafylakis and Georgios Tzimiropoulos. 2017. Combining residual networks with lstms for lipreading, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_126",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_127@0",
            "content": "Fei Tao, Carlos Busso, End-to-end audiovisual speech recognition system with multitask learning, 2020, IEEE Transactions on Multimedia, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_127",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_128@0",
            "content": "Shinji Watanabe, Takaaki Hori, Suyoun Kim, R John, Tomoki Hershey,  Hayashi, Hybrid ctc/attention architecture for end-to-end speech recognition, 2017, IEEE Journal of Selected Topics in Signal Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_128",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_129@0",
            "content": "Jianwei Yu,  Shi-Xiong, Jian Zhang, Shahram Wu, Bo Ghorbani, Shiyin Wu, Shansong Kang, Xunying Liu, Helen Liu, Dong Meng,  Yu, Audio-visual recognition of overlapped speech for the lrs2 dataset, 2020, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_129",
            "start": 0,
            "end": 305,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_130@0",
            "content": "Richard Zhang, Phillip Isola, Alexei Efros, Colorful image colorization, 2016, European conference on computer vision, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_130",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_131@0",
            "content": "Xingxuan Zhang, Feng Cheng, Shilin Wang, Spatio-temporal fusion based convolutional sequence learning for lip reading, 2019, Proceedings of the IEEE/CVF International Conference on Computer Vision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_131",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "470-ARR_v1_132@0",
            "content": "Ya Zhao, Rui Xu, Xinchao Wang, Peng Hou, Hearing lips: Improving lip reading by distilling speech recognizers, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "470-ARR_v1_132",
            "start": 0,
            "end": 180,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "470-ARR_v1_0",
            "tgt_ix": "470-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_0",
            "tgt_ix": "470-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_1",
            "tgt_ix": "470-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_1",
            "tgt_ix": "470-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_0",
            "tgt_ix": "470-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_2",
            "tgt_ix": "470-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_4",
            "tgt_ix": "470-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_5",
            "tgt_ix": "470-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_6",
            "tgt_ix": "470-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_7",
            "tgt_ix": "470-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_3",
            "tgt_ix": "470-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_3",
            "tgt_ix": "470-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_3",
            "tgt_ix": "470-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_3",
            "tgt_ix": "470-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_3",
            "tgt_ix": "470-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_3",
            "tgt_ix": "470-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_0",
            "tgt_ix": "470-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_8",
            "tgt_ix": "470-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_9",
            "tgt_ix": "470-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_9",
            "tgt_ix": "470-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_11",
            "tgt_ix": "470-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_12",
            "tgt_ix": "470-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_10",
            "tgt_ix": "470-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_10",
            "tgt_ix": "470-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_10",
            "tgt_ix": "470-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_10",
            "tgt_ix": "470-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_9",
            "tgt_ix": "470-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_13",
            "tgt_ix": "470-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_15",
            "tgt_ix": "470-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_14",
            "tgt_ix": "470-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_14",
            "tgt_ix": "470-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_14",
            "tgt_ix": "470-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_0",
            "tgt_ix": "470-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_16",
            "tgt_ix": "470-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_17",
            "tgt_ix": "470-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_17",
            "tgt_ix": "470-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_17",
            "tgt_ix": "470-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_18",
            "tgt_ix": "470-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_20",
            "tgt_ix": "470-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_21",
            "tgt_ix": "470-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_19",
            "tgt_ix": "470-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_19",
            "tgt_ix": "470-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_19",
            "tgt_ix": "470-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_19",
            "tgt_ix": "470-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_17",
            "tgt_ix": "470-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_22",
            "tgt_ix": "470-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_24",
            "tgt_ix": "470-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_23",
            "tgt_ix": "470-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_23",
            "tgt_ix": "470-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_23",
            "tgt_ix": "470-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_26",
            "tgt_ix": "470-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_27",
            "tgt_ix": "470-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_28",
            "tgt_ix": "470-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_29",
            "tgt_ix": "470-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_30",
            "tgt_ix": "470-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_31",
            "tgt_ix": "470-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_23",
            "tgt_ix": "470-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_23",
            "tgt_ix": "470-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_23",
            "tgt_ix": "470-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_23",
            "tgt_ix": "470-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_23",
            "tgt_ix": "470-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_23",
            "tgt_ix": "470-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_23",
            "tgt_ix": "470-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_25",
            "tgt_ix": "470-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_17",
            "tgt_ix": "470-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_32",
            "tgt_ix": "470-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_33",
            "tgt_ix": "470-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_33",
            "tgt_ix": "470-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_17",
            "tgt_ix": "470-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_34",
            "tgt_ix": "470-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_36",
            "tgt_ix": "470-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_37",
            "tgt_ix": "470-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_35",
            "tgt_ix": "470-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_35",
            "tgt_ix": "470-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_35",
            "tgt_ix": "470-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_35",
            "tgt_ix": "470-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_17",
            "tgt_ix": "470-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_38",
            "tgt_ix": "470-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_40",
            "tgt_ix": "470-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_41",
            "tgt_ix": "470-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_42",
            "tgt_ix": "470-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_43",
            "tgt_ix": "470-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_44",
            "tgt_ix": "470-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_45",
            "tgt_ix": "470-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_46",
            "tgt_ix": "470-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_39",
            "tgt_ix": "470-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_39",
            "tgt_ix": "470-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_39",
            "tgt_ix": "470-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_39",
            "tgt_ix": "470-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_39",
            "tgt_ix": "470-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_39",
            "tgt_ix": "470-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_39",
            "tgt_ix": "470-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_39",
            "tgt_ix": "470-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_39",
            "tgt_ix": "470-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_17",
            "tgt_ix": "470-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_47",
            "tgt_ix": "470-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_49",
            "tgt_ix": "470-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_50",
            "tgt_ix": "470-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_51",
            "tgt_ix": "470-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_52",
            "tgt_ix": "470-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_48",
            "tgt_ix": "470-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_48",
            "tgt_ix": "470-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_48",
            "tgt_ix": "470-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_48",
            "tgt_ix": "470-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_48",
            "tgt_ix": "470-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_48",
            "tgt_ix": "470-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_17",
            "tgt_ix": "470-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_53",
            "tgt_ix": "470-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_55",
            "tgt_ix": "470-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_56",
            "tgt_ix": "470-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_54",
            "tgt_ix": "470-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_54",
            "tgt_ix": "470-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_54",
            "tgt_ix": "470-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_54",
            "tgt_ix": "470-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_0",
            "tgt_ix": "470-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_57",
            "tgt_ix": "470-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_58",
            "tgt_ix": "470-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_58",
            "tgt_ix": "470-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_58",
            "tgt_ix": "470-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_59",
            "tgt_ix": "470-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_61",
            "tgt_ix": "470-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_60",
            "tgt_ix": "470-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_60",
            "tgt_ix": "470-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_60",
            "tgt_ix": "470-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_58",
            "tgt_ix": "470-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_62",
            "tgt_ix": "470-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_64",
            "tgt_ix": "470-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_65",
            "tgt_ix": "470-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_66",
            "tgt_ix": "470-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_63",
            "tgt_ix": "470-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_63",
            "tgt_ix": "470-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_63",
            "tgt_ix": "470-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_63",
            "tgt_ix": "470-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_63",
            "tgt_ix": "470-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_58",
            "tgt_ix": "470-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_67",
            "tgt_ix": "470-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_69",
            "tgt_ix": "470-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_68",
            "tgt_ix": "470-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_68",
            "tgt_ix": "470-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_68",
            "tgt_ix": "470-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_68",
            "tgt_ix": "470-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_70",
            "tgt_ix": "470-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_72",
            "tgt_ix": "470-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_68",
            "tgt_ix": "470-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_68",
            "tgt_ix": "470-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_71",
            "tgt_ix": "470-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_58",
            "tgt_ix": "470-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_73",
            "tgt_ix": "470-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_74",
            "tgt_ix": "470-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_74",
            "tgt_ix": "470-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_76",
            "tgt_ix": "470-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_77",
            "tgt_ix": "470-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_74",
            "tgt_ix": "470-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_74",
            "tgt_ix": "470-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_74",
            "tgt_ix": "470-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_75",
            "tgt_ix": "470-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_58",
            "tgt_ix": "470-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_78",
            "tgt_ix": "470-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_80",
            "tgt_ix": "470-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_79",
            "tgt_ix": "470-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_79",
            "tgt_ix": "470-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_79",
            "tgt_ix": "470-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_79",
            "tgt_ix": "470-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_83",
            "tgt_ix": "470-ARR_v1_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_84",
            "tgt_ix": "470-ARR_v1_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_85",
            "tgt_ix": "470-ARR_v1_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_86",
            "tgt_ix": "470-ARR_v1_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_87",
            "tgt_ix": "470-ARR_v1_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_79",
            "tgt_ix": "470-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_79",
            "tgt_ix": "470-ARR_v1_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_79",
            "tgt_ix": "470-ARR_v1_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_79",
            "tgt_ix": "470-ARR_v1_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_79",
            "tgt_ix": "470-ARR_v1_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_79",
            "tgt_ix": "470-ARR_v1_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_82",
            "tgt_ix": "470-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_79",
            "tgt_ix": "470-ARR_v1_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_88",
            "tgt_ix": "470-ARR_v1_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "470-ARR_v1_0",
            "tgt_ix": "470-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_1",
            "tgt_ix": "470-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_2",
            "tgt_ix": "470-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_2",
            "tgt_ix": "470-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_2",
            "tgt_ix": "470-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_2",
            "tgt_ix": "470-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_2",
            "tgt_ix": "470-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_2",
            "tgt_ix": "470-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_2",
            "tgt_ix": "470-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_2",
            "tgt_ix": "470-ARR_v1_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_3",
            "tgt_ix": "470-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_4",
            "tgt_ix": "470-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_4",
            "tgt_ix": "470-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_4",
            "tgt_ix": "470-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_5",
            "tgt_ix": "470-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_5",
            "tgt_ix": "470-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_5",
            "tgt_ix": "470-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_5",
            "tgt_ix": "470-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_5",
            "tgt_ix": "470-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_5",
            "tgt_ix": "470-ARR_v1_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_5",
            "tgt_ix": "470-ARR_v1_5@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_5",
            "tgt_ix": "470-ARR_v1_5@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_6",
            "tgt_ix": "470-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_6",
            "tgt_ix": "470-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_6",
            "tgt_ix": "470-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_6",
            "tgt_ix": "470-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_6",
            "tgt_ix": "470-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_7",
            "tgt_ix": "470-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_7",
            "tgt_ix": "470-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_7",
            "tgt_ix": "470-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_7",
            "tgt_ix": "470-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_8",
            "tgt_ix": "470-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_8",
            "tgt_ix": "470-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_8",
            "tgt_ix": "470-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_9",
            "tgt_ix": "470-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_10",
            "tgt_ix": "470-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_11",
            "tgt_ix": "470-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_11",
            "tgt_ix": "470-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_11",
            "tgt_ix": "470-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_11",
            "tgt_ix": "470-ARR_v1_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_12",
            "tgt_ix": "470-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_12",
            "tgt_ix": "470-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_12",
            "tgt_ix": "470-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_12",
            "tgt_ix": "470-ARR_v1_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_13",
            "tgt_ix": "470-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_13",
            "tgt_ix": "470-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_14",
            "tgt_ix": "470-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_15",
            "tgt_ix": "470-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_15",
            "tgt_ix": "470-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_15",
            "tgt_ix": "470-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_16",
            "tgt_ix": "470-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_16",
            "tgt_ix": "470-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_16",
            "tgt_ix": "470-ARR_v1_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_17",
            "tgt_ix": "470-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_18",
            "tgt_ix": "470-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_18",
            "tgt_ix": "470-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_19",
            "tgt_ix": "470-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_20",
            "tgt_ix": "470-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_20",
            "tgt_ix": "470-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_20",
            "tgt_ix": "470-ARR_v1_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_21",
            "tgt_ix": "470-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_21",
            "tgt_ix": "470-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_21",
            "tgt_ix": "470-ARR_v1_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_22",
            "tgt_ix": "470-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_22",
            "tgt_ix": "470-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_22",
            "tgt_ix": "470-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_22",
            "tgt_ix": "470-ARR_v1_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_23",
            "tgt_ix": "470-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_24",
            "tgt_ix": "470-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_24",
            "tgt_ix": "470-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_24",
            "tgt_ix": "470-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_25",
            "tgt_ix": "470-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_25",
            "tgt_ix": "470-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_25",
            "tgt_ix": "470-ARR_v1_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_25",
            "tgt_ix": "470-ARR_v1_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_26",
            "tgt_ix": "470-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_27",
            "tgt_ix": "470-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_28",
            "tgt_ix": "470-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_28",
            "tgt_ix": "470-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_28",
            "tgt_ix": "470-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_29",
            "tgt_ix": "470-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_30",
            "tgt_ix": "470-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_31",
            "tgt_ix": "470-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_32",
            "tgt_ix": "470-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_32",
            "tgt_ix": "470-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_33",
            "tgt_ix": "470-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_34",
            "tgt_ix": "470-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_34",
            "tgt_ix": "470-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_34",
            "tgt_ix": "470-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_34",
            "tgt_ix": "470-ARR_v1_34@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_35",
            "tgt_ix": "470-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_36",
            "tgt_ix": "470-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_37",
            "tgt_ix": "470-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_38",
            "tgt_ix": "470-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_38",
            "tgt_ix": "470-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_38",
            "tgt_ix": "470-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_39",
            "tgt_ix": "470-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_40",
            "tgt_ix": "470-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_41",
            "tgt_ix": "470-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_42",
            "tgt_ix": "470-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_43",
            "tgt_ix": "470-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_44",
            "tgt_ix": "470-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_45",
            "tgt_ix": "470-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_46",
            "tgt_ix": "470-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_47",
            "tgt_ix": "470-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_47",
            "tgt_ix": "470-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_48",
            "tgt_ix": "470-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_49",
            "tgt_ix": "470-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_50",
            "tgt_ix": "470-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_50",
            "tgt_ix": "470-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_51",
            "tgt_ix": "470-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_51",
            "tgt_ix": "470-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_52",
            "tgt_ix": "470-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_53",
            "tgt_ix": "470-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_53",
            "tgt_ix": "470-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_54",
            "tgt_ix": "470-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_55",
            "tgt_ix": "470-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_55",
            "tgt_ix": "470-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_56",
            "tgt_ix": "470-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_57",
            "tgt_ix": "470-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_58",
            "tgt_ix": "470-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_59",
            "tgt_ix": "470-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_59",
            "tgt_ix": "470-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_59",
            "tgt_ix": "470-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_60",
            "tgt_ix": "470-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_61",
            "tgt_ix": "470-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_61",
            "tgt_ix": "470-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_61",
            "tgt_ix": "470-ARR_v1_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_61",
            "tgt_ix": "470-ARR_v1_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_61",
            "tgt_ix": "470-ARR_v1_61@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_62",
            "tgt_ix": "470-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_62",
            "tgt_ix": "470-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_62",
            "tgt_ix": "470-ARR_v1_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_62",
            "tgt_ix": "470-ARR_v1_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_63",
            "tgt_ix": "470-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_64",
            "tgt_ix": "470-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_64",
            "tgt_ix": "470-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_65",
            "tgt_ix": "470-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_65",
            "tgt_ix": "470-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_65",
            "tgt_ix": "470-ARR_v1_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_65",
            "tgt_ix": "470-ARR_v1_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_65",
            "tgt_ix": "470-ARR_v1_65@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_65",
            "tgt_ix": "470-ARR_v1_65@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_65",
            "tgt_ix": "470-ARR_v1_65@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_66",
            "tgt_ix": "470-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_66",
            "tgt_ix": "470-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_66",
            "tgt_ix": "470-ARR_v1_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_66",
            "tgt_ix": "470-ARR_v1_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_66",
            "tgt_ix": "470-ARR_v1_66@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_66",
            "tgt_ix": "470-ARR_v1_66@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_67",
            "tgt_ix": "470-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_67",
            "tgt_ix": "470-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_67",
            "tgt_ix": "470-ARR_v1_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_67",
            "tgt_ix": "470-ARR_v1_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_67",
            "tgt_ix": "470-ARR_v1_67@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_67",
            "tgt_ix": "470-ARR_v1_67@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_68",
            "tgt_ix": "470-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_69",
            "tgt_ix": "470-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_69",
            "tgt_ix": "470-ARR_v1_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_70",
            "tgt_ix": "470-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_70",
            "tgt_ix": "470-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_70",
            "tgt_ix": "470-ARR_v1_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_71",
            "tgt_ix": "470-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_71",
            "tgt_ix": "470-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_72",
            "tgt_ix": "470-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_72",
            "tgt_ix": "470-ARR_v1_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_72",
            "tgt_ix": "470-ARR_v1_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_72",
            "tgt_ix": "470-ARR_v1_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_72",
            "tgt_ix": "470-ARR_v1_72@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_73",
            "tgt_ix": "470-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_74",
            "tgt_ix": "470-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_75",
            "tgt_ix": "470-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_75",
            "tgt_ix": "470-ARR_v1_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_75",
            "tgt_ix": "470-ARR_v1_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_75",
            "tgt_ix": "470-ARR_v1_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_75",
            "tgt_ix": "470-ARR_v1_75@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_75",
            "tgt_ix": "470-ARR_v1_75@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_75",
            "tgt_ix": "470-ARR_v1_75@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_75",
            "tgt_ix": "470-ARR_v1_75@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_75",
            "tgt_ix": "470-ARR_v1_75@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_75",
            "tgt_ix": "470-ARR_v1_75@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_75",
            "tgt_ix": "470-ARR_v1_75@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_75",
            "tgt_ix": "470-ARR_v1_75@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_76",
            "tgt_ix": "470-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_76",
            "tgt_ix": "470-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_76",
            "tgt_ix": "470-ARR_v1_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_76",
            "tgt_ix": "470-ARR_v1_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_76",
            "tgt_ix": "470-ARR_v1_76@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_76",
            "tgt_ix": "470-ARR_v1_76@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_76",
            "tgt_ix": "470-ARR_v1_76@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_76",
            "tgt_ix": "470-ARR_v1_76@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_77",
            "tgt_ix": "470-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_78",
            "tgt_ix": "470-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_78",
            "tgt_ix": "470-ARR_v1_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_78",
            "tgt_ix": "470-ARR_v1_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_78",
            "tgt_ix": "470-ARR_v1_78@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_78",
            "tgt_ix": "470-ARR_v1_78@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_78",
            "tgt_ix": "470-ARR_v1_78@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_78",
            "tgt_ix": "470-ARR_v1_78@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_79",
            "tgt_ix": "470-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_80",
            "tgt_ix": "470-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_80",
            "tgt_ix": "470-ARR_v1_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_80",
            "tgt_ix": "470-ARR_v1_80@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_80",
            "tgt_ix": "470-ARR_v1_80@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_80",
            "tgt_ix": "470-ARR_v1_80@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_81",
            "tgt_ix": "470-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_82",
            "tgt_ix": "470-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_82",
            "tgt_ix": "470-ARR_v1_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_82",
            "tgt_ix": "470-ARR_v1_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_83",
            "tgt_ix": "470-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_83",
            "tgt_ix": "470-ARR_v1_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_84",
            "tgt_ix": "470-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_85",
            "tgt_ix": "470-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_85",
            "tgt_ix": "470-ARR_v1_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_86",
            "tgt_ix": "470-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_87",
            "tgt_ix": "470-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_88",
            "tgt_ix": "470-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_88",
            "tgt_ix": "470-ARR_v1_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_88",
            "tgt_ix": "470-ARR_v1_88@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_89",
            "tgt_ix": "470-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_89",
            "tgt_ix": "470-ARR_v1_89@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_89",
            "tgt_ix": "470-ARR_v1_89@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_90",
            "tgt_ix": "470-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_91",
            "tgt_ix": "470-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_92",
            "tgt_ix": "470-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_93",
            "tgt_ix": "470-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_94",
            "tgt_ix": "470-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_95",
            "tgt_ix": "470-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_96",
            "tgt_ix": "470-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_97",
            "tgt_ix": "470-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_98",
            "tgt_ix": "470-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_99",
            "tgt_ix": "470-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_100",
            "tgt_ix": "470-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_101",
            "tgt_ix": "470-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_102",
            "tgt_ix": "470-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_103",
            "tgt_ix": "470-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_104",
            "tgt_ix": "470-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_105",
            "tgt_ix": "470-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_106",
            "tgt_ix": "470-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_107",
            "tgt_ix": "470-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_108",
            "tgt_ix": "470-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_109",
            "tgt_ix": "470-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_110",
            "tgt_ix": "470-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_111",
            "tgt_ix": "470-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_112",
            "tgt_ix": "470-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_113",
            "tgt_ix": "470-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_114",
            "tgt_ix": "470-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_115",
            "tgt_ix": "470-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_116",
            "tgt_ix": "470-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_117",
            "tgt_ix": "470-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_118",
            "tgt_ix": "470-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_119",
            "tgt_ix": "470-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_120",
            "tgt_ix": "470-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_121",
            "tgt_ix": "470-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_122",
            "tgt_ix": "470-ARR_v1_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_123",
            "tgt_ix": "470-ARR_v1_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_124",
            "tgt_ix": "470-ARR_v1_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_125",
            "tgt_ix": "470-ARR_v1_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_126",
            "tgt_ix": "470-ARR_v1_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_127",
            "tgt_ix": "470-ARR_v1_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_128",
            "tgt_ix": "470-ARR_v1_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_129",
            "tgt_ix": "470-ARR_v1_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_130",
            "tgt_ix": "470-ARR_v1_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_131",
            "tgt_ix": "470-ARR_v1_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "470-ARR_v1_132",
            "tgt_ix": "470-ARR_v1_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1425,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "470-ARR",
        "version": 1
    }
}