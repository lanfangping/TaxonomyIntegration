{
    "nodes": [
        {
            "ix": "331-ARR_v2_0",
            "content": "Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_2",
            "content": "Massively Multilingual Transformer based Language Models have been observed to be surprisingly effective on zero-shot transfer across languages, though the performance varies from language to language depending on the pivot language(s) used for fine-tuning. In this work, we build upon some of the existing techniques for predicting the zero-shot performance on a task, by modeling it as a multi-task learning problem. We jointly train predictive models for different tasks which helps us build more accurate predictors for tasks where we have test data in very few languages to measure the actual performance of the model. Our approach also lends us the ability to perform a much more robust feature selection, and identify a common set of features that influence zero-shot performance across a variety of tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "331-ARR_v2_4",
            "content": "Multilingual models like mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) have been recently shown to be surprisingly effective for zero-shot transfer (Pires et al., 2019) (Wu and Dredze, 2019), where on fine-tuning for a task on one or a few languages, called pivots, they can perform well on languages unseen during training. The zero-shot performance however, is often not uniform across the languages and the multilingual models turn out to be much less effective for low resource languages (Wu and Dredze, 2020;Lauscher et al., 2020) and the languages that are typologically distant from the pivots (Lauscher et al., 2020). What affects the zero-shot transfer across different languages is a subject of considerable interest and importance (K et al., 2020;Pires et al., 2019;Wu and Dredze, 2019;Lauscher et al., 2020), however there is little conclusive evidence and a few papers even show contradictory findings. Lauscher et al. (2020) recently, showed that it is possible to predict the zero shot performance of * Equal contribution mBERT and XLM-R on different languages by formulating it as a regression problem, with pretraining data size and typological similarities between the pivot and target languages as the input features, and the performance on downstream task as the prediction target. Along similar lines Srinivasan et al. (2021) and Dolicki and Spanakis (2021) explore zero-shot performance prediction with a larger set of features and different regression techniques.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_5",
            "content": "However, the efficacy of these solutions are severely limited by the lack of training data, that is, the number of languages for which performance metrics are available for a given task. For instance, for most tasks in the popular XTREME-R benchmark, there are data points for 7-11 languages. This not only makes zero-shot performance prediction a challenging problem, but also a very important one because for practical deployment of such multilingual models, one would ideally like to know its performance for all the languages the model is supposed to handle. As Srinivasan et al. (2021) shows, accurate performance predictors can also help us build better and fairer multilingual models by suggesting data labeling strategies.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_6",
            "content": "In this work, we propose multi-task learning (Zhang and Yang, 2017) as an approach to mitigate training-data constraints and consequent over-fitting of the performance predictors to tasks and/or datasets. The contributions of our work are fourfold. First, we experiment with different multitask learning approaches, such as Group Lasso (Yuan and Lin, 2006), Collective Matrix Factorization (Cortes, 2018), Multi-Task Deep Gaussian Process Regression (Bonilla et al., 2008) and Meta Agnostic Meta Learning (Finn et al., 2017) for 11 tasks. We observe an overall 10% reduction in performance prediction errors compared to the best performing single-task models. The gains are even stronger when we just consider the tasks with very few data points (\u2264 10), where we see a 20% drop in the mean absolute errors. Second, an interesting consequence of modelling this problem via multi-task learning is that we are able to predict performance on low resource languages much more accurately, where in some cases single-task approaches may perform even worse than the simple averaging baselines. Third, apart from the features used for zero-shot performance prediction in the previous work (Lauscher et al., 2020;Srinivasan et al., 2021;Dolicki and Spanakis, 2021), we also utilize metrics quantifying the quality of multilingual tokenizers as proposed in (Rust et al., 2021) as features in our predictive models, which turn out to have strong predictive power for certain tasks. To the best of our knowledge, our work is the first to explore the impact of tokenizer quality specifically on zero-shot transfer. And fourth, our multi-task framework in general lends us with a much more robust selection of features affecting the zero-shot performance. This, in turn, lets us investigate the critical open question on what influences the zero-shot performances across languages more rigorously. As we shall see, our findings corroborate some of the previous conclusions, while others are extended or annulled.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_7",
            "content": "Background and Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "331-ARR_v2_8",
            "content": "Zero Shot Transfer. Multilingual models like mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) have shown surprising effectiveness in zero-shot transfer, where fine-tuning the MMLM on a task in some source language often leads to impressive performance on the same task in other languages as well without explicitly training on them. Pires et al. (2019) first observed this phenomenon for NER (Tjong Kim Sang, 2002;Tjong Kim Sang and De Meulder, 2003;Levow, 2006) and POS tagging (Nivre et al., 2018) tasks. Concurrently, Wu and Dredze (2019) also showed this surprisingly cross lingual transfer ability of mBERT additionally on tasks like Document Classification (Schwenk and Li, 2018), Natural Language Inference (Conneau et al., 2018) and Dependency Parsing (Nivre et al., 2018). Factors Affecting Zero Shot Transfer. Pires et al. (2019) showed that vocabulary memorization played little role in zero-shot generalization as language pairs with little word piece overlap also exhibited impressive crosslingual performance. K et al. arrived at a similar conclusion by training BERT on an artificially generated language to zero out the word overlap with the target languages, and observed only minor drops in the performance compared to training the model on English. On the contrary Wu and Dredze (2019), observed strong correlations between the sub-word overlap and the zero-shot performance in four out of five tasks. Wu and Dredze (2020) showed that mBERT performed much worse for zero-shot transfer to low resource languages (i.e., less pre-training data) than high resource ones on POS Tagging, NER and Dependency Parsing tasks. Lauscher et al. (2020) also had a similar observation on tasks like XNLI and XQuAD (Artetxe et al., 2020), though they found that the zero-shot performance on NER, POS tagging and Dependency Parsing tasks might not strictly depend on the pre-training size and could be better explained by different linguistic relatedness features like syntactic and phonological similarities between the language pair. Similar dependence on the typological relatedness such as word order had also been observed by Pires et al. (2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_9",
            "content": "Performance Prediction. Prior work has explored predicting the performance of machine learning models from unlabelled data by either measuring (dis)agreements between multiple classifiers (Platanios et al., 2014(Platanios et al., , 2017 or by utilizing underlying information about data distribution (Domhan et al., 2015). In the context of NLP Birch et al. (2008) explored predicting the performance of a Machine Translation system by utilizing different explanatory variables for the language pairs. Lin et al. (2019) proposed a learning to rank approach to choose transfer languages for cross lingual learning using several linguistic and dataset specific features.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_10",
            "content": "Recently, there has been an interest in predicting the performance of NLP models without actually training or testing them, by formulating it as a regression problem. Xia et al. (2020) showed that using experimental settings for an NLP experiment as inputs it is possible to accurately predict the performance on different languages and model architectures. Ye et al. (2021) extended this work by proposing methods to do a fine-grained estimation of the performance as well as predicting well-callibrated confidence intervals. Specifically predicting the zero-shot performance of MMLMs was first explored in Lauscher et al. (2020), where they used a linear regression model to estimate the cross-lingual transfer performance based on pretraining data size and linguistic relatedness features. Srinivasan et al. (2021) tackled this problem by utilizing XGBoost Regressor for the prediction along with a larger set of features. Dolicki and Spanakis (2021) explored individual syntactic features for zero-shot performance prediction instead of working with aggregate similarity values, and showed about 2 to 4 times gain in performance. We extend all of these works by considering a multi-task learning approach, where performance prediction in a task utilizes not only the data available for that task, but also the patterns observed for other tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_11",
            "content": "Problem Setup",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "331-ARR_v2_12",
            "content": "We begin by defining the multi-task performance prediction problem and then describe the different linguistic and MMLM specific features used.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_13",
            "content": "Multi-Task Performance Prediction Problem",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "331-ARR_v2_14",
            "content": "Consider a pre-trained multilingual model M, trained using self supervision on a set of languages L. Let T be the set of downstream NLP tasks, P be the set of pivot (source) languages for which training data is available for the downstream tasks for fine-tuning and T be the set of target languages for which validation/test data is available. Note that P \u2282 L and T \u2286 L. We use the zero-shot setting similar to Lauscher et al. (2020) which enforces P and T to be disjoint sets 1 , i.e., P \u2229 T = \u2205.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_15",
            "content": "We then define y M,t p,t \u2208 R as the zero-shot performance on language t \u2208 T on finetuning M on task t \u2208 T in pivot language p \u2208 P. Let x M p,t \u2208 R n be the n-dimensional feature vector representing the corresponding train-test configuration. Since for our experiments we train and evaluate the performance prediction for a single model at a time, we will simplify the notations to y t p,t and x p,t . The predictor model can then be defined as the function f \u0398,\u03a6 : R n \u00d7 T \u2192 R, where \u0398 \u2208 R dg denotes the shared parameters across the tasks and the task specific parameters are given by \u03a6 \u2208 R ds\u00d7|T| . The objective function for training such a predictor model can be defined as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_16",
            "content": "J(\u0398, \u03a6) = t\u2208T p\u2208P t\u2208T \u2225f (x p,t , t; \u0398, \u03a6) \u2212 y t p,t \u2225 2 2 + \u03bb g \u2225\u0398\u2225 1 + \u03bb s \u2225\u03a6\u2225 1,1 + \u03bb group \u2225\u03a6\u2225 1,q(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_17",
            "content": "The second and third terms regularize the global and task specific parameters independently, while the last term, l 1 /l q norm with q > 1, ensures a block sparse selection of the task specific parameters. This term ensures a multi-task learning behavior even when there are no parameters shared across the tasks (i.e., \u0398 = \u2205) through selection of common features across the tasks. Setting \u0398 = \u2205 and \u03bb group = 0 leads to the single task setup of Lauscher et al. (2020) and Srinivasan et al. (2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_18",
            "content": "Features",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "331-ARR_v2_19",
            "content": "We divide the set of features into two higher level categories, viz. the pairwise features defined for the pivot and target that measure the typological relatedness of the languages, and the individual features defined for the target language reflecting the state of its representation in M.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_20",
            "content": "Pairwise Features",
            "ntype": "title",
            "meta": {
                "section": "3.2.1"
            }
        },
        {
            "ix": "331-ARR_v2_21",
            "content": "Instead of directly using the different typological properties of the the two languages as features, we use the pairwise relatedness to avoid feature explosion. Subword Overlap : We define the subword overlap as the percentage of unique tokens that are common to the vocabularies of both the pivot and target languages. Let V p and V t be the subword vocabularies of p and t. The subword overlap is then defined as :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_22",
            "content": "o sw (p, t) = |V p \u2229 V t | |V p \u222a V t |(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_23",
            "content": "Similarity between Lang2Vec vectors: Following Lin et al. (2019) and Lauscher et al. (2020), we compute the typological relatedness between p and t from the linguistic features provided by the URIEL project (Littell et al., 2017). We use syntactic (s syn (p, t)), phonological similarity (s pho (p, t)), genetic similarity (s gen (p, t)) and geographic distance (d geo (p, t)). For details, please see Littell et al. (2017).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_24",
            "content": "Individual Features",
            "ntype": "title",
            "meta": {
                "section": "3.2.2"
            }
        },
        {
            "ix": "331-ARR_v2_25",
            "content": "Pre-training Size: We use the log 10 of the size (in words) of the pre-training corpus in the target language, SIZE(t), as a feature. Rare Typological Traits: Srinivasan et al. (2021) proposed this metric to capture the rarity of the typological features of a language in the representation of M. Every typological feature in WALS database is ranked based on the amount of pretraining data for the languages that contain the feature. For the language t, Mean Reciprocal Rank (MRR) of all of its features is then calculated and used as a feature -WMRR(t).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_26",
            "content": "Tokenizer Features : In their recent work, Rust et al. (2021) proposed two metrics, viz. tokenizer's fertility and proportion of continued words, to evaluate the quality of multilingual tokenizers on a given language. For target t, they define the tokenizer's fertility, FERT(t), as the average number of sub-words produced for every tokenized word in t's corpus. On the other hand, the proportion of continued words, PCW(t), measures how often the tokenizer chooses to continue a word across at least two tokens. They show that the multilingual models perform much worse on a task than their monolingual counterparts when the values of these metrics are higher for the multilingual tokenizer. We include FERT(t) and PCW(t) as features.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_27",
            "content": "An important thing to note here is that the we do not use identity of a language as a feature while training the models, hence the performance prediction models are capable of generating predictions on new languages unseen during training. However, if the features of the new languages deviate significantly from the features seen during training, the predictions are expected to be less accurate as also observed in Xia et al. (2020); Srinivasan et al. (2021) and is one of the main reasons for exploring a multi-task approach.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_28",
            "content": "Approaches",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "331-ARR_v2_29",
            "content": "We extensively experiment with a wide-array of multi-task as well as single-task regression models to provide a fair comparison between different approaches to zero-shot performance prediction.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_30",
            "content": "Baselines",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "331-ARR_v2_31",
            "content": "Average Score Within a Task (AWT) : The performance for a pivot-target pair (p , t) on a task t is approximated by taking the average of the performance on all other target languages (pivot being fixed) in the same task t, i.e., f (x p,t , t) =",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_32",
            "content": "1 |T |\u22121 t \u2032 \u2208T \u2212{t} y t p,t \u2032 .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_33",
            "content": "Average Score across the Tasks (AAT) : Here instead of averaging over all the target languages within a task, we approximate the performance on a given target language by averaging the scores for that language across the other tasks, i.e.,",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_34",
            "content": "f (x p,t , t) = 1 |T|\u22121 t \u2032 \u2208T\u2212{t} y t \u2032 p,t .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_35",
            "content": "Single Task Models",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "331-ARR_v2_36",
            "content": "Lasso Regression: Lauscher et al. ( 2020) train different linear regression models for each task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_37",
            "content": "Along similar lines, we experiment with linear regression, but also add an L1 regularization term, as we observed it usually leads to better predictors.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_38",
            "content": "XGBoost Regressor: As shown in Srinivasan et al. (2021), XGBoost (Chen and Guestrin, 2016) generally obtains impressive performance on this task, and hence we include it in our experiments as well.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_39",
            "content": "Multi Task Models",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "331-ARR_v2_40",
            "content": "Group Lasso: l 1 /l q norm based blockregularization has been shown to be effective for multi-task learning in the setting of multi-linear regression (Yuan and Lin, 2006;Argyriou et al., 2008). For each task, consider separate linear regression models represented by the weight matrix \u03a6 \u2208 R n\u00d7|T| . The l 1 /l q regularization term is given as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_41",
            "content": "\u2225\u03a6\u2225 1,q = n j=1 ( |T| t=1 |\u03a6 jt | q ) 1/q",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_42",
            "content": ", where \u03a6 jt denotes the weight for the feature j in the task t. For q > 1, minimizing this term pushes the l q -norms corresponding to the weights of a given feature across the tasks to be sparse, which encourages multiple predictors to share similar sparsity patterns. In other words, a common set of features is selected for all the tasks. We use q = 2 for the group regularization term.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_43",
            "content": "Since this can be restrictive in certain scenarios, some natural extensions to Group Lasso, such as Dirty Models (Jalali et al., 2010) and Multi Level Lasso (Lozano and Swirszcz, 2012), have been proposed that separate out the task specific and global parameters. We experimented with these methods and observed equivalent or worse performance compared to Group Lasso. Collective Matrix Factorization (CMF) with Side Information: Low rank approximation for the task weights matrices forms one family of methods for multi-task learning (Zhang and Yang, 2017;Pong et al., 2010;Ando et al., 2005). As a direct analogue with collaborative filtering, here we can think of the tasks as users and pivot-target pairs as items. Consider the matrix Y \u2208 R |T|\u00d7|P\u00d7T | , where each element of the matrix correspond to y t p,t . We can then decompose the matrix into task and language-pair specific factors as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_44",
            "content": "Y \u223c TL T (3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_45",
            "content": "where T \u2208 R |T|\u00d7d latent and L \u2208 R |P\u00d7T |\u00d7d latent are the task and language-pair factor matrices, and d latent is the number of factors. Additionally, in order to incorporate the feature information about the language pairs as discussed in section 3.2, we incorporate Collective Matrix Factorization approach (Cortes, 2018). It incorporates the attribute information about items and/or users in the factorization algorithm by decomposing the language-pair feature matrix X \u2208 R |P\u00d7T |\u00d7n as LF T , such that L is shared across both decompositions. This helps to learn the latent representations for the pivot-language pairs from the task-wise performance as well as different linguistic and MMLM specific features 2 . In relation to Equation 1, we can think of task factors T to correspond to the task specific parameters \u03a6, languagepair factors L as the shared parameters \u0398 and the predictor model as f (x p,t , t; \u0398, \u03a6) = (TL T ) (p,t),t . Both L and T are regularized seperately, but there is no group regularization term (\u03bb group = 0).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_46",
            "content": "Ye et al. ( 2021) also uses a Tensor Factorization approach for performance prediction which is similar to our CMF method. However, they train separate models for each task and factorize over metric specific attributes instead for a fine-grained prediction.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_47",
            "content": "We use the multi-task variant of Gaussian Processes proposed in Bonilla et al. (2008) and utilize deep neural networks to define the kernel functions as in Deep GPs (Wilson et al., 2016). For comparison, we also report the scores of the single-task variant of this method which we denote as DGPR. See Appendix (section A.1) for details.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_48",
            "content": "Apart from these we also explore other multitask methods like Model Agnostic Meta Learning (MAML) (Finn et al., 2017), details of which we leave in the appendix (section A.1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_49",
            "content": "Experimental Setup",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "331-ARR_v2_50",
            "content": "In this section, we discuss our test conditions, datasets and training parameters for the different experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_51",
            "content": "Test Conditions",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "331-ARR_v2_52",
            "content": "We consider two different test conditions: Leave One Language Out (LOLO) and Leave Low Resource Languages Out (LLRO).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_53",
            "content": "Leave One Language Out: LOLO is a popular setup for multilingual performance prediction (Lauscher et al., 2020;Srinivasan et al., 2021), where for a given task, we choose a target language and move all of its instances from the prediction dataset to the test data. The models are then trained on the remaining languages and evaluated on the unseen test language. This is done for all the target languages available for a task, and the Mean Absolute Error (MAE) across languages is reported. In the multi-task setting we evaluate on one task at a time while considering the rest as helper tasks for which the entire data is used including the test language 3 . Leave Low Resource Languages Out: Through this evaluation strategy we try to emulate the real world use case where we only have test data available in high resource languages such as English, German and Chinese, and would like to estimate the performance on under-represented languages such as Swahili and Bengali. We use the language taxonomy provided by Joshi et al. (2020) to categorize the languages into six classes (0 = low to 5 = high) based on the number of resources available. We then move languages belonging to class 3 or below to our test set and train the models on class 4 and 5 languages only. Similar to LOLO, here too we allow the helper tasks to retain all the languages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_54",
            "content": "Tasks and Datasets",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "331-ARR_v2_55",
            "content": "We use the following 11 tasks provided in XTREME (Hu et al., 2020) and XTREME-R benchmarks: 1. Classification: XNLI (Conneau et al., 2018) , PAWS-X (Yang et al., 2019), andXCOPA (Ponti et al., 2020) 2. Structure Prediction: UDPOS (Nivre et al., 2018), and NER (Pan et al., 2017) 3. Question Answering: XQUAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), and TyDiQA-GoldP (Clark et al., 2020) 4. Retrieval: Tatoeba (Artetxe and Schwenk, 2019), Mewsli-X (Botha et al., 2020;, and LAReQA (Roy et al., 2020) All of these datasets have training data present only in English i.e. P = {en}, and majority of the tasks have fewer than 10 target languages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_56",
            "content": "Training Details",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "331-ARR_v2_57",
            "content": "We train and evaluate our performance prediction models for mBERT (bert-base-multilingual-cased) and XLM-R (xlm-roberta-large). For training XG-Boost, we used 100 estimators with a maximum depth of 10. For Group Lasso, we used the implementation provided in the MuTaR software package 4 , and used a regularization strength of 0.01.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_58",
            "content": "We optimized CMF's objective function using Alternating Least Squares (ALS), used 5 latent factors with a regularization parameter equal to 0.1, and used the Collective Matrix Factorization python library 5 . In case of MDGPR, we used Radial Basis Function as the kernel and a two-layer MLP for learning latent features, with 50 and 10 units followed by ReLU activation. We set the learning rate and epochs as 0.01 and 200, and implemented it using GPyTorch 6 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_59",
            "content": "6 Results and Discussion",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_60",
            "content": "LOLO Results",
            "ntype": "title",
            "meta": {
                "section": "6.1"
            }
        },
        {
            "ix": "331-ARR_v2_61",
            "content": "Table 1 shows MAE (in %) for LOLO for different single-task and multi-task models on the tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_62",
            "content": "For XLMR, we observe that multi-task models, primarily MDGPR, often outperform the best singletask models by significant margins, and for tasks like MewsliX we even see about 36% reduction in MAE. Overall, we see about 10% drop in LOLO errors on average for MDGPR compared to the best performing single-task model i.e. Lasso Regression. As expected, the benefit of multi-task learning is even more prominent when we consider the tasks for which only a few (\u2264 10) data points are available. Here we see about 20% reduction in errors. For mBERT as well, we have similar observations, except that CMF performs slightly better than MDGPR. Note that the Average across task baseline is quite competitive and performs better than singletask XGBoost and MAML in average, and better than all models for LAReQA.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_63",
            "content": "Figure 2 plots the dependence of the number of helper tasks on the performance of the multi-task models. As expected, MAE decreases as helper tasks increase, especially for MDGPR and CMF. On a related note, the Pearson Correlation coefficient between MAE and number of tasks a target language is part of is found to be \u22120.39, though the trend in this case is not as clear.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_64",
            "content": "LLRO Results",
            "ntype": "title",
            "meta": {
                "section": "6.2"
            }
        },
        {
            "ix": "331-ARR_v2_65",
            "content": "Predicting the performance on low resource languages, for which often standard training and test datasets are not available, can be an important use case where multi-task performance prediction can be helpful. Figure 6 in appendix shows the classwise (Joshi et al., 2020) distribution of languages for the tasks that we consider in our experiments. As one would expect, for most tasks, test data is available for languages belonging to class-4 and class-5. Training performance prediction models without any task to transfer from can therefore, possibly lead to poor generalization on the low resource languages. On the other hand, for the same reason -lack of test data, building accurate predictors for low-resource languages is necessary. MAE values for the LLRO evaluation setup are shown in figure 1 for XLMR. Results for mBERT follow similar trends and are reported in the Appendix (figure 7). For both XLMR and mBERT we observe that the three main multi-task models -Group Lasso, CMF and MDGPR -outperform the single-task models and baselines. Interestingly, for XLMR, the single task models XGBoost and Lasso perform even worse than the Average within Tasks baseline. Overall we see around 18% and 11% drop in MAE for Group Lasso over the best performing single-task model, for XLMR and mBERT respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_66",
            "content": "Feature Importance",
            "ntype": "title",
            "meta": {
                "section": "6.3"
            }
        },
        {
            "ix": "331-ARR_v2_67",
            "content": "An interesting consequence of zero-shot performance prediction is that the models can be directly used to infer the correlation (and possibly causation) between linguistic relatedness and pretraining conditions and zero-shot transferability. Multi-task learning, in this context, help us make more robust inferences, as the models are less prone to overfitting to a particular or dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_68",
            "content": "Figure 3 shows the SHAP values of the features for the Group Lasso model trained on XLMR's zero-shot performance data. As expected for Group Lasso, we see a block-sparsity behavior among the tasks. Features such as Rare Typological Traits (WMRR(t)), Tokenizer's Fertility (FERT(t)) and Genetic Similarity (s gen (p, t)) are ignored in all the tasks. In contrast, for the single-task lasso regression (Figure 9 in Appendix), we see different sets of features selected for different tasks, which for the scale at which we operate, might not be indicative of the actual factors that affect the zero-shot performance in these tasks. Subword Overlap. Among the features that get selected for all tasks, we observe that Subword Overlap (o sw (p, t)) typically gets higher importance in retrieval (LAReQA and MewsliX) and sentence classification tasks (PAWS-X, XNLI). Since the retrieval tasks that we consider, as described in , measure the alignment between the cross lingual representations of semantically similar sentences, having a shared vocabulary between the languages can leak information from one to another (Wu and Dredze, 2019) which might improve the retrieval performance. Interestingly, if we compare this with the feature importance scores for the single task lasso model (Figure 9 in Appendix), we do see MewsliX task getting higher importance for the subword overlap, but LAReQA gets virtually zero SHAP value for this feature, showcasing how single-task models can misinterpret two similar tasks as requiring very different features. Our observation reinforce the generally held notion that vocabulary overlap between the pivot and target is beneficial for zero-shot transfer (Wu and Dredze, 2019), especially for retrieval tasks, though some studies have argued otherwise (Pires et al., 2019;K et al., 2020). Tokenizer Features. For structure prediction (UDPOS and WikiAnn) and question answering (XQUAD and TyDiQA) tasks that require making predictions for each token in the input, we see that the tokenizer feature, PCW(t), receive a higher SHAP value. In contrast, for single-task lasso, here too we do not observe high importance of this feature across these related tasks. Rust et al. (2021) note that languages such as Arabic where mBERT's multilingual tokenizer was found to be much worse than it's monolingual counterpart, there was a sharper drop in performance of mBERT compared to the monolingual model for QA, UDPOS and NER tasks than for sentiment classification. We believe that XLMR's surprisingly worse performance than mBERT for Chinese and Japanese UDPOS might be correlated with it's significantly worse tokenizer for these languages based on the fertility (FERT) and Percentage Continued Words (PCW) feature values (see Appendix A.2 for exact values). The high SHAP values for PCW(t) further strengthen our belief 7 . Pre-training Size. Similar to the findings of Lauscher et al. (2020), we observe that pre-training corpus size has low SHAP value, and therefore, lower importance for lower level tasks such as UDPOS and NER, and higher SHAP values for higher level tasks like XNLI. Additionally, we extend their observations to tasks such as XCOPA, Tatoeba, MLQA and LAReQA where pre-training size seem to play a significant role in the performance prediction. Again, compared to single Lasso Regression model, we see a different selection pattern: Pre-training size receives a high SHAP value for UDPOS while for XNLI it is negligible. This neither fully conforms with our observations on the multi-task feature selections, nor with the previous work (Lauscher et al., 2020). Typological Relatedness Features. Out of all the typological relatedness features, we found Geographical Distance (d geo (p, t)) receiving highest SHAP values for all tasks, implying that geographical proximity between the pivot-target pair is an important factor in determining the zero-shot transferability between them. Lauscher et al. (2020) also observe positive correlations between geographical relatedness and zero-shot performance. The crosstask importance of geographic distance (unlike the other relatedness features) might be attributed to the 100% coverage across languages for the geographical vectors in the URIEL database. In contrast, Syntactic and Phonological vectors have missing values for a majority of the languages (Littell et al., 2017). Like Lauscher et al. (2020), we also see some dependence on syntactic (s syn (p, t)) and phonological (s pho (p, t)) similarities for XLMR's zero shot performance on XNLI and XQUAD tasks respectively. However, in both cases we found that the tokenizer feature PCW(t) receives a much higher SHAP value. Interestingly, genetic similarity (s gen (p, t)) is not selected for any task, arguably due to the block sparsity in feature selection of Group Lasso. We do see some tasks receiving high SHAP values for s gen (p, t) in single-task lasso (Figure 9 in Appendix). However, the number of such tasks as well as the SHAP values are on the lower side, implying that genetic similarity might not provide any additional information for zero-shot transfer over and above the geographical, syntactic and phonological similarities.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_69",
            "content": "Similar trends are observed in the case of mBERT as well (Figure 10 in appendix), with some minor differences. For instance, instead of PCW(t), FERT(t) receives higher SHAP value; s syn (p, t) also receives higher importance, especially for tasks like UDPOS and XNLI, which is consistent with the findings of Lauscher et al. (2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_70",
            "content": "Conclusion and Future Work",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "331-ARR_v2_71",
            "content": "In this paper, we showed that the zero-shot performance prediction problem can be much more effectively and robustly solved by using multi-task learning approaches. We see significant reduction in errors compared to the baselines and single-task models, specifically for the tasks which have test sets available in a very few languages or when trying to predict the performance for low resource languages. Additionally, this approach allows us to robustly identify factors that influence zero-shot performance. Our findings in this context can be summarized as follows.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_72",
            "content": "1. Subword overlap between the pivot and target has a strong positive influence on zero-shot transfer, especially for Retrieval tasks. 2. Quality of the target tokenizer, defined in terms of how often or how aggressively it splits the target tokens negatively influences zero-shot performance for word-level tasks such as POS tagging and Span extraction. 3. Pre-training size of the target positively influences zero-shot performance in many tasks, including XCOPA, Tatoeba, MLQA and LAReQA. 4. Geographical proximity between pivot and target is found to be uniformly important across all the tasks, unlike syntactic and phonological similarities, which are important for only some tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_73",
            "content": "This last finding is especially interesting. As described earlier, geographical proximity is a more clear, noise-free and complete feature compared to the other relatedness However, one could also argue that since neighboring languages tend to have high vocabulary and typological feature overlap due to contact processes and shared areal features, geographical distance is an extremely informative feature for zero-shot transfer. Two direct implications of these findings are: (1) for effective use of MMLMs, one should develop resources in at least one pivot language per geographic regions, and (2) one should work towards multilingual tokenizers that are effective for most languages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_74",
            "content": "There are a number of directions that can be explored in future related to our work. The prediction models can be extended to a multi-pivot and few-shot settings, as described in Srinivasan et al. (2021). Further probing experiments could be designed to understand the role of sub-word overlap on zero-shot transfer of Retrieval tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_75",
            "content": "where inter-task similarities are learnt solely based on the task identities and the observed data for each task. Instead of learning task-specific kernels k t (g(x p,t ), g(x p \u2032 ,t \u2032 )), we will have a common kernel over the inputs as k(g(x p,t ), g(x p \u2032 ,t \u2032 )) and a positive semi-definite matrix K task for learning inter-task similarities. Specifically, we define the multi-task kernel K m as follows",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_76",
            "content": "k m ([x p,t , t], [x p \u2032 ,t \u2032 , t \u2032 ]) = k(g(x p,t ), g(x p \u2032 ,t \u2032 )) * k task (t, t \u2032 ) (6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_77",
            "content": "The GP prior will be defined by replacing the task specific kernel K t in the equation 4 with the multi-task kernel K m . We use the optimization steps similar to DGP and the inference is done by using the standard GP formulae.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_78",
            "content": "Relating MDGPR to equation 1, the global parameters \u0398 are the parameters of the deep network g, and the task specific parameter \u03a6 is the positive semi-definite matrix K task . Model Agnostic Meta Learning (MAML): MAML (Finn et al., 2017) is a popular meta learning algorithm that can be used to quickly adapt Deep Neural Networks on new tasks in a few-shot setting. In MAML, the set of initialization parameters for the neural network are explicitly learned such that the network can generalize well on a new task with a small number of gradient steps and training samples.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_79",
            "content": "Relating to equation 1, the global parameters \u0398 can be considered as the initial set of parameters for the neural network that are learned and shared across all the tasks. Task specific parameters \u03a6 are adapted from \u0398 by taking K gradient steps using the task's performance data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_80",
            "content": "For evaluating a task t, we consider rest of the tasks in our dataset as helpers (t \u2032 \u2208 T\u2212{t}) and use them to train the initial set of parameters \u0398. The initial parameters are then updated by fine-tuning the network on the training set for t using gradient descent.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_81",
            "content": "The FERT and PCW metrics as proposed by Rust et al. (2021), have been compared for mBERT and XLMR in figure 4. As can be seen, for most languages the metric values are similar across the two tokenizers, however for languages like Chinese and Japanese, there is a dramatic increase in the values for XLMR. Interestingly, when we compare the zero-shot performance between mBERT and XLMR on structure prediction tasks like UD-POS and WikiANN, we see a surprisingly large drop (upto 20% absolute drop) in the performance for XLMR on these both Chinese and Japanese, whereas usually XLMR outperforms mBERT on these tasks (Refer to figure 5). This observation along with the feature importance for the tokenizer features that we observed for Group Lasso (3) indicate that tokenizer quality might play some role in the zero-shot transfer capabilities of the multilingual models. Table 2: Mean Absolute Errors (Scaled by 100 for readability) for different models trained to predict the zero shot performance of mBERT. In the \"Average\" row we average the MAEs across all the tasks and in the \"Average Low\" Res Tasks\", we consider the tasks with fewer than 10 target languages and take the average of the MAEs for those tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "331-ARR_v2_82",
            "content": "Tong Rie Kubota Ando, Peter Zhang,  Bartlett, A framework for learning predictive structures from multiple tasks and unlabeled data, 2005, Journal of Machine Learning Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Tong Rie Kubota Ando",
                    "Peter Zhang",
                    " Bartlett"
                ],
                "title": "A framework for learning predictive structures from multiple tasks and unlabeled data",
                "pub_date": "2005",
                "pub_title": "Journal of Machine Learning Research",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_83",
            "content": "Andreas Argyriou, Theodoros Evgeniou, Massimiliano Pontil, Convex multi-task feature learning, 2008, Machine learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Andreas Argyriou",
                    "Theodoros Evgeniou",
                    "Massimiliano Pontil"
                ],
                "title": "Convex multi-task feature learning",
                "pub_date": "2008",
                "pub_title": "Machine learning",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_84",
            "content": "Mikel Artetxe, Sebastian Ruder, Dani Yogatama, On the Cross-lingual Transferability of Monolingual Representations, 2020, Proceedings of ACL 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Mikel Artetxe",
                    "Sebastian Ruder",
                    "Dani Yogatama"
                ],
                "title": "On the Cross-lingual Transferability of Monolingual Representations",
                "pub_date": "2020",
                "pub_title": "Proceedings of ACL 2020",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_85",
            "content": "Mikel Artetxe, Holger Schwenk, Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond, 2019, Transactions of the ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Mikel Artetxe",
                    "Holger Schwenk"
                ],
                "title": "Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond",
                "pub_date": "2019",
                "pub_title": "Transactions of the ACL",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_86",
            "content": "Alexandra Birch, Miles Osborne, Philipp Koehn, Predicting success in machine translation, 2008, Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Alexandra Birch",
                    "Miles Osborne",
                    "Philipp Koehn"
                ],
                "title": "Predicting success in machine translation",
                "pub_date": "2008",
                "pub_title": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_87",
            "content": "V Edwin, Kian Bonilla, Christopher Chai,  Williams, Multi-task gaussian process prediction, 2008, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "V Edwin",
                    "Kian Bonilla",
                    "Christopher Chai",
                    " Williams"
                ],
                "title": "Multi-task gaussian process prediction",
                "pub_date": "2008",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "331-ARR_v2_88",
            "content": "Jan Botha, Zifei Shan, Daniel Gillick, Entity Linking in 100 Languages, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Jan Botha",
                    "Zifei Shan",
                    "Daniel Gillick"
                ],
                "title": "Entity Linking in 100 Languages",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "331-ARR_v2_89",
            "content": "Tianqi Chen, Carlos Guestrin, Xgboost: A scalable tree boosting system, 2016, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16, Association for Computing Machinery.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Tianqi Chen",
                    "Carlos Guestrin"
                ],
                "title": "Xgboost: A scalable tree boosting system",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16",
                "pub": "Association for Computing Machinery"
            }
        },
        {
            "ix": "331-ARR_v2_90",
            "content": "Jonathan Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, Jennimaria Palomaki, TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages, 2020, Transactions of the Association of Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Jonathan Clark",
                    "Eunsol Choi",
                    "Michael Collins",
                    "Dan Garrette",
                    "Tom Kwiatkowski",
                    "Vitaly Nikolaev",
                    "Jennimaria Palomaki"
                ],
                "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages",
                "pub_date": "2020",
                "pub_title": "Transactions of the Association of Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_91",
            "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Unsupervised cross-lingual representation learning at scale, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Alexis Conneau",
                    "Kartikay Khandelwal",
                    "Naman Goyal",
                    "Vishrav Chaudhary",
                    "Guillaume Wenzek",
                    "Francisco Guzm\u00e1n",
                    "Edouard Grave",
                    "Myle Ott",
                    "Luke Zettlemoyer",
                    "Veselin Stoyanov"
                ],
                "title": "Unsupervised cross-lingual representation learning at scale",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_92",
            "content": "Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, Veselin Stoyanov, XNLI: Evaluating crosslingual sentence representations, 2018, Proceedings of EMNLP 2018, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Alexis Conneau",
                    "Ruty Rinott",
                    "Guillaume Lample",
                    "Adina Williams",
                    "Samuel Bowman",
                    "Holger Schwenk",
                    "Veselin Stoyanov"
                ],
                "title": "XNLI: Evaluating crosslingual sentence representations",
                "pub_date": "2018",
                "pub_title": "Proceedings of EMNLP 2018",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_93",
            "content": "UNKNOWN, None, 2018, Cold-start recommendations in collective matrix factorization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Cold-start recommendations in collective matrix factorization",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_94",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "331-ARR_v2_95",
            "content": "UNKNOWN, None, 2021, Analysing the impact of linguistic features on crosslingual transfer, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Analysing the impact of linguistic features on crosslingual transfer",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_96",
            "content": "Tobias Domhan, Jost Springenberg, Frank Hutter, Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves, 2015, Twenty-fourth international joint conference on artificial intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Tobias Domhan",
                    "Jost Springenberg",
                    "Frank Hutter"
                ],
                "title": "Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves",
                "pub_date": "2015",
                "pub_title": "Twenty-fourth international joint conference on artificial intelligence",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_97",
            "content": "Chelsea Finn, Pieter Abbeel, Sergey Levine, Model-agnostic meta-learning for fast adaptation of deep networks, 2017, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Chelsea Finn",
                    "Pieter Abbeel",
                    "Sergey Levine"
                ],
                "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
                "pub_date": "2017",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "331-ARR_v2_98",
            "content": "Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, Melvin Johnson, XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation, 2020, Proceedings of the 37th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Junjie Hu",
                    "Sebastian Ruder",
                    "Aditya Siddhant",
                    "Graham Neubig",
                    "Orhan Firat",
                    "Melvin Johnson"
                ],
                "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 37th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "331-ARR_v2_99",
            "content": "Ali Jalali, Sujay Sanghavi, Chao Ruan, Pradeep Ravikumar, A dirty model for multi-task learning, 2010, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Ali Jalali",
                    "Sujay Sanghavi",
                    "Chao Ruan",
                    "Pradeep Ravikumar"
                ],
                "title": "A dirty model for multi-task learning",
                "pub_date": "2010",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "331-ARR_v2_100",
            "content": "Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, Monojit Choudhury, The state and fate of linguistic diversity and inclusion in the NLP world, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Pratik Joshi",
                    "Sebastin Santy",
                    "Amar Budhiraja",
                    "Kalika Bali",
                    "Monojit Choudhury"
                ],
                "title": "The state and fate of linguistic diversity and inclusion in the NLP world",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "331-ARR_v2_101",
            "content": "K Karthikeyan, Zihan Wang, Stephen Mayhew, Dan Roth, Cross-lingual ability of multilingual bert: An empirical study, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "K Karthikeyan",
                    "Zihan Wang",
                    "Stephen Mayhew",
                    "Dan Roth"
                ],
                "title": "Cross-lingual ability of multilingual bert: An empirical study",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_102",
            "content": "Anne Lauscher, Vinit Ravishankar, Ivan Vuli\u0107, Goran Glava\u0161, From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Anne Lauscher",
                    "Vinit Ravishankar",
                    "Ivan Vuli\u0107",
                    "Goran Glava\u0161"
                ],
                "title": "From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "331-ARR_v2_103",
            "content": "Gina-Anne Levow, The third international Chinese language processing bakeoff: Word segmentation and named entity recognition, 2006, Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Gina-Anne Levow"
                ],
                "title": "The third international Chinese language processing bakeoff: Word segmentation and named entity recognition",
                "pub_date": "2006",
                "pub_title": "Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "331-ARR_v2_104",
            "content": "Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, Holger Schwenk, MLQA: Evaluating Cross-lingual Extractive Question Answering, 2020, Proceedings of ACL 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Patrick Lewis",
                    "Barlas Oguz",
                    "Ruty Rinott",
                    "Sebastian Riedel",
                    "Holger Schwenk"
                ],
                "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering",
                "pub_date": "2020",
                "pub_title": "Proceedings of ACL 2020",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_105",
            "content": "Yu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li, Yuyan Zhang, Mengzhou Xia, Shruti Rijhwani, Junxian He, Zhisong Zhang, Xuezhe Ma, Antonios Anastasopoulos, Patrick Littell, and Graham Neubig, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Yu-Hsiang Lin",
                    "Chian-Yu Chen",
                    "Jean Lee",
                    "Zirui Li",
                    "Yuyan Zhang",
                    "Mengzhou Xia",
                    "Shruti Rijhwani",
                    "Junxian He",
                    "Zhisong Zhang",
                    "Xuezhe Ma"
                ],
                "title": "Antonios Anastasopoulos, Patrick Littell, and Graham Neubig",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_106",
            "content": "Patrick Littell, David Mortensen, Ke Lin, Katherine Kairis, Carlisle Turner, Lori Levin, URIEL and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors, 2017, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Patrick Littell",
                    "David Mortensen",
                    "Ke Lin",
                    "Katherine Kairis",
                    "Carlisle Turner",
                    "Lori Levin"
                ],
                "title": "URIEL and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "331-ARR_v2_107",
            "content": "C Aurelie, Grzegorz Lozano,  Swirszcz, Multilevel lasso for sparse multi-task regression, 2012, Proceedings of the 29th International Coference on International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "C Aurelie",
                    "Grzegorz Lozano",
                    " Swirszcz"
                ],
                "title": "Multilevel lasso for sparse multi-task regression",
                "pub_date": "2012",
                "pub_title": "Proceedings of the 29th International Coference on International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_108",
            "content": "UNKNOWN, None, , , Mohammed Attia.",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": "Mohammed Attia"
            }
        },
        {
            "ix": "331-ARR_v2_109",
            "content": "Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, Heng Ji, Cross-lingual name tagging and linking for 282 languages, 2017, Proceedings of ACL 2017, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Xiaoman Pan",
                    "Boliang Zhang",
                    "Jonathan May",
                    "Joel Nothman",
                    "Kevin Knight",
                    "Heng Ji"
                ],
                "title": "Cross-lingual name tagging and linking for 282 languages",
                "pub_date": "2017",
                "pub_title": "Proceedings of ACL 2017",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_110",
            "content": "Telmo Pires, Eva Schlinger, Dan Garrette, How multilingual is multilingual BERT?, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Telmo Pires",
                    "Eva Schlinger",
                    "Dan Garrette"
                ],
                "title": "How multilingual is multilingual BERT?",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_111",
            "content": "Emmanouil Platanios, Hoifung Poon, M Tom, Eric Mitchell,  Horvitz, Estimating accuracy from unlabeled data: A probabilistic logic approach, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Emmanouil Platanios",
                    "Hoifung Poon",
                    "M Tom",
                    "Eric Mitchell",
                    " Horvitz"
                ],
                "title": "Estimating accuracy from unlabeled data: A probabilistic logic approach",
                "pub_date": "2017",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "331-ARR_v2_112",
            "content": "UNKNOWN, None, 2014, Estimating accuracy from unlabeled data, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": null,
                "title": null,
                "pub_date": "2014",
                "pub_title": "Estimating accuracy from unlabeled data",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_113",
            "content": "Paul Ting Kei Pong, Shuiwang Tseng, Jieping Ji,  Ye, Trace norm regularization: Reformulations, algorithms, and multi-task learning, 2010, SIAM Journal on Optimization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Paul Ting Kei Pong",
                    "Shuiwang Tseng",
                    "Jieping Ji",
                    " Ye"
                ],
                "title": "Trace norm regularization: Reformulations, algorithms, and multi-task learning",
                "pub_date": "2010",
                "pub_title": "SIAM Journal on Optimization",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_114",
            "content": "Goran Edoardo Maria Ponti, Olga Glava\u0161, Qianchu Majewska, Ivan Liu, Anna Vuli\u0107,  Korhonen, XCOPA: A multilingual dataset for causal commonsense reasoning, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Goran Edoardo Maria Ponti",
                    "Olga Glava\u0161",
                    "Qianchu Majewska",
                    "Ivan Liu",
                    "Anna Vuli\u0107",
                    " Korhonen"
                ],
                "title": "XCOPA: A multilingual dataset for causal commonsense reasoning",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_115",
            "content": "Uma Roy, Noah Constant, Rami Al-Rfou, Aditya Barua, Aaron Phillips, Yinfei Yang, LAReQA: Language-agnostic answer retrieval from a multilingual pool, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Uma Roy",
                    "Noah Constant",
                    "Rami Al-Rfou",
                    "Aditya Barua",
                    "Aaron Phillips",
                    "Yinfei Yang"
                ],
                "title": "LAReQA: Language-agnostic answer retrieval from a multilingual pool",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "331-ARR_v2_116",
            "content": "Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, Melvin Johnson, XTREME-R: Towards more challenging and nuanced multilingual evaluation, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Sebastian Ruder",
                    "Noah Constant",
                    "Jan Botha",
                    "Aditya Siddhant",
                    "Orhan Firat",
                    "Jinlan Fu",
                    "Pengfei Liu",
                    "Junjie Hu",
                    "Dan Garrette",
                    "Graham Neubig",
                    "Melvin Johnson"
                ],
                "title": "XTREME-R: Towards more challenging and nuanced multilingual evaluation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_117",
            "content": "Phillip Rust, Jonas Pfeiffer, Ivan Vuli\u0107, Sebastian Ruder, Iryna Gurevych, How good is your tokenizer? on the monolingual performance of multilingual language models, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Phillip Rust",
                    "Jonas Pfeiffer",
                    "Ivan Vuli\u0107",
                    "Sebastian Ruder",
                    "Iryna Gurevych"
                ],
                "title": "How good is your tokenizer? on the monolingual performance of multilingual language models",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "331-ARR_v2_118",
            "content": "Holger Schwenk, Xian Li, A corpus for multilingual document classification in eight languages, 2018, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Holger Schwenk",
                    "Xian Li"
                ],
                "title": "A corpus for multilingual document classification in eight languages",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_119",
            "content": "UNKNOWN, None, 2021, Sandipan Dandapat, Kalika Bali, and Monojit Choudhury, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Sandipan Dandapat, Kalika Bali, and Monojit Choudhury",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_120",
            "content": "Erik , Tjong Kim Sang, Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition, 2002, COLING-02: The 6th Conference on Natural Language Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Erik ",
                    "Tjong Kim Sang"
                ],
                "title": "Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition",
                "pub_date": "2002",
                "pub_title": "COLING-02: The 6th Conference on Natural Language Learning",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_121",
            "content": "Erik Tjong, Kim Sang, Fien De Meulder, Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition, 2003, Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Erik Tjong",
                    "Kim Sang",
                    "Fien De Meulder"
                ],
                "title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
                "pub_date": "2003",
                "pub_title": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_122",
            "content": "Zhiting Andrew Gordon Wilson, Ruslan Hu, Eric Salakhutdinov,  Xing, Deep kernel learning, 2016, Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Zhiting Andrew Gordon Wilson",
                    "Ruslan Hu",
                    "Eric Salakhutdinov",
                    " Xing"
                ],
                "title": "Deep kernel learning",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 19th International Conference on Artificial Intelligence and Statistics",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_123",
            "content": "Shijie Wu, Mark Dredze, Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Shijie Wu",
                    "Mark Dredze"
                ],
                "title": "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_124",
            "content": "Shijie Wu, Mark Dredze, Are all languages created equal in multilingual BERT?, 2020, Proceedings of the 5th Workshop on Representation Learning for NLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Shijie Wu",
                    "Mark Dredze"
                ],
                "title": "Are all languages created equal in multilingual BERT?",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 5th Workshop on Representation Learning for NLP",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_125",
            "content": "Mengzhou Xia, Antonios Anastasopoulos, Ruochen Xu, Yiming Yang, Graham Neubig, Predicting performance for natural language processing tasks, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Mengzhou Xia",
                    "Antonios Anastasopoulos",
                    "Ruochen Xu",
                    "Yiming Yang",
                    "Graham Neubig"
                ],
                "title": "Predicting performance for natural language processing tasks",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_126",
            "content": "Yinfei Yang, Yuan Zhang, Chris Tar, Jason Baldridge, PAWS-X: A cross-lingual adversarial dataset for paraphrase identification, 2019, Proceedings of EMNLP 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Yinfei Yang",
                    "Yuan Zhang",
                    "Chris Tar",
                    "Jason Baldridge"
                ],
                "title": "PAWS-X: A cross-lingual adversarial dataset for paraphrase identification",
                "pub_date": "2019",
                "pub_title": "Proceedings of EMNLP 2019",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_127",
            "content": "Zihuiwen Ye, Pengfei Liu, Jinlan Fu, Graham Neubig, Towards more fine-grained and reliable NLP performance prediction, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Zihuiwen Ye",
                    "Pengfei Liu",
                    "Jinlan Fu",
                    "Graham Neubig"
                ],
                "title": "Towards more fine-grained and reliable NLP performance prediction",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_128",
            "content": "Ming Yuan, Yi Lin, Model selection and estimation in regression with grouped variables, 2006, Journal of the Royal Statistical Society: Series B (Statistical Methodology), .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Ming Yuan",
                    "Yi Lin"
                ],
                "title": "Model selection and estimation in regression with grouped variables",
                "pub_date": "2006",
                "pub_title": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)",
                "pub": null
            }
        },
        {
            "ix": "331-ARR_v2_129",
            "content": "Yu Zhang, Qiang Yang, A survey on multitask learning, 2017, IEEE Transactions on Knowledge and Data Engineering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Yu Zhang",
                    "Qiang Yang"
                ],
                "title": "A survey on multitask learning",
                "pub_date": "2017",
                "pub_title": "IEEE Transactions on Knowledge and Data Engineering",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "331-ARR_v2_0@0",
            "content": "Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_0",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_2@0",
            "content": "Massively Multilingual Transformer based Language Models have been observed to be surprisingly effective on zero-shot transfer across languages, though the performance varies from language to language depending on the pivot language(s) used for fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_2",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_2@1",
            "content": "In this work, we build upon some of the existing techniques for predicting the zero-shot performance on a task, by modeling it as a multi-task learning problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_2",
            "start": 258,
            "end": 417,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_2@2",
            "content": "We jointly train predictive models for different tasks which helps us build more accurate predictors for tasks where we have test data in very few languages to measure the actual performance of the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_2",
            "start": 419,
            "end": 622,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_2@3",
            "content": "Our approach also lends us the ability to perform a much more robust feature selection, and identify a common set of features that influence zero-shot performance across a variety of tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_2",
            "start": 624,
            "end": 812,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_4@0",
            "content": "Multilingual models like mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) have been recently shown to be surprisingly effective for zero-shot transfer (Pires et al., 2019) (Wu and Dredze, 2019), where on fine-tuning for a task on one or a few languages, called pivots, they can perform well on languages unseen during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_4",
            "start": 0,
            "end": 338,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_4@1",
            "content": "The zero-shot performance however, is often not uniform across the languages and the multilingual models turn out to be much less effective for low resource languages (Wu and Dredze, 2020;Lauscher et al., 2020) and the languages that are typologically distant from the pivots (Lauscher et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_4",
            "start": 340,
            "end": 639,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_4@2",
            "content": "What affects the zero-shot transfer across different languages is a subject of considerable interest and importance (K et al., 2020;Pires et al., 2019;Wu and Dredze, 2019;Lauscher et al., 2020), however there is little conclusive evidence and a few papers even show contradictory findings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_4",
            "start": 641,
            "end": 929,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_4@3",
            "content": "Lauscher et al. (2020) recently, showed that it is possible to predict the zero shot performance of * Equal contribution mBERT and XLM-R on different languages by formulating it as a regression problem, with pretraining data size and typological similarities between the pivot and target languages as the input features, and the performance on downstream task as the prediction target.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_4",
            "start": 931,
            "end": 1315,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_4@4",
            "content": "Along similar lines Srinivasan et al. (2021) and Dolicki and Spanakis (2021) explore zero-shot performance prediction with a larger set of features and different regression techniques.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_4",
            "start": 1317,
            "end": 1500,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_5@0",
            "content": "However, the efficacy of these solutions are severely limited by the lack of training data, that is, the number of languages for which performance metrics are available for a given task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_5",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_5@1",
            "content": "For instance, for most tasks in the popular XTREME-R benchmark, there are data points for 7-11 languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_5",
            "start": 187,
            "end": 291,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_5@2",
            "content": "This not only makes zero-shot performance prediction a challenging problem, but also a very important one because for practical deployment of such multilingual models, one would ideally like to know its performance for all the languages the model is supposed to handle.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_5",
            "start": 293,
            "end": 561,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_5@3",
            "content": "As Srinivasan et al. (2021) shows, accurate performance predictors can also help us build better and fairer multilingual models by suggesting data labeling strategies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_5",
            "start": 563,
            "end": 729,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_6@0",
            "content": "In this work, we propose multi-task learning (Zhang and Yang, 2017) as an approach to mitigate training-data constraints and consequent over-fitting of the performance predictors to tasks and/or datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_6",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_6@1",
            "content": "The contributions of our work are fourfold.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_6",
            "start": 205,
            "end": 247,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_6@2",
            "content": "First, we experiment with different multitask learning approaches, such as Group Lasso (Yuan and Lin, 2006), Collective Matrix Factorization (Cortes, 2018), Multi-Task Deep Gaussian Process Regression (Bonilla et al., 2008) and Meta Agnostic Meta Learning (Finn et al., 2017) for 11 tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_6",
            "start": 249,
            "end": 537,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_6@3",
            "content": "We observe an overall 10% reduction in performance prediction errors compared to the best performing single-task models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_6",
            "start": 539,
            "end": 658,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_6@4",
            "content": "The gains are even stronger when we just consider the tasks with very few data points (\u2264 10), where we see a 20% drop in the mean absolute errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_6",
            "start": 660,
            "end": 805,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_6@5",
            "content": "Second, an interesting consequence of modelling this problem via multi-task learning is that we are able to predict performance on low resource languages much more accurately, where in some cases single-task approaches may perform even worse than the simple averaging baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_6",
            "start": 807,
            "end": 1084,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_6@6",
            "content": "Third, apart from the features used for zero-shot performance prediction in the previous work (Lauscher et al., 2020;Srinivasan et al., 2021;Dolicki and Spanakis, 2021), we also utilize metrics quantifying the quality of multilingual tokenizers as proposed in (Rust et al., 2021) as features in our predictive models, which turn out to have strong predictive power for certain tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_6",
            "start": 1086,
            "end": 1468,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_6@7",
            "content": "To the best of our knowledge, our work is the first to explore the impact of tokenizer quality specifically on zero-shot transfer. And fourth, our multi-task framework in general lends us with a much more robust selection of features affecting the zero-shot performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_6",
            "start": 1470,
            "end": 1739,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_6@8",
            "content": "This, in turn, lets us investigate the critical open question on what influences the zero-shot performances across languages more rigorously.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_6",
            "start": 1741,
            "end": 1881,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_6@9",
            "content": "As we shall see, our findings corroborate some of the previous conclusions, while others are extended or annulled.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_6",
            "start": 1883,
            "end": 1996,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_7@0",
            "content": "Background and Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_7",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_8@0",
            "content": "Zero Shot Transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_8",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_8@1",
            "content": "Multilingual models like mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) have shown surprising effectiveness in zero-shot transfer, where fine-tuning the MMLM on a task in some source language often leads to impressive performance on the same task in other languages as well without explicitly training on them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_8",
            "start": 20,
            "end": 343,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_8@2",
            "content": "Pires et al. (2019) first observed this phenomenon for NER (Tjong Kim Sang, 2002;Tjong Kim Sang and De Meulder, 2003;Levow, 2006) and POS tagging (Nivre et al., 2018) tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_8",
            "start": 345,
            "end": 517,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_8@3",
            "content": "Concurrently, Wu and Dredze (2019) also showed this surprisingly cross lingual transfer ability of mBERT additionally on tasks like Document Classification (Schwenk and Li, 2018), Natural Language Inference (Conneau et al., 2018) and Dependency Parsing (Nivre et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_8",
            "start": 519,
            "end": 792,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_8@4",
            "content": "Factors Affecting Zero Shot Transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_8",
            "start": 794,
            "end": 830,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_8@5",
            "content": "Pires et al. (2019) showed that vocabulary memorization played little role in zero-shot generalization as language pairs with little word piece overlap also exhibited impressive crosslingual performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_8",
            "start": 832,
            "end": 1034,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_8@6",
            "content": "K et al. arrived at a similar conclusion by training BERT on an artificially generated language to zero out the word overlap with the target languages, and observed only minor drops in the performance compared to training the model on English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_8",
            "start": 1036,
            "end": 1278,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_8@7",
            "content": "On the contrary Wu and Dredze (2019), observed strong correlations between the sub-word overlap and the zero-shot performance in four out of five tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_8",
            "start": 1280,
            "end": 1431,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_8@8",
            "content": "Wu and Dredze (2020) showed that mBERT performed much worse for zero-shot transfer to low resource languages (i.e., less pre-training data) than high resource ones on POS Tagging, NER and Dependency Parsing tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_8",
            "start": 1433,
            "end": 1645,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_8@9",
            "content": "Lauscher et al. (2020) also had a similar observation on tasks like XNLI and XQuAD (Artetxe et al., 2020), though they found that the zero-shot performance on NER, POS tagging and Dependency Parsing tasks might not strictly depend on the pre-training size and could be better explained by different linguistic relatedness features like syntactic and phonological similarities between the language pair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_8",
            "start": 1647,
            "end": 2048,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_8@10",
            "content": "Similar dependence on the typological relatedness such as word order had also been observed by Pires et al. (2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_8",
            "start": 2050,
            "end": 2164,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_9@0",
            "content": "Performance Prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_9",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_9@1",
            "content": "Prior work has explored predicting the performance of machine learning models from unlabelled data by either measuring (dis)agreements between multiple classifiers (Platanios et al., 2014(Platanios et al., , 2017 or by utilizing underlying information about data distribution (Domhan et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_9",
            "start": 24,
            "end": 321,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_9@2",
            "content": "In the context of NLP Birch et al. (2008) explored predicting the performance of a Machine Translation system by utilizing different explanatory variables for the language pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_9",
            "start": 323,
            "end": 500,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_9@3",
            "content": "Lin et al. (2019) proposed a learning to rank approach to choose transfer languages for cross lingual learning using several linguistic and dataset specific features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_9",
            "start": 502,
            "end": 667,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_10@0",
            "content": "Recently, there has been an interest in predicting the performance of NLP models without actually training or testing them, by formulating it as a regression problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_10",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_10@1",
            "content": "Xia et al. (2020) showed that using experimental settings for an NLP experiment as inputs it is possible to accurately predict the performance on different languages and model architectures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_10",
            "start": 167,
            "end": 356,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_10@2",
            "content": "Ye et al. (2021) extended this work by proposing methods to do a fine-grained estimation of the performance as well as predicting well-callibrated confidence intervals.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_10",
            "start": 358,
            "end": 525,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_10@3",
            "content": "Specifically predicting the zero-shot performance of MMLMs was first explored in Lauscher et al. (2020), where they used a linear regression model to estimate the cross-lingual transfer performance based on pretraining data size and linguistic relatedness features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_10",
            "start": 527,
            "end": 791,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_10@4",
            "content": "Srinivasan et al. (2021) tackled this problem by utilizing XGBoost Regressor for the prediction along with a larger set of features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_10",
            "start": 793,
            "end": 924,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_10@5",
            "content": "Dolicki and Spanakis (2021) explored individual syntactic features for zero-shot performance prediction instead of working with aggregate similarity values, and showed about 2 to 4 times gain in performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_10",
            "start": 926,
            "end": 1132,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_10@6",
            "content": "We extend all of these works by considering a multi-task learning approach, where performance prediction in a task utilizes not only the data available for that task, but also the patterns observed for other tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_10",
            "start": 1134,
            "end": 1347,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_11@0",
            "content": "Problem Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_11",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_12@0",
            "content": "We begin by defining the multi-task performance prediction problem and then describe the different linguistic and MMLM specific features used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_12",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_13@0",
            "content": "Multi-Task Performance Prediction Problem",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_13",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_14@0",
            "content": "Consider a pre-trained multilingual model M, trained using self supervision on a set of languages L. Let T be the set of downstream NLP tasks, P be the set of pivot (source) languages for which training data is available for the downstream tasks for fine-tuning and T be the set of target languages for which validation/test data is available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_14",
            "start": 0,
            "end": 342,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_14@1",
            "content": "Note that P \u2282 L and T \u2286 L. We use the zero-shot setting similar to Lauscher et al. (2020) which enforces P and T to be disjoint sets 1 , i.e., P \u2229 T = \u2205.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_14",
            "start": 344,
            "end": 496,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_15@0",
            "content": "We then define y M,t p,t \u2208 R as the zero-shot performance on language t \u2208 T on finetuning M on task t \u2208 T in pivot language p \u2208 P. Let x M p,t \u2208 R n be the n-dimensional feature vector representing the corresponding train-test configuration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_15",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_15@1",
            "content": "Since for our experiments we train and evaluate the performance prediction for a single model at a time, we will simplify the notations to y t p,t and x p,t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_15",
            "start": 242,
            "end": 399,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_15@2",
            "content": "The predictor model can then be defined as the function f \u0398,\u03a6 : R n \u00d7 T \u2192 R, where \u0398 \u2208 R dg denotes the shared parameters across the tasks and the task specific parameters are given by \u03a6 \u2208 R ds\u00d7|T| .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_15",
            "start": 401,
            "end": 599,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_15@3",
            "content": "The objective function for training such a predictor model can be defined as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_15",
            "start": 601,
            "end": 677,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_16@0",
            "content": "J(\u0398, \u03a6) = t\u2208T p\u2208P t\u2208T \u2225f (x p,t , t; \u0398, \u03a6) \u2212 y t p,t \u2225 2 2 + \u03bb g \u2225\u0398\u2225 1 + \u03bb s \u2225\u03a6\u2225 1,1 + \u03bb group \u2225\u03a6\u2225 1,q(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_16",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_17@0",
            "content": "The second and third terms regularize the global and task specific parameters independently, while the last term, l 1 /l q norm with q > 1, ensures a block sparse selection of the task specific parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_17",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_17@1",
            "content": "This term ensures a multi-task learning behavior even when there are no parameters shared across the tasks (i.e., \u0398 = \u2205) through selection of common features across the tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_17",
            "start": 206,
            "end": 380,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_17@2",
            "content": "Setting \u0398 = \u2205 and \u03bb group = 0 leads to the single task setup of Lauscher et al. (2020) and Srinivasan et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_17",
            "start": 382,
            "end": 497,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_18@0",
            "content": "Features",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_18",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_19@0",
            "content": "We divide the set of features into two higher level categories, viz. the pairwise features defined for the pivot and target that measure the typological relatedness of the languages, and the individual features defined for the target language reflecting the state of its representation in M.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_19",
            "start": 0,
            "end": 290,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_20@0",
            "content": "Pairwise Features",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_20",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_21@0",
            "content": "Instead of directly using the different typological properties of the the two languages as features, we use the pairwise relatedness to avoid feature explosion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_21",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_21@1",
            "content": "Subword Overlap : We define the subword overlap as the percentage of unique tokens that are common to the vocabularies of both the pivot and target languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_21",
            "start": 161,
            "end": 318,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_21@2",
            "content": "Let V p and V t be the subword vocabularies of p and t. The subword overlap is then defined as :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_21",
            "start": 320,
            "end": 415,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_22@0",
            "content": "o sw (p, t) = |V p \u2229 V t | |V p \u222a V t |(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_22",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_23@0",
            "content": "Similarity between Lang2Vec vectors: Following Lin et al. (2019) and Lauscher et al. (2020), we compute the typological relatedness between p and t from the linguistic features provided by the URIEL project (Littell et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_23",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_23@1",
            "content": "We use syntactic (s syn (p, t)), phonological similarity (s pho (p, t)), genetic similarity (s gen (p, t)) and geographic distance (d geo (p, t)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_23",
            "start": 231,
            "end": 376,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_23@2",
            "content": "For details, please see Littell et al. (2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_23",
            "start": 378,
            "end": 423,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_24@0",
            "content": "Individual Features",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_24",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_25@0",
            "content": "Pre-training Size: We use the log 10 of the size (in words) of the pre-training corpus in the target language, SIZE(t), as a feature.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_25",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_25@1",
            "content": "Rare Typological Traits: Srinivasan et al. (2021) proposed this metric to capture the rarity of the typological features of a language in the representation of M. Every typological feature in WALS database is ranked based on the amount of pretraining data for the languages that contain the feature.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_25",
            "start": 134,
            "end": 432,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_25@2",
            "content": "For the language t, Mean Reciprocal Rank (MRR) of all of its features is then calculated and used as a feature -WMRR(t).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_25",
            "start": 434,
            "end": 553,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_26@0",
            "content": "Tokenizer Features : In their recent work, Rust et al. (2021) proposed two metrics, viz. tokenizer's fertility and proportion of continued words, to evaluate the quality of multilingual tokenizers on a given language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_26",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_26@1",
            "content": "For target t, they define the tokenizer's fertility, FERT(t), as the average number of sub-words produced for every tokenized word in t's corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_26",
            "start": 218,
            "end": 362,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_26@2",
            "content": "On the other hand, the proportion of continued words, PCW(t), measures how often the tokenizer chooses to continue a word across at least two tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_26",
            "start": 364,
            "end": 512,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_26@3",
            "content": "They show that the multilingual models perform much worse on a task than their monolingual counterparts when the values of these metrics are higher for the multilingual tokenizer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_26",
            "start": 514,
            "end": 692,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_26@4",
            "content": "We include FERT(t) and PCW(t) as features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_26",
            "start": 694,
            "end": 735,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_27@0",
            "content": "An important thing to note here is that the we do not use identity of a language as a feature while training the models, hence the performance prediction models are capable of generating predictions on new languages unseen during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_27",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_27@1",
            "content": "However, if the features of the new languages deviate significantly from the features seen during training, the predictions are expected to be less accurate as also observed in Xia et al. (2020); Srinivasan et al. (2021) and is one of the main reasons for exploring a multi-task approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_27",
            "start": 240,
            "end": 527,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_28@0",
            "content": "Approaches",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_28",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_29@0",
            "content": "We extensively experiment with a wide-array of multi-task as well as single-task regression models to provide a fair comparison between different approaches to zero-shot performance prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_29",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_30@0",
            "content": "Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_30",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_31@0",
            "content": "Average Score Within a Task (AWT) : The performance for a pivot-target pair (p , t) on a task t is approximated by taking the average of the performance on all other target languages (pivot being fixed) in the same task t, i.e., f (x p,t , t) =",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_31",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_32@0",
            "content": "1 |T |\u22121 t \u2032 \u2208T \u2212{t} y t p,t \u2032 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_32",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_33@0",
            "content": "Average Score across the Tasks (AAT) : Here instead of averaging over all the target languages within a task, we approximate the performance on a given target language by averaging the scores for that language across the other tasks, i.e.,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_33",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_34@0",
            "content": "f (x p,t , t) = 1 |T|\u22121 t \u2032 \u2208T\u2212{t} y t \u2032 p,t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_34",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_35@0",
            "content": "Single Task Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_35",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_36@0",
            "content": "Lasso Regression: Lauscher et al. ( 2020) train different linear regression models for each task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_36",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_37@0",
            "content": "Along similar lines, we experiment with linear regression, but also add an L1 regularization term, as we observed it usually leads to better predictors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_37",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_38@0",
            "content": "XGBoost Regressor: As shown in Srinivasan et al. (2021), XGBoost (Chen and Guestrin, 2016) generally obtains impressive performance on this task, and hence we include it in our experiments as well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_38",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_39@0",
            "content": "Multi Task Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_39",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_40@0",
            "content": "Group Lasso: l 1 /l q norm based blockregularization has been shown to be effective for multi-task learning in the setting of multi-linear regression (Yuan and Lin, 2006;Argyriou et al., 2008).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_40",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_40@1",
            "content": "For each task, consider separate linear regression models represented by the weight matrix \u03a6 \u2208 R n\u00d7|T| .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_40",
            "start": 194,
            "end": 297,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_40@2",
            "content": "The l 1 /l q regularization term is given as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_40",
            "start": 299,
            "end": 343,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_41@0",
            "content": "\u2225\u03a6\u2225 1,q = n j=1 ( |T| t=1 |\u03a6 jt | q ) 1/q",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_41",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_42@0",
            "content": ", where \u03a6 jt denotes the weight for the feature j in the task t. For q > 1, minimizing this term pushes the l q -norms corresponding to the weights of a given feature across the tasks to be sparse, which encourages multiple predictors to share similar sparsity patterns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_42",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_42@1",
            "content": "In other words, a common set of features is selected for all the tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_42",
            "start": 271,
            "end": 341,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_42@2",
            "content": "We use q = 2 for the group regularization term.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_42",
            "start": 343,
            "end": 389,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_43@0",
            "content": "Since this can be restrictive in certain scenarios, some natural extensions to Group Lasso, such as Dirty Models (Jalali et al., 2010) and Multi Level Lasso (Lozano and Swirszcz, 2012), have been proposed that separate out the task specific and global parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_43",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_43@1",
            "content": "We experimented with these methods and observed equivalent or worse performance compared to Group Lasso.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_43",
            "start": 264,
            "end": 367,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_43@2",
            "content": "Collective Matrix Factorization (CMF) with Side Information: Low rank approximation for the task weights matrices forms one family of methods for multi-task learning (Zhang and Yang, 2017;Pong et al., 2010;Ando et al., 2005).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_43",
            "start": 369,
            "end": 593,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_43@3",
            "content": "As a direct analogue with collaborative filtering, here we can think of the tasks as users and pivot-target pairs as items.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_43",
            "start": 595,
            "end": 717,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_43@4",
            "content": "Consider the matrix Y \u2208 R |T|\u00d7|P\u00d7T | , where each element of the matrix correspond to y t p,t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_43",
            "start": 719,
            "end": 813,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_43@5",
            "content": "We can then decompose the matrix into task and language-pair specific factors as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_43",
            "start": 815,
            "end": 894,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_44@0",
            "content": "Y \u223c TL T (3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_44",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_45@0",
            "content": "where T \u2208 R |T|\u00d7d latent and L \u2208 R |P\u00d7T |\u00d7d latent are the task and language-pair factor matrices, and d latent is the number of factors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_45",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_45@1",
            "content": "Additionally, in order to incorporate the feature information about the language pairs as discussed in section 3.2, we incorporate Collective Matrix Factorization approach (Cortes, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_45",
            "start": 138,
            "end": 324,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_45@2",
            "content": "It incorporates the attribute information about items and/or users in the factorization algorithm by decomposing the language-pair feature matrix X \u2208 R |P\u00d7T |\u00d7n as LF T , such that L is shared across both decompositions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_45",
            "start": 326,
            "end": 545,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_45@3",
            "content": "This helps to learn the latent representations for the pivot-language pairs from the task-wise performance as well as different linguistic and MMLM specific features 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_45",
            "start": 547,
            "end": 715,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_45@4",
            "content": "In relation to Equation 1, we can think of task factors T to correspond to the task specific parameters \u03a6, languagepair factors L as the shared parameters \u0398 and the predictor model as f (x p,t , t; \u0398, \u03a6) = (TL T ) (p,t),t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_45",
            "start": 717,
            "end": 939,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_45@5",
            "content": "Both L and T are regularized seperately, but there is no group regularization term (\u03bb group = 0).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_45",
            "start": 941,
            "end": 1037,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_46@0",
            "content": "Ye et al. ( 2021) also uses a Tensor Factorization approach for performance prediction which is similar to our CMF method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_46",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_46@1",
            "content": "However, they train separate models for each task and factorize over metric specific attributes instead for a fine-grained prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_46",
            "start": 123,
            "end": 256,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_47@0",
            "content": "We use the multi-task variant of Gaussian Processes proposed in Bonilla et al. (2008) and utilize deep neural networks to define the kernel functions as in Deep GPs (Wilson et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_47",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_47@1",
            "content": "For comparison, we also report the scores of the single-task variant of this method which we denote as DGPR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_47",
            "start": 188,
            "end": 295,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_47@2",
            "content": "See Appendix (section A.1) for details.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_47",
            "start": 297,
            "end": 335,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_48@0",
            "content": "Apart from these we also explore other multitask methods like Model Agnostic Meta Learning (MAML) (Finn et al., 2017), details of which we leave in the appendix (section A.1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_48",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_49@0",
            "content": "Experimental Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_49",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_50@0",
            "content": "In this section, we discuss our test conditions, datasets and training parameters for the different experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_50",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_51@0",
            "content": "Test Conditions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_51",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_52@0",
            "content": "We consider two different test conditions: Leave One Language Out (LOLO) and Leave Low Resource Languages Out (LLRO).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_52",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_53@0",
            "content": "Leave One Language Out: LOLO is a popular setup for multilingual performance prediction (Lauscher et al., 2020;Srinivasan et al., 2021), where for a given task, we choose a target language and move all of its instances from the prediction dataset to the test data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_53",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_53@1",
            "content": "The models are then trained on the remaining languages and evaluated on the unseen test language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_53",
            "start": 265,
            "end": 361,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_53@2",
            "content": "This is done for all the target languages available for a task, and the Mean Absolute Error (MAE) across languages is reported.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_53",
            "start": 363,
            "end": 489,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_53@3",
            "content": "In the multi-task setting we evaluate on one task at a time while considering the rest as helper tasks for which the entire data is used including the test language 3 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_53",
            "start": 491,
            "end": 658,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_53@4",
            "content": "Leave Low Resource Languages Out: Through this evaluation strategy we try to emulate the real world use case where we only have test data available in high resource languages such as English, German and Chinese, and would like to estimate the performance on under-represented languages such as Swahili and Bengali.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_53",
            "start": 660,
            "end": 973,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_53@5",
            "content": "We use the language taxonomy provided by Joshi et al. (2020) to categorize the languages into six classes (0 = low to 5 = high) based on the number of resources available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_53",
            "start": 975,
            "end": 1145,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_53@6",
            "content": "We then move languages belonging to class 3 or below to our test set and train the models on class 4 and 5 languages only.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_53",
            "start": 1147,
            "end": 1268,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_53@7",
            "content": "Similar to LOLO, here too we allow the helper tasks to retain all the languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_53",
            "start": 1270,
            "end": 1349,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_54@0",
            "content": "Tasks and Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_54",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_55@0",
            "content": "We use the following 11 tasks provided in XTREME (Hu et al., 2020) and XTREME-R benchmarks:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_55",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_55@1",
            "content": "1. Classification: XNLI (Conneau et al., 2018) , PAWS-X (Yang et al., 2019), andXCOPA (Ponti et al., 2020)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_55",
            "start": 92,
            "end": 197,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_55@2",
            "content": "2. Structure Prediction: UDPOS (Nivre et al., 2018), and NER (Pan et al., 2017)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_55",
            "start": 199,
            "end": 277,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_55@3",
            "content": "3. Question Answering: XQUAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), and TyDiQA-GoldP (Clark et al., 2020)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_55",
            "start": 279,
            "end": 395,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_55@4",
            "content": "4. Retrieval: Tatoeba (Artetxe and Schwenk, 2019), Mewsli-X (Botha et al., 2020;, and LAReQA (Roy et al., 2020) All of these datasets have training data present only in English i.e. P = {en}, and majority of the tasks have fewer than 10 target languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_55",
            "start": 397,
            "end": 650,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_56@0",
            "content": "Training Details",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_56",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_57@0",
            "content": "We train and evaluate our performance prediction models for mBERT (bert-base-multilingual-cased) and XLM-R (xlm-roberta-large).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_57",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_57@1",
            "content": "For training XG-Boost, we used 100 estimators with a maximum depth of 10.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_57",
            "start": 128,
            "end": 200,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_57@2",
            "content": "For Group Lasso, we used the implementation provided in the MuTaR software package 4 , and used a regularization strength of 0.01.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_57",
            "start": 202,
            "end": 331,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_58@0",
            "content": "We optimized CMF's objective function using Alternating Least Squares (ALS), used 5 latent factors with a regularization parameter equal to 0.1, and used the Collective Matrix Factorization python library 5 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_58",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_58@1",
            "content": "In case of MDGPR, we used Radial Basis Function as the kernel and a two-layer MLP for learning latent features, with 50 and 10 units followed by ReLU activation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_58",
            "start": 209,
            "end": 369,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_58@2",
            "content": "We set the learning rate and epochs as 0.01 and 200, and implemented it using GPyTorch 6 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_58",
            "start": 371,
            "end": 460,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_59@0",
            "content": "6 Results and Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_59",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_60@0",
            "content": "LOLO Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_60",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_61@0",
            "content": "Table 1 shows MAE (in %) for LOLO for different single-task and multi-task models on the tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_61",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_62@0",
            "content": "For XLMR, we observe that multi-task models, primarily MDGPR, often outperform the best singletask models by significant margins, and for tasks like MewsliX we even see about 36% reduction in MAE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_62",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_62@1",
            "content": "Overall, we see about 10% drop in LOLO errors on average for MDGPR compared to the best performing single-task model i.e. Lasso Regression.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_62",
            "start": 197,
            "end": 335,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_62@2",
            "content": "As expected, the benefit of multi-task learning is even more prominent when we consider the tasks for which only a few (\u2264 10) data points are available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_62",
            "start": 337,
            "end": 488,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_62@3",
            "content": "Here we see about 20% reduction in errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_62",
            "start": 490,
            "end": 531,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_62@4",
            "content": "For mBERT as well, we have similar observations, except that CMF performs slightly better than MDGPR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_62",
            "start": 533,
            "end": 633,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_62@5",
            "content": "Note that the Average across task baseline is quite competitive and performs better than singletask XGBoost and MAML in average, and better than all models for LAReQA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_62",
            "start": 635,
            "end": 801,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_63@0",
            "content": "Figure 2 plots the dependence of the number of helper tasks on the performance of the multi-task models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_63",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_63@1",
            "content": "As expected, MAE decreases as helper tasks increase, especially for MDGPR and CMF.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_63",
            "start": 105,
            "end": 186,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_63@2",
            "content": "On a related note, the Pearson Correlation coefficient between MAE and number of tasks a target language is part of is found to be \u22120.39, though the trend in this case is not as clear.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_63",
            "start": 188,
            "end": 371,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_64@0",
            "content": "LLRO Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_64",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_65@0",
            "content": "Predicting the performance on low resource languages, for which often standard training and test datasets are not available, can be an important use case where multi-task performance prediction can be helpful.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_65",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_65@1",
            "content": "Figure 6 in appendix shows the classwise (Joshi et al., 2020) distribution of languages for the tasks that we consider in our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_65",
            "start": 210,
            "end": 347,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_65@2",
            "content": "As one would expect, for most tasks, test data is available for languages belonging to class-4 and class-5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_65",
            "start": 349,
            "end": 455,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_65@3",
            "content": "Training performance prediction models without any task to transfer from can therefore, possibly lead to poor generalization on the low resource languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_65",
            "start": 457,
            "end": 611,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_65@4",
            "content": "On the other hand, for the same reason -lack of test data, building accurate predictors for low-resource languages is necessary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_65",
            "start": 613,
            "end": 740,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_65@5",
            "content": "MAE values for the LLRO evaluation setup are shown in figure 1 for XLMR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_65",
            "start": 742,
            "end": 813,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_65@6",
            "content": "Results for mBERT follow similar trends and are reported in the Appendix (figure 7).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_65",
            "start": 815,
            "end": 898,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_65@7",
            "content": "For both XLMR and mBERT we observe that the three main multi-task models -Group Lasso, CMF and MDGPR -outperform the single-task models and baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_65",
            "start": 900,
            "end": 1049,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_65@8",
            "content": "Interestingly, for XLMR, the single task models XGBoost and Lasso perform even worse than the Average within Tasks baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_65",
            "start": 1051,
            "end": 1174,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_65@9",
            "content": "Overall we see around 18% and 11% drop in MAE for Group Lasso over the best performing single-task model, for XLMR and mBERT respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_65",
            "start": 1176,
            "end": 1313,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_66@0",
            "content": "Feature Importance",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_66",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_67@0",
            "content": "An interesting consequence of zero-shot performance prediction is that the models can be directly used to infer the correlation (and possibly causation) between linguistic relatedness and pretraining conditions and zero-shot transferability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_67",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_67@1",
            "content": "Multi-task learning, in this context, help us make more robust inferences, as the models are less prone to overfitting to a particular or dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_67",
            "start": 242,
            "end": 387,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@0",
            "content": "Figure 3 shows the SHAP values of the features for the Group Lasso model trained on XLMR's zero-shot performance data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@1",
            "content": "As expected for Group Lasso, we see a block-sparsity behavior among the tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 119,
            "end": 196,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@2",
            "content": "Features such as Rare Typological Traits (WMRR(t)), Tokenizer's Fertility (FERT(t)) and Genetic Similarity (s gen (p, t)) are ignored in all the tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 198,
            "end": 348,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@3",
            "content": "In contrast, for the single-task lasso regression (Figure 9 in Appendix), we see different sets of features selected for different tasks, which for the scale at which we operate, might not be indicative of the actual factors that affect the zero-shot performance in these tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 350,
            "end": 627,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@4",
            "content": "Subword Overlap.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 629,
            "end": 644,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@5",
            "content": "Among the features that get selected for all tasks, we observe that Subword Overlap (o sw (p, t)) typically gets higher importance in retrieval (LAReQA and MewsliX) and sentence classification tasks (PAWS-X, XNLI).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 646,
            "end": 859,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@6",
            "content": "Since the retrieval tasks that we consider, as described in , measure the alignment between the cross lingual representations of semantically similar sentences, having a shared vocabulary between the languages can leak information from one to another (Wu and Dredze, 2019) which might improve the retrieval performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 861,
            "end": 1179,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@7",
            "content": "Interestingly, if we compare this with the feature importance scores for the single task lasso model (Figure 9 in Appendix), we do see MewsliX task getting higher importance for the subword overlap, but LAReQA gets virtually zero SHAP value for this feature, showcasing how single-task models can misinterpret two similar tasks as requiring very different features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 1181,
            "end": 1545,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@8",
            "content": "Our observation reinforce the generally held notion that vocabulary overlap between the pivot and target is beneficial for zero-shot transfer (Wu and Dredze, 2019), especially for retrieval tasks, though some studies have argued otherwise (Pires et al., 2019;K et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 1547,
            "end": 1821,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@9",
            "content": "Tokenizer Features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 1823,
            "end": 1841,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@10",
            "content": "For structure prediction (UDPOS and WikiAnn) and question answering (XQUAD and TyDiQA) tasks that require making predictions for each token in the input, we see that the tokenizer feature, PCW(t), receive a higher SHAP value.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 1843,
            "end": 2067,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@11",
            "content": "In contrast, for single-task lasso, here too we do not observe high importance of this feature across these related tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 2069,
            "end": 2190,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@12",
            "content": "Rust et al. (2021) note that languages such as Arabic where mBERT's multilingual tokenizer was found to be much worse than it's monolingual counterpart, there was a sharper drop in performance of mBERT compared to the monolingual model for QA, UDPOS and NER tasks than for sentiment classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 2192,
            "end": 2489,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@13",
            "content": "We believe that XLMR's surprisingly worse performance than mBERT for Chinese and Japanese UDPOS might be correlated with it's significantly worse tokenizer for these languages based on the fertility (FERT) and Percentage Continued Words (PCW) feature values (see Appendix A.2 for exact values).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 2491,
            "end": 2784,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@14",
            "content": "The high SHAP values for PCW(t) further strengthen our belief 7 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 2786,
            "end": 2850,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@15",
            "content": "Pre-training Size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 2852,
            "end": 2869,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@16",
            "content": "Similar to the findings of Lauscher et al. (2020), we observe that pre-training corpus size has low SHAP value, and therefore, lower importance for lower level tasks such as UDPOS and NER, and higher SHAP values for higher level tasks like XNLI.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 2871,
            "end": 3115,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@17",
            "content": "Additionally, we extend their observations to tasks such as XCOPA, Tatoeba, MLQA and LAReQA where pre-training size seem to play a significant role in the performance prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 3117,
            "end": 3294,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@18",
            "content": "Again, compared to single Lasso Regression model, we see a different selection pattern: Pre-training size receives a high SHAP value for UDPOS while for XNLI it is negligible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 3296,
            "end": 3470,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@19",
            "content": "This neither fully conforms with our observations on the multi-task feature selections, nor with the previous work (Lauscher et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 3472,
            "end": 3610,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@20",
            "content": "Typological Relatedness Features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 3612,
            "end": 3644,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@21",
            "content": "Out of all the typological relatedness features, we found Geographical Distance (d geo (p, t)) receiving highest SHAP values for all tasks, implying that geographical proximity between the pivot-target pair is an important factor in determining the zero-shot transferability between them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 3646,
            "end": 3933,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@22",
            "content": "Lauscher et al. (2020) also observe positive correlations between geographical relatedness and zero-shot performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 3935,
            "end": 4051,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@23",
            "content": "The crosstask importance of geographic distance (unlike the other relatedness features) might be attributed to the 100% coverage across languages for the geographical vectors in the URIEL database.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 4053,
            "end": 4249,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@24",
            "content": "In contrast, Syntactic and Phonological vectors have missing values for a majority of the languages (Littell et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 4251,
            "end": 4373,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@25",
            "content": "Like Lauscher et al. (2020), we also see some dependence on syntactic (s syn (p, t)) and phonological (s pho (p, t)) similarities for XLMR's zero shot performance on XNLI and XQUAD tasks respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 4375,
            "end": 4574,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@26",
            "content": "However, in both cases we found that the tokenizer feature PCW(t) receives a much higher SHAP value.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 4576,
            "end": 4675,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@27",
            "content": "Interestingly, genetic similarity (s gen (p, t)) is not selected for any task, arguably due to the block sparsity in feature selection of Group Lasso.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 4677,
            "end": 4826,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@28",
            "content": "We do see some tasks receiving high SHAP values for s gen (p, t) in single-task lasso (Figure 9 in Appendix).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 4828,
            "end": 4936,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_68@29",
            "content": "However, the number of such tasks as well as the SHAP values are on the lower side, implying that genetic similarity might not provide any additional information for zero-shot transfer over and above the geographical, syntactic and phonological similarities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_68",
            "start": 4938,
            "end": 5195,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_69@0",
            "content": "Similar trends are observed in the case of mBERT as well (Figure 10 in appendix), with some minor differences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_69",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_69@1",
            "content": "For instance, instead of PCW(t), FERT(t) receives higher SHAP value; s syn (p, t) also receives higher importance, especially for tasks like UDPOS and XNLI, which is consistent with the findings of Lauscher et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_69",
            "start": 111,
            "end": 331,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_70@0",
            "content": "Conclusion and Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_70",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_71@0",
            "content": "In this paper, we showed that the zero-shot performance prediction problem can be much more effectively and robustly solved by using multi-task learning approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_71",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_71@1",
            "content": "We see significant reduction in errors compared to the baselines and single-task models, specifically for the tasks which have test sets available in a very few languages or when trying to predict the performance for low resource languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_71",
            "start": 165,
            "end": 404,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_71@2",
            "content": "Additionally, this approach allows us to robustly identify factors that influence zero-shot performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_71",
            "start": 406,
            "end": 509,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_71@3",
            "content": "Our findings in this context can be summarized as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_71",
            "start": 511,
            "end": 568,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_72@0",
            "content": "1. Subword overlap between the pivot and target has a strong positive influence on zero-shot transfer, especially for Retrieval tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_72",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_72@1",
            "content": "2. Quality of the target tokenizer, defined in terms of how often or how aggressively it splits the target tokens negatively influences zero-shot performance for word-level tasks such as POS tagging and Span extraction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_72",
            "start": 135,
            "end": 353,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_72@2",
            "content": "3. Pre-training size of the target positively influences zero-shot performance in many tasks, including XCOPA, Tatoeba, MLQA and LAReQA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_72",
            "start": 355,
            "end": 490,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_72@3",
            "content": "4. Geographical proximity between pivot and target is found to be uniformly important across all the tasks, unlike syntactic and phonological similarities, which are important for only some tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_72",
            "start": 492,
            "end": 687,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_73@0",
            "content": "This last finding is especially interesting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_73",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_73@1",
            "content": "As described earlier, geographical proximity is a more clear, noise-free and complete feature compared to the other relatedness However, one could also argue that since neighboring languages tend to have high vocabulary and typological feature overlap due to contact processes and shared areal features, geographical distance is an extremely informative feature for zero-shot transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_73",
            "start": 45,
            "end": 429,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_73@2",
            "content": "Two direct implications of these findings are: (1) for effective use of MMLMs, one should develop resources in at least one pivot language per geographic regions, and (2) one should work towards multilingual tokenizers that are effective for most languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_73",
            "start": 431,
            "end": 687,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_74@0",
            "content": "There are a number of directions that can be explored in future related to our work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_74",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_74@1",
            "content": "The prediction models can be extended to a multi-pivot and few-shot settings, as described in Srinivasan et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_74",
            "start": 85,
            "end": 203,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_74@2",
            "content": "Further probing experiments could be designed to understand the role of sub-word overlap on zero-shot transfer of Retrieval tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_74",
            "start": 205,
            "end": 334,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_75@0",
            "content": "where inter-task similarities are learnt solely based on the task identities and the observed data for each task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_75",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_75@1",
            "content": "Instead of learning task-specific kernels k t (g(x p,t ), g(x p \u2032 ,t \u2032 )), we will have a common kernel over the inputs as k(g(x p,t ), g(x p \u2032 ,t \u2032 )) and a positive semi-definite matrix K task for learning inter-task similarities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_75",
            "start": 114,
            "end": 345,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_75@2",
            "content": "Specifically, we define the multi-task kernel K m as follows",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_75",
            "start": 347,
            "end": 406,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_76@0",
            "content": "k m ([x p,t , t], [x p \u2032 ,t \u2032 , t \u2032 ]) = k(g(x p,t ), g(x p \u2032 ,t \u2032 )) * k task (t, t \u2032 ) (6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_76",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_77@0",
            "content": "The GP prior will be defined by replacing the task specific kernel K t in the equation 4 with the multi-task kernel K m .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_77",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_77@1",
            "content": "We use the optimization steps similar to DGP and the inference is done by using the standard GP formulae.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_77",
            "start": 122,
            "end": 226,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_78@0",
            "content": "Relating MDGPR to equation 1, the global parameters \u0398 are the parameters of the deep network g, and the task specific parameter \u03a6 is the positive semi-definite matrix K task .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_78",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_78@1",
            "content": "Model Agnostic Meta Learning (MAML): MAML (Finn et al., 2017) is a popular meta learning algorithm that can be used to quickly adapt Deep Neural Networks on new tasks in a few-shot setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_78",
            "start": 176,
            "end": 364,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_78@2",
            "content": "In MAML, the set of initialization parameters for the neural network are explicitly learned such that the network can generalize well on a new task with a small number of gradient steps and training samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_78",
            "start": 366,
            "end": 572,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_79@0",
            "content": "Relating to equation 1, the global parameters \u0398 can be considered as the initial set of parameters for the neural network that are learned and shared across all the tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_79",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_79@1",
            "content": "Task specific parameters \u03a6 are adapted from \u0398 by taking K gradient steps using the task's performance data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_79",
            "start": 172,
            "end": 278,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_80@0",
            "content": "For evaluating a task t, we consider rest of the tasks in our dataset as helpers (t \u2032 \u2208 T\u2212{t}) and use them to train the initial set of parameters \u0398. The initial parameters are then updated by fine-tuning the network on the training set for t using gradient descent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_80",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_81@0",
            "content": "The FERT and PCW metrics as proposed by Rust et al. (2021), have been compared for mBERT and XLMR in figure 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_81",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_81@1",
            "content": "As can be seen, for most languages the metric values are similar across the two tokenizers, however for languages like Chinese and Japanese, there is a dramatic increase in the values for XLMR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_81",
            "start": 111,
            "end": 303,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_81@2",
            "content": "Interestingly, when we compare the zero-shot performance between mBERT and XLMR on structure prediction tasks like UD-POS and WikiANN, we see a surprisingly large drop (upto 20% absolute drop) in the performance for XLMR on these both Chinese and Japanese, whereas usually XLMR outperforms mBERT on these tasks (Refer to figure 5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_81",
            "start": 305,
            "end": 635,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_81@3",
            "content": "This observation along with the feature importance for the tokenizer features that we observed for Group Lasso (3) indicate that tokenizer quality might play some role in the zero-shot transfer capabilities of the multilingual models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_81",
            "start": 637,
            "end": 870,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_81@4",
            "content": "Table 2: Mean Absolute Errors (Scaled by 100 for readability) for different models trained to predict the zero shot performance of mBERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_81",
            "start": 872,
            "end": 1008,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_81@5",
            "content": "In the \"Average\" row we average the MAEs across all the tasks and in the \"Average Low\" Res Tasks\", we consider the tasks with fewer than 10 target languages and take the average of the MAEs for those tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_81",
            "start": 1010,
            "end": 1215,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_82@0",
            "content": "Tong Rie Kubota Ando, Peter Zhang,  Bartlett, A framework for learning predictive structures from multiple tasks and unlabeled data, 2005, Journal of Machine Learning Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_82",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_83@0",
            "content": "Andreas Argyriou, Theodoros Evgeniou, Massimiliano Pontil, Convex multi-task feature learning, 2008, Machine learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_83",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_84@0",
            "content": "Mikel Artetxe, Sebastian Ruder, Dani Yogatama, On the Cross-lingual Transferability of Monolingual Representations, 2020, Proceedings of ACL 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_84",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_85@0",
            "content": "Mikel Artetxe, Holger Schwenk, Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond, 2019, Transactions of the ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_85",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_86@0",
            "content": "Alexandra Birch, Miles Osborne, Philipp Koehn, Predicting success in machine translation, 2008, Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_86",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_87@0",
            "content": "V Edwin, Kian Bonilla, Christopher Chai,  Williams, Multi-task gaussian process prediction, 2008, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_87",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_88@0",
            "content": "Jan Botha, Zifei Shan, Daniel Gillick, Entity Linking in 100 Languages, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_88",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_89@0",
            "content": "Tianqi Chen, Carlos Guestrin, Xgboost: A scalable tree boosting system, 2016, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16, Association for Computing Machinery.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_89",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_90@0",
            "content": "Jonathan Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, Jennimaria Palomaki, TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages, 2020, Transactions of the Association of Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_90",
            "start": 0,
            "end": 283,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_91@0",
            "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Unsupervised cross-lingual representation learning at scale, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_91",
            "start": 0,
            "end": 322,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_92@0",
            "content": "Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, Veselin Stoyanov, XNLI: Evaluating crosslingual sentence representations, 2018, Proceedings of EMNLP 2018, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_92",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_93@0",
            "content": "UNKNOWN, None, 2018, Cold-start recommendations in collective matrix factorization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_93",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_94@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_94",
            "start": 0,
            "end": 335,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_95@0",
            "content": "UNKNOWN, None, 2021, Analysing the impact of linguistic features on crosslingual transfer, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_95",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_96@0",
            "content": "Tobias Domhan, Jost Springenberg, Frank Hutter, Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves, 2015, Twenty-fourth international joint conference on artificial intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_96",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_97@0",
            "content": "Chelsea Finn, Pieter Abbeel, Sergey Levine, Model-agnostic meta-learning for fast adaptation of deep networks, 2017, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_97",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_98@0",
            "content": "Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, Melvin Johnson, XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation, 2020, Proceedings of the 37th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_98",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_99@0",
            "content": "Ali Jalali, Sujay Sanghavi, Chao Ruan, Pradeep Ravikumar, A dirty model for multi-task learning, 2010, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_99",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_100@0",
            "content": "Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, Monojit Choudhury, The state and fate of linguistic diversity and inclusion in the NLP world, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_100",
            "start": 0,
            "end": 297,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_101@0",
            "content": "K Karthikeyan, Zihan Wang, Stephen Mayhew, Dan Roth, Cross-lingual ability of multilingual bert: An empirical study, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_101",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_102@0",
            "content": "Anne Lauscher, Vinit Ravishankar, Ivan Vuli\u0107, Goran Glava\u0161, From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_102",
            "start": 0,
            "end": 304,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_103@0",
            "content": "Gina-Anne Levow, The third international Chinese language processing bakeoff: Word segmentation and named entity recognition, 2006, Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_103",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_104@0",
            "content": "Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, Holger Schwenk, MLQA: Evaluating Cross-lingual Extractive Question Answering, 2020, Proceedings of ACL 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_104",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_105@0",
            "content": "Yu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li, Yuyan Zhang, Mengzhou Xia, Shruti Rijhwani, Junxian He, Zhisong Zhang, Xuezhe Ma, Antonios Anastasopoulos, Patrick Littell, and Graham Neubig, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_105",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_106@0",
            "content": "Patrick Littell, David Mortensen, Ke Lin, Katherine Kairis, Carlisle Turner, Lori Levin, URIEL and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors, 2017, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_106",
            "start": 0,
            "end": 344,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_107@0",
            "content": "C Aurelie, Grzegorz Lozano,  Swirszcz, Multilevel lasso for sparse multi-task regression, 2012, Proceedings of the 29th International Coference on International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_107",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_108@0",
            "content": "UNKNOWN, None, , , Mohammed Attia.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_108",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_109@0",
            "content": "Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, Heng Ji, Cross-lingual name tagging and linking for 282 languages, 2017, Proceedings of ACL 2017, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_109",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_110@0",
            "content": "Telmo Pires, Eva Schlinger, Dan Garrette, How multilingual is multilingual BERT?, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_110",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_111@0",
            "content": "Emmanouil Platanios, Hoifung Poon, M Tom, Eric Mitchell,  Horvitz, Estimating accuracy from unlabeled data: A probabilistic logic approach, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_111",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_112@0",
            "content": "UNKNOWN, None, 2014, Estimating accuracy from unlabeled data, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_112",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_113@0",
            "content": "Paul Ting Kei Pong, Shuiwang Tseng, Jieping Ji,  Ye, Trace norm regularization: Reformulations, algorithms, and multi-task learning, 2010, SIAM Journal on Optimization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_113",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_114@0",
            "content": "Goran Edoardo Maria Ponti, Olga Glava\u0161, Qianchu Majewska, Ivan Liu, Anna Vuli\u0107,  Korhonen, XCOPA: A multilingual dataset for causal commonsense reasoning, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_114",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_115@0",
            "content": "Uma Roy, Noah Constant, Rami Al-Rfou, Aditya Barua, Aaron Phillips, Yinfei Yang, LAReQA: Language-agnostic answer retrieval from a multilingual pool, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_115",
            "start": 0,
            "end": 301,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_116@0",
            "content": "Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, Melvin Johnson, XTREME-R: Towards more challenging and nuanced multilingual evaluation, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_116",
            "start": 0,
            "end": 319,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_117@0",
            "content": "Phillip Rust, Jonas Pfeiffer, Ivan Vuli\u0107, Sebastian Ruder, Iryna Gurevych, How good is your tokenizer? on the monolingual performance of multilingual language models, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_117",
            "start": 0,
            "end": 348,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_118@0",
            "content": "Holger Schwenk, Xian Li, A corpus for multilingual document classification in eight languages, 2018, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_118",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_119@0",
            "content": "UNKNOWN, None, 2021, Sandipan Dandapat, Kalika Bali, and Monojit Choudhury, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_119",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_120@0",
            "content": "Erik , Tjong Kim Sang, Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition, 2002, COLING-02: The 6th Conference on Natural Language Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_120",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_121@0",
            "content": "Erik Tjong, Kim Sang, Fien De Meulder, Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition, 2003, Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_121",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_122@0",
            "content": "Zhiting Andrew Gordon Wilson, Ruslan Hu, Eric Salakhutdinov,  Xing, Deep kernel learning, 2016, Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_122",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_123@0",
            "content": "Shijie Wu, Mark Dredze, Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_123",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_124@0",
            "content": "Shijie Wu, Mark Dredze, Are all languages created equal in multilingual BERT?, 2020, Proceedings of the 5th Workshop on Representation Learning for NLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_124",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_125@0",
            "content": "Mengzhou Xia, Antonios Anastasopoulos, Ruochen Xu, Yiming Yang, Graham Neubig, Predicting performance for natural language processing tasks, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_125",
            "start": 0,
            "end": 236,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_126@0",
            "content": "Yinfei Yang, Yuan Zhang, Chris Tar, Jason Baldridge, PAWS-X: A cross-lingual adversarial dataset for paraphrase identification, 2019, Proceedings of EMNLP 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_126",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_127@0",
            "content": "Zihuiwen Ye, Pengfei Liu, Jinlan Fu, Graham Neubig, Towards more fine-grained and reliable NLP performance prediction, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_127",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_128@0",
            "content": "Ming Yuan, Yi Lin, Model selection and estimation in regression with grouped variables, 2006, Journal of the Royal Statistical Society: Series B (Statistical Methodology), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_128",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "331-ARR_v2_129@0",
            "content": "Yu Zhang, Qiang Yang, A survey on multitask learning, 2017, IEEE Transactions on Knowledge and Data Engineering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "331-ARR_v2_129",
            "start": 0,
            "end": 113,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "331-ARR_v2_0",
            "tgt_ix": "331-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_0",
            "tgt_ix": "331-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_1",
            "tgt_ix": "331-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_1",
            "tgt_ix": "331-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_0",
            "tgt_ix": "331-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_2",
            "tgt_ix": "331-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_4",
            "tgt_ix": "331-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_5",
            "tgt_ix": "331-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_3",
            "tgt_ix": "331-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_3",
            "tgt_ix": "331-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_3",
            "tgt_ix": "331-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_3",
            "tgt_ix": "331-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_0",
            "tgt_ix": "331-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_6",
            "tgt_ix": "331-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_8",
            "tgt_ix": "331-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_9",
            "tgt_ix": "331-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_7",
            "tgt_ix": "331-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_7",
            "tgt_ix": "331-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_7",
            "tgt_ix": "331-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_7",
            "tgt_ix": "331-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_0",
            "tgt_ix": "331-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_10",
            "tgt_ix": "331-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_11",
            "tgt_ix": "331-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_11",
            "tgt_ix": "331-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_11",
            "tgt_ix": "331-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_12",
            "tgt_ix": "331-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_14",
            "tgt_ix": "331-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_15",
            "tgt_ix": "331-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_16",
            "tgt_ix": "331-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_13",
            "tgt_ix": "331-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_13",
            "tgt_ix": "331-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_13",
            "tgt_ix": "331-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_13",
            "tgt_ix": "331-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_13",
            "tgt_ix": "331-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_11",
            "tgt_ix": "331-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_17",
            "tgt_ix": "331-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_18",
            "tgt_ix": "331-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_18",
            "tgt_ix": "331-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_11",
            "tgt_ix": "331-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_19",
            "tgt_ix": "331-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_21",
            "tgt_ix": "331-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_22",
            "tgt_ix": "331-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_20",
            "tgt_ix": "331-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_20",
            "tgt_ix": "331-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_20",
            "tgt_ix": "331-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_20",
            "tgt_ix": "331-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_11",
            "tgt_ix": "331-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_23",
            "tgt_ix": "331-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_25",
            "tgt_ix": "331-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_26",
            "tgt_ix": "331-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_24",
            "tgt_ix": "331-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_24",
            "tgt_ix": "331-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_24",
            "tgt_ix": "331-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_24",
            "tgt_ix": "331-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_0",
            "tgt_ix": "331-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_27",
            "tgt_ix": "331-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_28",
            "tgt_ix": "331-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_28",
            "tgt_ix": "331-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_28",
            "tgt_ix": "331-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_29",
            "tgt_ix": "331-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_31",
            "tgt_ix": "331-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_32",
            "tgt_ix": "331-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_33",
            "tgt_ix": "331-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_30",
            "tgt_ix": "331-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_30",
            "tgt_ix": "331-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_30",
            "tgt_ix": "331-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_30",
            "tgt_ix": "331-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_30",
            "tgt_ix": "331-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_28",
            "tgt_ix": "331-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_34",
            "tgt_ix": "331-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_36",
            "tgt_ix": "331-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_37",
            "tgt_ix": "331-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_35",
            "tgt_ix": "331-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_35",
            "tgt_ix": "331-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_35",
            "tgt_ix": "331-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_35",
            "tgt_ix": "331-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_28",
            "tgt_ix": "331-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_38",
            "tgt_ix": "331-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_40",
            "tgt_ix": "331-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_41",
            "tgt_ix": "331-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_42",
            "tgt_ix": "331-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_43",
            "tgt_ix": "331-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_44",
            "tgt_ix": "331-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_45",
            "tgt_ix": "331-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_39",
            "tgt_ix": "331-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_39",
            "tgt_ix": "331-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_39",
            "tgt_ix": "331-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_39",
            "tgt_ix": "331-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_39",
            "tgt_ix": "331-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_39",
            "tgt_ix": "331-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_39",
            "tgt_ix": "331-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_39",
            "tgt_ix": "331-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_47",
            "tgt_ix": "331-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_39",
            "tgt_ix": "331-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_39",
            "tgt_ix": "331-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_46",
            "tgt_ix": "331-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_0",
            "tgt_ix": "331-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_48",
            "tgt_ix": "331-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_49",
            "tgt_ix": "331-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_49",
            "tgt_ix": "331-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_49",
            "tgt_ix": "331-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_50",
            "tgt_ix": "331-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_52",
            "tgt_ix": "331-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_51",
            "tgt_ix": "331-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_51",
            "tgt_ix": "331-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_51",
            "tgt_ix": "331-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_49",
            "tgt_ix": "331-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_53",
            "tgt_ix": "331-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_54",
            "tgt_ix": "331-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_54",
            "tgt_ix": "331-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_49",
            "tgt_ix": "331-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_55",
            "tgt_ix": "331-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_57",
            "tgt_ix": "331-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_58",
            "tgt_ix": "331-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_56",
            "tgt_ix": "331-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_56",
            "tgt_ix": "331-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_56",
            "tgt_ix": "331-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_56",
            "tgt_ix": "331-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_0",
            "tgt_ix": "331-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_59",
            "tgt_ix": "331-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_61",
            "tgt_ix": "331-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_62",
            "tgt_ix": "331-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_60",
            "tgt_ix": "331-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_60",
            "tgt_ix": "331-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_60",
            "tgt_ix": "331-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_60",
            "tgt_ix": "331-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_0",
            "tgt_ix": "331-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_63",
            "tgt_ix": "331-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_64",
            "tgt_ix": "331-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_64",
            "tgt_ix": "331-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_0",
            "tgt_ix": "331-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_65",
            "tgt_ix": "331-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_67",
            "tgt_ix": "331-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_66",
            "tgt_ix": "331-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_66",
            "tgt_ix": "331-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_66",
            "tgt_ix": "331-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_66",
            "tgt_ix": "331-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_0",
            "tgt_ix": "331-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_69",
            "tgt_ix": "331-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_71",
            "tgt_ix": "331-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_72",
            "tgt_ix": "331-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_73",
            "tgt_ix": "331-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_74",
            "tgt_ix": "331-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_75",
            "tgt_ix": "331-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_76",
            "tgt_ix": "331-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_77",
            "tgt_ix": "331-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_78",
            "tgt_ix": "331-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_79",
            "tgt_ix": "331-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_70",
            "tgt_ix": "331-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_70",
            "tgt_ix": "331-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_70",
            "tgt_ix": "331-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_70",
            "tgt_ix": "331-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_70",
            "tgt_ix": "331-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_70",
            "tgt_ix": "331-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_70",
            "tgt_ix": "331-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_70",
            "tgt_ix": "331-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_70",
            "tgt_ix": "331-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_70",
            "tgt_ix": "331-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_70",
            "tgt_ix": "331-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_70",
            "tgt_ix": "331-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_80",
            "tgt_ix": "331-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "331-ARR_v2_0",
            "tgt_ix": "331-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_1",
            "tgt_ix": "331-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_2",
            "tgt_ix": "331-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_2",
            "tgt_ix": "331-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_2",
            "tgt_ix": "331-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_2",
            "tgt_ix": "331-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_3",
            "tgt_ix": "331-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_4",
            "tgt_ix": "331-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_4",
            "tgt_ix": "331-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_4",
            "tgt_ix": "331-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_4",
            "tgt_ix": "331-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_4",
            "tgt_ix": "331-ARR_v2_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_5",
            "tgt_ix": "331-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_5",
            "tgt_ix": "331-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_5",
            "tgt_ix": "331-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_5",
            "tgt_ix": "331-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_6",
            "tgt_ix": "331-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_6",
            "tgt_ix": "331-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_6",
            "tgt_ix": "331-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_6",
            "tgt_ix": "331-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_6",
            "tgt_ix": "331-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_6",
            "tgt_ix": "331-ARR_v2_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_6",
            "tgt_ix": "331-ARR_v2_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_6",
            "tgt_ix": "331-ARR_v2_6@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_6",
            "tgt_ix": "331-ARR_v2_6@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_6",
            "tgt_ix": "331-ARR_v2_6@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_7",
            "tgt_ix": "331-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_8",
            "tgt_ix": "331-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_8",
            "tgt_ix": "331-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_8",
            "tgt_ix": "331-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_8",
            "tgt_ix": "331-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_8",
            "tgt_ix": "331-ARR_v2_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_8",
            "tgt_ix": "331-ARR_v2_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_8",
            "tgt_ix": "331-ARR_v2_8@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_8",
            "tgt_ix": "331-ARR_v2_8@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_8",
            "tgt_ix": "331-ARR_v2_8@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_8",
            "tgt_ix": "331-ARR_v2_8@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_8",
            "tgt_ix": "331-ARR_v2_8@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_9",
            "tgt_ix": "331-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_9",
            "tgt_ix": "331-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_9",
            "tgt_ix": "331-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_9",
            "tgt_ix": "331-ARR_v2_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_10",
            "tgt_ix": "331-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_10",
            "tgt_ix": "331-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_10",
            "tgt_ix": "331-ARR_v2_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_10",
            "tgt_ix": "331-ARR_v2_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_10",
            "tgt_ix": "331-ARR_v2_10@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_10",
            "tgt_ix": "331-ARR_v2_10@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_10",
            "tgt_ix": "331-ARR_v2_10@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_11",
            "tgt_ix": "331-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_12",
            "tgt_ix": "331-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_13",
            "tgt_ix": "331-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_14",
            "tgt_ix": "331-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_14",
            "tgt_ix": "331-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_15",
            "tgt_ix": "331-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_15",
            "tgt_ix": "331-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_15",
            "tgt_ix": "331-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_15",
            "tgt_ix": "331-ARR_v2_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_16",
            "tgt_ix": "331-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_17",
            "tgt_ix": "331-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_17",
            "tgt_ix": "331-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_17",
            "tgt_ix": "331-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_18",
            "tgt_ix": "331-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_19",
            "tgt_ix": "331-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_20",
            "tgt_ix": "331-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_21",
            "tgt_ix": "331-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_21",
            "tgt_ix": "331-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_21",
            "tgt_ix": "331-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_22",
            "tgt_ix": "331-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_23",
            "tgt_ix": "331-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_23",
            "tgt_ix": "331-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_23",
            "tgt_ix": "331-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_24",
            "tgt_ix": "331-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_25",
            "tgt_ix": "331-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_25",
            "tgt_ix": "331-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_25",
            "tgt_ix": "331-ARR_v2_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_26",
            "tgt_ix": "331-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_26",
            "tgt_ix": "331-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_26",
            "tgt_ix": "331-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_26",
            "tgt_ix": "331-ARR_v2_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_26",
            "tgt_ix": "331-ARR_v2_26@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_27",
            "tgt_ix": "331-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_27",
            "tgt_ix": "331-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_28",
            "tgt_ix": "331-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_29",
            "tgt_ix": "331-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_30",
            "tgt_ix": "331-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_31",
            "tgt_ix": "331-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_32",
            "tgt_ix": "331-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_33",
            "tgt_ix": "331-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_34",
            "tgt_ix": "331-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_35",
            "tgt_ix": "331-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_36",
            "tgt_ix": "331-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_37",
            "tgt_ix": "331-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_38",
            "tgt_ix": "331-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_39",
            "tgt_ix": "331-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_40",
            "tgt_ix": "331-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_40",
            "tgt_ix": "331-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_40",
            "tgt_ix": "331-ARR_v2_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_41",
            "tgt_ix": "331-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_42",
            "tgt_ix": "331-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_42",
            "tgt_ix": "331-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_42",
            "tgt_ix": "331-ARR_v2_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_43",
            "tgt_ix": "331-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_43",
            "tgt_ix": "331-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_43",
            "tgt_ix": "331-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_43",
            "tgt_ix": "331-ARR_v2_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_43",
            "tgt_ix": "331-ARR_v2_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_43",
            "tgt_ix": "331-ARR_v2_43@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_44",
            "tgt_ix": "331-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_45",
            "tgt_ix": "331-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_45",
            "tgt_ix": "331-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_45",
            "tgt_ix": "331-ARR_v2_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_45",
            "tgt_ix": "331-ARR_v2_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_45",
            "tgt_ix": "331-ARR_v2_45@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_45",
            "tgt_ix": "331-ARR_v2_45@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_46",
            "tgt_ix": "331-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_46",
            "tgt_ix": "331-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_47",
            "tgt_ix": "331-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_47",
            "tgt_ix": "331-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_47",
            "tgt_ix": "331-ARR_v2_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_48",
            "tgt_ix": "331-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_49",
            "tgt_ix": "331-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_50",
            "tgt_ix": "331-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_51",
            "tgt_ix": "331-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_52",
            "tgt_ix": "331-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_53",
            "tgt_ix": "331-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_53",
            "tgt_ix": "331-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_53",
            "tgt_ix": "331-ARR_v2_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_53",
            "tgt_ix": "331-ARR_v2_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_53",
            "tgt_ix": "331-ARR_v2_53@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_53",
            "tgt_ix": "331-ARR_v2_53@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_53",
            "tgt_ix": "331-ARR_v2_53@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_53",
            "tgt_ix": "331-ARR_v2_53@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_54",
            "tgt_ix": "331-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_55",
            "tgt_ix": "331-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_55",
            "tgt_ix": "331-ARR_v2_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_55",
            "tgt_ix": "331-ARR_v2_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_55",
            "tgt_ix": "331-ARR_v2_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_55",
            "tgt_ix": "331-ARR_v2_55@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_56",
            "tgt_ix": "331-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_57",
            "tgt_ix": "331-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_57",
            "tgt_ix": "331-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_57",
            "tgt_ix": "331-ARR_v2_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_58",
            "tgt_ix": "331-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_58",
            "tgt_ix": "331-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_58",
            "tgt_ix": "331-ARR_v2_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_59",
            "tgt_ix": "331-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_60",
            "tgt_ix": "331-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_61",
            "tgt_ix": "331-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_62",
            "tgt_ix": "331-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_62",
            "tgt_ix": "331-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_62",
            "tgt_ix": "331-ARR_v2_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_62",
            "tgt_ix": "331-ARR_v2_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_62",
            "tgt_ix": "331-ARR_v2_62@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_62",
            "tgt_ix": "331-ARR_v2_62@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_63",
            "tgt_ix": "331-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_63",
            "tgt_ix": "331-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_63",
            "tgt_ix": "331-ARR_v2_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_64",
            "tgt_ix": "331-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_65",
            "tgt_ix": "331-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_65",
            "tgt_ix": "331-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_65",
            "tgt_ix": "331-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_65",
            "tgt_ix": "331-ARR_v2_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_65",
            "tgt_ix": "331-ARR_v2_65@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_65",
            "tgt_ix": "331-ARR_v2_65@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_65",
            "tgt_ix": "331-ARR_v2_65@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_65",
            "tgt_ix": "331-ARR_v2_65@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_65",
            "tgt_ix": "331-ARR_v2_65@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_65",
            "tgt_ix": "331-ARR_v2_65@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_66",
            "tgt_ix": "331-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_67",
            "tgt_ix": "331-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_67",
            "tgt_ix": "331-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@15",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@16",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@17",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@18",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@19",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@20",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@21",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@22",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@23",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@24",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@25",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@26",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@27",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@28",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_68",
            "tgt_ix": "331-ARR_v2_68@29",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_69",
            "tgt_ix": "331-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_69",
            "tgt_ix": "331-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_70",
            "tgt_ix": "331-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_71",
            "tgt_ix": "331-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_71",
            "tgt_ix": "331-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_71",
            "tgt_ix": "331-ARR_v2_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_71",
            "tgt_ix": "331-ARR_v2_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_72",
            "tgt_ix": "331-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_72",
            "tgt_ix": "331-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_72",
            "tgt_ix": "331-ARR_v2_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_72",
            "tgt_ix": "331-ARR_v2_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_73",
            "tgt_ix": "331-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_73",
            "tgt_ix": "331-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_73",
            "tgt_ix": "331-ARR_v2_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_74",
            "tgt_ix": "331-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_74",
            "tgt_ix": "331-ARR_v2_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_74",
            "tgt_ix": "331-ARR_v2_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_75",
            "tgt_ix": "331-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_75",
            "tgt_ix": "331-ARR_v2_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_75",
            "tgt_ix": "331-ARR_v2_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_76",
            "tgt_ix": "331-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_77",
            "tgt_ix": "331-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_77",
            "tgt_ix": "331-ARR_v2_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_78",
            "tgt_ix": "331-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_78",
            "tgt_ix": "331-ARR_v2_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_78",
            "tgt_ix": "331-ARR_v2_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_79",
            "tgt_ix": "331-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_79",
            "tgt_ix": "331-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_80",
            "tgt_ix": "331-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_81",
            "tgt_ix": "331-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_81",
            "tgt_ix": "331-ARR_v2_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_81",
            "tgt_ix": "331-ARR_v2_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_81",
            "tgt_ix": "331-ARR_v2_81@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_81",
            "tgt_ix": "331-ARR_v2_81@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_81",
            "tgt_ix": "331-ARR_v2_81@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_82",
            "tgt_ix": "331-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_83",
            "tgt_ix": "331-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_84",
            "tgt_ix": "331-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_85",
            "tgt_ix": "331-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_86",
            "tgt_ix": "331-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_87",
            "tgt_ix": "331-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_88",
            "tgt_ix": "331-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_89",
            "tgt_ix": "331-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_90",
            "tgt_ix": "331-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_91",
            "tgt_ix": "331-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_92",
            "tgt_ix": "331-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_93",
            "tgt_ix": "331-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_94",
            "tgt_ix": "331-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_95",
            "tgt_ix": "331-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_96",
            "tgt_ix": "331-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_97",
            "tgt_ix": "331-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_98",
            "tgt_ix": "331-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_99",
            "tgt_ix": "331-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_100",
            "tgt_ix": "331-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_101",
            "tgt_ix": "331-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_102",
            "tgt_ix": "331-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_103",
            "tgt_ix": "331-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_104",
            "tgt_ix": "331-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_105",
            "tgt_ix": "331-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_106",
            "tgt_ix": "331-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_107",
            "tgt_ix": "331-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_108",
            "tgt_ix": "331-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_109",
            "tgt_ix": "331-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_110",
            "tgt_ix": "331-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_111",
            "tgt_ix": "331-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_112",
            "tgt_ix": "331-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_113",
            "tgt_ix": "331-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_114",
            "tgt_ix": "331-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_115",
            "tgt_ix": "331-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_116",
            "tgt_ix": "331-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_117",
            "tgt_ix": "331-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_118",
            "tgt_ix": "331-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_119",
            "tgt_ix": "331-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_120",
            "tgt_ix": "331-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_121",
            "tgt_ix": "331-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_122",
            "tgt_ix": "331-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_123",
            "tgt_ix": "331-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_124",
            "tgt_ix": "331-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_125",
            "tgt_ix": "331-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_126",
            "tgt_ix": "331-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_127",
            "tgt_ix": "331-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_128",
            "tgt_ix": "331-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "331-ARR_v2_129",
            "tgt_ix": "331-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 923,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "331-ARR",
        "version": 2
    }
}