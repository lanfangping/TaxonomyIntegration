{
    "nodes": [
        {
            "ix": "405-ARR_v1_0",
            "content": "Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_2",
            "content": "Prompting language models (LMs) with training examples and task descriptions has been seen as critical to recent successes in few-shot learning. In this work, we show that finetuning LMs in the few-shot setting can considerably reduce the need for prompt engineering. In fact, one can use null prompts, prompts that contain neither task-specific templates nor training examples, and achieve competitive accuracy to manually-tuned prompts across a wide range of tasks. While finetuning LMs does introduce new parameters for each downstream task, we show that this memory overhead can be substantially reduced: finetuning only the bias terms can achieve comparable or better accuracy than standard finetuning while only updating 0.1% of the parameters. All in all, we recommend finetuning LMs for fewshot learning as it is more accurate, robust to different prompts, and can be made nearly as efficient as using frozen LMs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "405-ARR_v1_4",
            "content": "Few-shot learning-the ability to learn tasks with limited examples-is an important academic and practical challenge (Lake et al., 2015). In stateof-the-art NLP, few-shot learning is performed by reformulating tasks as natural language \"prompts\" and completing those prompts with pre-trained language models (Brown et al., 2020;Schick and Sch\u00fctze, 2021a). Prompts that are well-designed can substantially improve accuracy (Zhao et al., 2021;Lu et al., 2021). However, finding these prompts is difficult: it requires a non-trivial combinatorial search over the prompt's wording (a.k.a. its pattern or template), whether and how to include training examples, and how to convert language model probabilities into class predictions. Consequently, prompts are often designed using human intuition that is hard to replicate and apply in a principled manner (Perez et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_5",
            "content": "In this work, we seek to mitigate prompt engineering by identifying a class of simple prompts that are effective across many tasks for masked language models (LMs). We find that, when using prompt-based finetuning (Schick and Sch\u00fctze, 2021a;Gao et al., 2021), the prompt requires less optimization than previously thought; in fact, the pattern and training examples can be completely cut out (e.g., Figure 1, right). These null prompts-simple concatenations of the inputs and the [MASK] token-achieve comparable accuracy to manually-written patterns while drastically simplifying prompt design: users only need to decide the label names (a.k.a. the verbalizer) and where to place the [MASK] token. The effectiveness of null prompts also challenges the common wisdom that the success of few-shot learning is due to inductive biases present in the prompt.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_6",
            "content": "A key drawback of prompt-based finetuning is that it has large memory requirements for each new downstream task at inference time (Figure 1, left). In contrast, in-context learning (Brown et al., 2020) allows reusing the large-scale LM across tasks, but it requires significant prompt engineering. To determine whether memory efficiency and simple prompt selection can be simultaneously achieved, we experiment with either: (a) making prompts for in-context learning similarly easy to create, or (b) making prompt-based finetuning more memory efficient. For (a), we simplify prompt engineering for in-context learning by automatically tuning the prompt's tokens or embeddings, an approach that has been successful in the non-few-shot setting (Shin et al., 2020;Lester et al., 2021). For (b), we study lightweight finetuning alternatives that update a smaller set of parameters: BitFit (Ben-Zaken et al., 2021), Adapters (Houlsby et al., 2019), and calibration layers (Zhao et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_7",
            "content": "We show that the latter approach-prompt-based finetuning with lightweight updates-is considerably more successful. In particular, learning only the model's bias terms (BitFit) can achieve competitive or better few-shot accuracy than standard finetuning while only requiring switching out 0.1% of the parameters at inference time to perform different tasks. On the other hand, automated prompt tuning for in-context learning generally fails to find prompts that are competitive with manual ones. Taken together, our results show that prompt-based finetuning is preferable because it is more accurate, more robust across prompts, and can be made nearly as efficient as using frozen LMs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_8",
            "content": "Prompting Language Models",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "405-ARR_v1_9",
            "content": "We use masked LMs for few-shot learning. Following Schick and Sch\u00fctze (2021a), we have:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_10",
            "content": "\u2022 a pre-trained masked LM, with T denoting its vocabulary and T * the set of all token sequences. \u2022 a small set of training inputs x i \u2208 X and their corresponding labels y i \u2208 Y . \u2022 a pattern P : X \u2192 T * that maps inputs to cloze questions containing a single [MASK] token. Additionally, a verbalizer v : Y \u2192 T that maps each label to a single vocabulary token. We call the pattern and verbalizer together the prompt. In our work, we consider different ways of constructing the prompt (Section 2.1) and updating the masked LM's parameters (Section 2.2). Table 1 contains an overview of existing prompting methods and the settings they are evaluated in.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_11",
            "content": "Constructing the Prompt",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "405-ARR_v1_12",
            "content": "The prompt is important: different prompts can cause accuracy to vary from near chance to near state-of-the-art (Zhao et al., 2021). However, finding good prompts can be difficult. Prompt construction requires a non-trivial combinatorial search over the prompt's wording, whether to include training examples, and how to convert LM probabilities to class predictions. As a consequence, prompts are either designed using human intuition that is hard to replicate and apply in a principled manner (Perez et al., 2021), or using automated methods (Shin et al., 2020;Gao et al., 2021;Lu et al., 2021). These methods search for elements such as (1) the text of the pattern, (2) the tokens in the verbalizers, and (3) whether and how training examples are prepended before the test input. Although automated prompt search can match the accuracy of manual tuning, it introduces its own complexities. For example, the prompts from Gao et al. (2021) achieve comparable results to manually-designed prompts but are found using generative models and careful validation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_13",
            "content": "In this paper, we show that prompt-based finetuning (see Section 2.2) can considerably reduce the importance of the prompt. This does not contradict past work-the extreme importance of the prompt is only true when models are not finetuned.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_14",
            "content": "Prompting Approaches for Few-Shot Learning",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "405-ARR_v1_15",
            "content": "In-context Learning An increasingly popular strategy for few-shot learning is prompting frozen LMs (Brown et al., 2020). This strategy relies solely on in-context learning (a.k.a. priming), where the LM learns by conditioning on the prompt rather than updating its parameters. In-context",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_16",
            "content": "Finetuned Params Prompt Design Few-shot AUTOPROMPT (Shin et al., 2020) None Learned (Discrete) Prompt Tuning (Lester et al., 2021) Prompt Token Embeds Learned (Continuous) OPTIPROMPT (Zhong et al., 2021) Prompt Token Embeds Learned (Continuous) Soft Prompts (Qin and Eisner, 2021) All Contextualized Embeds Learned (Continuous)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_17",
            "content": "GPT-3 (Brown et al., 2020) None Manual PET (Schick and Sch\u00fctze, 2021a) All Manual LM-BFF (Gao et al., 2021) All Learned (Discrete) P-Tuning (Liu et al., 2021) All + Prompt Token Embeds Learned (Continuous) Null Prompts + Bitfit (Ours)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_18",
            "content": "Bias Terms None learning has been shown to be successful when using very large (e.g., billions of parameters) LMs, as these models better leverage the prompt.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_19",
            "content": "Prompt-Based Finetuning Rather than using frozen LMs, prompt-based finetuning methods finetune all of the LM's parameters (Schick and Sch\u00fctze, 2021a;Scao and Rush, 2021;Gao et al., 2021). For masked LMs, this is done by constructing training examples that contain a [MASK] token and finetuning the masked LM to generate the correct verbalizer token in that position.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_20",
            "content": "The main advantage of prompt-based finetuning over in-context learning is that it achieves higher accuracy, especially when the LM is relatively small, e.g., millions of parameters (Schick and Sch\u00fctze, 2021b). The main downside is that the same model can no longer be reused across different tasks, thus reducing efficiency. The efficiency is impacted in two ways. First, it requires large amounts of disk space at test time because numerous model checkpoints must be stored. Second, during training time, it requires large amounts of GPU memory to perform updates on massive LMs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_21",
            "content": "In this paper, we will show an additional benefit to prompt-based finetuning-it makes prompt engineering easier. Moreover, we will show that the memory inefficiency of prompt-based finetuning can be drastically mitigated using lightweight finetuning alternatives. These lightweight methods allow one to switch out only a small subset of model parameters at inference time in order to solve multiple tasks, and also drastically reduce training-time memory costs. Moreover, in many cases these lightweight methods also improve model accuracy. Our work is related to Scao and Rush (2021), who concurrently show that different manually-written patterns lead to similar accuracy for prompt-based finetuning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_22",
            "content": "3 Experimental Setup",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_23",
            "content": "Datasets and Hyperparameter Tuning",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "405-ARR_v1_24",
            "content": "We use the following classification datasets from GLUE (Wang et al., 2019b) and Super-GLUE (Wang et al., 2019a): BoolQ, CB, MNLI, MRPC, QNLI, QQP, RTE, and SST-2. 1 To build few-shot datasets, past work collects K examples from each label for training and K examples from each label for development (Gao et al., 2021). Despite this setup often being denoted as K-shot learning, it effectively uses 2K examples and splits the examples evenly into train and development. We instead propose to use cross validation to perform more principled model selection. Concretely, we sample 2K examples from each label and use 4-fold cross validation to determine the best hyperparameters. After finding the best hyperparameters, we train on the first K examples and early stop on the second K examples. We use K = 16 following past work (Gao et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_25",
            "content": "We sample our examples from each dataset's original training set. Since transformers have been observed can be high variance (Dodge et al., 2020), we initialize the model parameters with 10 different random seeds and report the mean and variance of the model performance. We use each dataset's original development set for our final evaluation and use the standard evaluation metrics (accuracy or F 1 ) associated with each dataset. We do not check the final evaluation metrics during any tuning of the hyperparameters to ensure that we are doing \"true\" few-shot learning (Perez et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_26",
            "content": "Figure 2: How # Wins are Computed. For a given dataset, we perform a Welch's t-test to determine if there is a significant difference in accuracy for each pair of methods. The method which performs better than most other methods (i.e., the row with the most yellow squares; BitFit in this case) is considered the \"winner\" of the task, and its # Wins is incremented by 1. In the figure above, we show a subset of methods evaluated on a single dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_27",
            "content": "Masked Language Models",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "405-ARR_v1_28",
            "content": "Following past work (Schick and Sch\u00fctze, 2021b), we use the RoBERTa (large, 330M params, Liu et al., 2019) and ALBERT (xxl-v2, 223M params, Lan et al., 2019) masked LMs provided by the Hug-gingFace transformers library (Wolf et al., 2020). Training and evaluation were performed on a heterogeneous compute cluster with the following minimum specs: 2xNVIDIA GeForce GTX 1080 Ti's, 8-core Intel Core i7 CPU, 64 GB RAM.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_29",
            "content": "Comparing Few-shot Methods by # Wins",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "405-ARR_v1_30",
            "content": "The results for different few-shot learning methods can be quite different across datasets and seeds for the training set (Zhao et al., 2021;Schick and Sch\u00fctze, 2021a). To compare different methods at a high level, we use a metric denoted as # Wins: the number of datasets that a given method performs significantly better than all other methods on. We compute this metric for a given dataset by first performing a Welch's t-test to determine if there is a significant difference in accuracy for each pair of methods. The method which performs better than most other methods is considered the \"winner\" of the task and its # Wins is incremented by 1. There are multiple winners in the case of a tie. See Figure 2 for a demonstration.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_31",
            "content": "Simplifying Prompt Engineering",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "405-ARR_v1_32",
            "content": "In this section, we run prompt-based finetuning and ablate different elements of the prompt. We consider the following ablations:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_33",
            "content": "\u2022 Manual Prompt (Prior): We use manuallywritten prompts from Schick and Sch\u00fctze (2021a,b), and Gao et al. (2021). We show the patterns and verbalizers in Appendix A1. \u2022 Manual Prompt (w/o Engineering): We simulate standard prompt design by manually writing one prompt for each task using our intuition. We show the prompts in Appendix A2. \u2022 Prompt Tuning: Inspired by Liu et al. (2021) and Lester et al. (2021), we use the pattern from Manual Prompt (Prior) but randomly initialize the embeddings of the pattern tokens and learn them using gradient-based optimization. This ablates the gains from human-designed patterns. \u2022 Null Prompt: We use the same verbalizer as Manual Prompt (Prior) but use a pattern that consists of only the input fields and a [MASK] token (Appendix A3). This ablates the pattern entirely. \u2022 Null Verbalizer: We use the same pattern as Manual Prompt (Prior) but-following Opitz (2019) and Scao and Rush (2021)-select random tokens for the verbalizer. This ablates the gains from a human-designed verbalizer. \u2022 Null Prompt + Verbalizer We use both null prompts and random tokens for the verbalizer.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_34",
            "content": "In all cases, we finetune all of the masked LM parameters. We show the accuracy of the above prompts as well as traditional finetuning (using a",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_35",
            "content": "[CLS] token and a classification head) in Figure 3. 2",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_36",
            "content": "Manual Prompts Perform Best The manuallywritten prompts from prior work perform best on average for both models. On the other hand, our manual prompts (w/o Engineering) are noticeably worse than the ones from prior work and are outperformed by many other methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_37",
            "content": "Null Prompts Are Competitive In many cases, prompt tuning and null prompts perform comparably to manually-written prompts, especially for RoBERTa. For instance, both of these methods outperform our manually-written prompts in terms The only decision to make when using null prompts is which order to concatenate the mask token and the input fields. One can robustly choose the best option using a tiny held-out development set. We show the results for MNLI, with the fewshot development set accuracy on the x-axis.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_38",
            "content": "B o o l Q C B M N L I -m M N L I -m m M R P C Q N L I Q Q P R T E S S T -2 0 25 50 75 100 RoBERTa (Large) # Wins 0 5 B o o l Q C B M N L I -m M N L I -m m M R P C Q N L I Q Q P R T E S S",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_39",
            "content": "of # Wins. These results are exciting from a practical perspective as they show that one can achieve competitive few-shot results without resorting to any tuning of the prompt.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_40",
            "content": "From an analysis perspective, these results also show that effective few-shot learning can be accomplished without any inductive bias from a manuallywritten pattern. In fact, combining null prompts with null verbalizers, which involves no human design at all, still significantly outperforms standard [CLS] finetuning for numerous tasks (3 for RoBERTa and 5 for ALBERT at p = 0.05). This shows that some of the effectiveness of promptbased finetuning is due to its basic setup, i.e., predicting on a [MASK] token with an MLM head.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_41",
            "content": "Null Prompts or Prompt Tuning? Both null prompts and prompt tuning achieve competitive results without resorting to manual prompt design. We advocate for using null prompts over prompt tuning because they are easier to use. Null prompts only require choosing which order to concatenate the input fields and the [MASK] token. Prompt tuning requires choosing the number of embeddings, their placement, their initialization, etc.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_42",
            "content": "Null Prompts Simplify Prompt Search One complication that arises in standard prompt-based finetuning is that prompts become a hyperparameter of the finetuning procedure, and have a combinatorially large search space. On the other hand, determining the concatenation order for null prompts is trivial by just trying all of the few possible options and choosing which one works best on the validation set. To see this, in Figure 4 we plot the accuracy on the few-shot development set and the full test set for different concatenation orders for RoBERTa on MNLI. 3 The development and test accuracy is strongly correlated (R 2 = 79.05), which demonstrates that tuning the concatenation order is easy even when validation data is scarce.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_43",
            "content": "We next investigate whether the observations made in the previous para- Figure 5: Impact of Dataset Size. We plot a subset of learning curves for K \u2208 {4, 8, 16, 32} (results for all datasets are provided in Appendix A1). Shaded regions indicate the range of performance across 10 different random seeds. In general, we find that as K increases the accuracy of prompt tuning with null prompts tends to be close to that of manual prompts, and substantially better than traditional finetuning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_44",
            "content": "graphs hold across different dataset sizes. Intuitively, when the amount of data is small, manual prompts may outperform other approaches because the inductive bias provided by the prompt has the most impact when there is little data to learn the task at hand. In Figure 5 we compare the accuracy of prompt-based finetuning using manually-written prompts and null prompts to traditional finetuning, using the same setup described in Section 3.1 but varying K \u2208 {4, 8, 16, 32}. Full results for all datasets are provided in Appendix A1). Although there is some instability at lower values of K, we find that the accuracy of both prompt-based finetuning approaches tends to be similar, and is either substantially better or on-par with traditional finetuning. In other words, null prompts are competitive with manual prompts, even when K is small.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_45",
            "content": "Achieving Simplicity and Efficiency",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "405-ARR_v1_46",
            "content": "Thus far, we have shown that prompt-based finetuning can simplify prompt engineering at the cost of memory inefficiency-a new set of parameters must be learned for each task. This is in contrast to in-context learning, which holds all model weights fixed but is heavily influenced by small prompt modifications (Zhao et al., 2021;Lu et al., 2021). In this section, we investigate how to memory efficiency and simple prompts. Concretely, in Section 5.1 we try to simplify prompt engineering for in-context learning by tuning the prompt, and in Section 5.2, we reduce the number of learned parameters for prompt-based finetuning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_47",
            "content": "Simplifying In-Context Learning With",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "405-ARR_v1_48",
            "content": "Prompt-Only Tuning Here, we try to make prompt engineering for incontext learning as simple as prompt-based finetuning by automatically finding the prompt. Concretely, we focus on the emerging class of methods that do prompt-only tuning: learning the prompt while keeping the rest of the model fixed (Shin et al., 2020;Lester et al., 2021). We consider:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_49",
            "content": "\u2022 AUTOPROMPT: Following (Shin et al., 2020), we search for discrete tokens to use in the input instead of manually-designed patterns. We use the hyperparameters from Shin et al. (2020). \u2022 Prompt Tuning (Short): We use the same prompt tuning approach described in the previous section but we keep the masked LM fixed. \u2022 Prompt Tuning (Long): Based on the advice of Lester et al. (2021), we increase the number of learned prompt embeddings to 20 in order to expand the learning capacity.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_50",
            "content": "For reference, we also report the results from prompt-based finetuning with null prompts. We show the results for RoBERTa in Figure 6. We find that only tuning the prompt is relatively unsuccessful. First, on average it fails to match the performance of manually-designed prompts. Second, all methods struggle to match the accuracy of prompt-based finetuning. In fact, for many of the datasets, prompt-only methods perform worse by a wide margin (e.g., 40% absolute difference in F 1 score on CB). This shows that finetuning masked LMs in the few-shot setting leads to substantially higher accuracy than prompt-only tuning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_51",
            "content": "Work We find that only tuning the prompt performs substantially worse than finetuning the entire LM. This is in contrast to recent work, which argues that prompt-only tuning is competitive with finetuning (Lester et al., 2021;Li and Liang, 2021). We believe these are not contradictions but rather differences in the models and settings. Li and Liang Figure 6: Prompt-Only Tuning. We try to simplify prompt engineering for in-context learning (i.e., using frozen models) by directly learning the prompt. The performance (accuracy/F 1 ) for prompt-only tuning is substantially lower than finetuning the LM parameters for RoBERTa-large. Thus, we recommend finetuning over in-context learning in the few-shot setting. (2021) focus on left-to-right LMs for generation tasks, whereas we focus on masked LMs for classification tasks. This may explain the difference in the results. Moreover, Lester et al. (2021) show that prompt-only tuning becomes less competitive as models get smaller; we use even smaller models than evaluated in their work. Consequently, although we find that finetuning a masked LM is superior to prompt-only tuning, there may be other settings in which they fair similarly.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_52",
            "content": "B o o l Q C B M N L I -m M N L I -m m M R P C Q N L I Q Q P R T E S S",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_53",
            "content": "B o o l Q C B M N L I -m M N L I -m m M R P C Q N L I Q Q P R T E S S",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_54",
            "content": "Memory-Efficient Finetuning",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "405-ARR_v1_55",
            "content": "Given the inadequacies of prompt-only tuning, we next study if prompt-based finetuning can be made memory-efficient. To do so, we focus on reducing the number of trainable parameters, taking inspiration from recent work in the non-few-shot setting. The benefits of these methods is that they (1) reduce storage costs at test time when running many tasks (one can store only the modified parameters for each task), and (2) reduce memory costs at training time, as fewer optimized parameters means much smaller statistics in optimizers like Adam. We consider four lightweight finetuning methods:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_56",
            "content": "\u2022 Adapters: We use Adapters (Houlsby et al., 2019), neural networks layers that are inserted between the feedforward portion of the Transformer architecture. We use the default Adapters hyperparameters from Houlsby et al. ( 2019) (\u2248 10 7 parameters per task). \u2022 BitFit: Following Ben-Zaken et al. ( 2021), we only update the bias terms inside the Transformer (\u2248 10 5 parameters per task). \u2022 LM Head Tuning: We update the embeddings in the MLM output layer that are associated with the verbalizer tokens (\u2248 10 3 parameters per task). \u2022 Calibration: Following Zhao et al. ( 2021), we learn an affine transformation on top of the logits associated with the verbalizer tokens (\u2248 10 1 parameters per task). We run prompt-based finetuning for each method with the prompts from Manual Prompts (Prior). We also report the accuracy of finetuning all of the parameters for reference.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_57",
            "content": "We show the results in Figure 7. There are diminishing returns as the parameter count is increased. In particular, substantial gains are made when going from calibration to LM head tuning to BitFit, however, there is either a marginal improvement or even a decrease in performance when going to Adapters or All Parameters. The BitFit method provides the best accuracy-efficiency trade-off, and even outperforms finetuning all of the parameters in terms of # Wins. This suggests that updating all of the LM's hundreds of millions of parameters on only 16 data points is suboptimal.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_58",
            "content": "Putting Everything Together",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "405-ARR_v1_59",
            "content": "We finally combine null prompts and memoryefficient finetuning. We show the results from this method, as well as the other best few-shot methods, in Table 2. Overall, we recommend finetuning with null prompts and BitFit: it achieves competitive accuracy, is simple to set up, and introduces small memory costs for each new task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_60",
            "content": "Conclusion and Future Work",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "405-ARR_v1_61",
            "content": "Two high-level methods exist in few-shot prompting: using a frozen LM (in-context learning) and finetuning the LM on the few training examples (prompt-based finetuning). In this work, we demonstrate two new advantages of prompt-based finetuning. First, we show that it is robust to different choices of the prompt. In fact, there is a simple class of prompts-null prompts-that can be flexibly applied to different tasks without degrading performance relative to manually-written and learned prompts. Second, we demonstrate that promptbased finetuning can be made memory efficient: finetuning only the bias terms (BitFit) achieves comparable or better accuracy than finetuning all the parameters while being 1000x more memory efficient. Taken together, using null patterns with BitFit is an approach that is efficient, simple-totune, and competitive in accuracy. Code and instructions for reproducing our results is available at: omitted.link. 4 Our results motivate future analysis of few-shot learning methods. Concretely, we show that the success of prompt-based finetuning is not solely explained by carefully-chosen patterns or verbalizers. This suggests that the gains from promptbased finetuning are partially due to its low-level setup, i.e., predicting on a [MASK] token with a pre-trained MLM head. More generally, we hope to further analyze why and how small changes to different few-shot learning methods can lead to wildly different accuracies. We also hope to extend our findings to both very large and left-to-right LMs, as our current results are for masked LMs that are relatively small by modern standards.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "405-ARR_v1_62",
            "content": "UNKNOWN, None, 2021, BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_63",
            "content": "UNKNOWN, None, , NeurIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "NeurIPS",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_64",
            "content": "UNKNOWN, None, 2020, Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_65",
            "content": "Tianyu Gao, Adam Fisch, Danqi Chen, Making pre-trained language models better few-shot learners, 2021, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Tianyu Gao",
                    "Adam Fisch",
                    "Danqi Chen"
                ],
                "title": "Making pre-trained language models better few-shot learners",
                "pub_date": "2021",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_66",
            "content": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, Parameter-efficient transfer learning for NLP, 2019, ICML, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Neil Houlsby",
                    "Andrei Giurgiu",
                    "Stanislaw Jastrzebski",
                    "Bruna Morrone",
                    "Quentin De Laroussilhe",
                    "Andrea Gesmundo",
                    "Mona Attariyan",
                    "Sylvain Gelly"
                ],
                "title": "Parameter-efficient transfer learning for NLP",
                "pub_date": "2019",
                "pub_title": "ICML",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_67",
            "content": "UNKNOWN, None, 2015, Human-level concept learning through probabilistic program induction, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Human-level concept learning through probabilistic program induction",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_68",
            "content": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, ALBERT: A lite BERT for self-supervised learning of language representations, 2019, ICLR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Zhenzhong Lan",
                    "Mingda Chen",
                    "Sebastian Goodman",
                    "Kevin Gimpel",
                    "Piyush Sharma",
                    "Radu Soricut"
                ],
                "title": "ALBERT: A lite BERT for self-supervised learning of language representations",
                "pub_date": "2019",
                "pub_title": "ICLR",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_69",
            "content": "UNKNOWN, None, 2021, The power of scale for parameter-efficient prompt tuning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "The power of scale for parameter-efficient prompt tuning",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_70",
            "content": "UNKNOWN, None, 2021, Prefix-Tuning: Optimizing continuous prompts for generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Prefix-Tuning: Optimizing continuous prompts for generation",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_71",
            "content": "UNKNOWN, None, , Zhilin Yang, and Jie Tang. 2021. GPT understands, too, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Zhilin Yang, and Jie Tang. 2021. GPT understands, too",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_72",
            "content": "UNKNOWN, None, 2019, RoBERTa: A robustly optimized BERT pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "RoBERTa: A robustly optimized BERT pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_73",
            "content": "UNKNOWN, None, 2021, Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_74",
            "content": "Marius Mosbach, Maksym Andriushchenko, Dietrich Klakow, On the stability of fine-tuning BERT: Misconceptions, explanations, and strong baselines, 2021, ICLR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Marius Mosbach",
                    "Maksym Andriushchenko",
                    "Dietrich Klakow"
                ],
                "title": "On the stability of fine-tuning BERT: Misconceptions, explanations, and strong baselines",
                "pub_date": "2021",
                "pub_title": "ICLR",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_75",
            "content": "Juri Opitz, Argumentative relation classification as plausibility ranking, 2019, Proceedings of the 15th Conference on Natural Language Processing, German Society for Computational Linguistics & Language Technology.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Juri Opitz"
                ],
                "title": "Argumentative relation classification as plausibility ranking",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 15th Conference on Natural Language Processing",
                "pub": "German Society for Computational Linguistics & Language Technology"
            }
        },
        {
            "ix": "405-ARR_v1_76",
            "content": "UNKNOWN, None, 2021, True few-shot learning with language models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "True few-shot learning with language models",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_77",
            "content": "Guanghui Qin, Jason Eisner, Learning how to ask: Querying LMs with mixtures of soft prompts, 2021, NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Guanghui Qin",
                    "Jason Eisner"
                ],
                "title": "Learning how to ask: Querying LMs with mixtures of soft prompts",
                "pub_date": "2021",
                "pub_title": "NAACL",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_78",
            "content": "Le Teven, Alexander Scao,  Rush, How many data points is a prompt worth, 2021, NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Le Teven",
                    "Alexander Scao",
                    " Rush"
                ],
                "title": "How many data points is a prompt worth",
                "pub_date": "2021",
                "pub_title": "NAACL",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_79",
            "content": "Timo Schick, Hinrich Sch\u00fctze, Exploiting cloze questions for few-shot text classification and natural language inference, 2021, EACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Timo Schick",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "Exploiting cloze questions for few-shot text classification and natural language inference",
                "pub_date": "2021",
                "pub_title": "EACL",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_80",
            "content": "Timo Schick, Hinrich Sch\u00fctze, It's not just size that matters: Small language models are also few-shot learners, 2021, NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Timo Schick",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "It's not just size that matters: Small language models are also few-shot learners",
                "pub_date": "2021",
                "pub_title": "NAACL",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_81",
            "content": "Taylor Shin, Yasaman Razeghi, I Robert L Logan, Eric Wallace, Sameer Singh, AutoPrompt: Eliciting knowledge from language models with automatically generated prompts, 2020, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Taylor Shin",
                    "Yasaman Razeghi",
                    "I Robert L Logan",
                    "Eric Wallace",
                    "Sameer Singh"
                ],
                "title": "AutoPrompt: Eliciting knowledge from language models with automatically generated prompts",
                "pub_date": "2020",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_82",
            "content": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, SuperGLUE: A stickier benchmark for general-purpose language understanding systems, 2019, NeurIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Alex Wang",
                    "Yada Pruksachatkun",
                    "Nikita Nangia",
                    "Amanpreet Singh",
                    "Julian Michael",
                    "Felix Hill",
                    "Omer Levy",
                    "Samuel Bowman"
                ],
                "title": "SuperGLUE: A stickier benchmark for general-purpose language understanding systems",
                "pub_date": "2019",
                "pub_title": "NeurIPS",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_83",
            "content": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, GLUE: A multi-task benchmark and analysis platform for natural language understanding, 2019, ICLR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Alex Wang",
                    "Amanpreet Singh",
                    "Julian Michael",
                    "Felix Hill",
                    "Omer Levy",
                    "Samuel R Bowman"
                ],
                "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
                "pub_date": "2019",
                "pub_title": "ICLR",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_84",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R'emi Louf, Morgan Funtowicz, Jamie Brew, HuggingFace's Transformers: State-of-the-art natural language processing, 2020, EMNLP Demo Track, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Thomas Wolf",
                    "Lysandre Debut",
                    "Victor Sanh",
                    "Julien Chaumond",
                    "Clement Delangue",
                    "Anthony Moi",
                    "Pierric Cistac",
                    "Tim Rault",
                    "R'emi Louf",
                    "Morgan Funtowicz",
                    "Jamie Brew"
                ],
                "title": "HuggingFace's Transformers: State-of-the-art natural language processing",
                "pub_date": "2020",
                "pub_title": "EMNLP Demo Track",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_85",
            "content": "Z Tony, Eric Zhao, Shi Wallace, Dan Feng, Sameer Klein,  Singh, Calibrate before use: Improving few-shot performance of language models, 2021, ICML, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Z Tony",
                    "Eric Zhao",
                    "Shi Wallace",
                    "Dan Feng",
                    "Sameer Klein",
                    " Singh"
                ],
                "title": "Calibrate before use: Improving few-shot performance of language models",
                "pub_date": "2021",
                "pub_title": "ICML",
                "pub": null
            }
        },
        {
            "ix": "405-ARR_v1_86",
            "content": "Zexuan Zhong, Dan Friedman, Danqi Chen, Factual probing is [MASK]: Learning vs. learning to recall, 2021, NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Zexuan Zhong",
                    "Dan Friedman",
                    "Danqi Chen"
                ],
                "title": "Factual probing is [MASK]: Learning vs. learning to recall",
                "pub_date": "2021",
                "pub_title": "NAACL",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "405-ARR_v1_0@0",
            "content": "Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_0",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_2@0",
            "content": "Prompting language models (LMs) with training examples and task descriptions has been seen as critical to recent successes in few-shot learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_2",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_2@1",
            "content": "In this work, we show that finetuning LMs in the few-shot setting can considerably reduce the need for prompt engineering.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_2",
            "start": 145,
            "end": 266,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_2@2",
            "content": "In fact, one can use null prompts, prompts that contain neither task-specific templates nor training examples, and achieve competitive accuracy to manually-tuned prompts across a wide range of tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_2",
            "start": 268,
            "end": 466,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_2@3",
            "content": "While finetuning LMs does introduce new parameters for each downstream task, we show that this memory overhead can be substantially reduced: finetuning only the bias terms can achieve comparable or better accuracy than standard finetuning while only updating 0.1% of the parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_2",
            "start": 468,
            "end": 749,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_2@4",
            "content": "All in all, we recommend finetuning LMs for fewshot learning as it is more accurate, robust to different prompts, and can be made nearly as efficient as using frozen LMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_2",
            "start": 751,
            "end": 920,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_4@0",
            "content": "Few-shot learning-the ability to learn tasks with limited examples-is an important academic and practical challenge (Lake et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_4",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_4@1",
            "content": "In stateof-the-art NLP, few-shot learning is performed by reformulating tasks as natural language \"prompts\" and completing those prompts with pre-trained language models (Brown et al., 2020;Schick and Sch\u00fctze, 2021a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_4",
            "start": 137,
            "end": 353,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_4@2",
            "content": "Prompts that are well-designed can substantially improve accuracy (Zhao et al., 2021;Lu et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_4",
            "start": 355,
            "end": 456,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_4@3",
            "content": "However, finding these prompts is difficult: it requires a non-trivial combinatorial search over the prompt's wording (a.k.a. its pattern or template), whether and how to include training examples, and how to convert language model probabilities into class predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_4",
            "start": 458,
            "end": 726,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_4@4",
            "content": "Consequently, prompts are often designed using human intuition that is hard to replicate and apply in a principled manner (Perez et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_4",
            "start": 728,
            "end": 870,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_5@0",
            "content": "In this work, we seek to mitigate prompt engineering by identifying a class of simple prompts that are effective across many tasks for masked language models (LMs).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_5",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_5@1",
            "content": "We find that, when using prompt-based finetuning (Schick and Sch\u00fctze, 2021a;Gao et al., 2021), the prompt requires less optimization than previously thought; in fact, the pattern and training examples can be completely cut out (e.g., Figure 1, right).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_5",
            "start": 165,
            "end": 415,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_5@2",
            "content": "These null prompts-simple concatenations of the inputs and the [MASK] token-achieve comparable accuracy to manually-written patterns while drastically simplifying prompt design: users only need to decide the label names (a.k.a. the verbalizer) and where to place the [MASK] token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_5",
            "start": 417,
            "end": 696,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_5@3",
            "content": "The effectiveness of null prompts also challenges the common wisdom that the success of few-shot learning is due to inductive biases present in the prompt.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_5",
            "start": 698,
            "end": 852,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_6@0",
            "content": "A key drawback of prompt-based finetuning is that it has large memory requirements for each new downstream task at inference time (Figure 1, left).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_6",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_6@1",
            "content": "In contrast, in-context learning (Brown et al., 2020) allows reusing the large-scale LM across tasks, but it requires significant prompt engineering.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_6",
            "start": 148,
            "end": 296,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_6@2",
            "content": "To determine whether memory efficiency and simple prompt selection can be simultaneously achieved, we experiment with either: (a) making prompts for in-context learning similarly easy to create, or (b) making prompt-based finetuning more memory efficient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_6",
            "start": 298,
            "end": 552,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_6@3",
            "content": "For (a), we simplify prompt engineering for in-context learning by automatically tuning the prompt's tokens or embeddings, an approach that has been successful in the non-few-shot setting (Shin et al., 2020;Lester et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_6",
            "start": 554,
            "end": 781,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_6@4",
            "content": "For (b), we study lightweight finetuning alternatives that update a smaller set of parameters: BitFit (Ben-Zaken et al., 2021), Adapters (Houlsby et al., 2019), and calibration layers (Zhao et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_6",
            "start": 783,
            "end": 986,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_7@0",
            "content": "We show that the latter approach-prompt-based finetuning with lightweight updates-is considerably more successful.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_7",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_7@1",
            "content": "In particular, learning only the model's bias terms (BitFit) can achieve competitive or better few-shot accuracy than standard finetuning while only requiring switching out 0.1% of the parameters at inference time to perform different tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_7",
            "start": 115,
            "end": 355,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_7@2",
            "content": "On the other hand, automated prompt tuning for in-context learning generally fails to find prompts that are competitive with manual ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_7",
            "start": 357,
            "end": 493,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_7@3",
            "content": "Taken together, our results show that prompt-based finetuning is preferable because it is more accurate, more robust across prompts, and can be made nearly as efficient as using frozen LMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_7",
            "start": 495,
            "end": 683,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_8@0",
            "content": "Prompting Language Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_8",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_9@0",
            "content": "We use masked LMs for few-shot learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_9",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_9@1",
            "content": "Following Schick and Sch\u00fctze (2021a), we have:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_9",
            "start": 41,
            "end": 86,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_10@0",
            "content": "\u2022 a pre-trained masked LM, with T denoting its vocabulary and T * the set of all token sequences. \u2022 a small set of training inputs x i \u2208 X and their corresponding labels y i \u2208 Y . \u2022 a pattern P : X \u2192 T * that maps inputs to cloze questions containing a single [MASK] token. Additionally, a verbalizer v : Y \u2192 T that maps each label to a single vocabulary token. We call the pattern and verbalizer together the prompt. In our work, we consider different ways of constructing the prompt (Section 2.1) and updating the masked LM's parameters (Section 2.2). Table 1 contains an overview of existing prompting methods and the settings they are evaluated in.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_10",
            "start": 0,
            "end": 651,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_11@0",
            "content": "Constructing the Prompt",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_11",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_12@0",
            "content": "The prompt is important: different prompts can cause accuracy to vary from near chance to near state-of-the-art (Zhao et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_12",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_12@1",
            "content": "However, finding good prompts can be difficult.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_12",
            "start": 133,
            "end": 179,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_12@2",
            "content": "Prompt construction requires a non-trivial combinatorial search over the prompt's wording, whether to include training examples, and how to convert LM probabilities to class predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_12",
            "start": 181,
            "end": 366,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_12@3",
            "content": "As a consequence, prompts are either designed using human intuition that is hard to replicate and apply in a principled manner (Perez et al., 2021), or using automated methods (Shin et al., 2020;Gao et al., 2021;Lu et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_12",
            "start": 368,
            "end": 596,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_12@4",
            "content": "These methods search for elements such as (1) the text of the pattern, (2) the tokens in the verbalizers, and (3) whether and how training examples are prepended before the test input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_12",
            "start": 598,
            "end": 781,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_12@5",
            "content": "Although automated prompt search can match the accuracy of manual tuning, it introduces its own complexities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_12",
            "start": 783,
            "end": 891,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_12@6",
            "content": "For example, the prompts from Gao et al. (2021) achieve comparable results to manually-designed prompts but are found using generative models and careful validation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_12",
            "start": 893,
            "end": 1057,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_13@0",
            "content": "In this paper, we show that prompt-based finetuning (see Section 2.2) can considerably reduce the importance of the prompt.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_13",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_13@1",
            "content": "This does not contradict past work-the extreme importance of the prompt is only true when models are not finetuned.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_13",
            "start": 124,
            "end": 238,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_14@0",
            "content": "Prompting Approaches for Few-Shot Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_14",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_15@0",
            "content": "In-context Learning An increasingly popular strategy for few-shot learning is prompting frozen LMs (Brown et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_15",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_15@1",
            "content": "This strategy relies solely on in-context learning (a.k.a. priming), where the LM learns by conditioning on the prompt rather than updating its parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_15",
            "start": 121,
            "end": 275,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_15@2",
            "content": "In-context",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_15",
            "start": 277,
            "end": 286,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_16@0",
            "content": "Finetuned Params Prompt Design Few-shot AUTOPROMPT (Shin et al., 2020) None Learned (Discrete) Prompt Tuning (Lester et al., 2021) Prompt Token Embeds Learned (Continuous) OPTIPROMPT (Zhong et al., 2021) Prompt Token Embeds Learned (Continuous) Soft Prompts (Qin and Eisner, 2021) All Contextualized Embeds Learned (Continuous)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_16",
            "start": 0,
            "end": 326,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_17@0",
            "content": "GPT-3 (Brown et al., 2020) None Manual PET (Schick and Sch\u00fctze, 2021a) All Manual LM-BFF (Gao et al., 2021) All Learned (Discrete) P-Tuning (Liu et al., 2021) All + Prompt Token Embeds Learned (Continuous) Null Prompts + Bitfit (Ours)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_17",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_18@0",
            "content": "Bias Terms None learning has been shown to be successful when using very large (e.g., billions of parameters) LMs, as these models better leverage the prompt.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_18",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_19@0",
            "content": "Prompt-Based Finetuning Rather than using frozen LMs, prompt-based finetuning methods finetune all of the LM's parameters (Schick and Sch\u00fctze, 2021a;Scao and Rush, 2021;Gao et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_19",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_19@1",
            "content": "For masked LMs, this is done by constructing training examples that contain a [MASK] token and finetuning the masked LM to generate the correct verbalizer token in that position.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_19",
            "start": 188,
            "end": 365,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_20@0",
            "content": "The main advantage of prompt-based finetuning over in-context learning is that it achieves higher accuracy, especially when the LM is relatively small, e.g., millions of parameters (Schick and Sch\u00fctze, 2021b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_20",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_20@1",
            "content": "The main downside is that the same model can no longer be reused across different tasks, thus reducing efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_20",
            "start": 210,
            "end": 323,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_20@2",
            "content": "The efficiency is impacted in two ways.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_20",
            "start": 325,
            "end": 363,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_20@3",
            "content": "First, it requires large amounts of disk space at test time because numerous model checkpoints must be stored.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_20",
            "start": 365,
            "end": 474,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_20@4",
            "content": "Second, during training time, it requires large amounts of GPU memory to perform updates on massive LMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_20",
            "start": 476,
            "end": 579,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_21@0",
            "content": "In this paper, we will show an additional benefit to prompt-based finetuning-it makes prompt engineering easier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_21",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_21@1",
            "content": "Moreover, we will show that the memory inefficiency of prompt-based finetuning can be drastically mitigated using lightweight finetuning alternatives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_21",
            "start": 113,
            "end": 262,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_21@2",
            "content": "These lightweight methods allow one to switch out only a small subset of model parameters at inference time in order to solve multiple tasks, and also drastically reduce training-time memory costs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_21",
            "start": 264,
            "end": 460,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_21@3",
            "content": "Moreover, in many cases these lightweight methods also improve model accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_21",
            "start": 462,
            "end": 539,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_21@4",
            "content": "Our work is related to Scao and Rush (2021), who concurrently show that different manually-written patterns lead to similar accuracy for prompt-based finetuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_21",
            "start": 541,
            "end": 701,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_22@0",
            "content": "3 Experimental Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_22",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_23@0",
            "content": "Datasets and Hyperparameter Tuning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_23",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_24@0",
            "content": "We use the following classification datasets from GLUE (Wang et al., 2019b) and Super-GLUE (Wang et al., 2019a): BoolQ, CB, MNLI, MRPC, QNLI, QQP, RTE, and SST-2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_24",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_24@1",
            "content": "1 To build few-shot datasets, past work collects K examples from each label for training and K examples from each label for development (Gao et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_24",
            "start": 163,
            "end": 317,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_24@2",
            "content": "Despite this setup often being denoted as K-shot learning, it effectively uses 2K examples and splits the examples evenly into train and development.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_24",
            "start": 319,
            "end": 467,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_24@3",
            "content": "We instead propose to use cross validation to perform more principled model selection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_24",
            "start": 469,
            "end": 554,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_24@4",
            "content": "Concretely, we sample 2K examples from each label and use 4-fold cross validation to determine the best hyperparameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_24",
            "start": 556,
            "end": 675,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_24@5",
            "content": "After finding the best hyperparameters, we train on the first K examples and early stop on the second K examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_24",
            "start": 677,
            "end": 789,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_24@6",
            "content": "We use K = 16 following past work (Gao et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_24",
            "start": 791,
            "end": 843,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_25@0",
            "content": "We sample our examples from each dataset's original training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_25",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_25@1",
            "content": "Since transformers have been observed can be high variance (Dodge et al., 2020), we initialize the model parameters with 10 different random seeds and report the mean and variance of the model performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_25",
            "start": 66,
            "end": 270,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_25@2",
            "content": "We use each dataset's original development set for our final evaluation and use the standard evaluation metrics (accuracy or F 1 ) associated with each dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_25",
            "start": 272,
            "end": 431,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_25@3",
            "content": "We do not check the final evaluation metrics during any tuning of the hyperparameters to ensure that we are doing \"true\" few-shot learning (Perez et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_25",
            "start": 433,
            "end": 592,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_26@0",
            "content": "Figure 2: How # Wins are Computed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_26",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_26@1",
            "content": "For a given dataset, we perform a Welch's t-test to determine if there is a significant difference in accuracy for each pair of methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_26",
            "start": 35,
            "end": 170,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_26@2",
            "content": "The method which performs better than most other methods (i.e., the row with the most yellow squares; BitFit in this case) is considered the \"winner\" of the task, and its # Wins is incremented by 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_26",
            "start": 172,
            "end": 369,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_26@3",
            "content": "In the figure above, we show a subset of methods evaluated on a single dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_26",
            "start": 371,
            "end": 449,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_27@0",
            "content": "Masked Language Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_27",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_28@0",
            "content": "Following past work (Schick and Sch\u00fctze, 2021b), we use the RoBERTa (large, 330M params, Liu et al., 2019) and ALBERT (xxl-v2, 223M params, Lan et al., 2019) masked LMs provided by the Hug-gingFace transformers library (Wolf et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_28",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_28@1",
            "content": "Training and evaluation were performed on a heterogeneous compute cluster with the following minimum specs: 2xNVIDIA GeForce GTX 1080 Ti's, 8-core Intel Core i7 CPU, 64 GB RAM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_28",
            "start": 240,
            "end": 415,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_29@0",
            "content": "Comparing Few-shot Methods by # Wins",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_29",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_30@0",
            "content": "The results for different few-shot learning methods can be quite different across datasets and seeds for the training set (Zhao et al., 2021;Schick and Sch\u00fctze, 2021a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_30",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_30@1",
            "content": "To compare different methods at a high level, we use a metric denoted as # Wins: the number of datasets that a given method performs significantly better than all other methods on.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_30",
            "start": 169,
            "end": 348,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_30@2",
            "content": "We compute this metric for a given dataset by first performing a Welch's t-test to determine if there is a significant difference in accuracy for each pair of methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_30",
            "start": 350,
            "end": 516,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_30@3",
            "content": "The method which performs better than most other methods is considered the \"winner\" of the task and its # Wins is incremented by 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_30",
            "start": 518,
            "end": 648,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_30@4",
            "content": "There are multiple winners in the case of a tie.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_30",
            "start": 650,
            "end": 697,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_30@5",
            "content": "See Figure 2 for a demonstration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_30",
            "start": 699,
            "end": 731,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_31@0",
            "content": "Simplifying Prompt Engineering",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_31",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_32@0",
            "content": "In this section, we run prompt-based finetuning and ablate different elements of the prompt.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_32",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_32@1",
            "content": "We consider the following ablations:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_32",
            "start": 93,
            "end": 128,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_33@0",
            "content": "\u2022 Manual Prompt (Prior): We use manuallywritten prompts from Schick and Sch\u00fctze (2021a,b), and Gao et al. (2021). We show the patterns and verbalizers in Appendix A1. \u2022 Manual Prompt (w/o Engineering): We simulate standard prompt design by manually writing one prompt for each task using our intuition. We show the prompts in Appendix A2. \u2022 Prompt Tuning: Inspired by Liu et al. (2021) and Lester et al. (2021), we use the pattern from Manual Prompt (Prior) but randomly initialize the embeddings of the pattern tokens and learn them using gradient-based optimization. This ablates the gains from human-designed patterns. \u2022 Null Prompt: We use the same verbalizer as Manual Prompt (Prior) but use a pattern that consists of only the input fields and a [MASK] token (Appendix A3). This ablates the pattern entirely. \u2022 Null Verbalizer: We use the same pattern as Manual Prompt (Prior) but-following Opitz (2019) and Scao and Rush (2021)-select random tokens for the verbalizer. This ablates the gains from a human-designed verbalizer. \u2022 Null Prompt + Verbalizer We use both null prompts and random tokens for the verbalizer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_33",
            "start": 0,
            "end": 1121,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_34@0",
            "content": "In all cases, we finetune all of the masked LM parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_34",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_34@1",
            "content": "We show the accuracy of the above prompts as well as traditional finetuning (using a",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_34",
            "start": 59,
            "end": 142,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_35@0",
            "content": "[CLS] token and a classification head) in Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_35",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_35@1",
            "content": "2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_35",
            "start": 52,
            "end": 52,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_36@0",
            "content": "Manual Prompts Perform Best The manuallywritten prompts from prior work perform best on average for both models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_36",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_36@1",
            "content": "On the other hand, our manual prompts (w/o Engineering) are noticeably worse than the ones from prior work and are outperformed by many other methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_36",
            "start": 113,
            "end": 262,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_37@0",
            "content": "Null Prompts Are Competitive In many cases, prompt tuning and null prompts perform comparably to manually-written prompts, especially for RoBERTa.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_37",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_37@1",
            "content": "For instance, both of these methods outperform our manually-written prompts in terms The only decision to make when using null prompts is which order to concatenate the mask token and the input fields.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_37",
            "start": 147,
            "end": 347,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_37@2",
            "content": "One can robustly choose the best option using a tiny held-out development set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_37",
            "start": 349,
            "end": 426,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_37@3",
            "content": "We show the results for MNLI, with the fewshot development set accuracy on the x-axis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_37",
            "start": 428,
            "end": 513,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_38@0",
            "content": "B o o l Q C B M N L I -m M N L I -m m M R P C Q N L I Q Q P R T E S S T -2 0 25 50 75 100 RoBERTa (Large) # Wins 0 5 B o o l Q C B M N L I -m M N L I -m m M R P C Q N L I Q Q P R T E S S",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_38",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_39@0",
            "content": "of # Wins.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_39",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_39@1",
            "content": "These results are exciting from a practical perspective as they show that one can achieve competitive few-shot results without resorting to any tuning of the prompt.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_39",
            "start": 11,
            "end": 175,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_40@0",
            "content": "From an analysis perspective, these results also show that effective few-shot learning can be accomplished without any inductive bias from a manuallywritten pattern.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_40",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_40@1",
            "content": "In fact, combining null prompts with null verbalizers, which involves no human design at all, still significantly outperforms standard [CLS] finetuning for numerous tasks (3 for RoBERTa and 5 for ALBERT at p = 0.05).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_40",
            "start": 166,
            "end": 381,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_40@2",
            "content": "This shows that some of the effectiveness of promptbased finetuning is due to its basic setup, i.e., predicting on a [MASK] token with an MLM head.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_40",
            "start": 383,
            "end": 529,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_41@0",
            "content": "Null Prompts or Prompt Tuning? Both null prompts and prompt tuning achieve competitive results without resorting to manual prompt design.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_41",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_41@1",
            "content": "We advocate for using null prompts over prompt tuning because they are easier to use.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_41",
            "start": 138,
            "end": 222,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_41@2",
            "content": "Null prompts only require choosing which order to concatenate the input fields and the [MASK] token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_41",
            "start": 224,
            "end": 323,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_41@3",
            "content": "Prompt tuning requires choosing the number of embeddings, their placement, their initialization, etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_41",
            "start": 325,
            "end": 425,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_42@0",
            "content": "Null Prompts Simplify Prompt Search One complication that arises in standard prompt-based finetuning is that prompts become a hyperparameter of the finetuning procedure, and have a combinatorially large search space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_42",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_42@1",
            "content": "On the other hand, determining the concatenation order for null prompts is trivial by just trying all of the few possible options and choosing which one works best on the validation set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_42",
            "start": 217,
            "end": 402,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_42@2",
            "content": "To see this, in Figure 4 we plot the accuracy on the few-shot development set and the full test set for different concatenation orders for RoBERTa on MNLI.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_42",
            "start": 404,
            "end": 558,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_42@3",
            "content": "3 The development and test accuracy is strongly correlated (R 2 = 79.05), which demonstrates that tuning the concatenation order is easy even when validation data is scarce.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_42",
            "start": 560,
            "end": 732,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_43@0",
            "content": "We next investigate whether the observations made in the previous para- Figure 5: Impact of Dataset Size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_43",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_43@1",
            "content": "We plot a subset of learning curves for K \u2208 {4, 8, 16, 32} (results for all datasets are provided in Appendix A1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_43",
            "start": 106,
            "end": 219,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_43@2",
            "content": "Shaded regions indicate the range of performance across 10 different random seeds.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_43",
            "start": 221,
            "end": 302,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_43@3",
            "content": "In general, we find that as K increases the accuracy of prompt tuning with null prompts tends to be close to that of manual prompts, and substantially better than traditional finetuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_43",
            "start": 304,
            "end": 489,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_44@0",
            "content": "graphs hold across different dataset sizes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_44",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_44@1",
            "content": "Intuitively, when the amount of data is small, manual prompts may outperform other approaches because the inductive bias provided by the prompt has the most impact when there is little data to learn the task at hand.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_44",
            "start": 44,
            "end": 259,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_44@2",
            "content": "In Figure 5 we compare the accuracy of prompt-based finetuning using manually-written prompts and null prompts to traditional finetuning, using the same setup described in Section 3.1 but varying K \u2208 {4, 8, 16, 32}. Full results for all datasets are provided in Appendix A1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_44",
            "start": 261,
            "end": 535,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_44@3",
            "content": "Although there is some instability at lower values of K, we find that the accuracy of both prompt-based finetuning approaches tends to be similar, and is either substantially better or on-par with traditional finetuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_44",
            "start": 537,
            "end": 756,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_44@4",
            "content": "In other words, null prompts are competitive with manual prompts, even when K is small.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_44",
            "start": 758,
            "end": 844,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_45@0",
            "content": "Achieving Simplicity and Efficiency",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_45",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_46@0",
            "content": "Thus far, we have shown that prompt-based finetuning can simplify prompt engineering at the cost of memory inefficiency-a new set of parameters must be learned for each task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_46",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_46@1",
            "content": "This is in contrast to in-context learning, which holds all model weights fixed but is heavily influenced by small prompt modifications (Zhao et al., 2021;Lu et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_46",
            "start": 175,
            "end": 346,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_46@2",
            "content": "In this section, we investigate how to memory efficiency and simple prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_46",
            "start": 348,
            "end": 423,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_46@3",
            "content": "Concretely, in Section 5.1 we try to simplify prompt engineering for in-context learning by tuning the prompt, and in Section 5.2, we reduce the number of learned parameters for prompt-based finetuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_46",
            "start": 425,
            "end": 626,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_47@0",
            "content": "Simplifying In-Context Learning With",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_47",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_48@0",
            "content": "Prompt-Only Tuning Here, we try to make prompt engineering for incontext learning as simple as prompt-based finetuning by automatically finding the prompt.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_48",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_48@1",
            "content": "Concretely, we focus on the emerging class of methods that do prompt-only tuning: learning the prompt while keeping the rest of the model fixed (Shin et al., 2020;Lester et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_48",
            "start": 156,
            "end": 339,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_48@2",
            "content": "We consider:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_48",
            "start": 341,
            "end": 352,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_49@0",
            "content": "\u2022 AUTOPROMPT: Following (Shin et al., 2020), we search for discrete tokens to use in the input instead of manually-designed patterns. We use the hyperparameters from Shin et al. (2020). \u2022 Prompt Tuning (Short): We use the same prompt tuning approach described in the previous section but we keep the masked LM fixed. \u2022 Prompt Tuning (Long): Based on the advice of Lester et al. (2021), we increase the number of learned prompt embeddings to 20 in order to expand the learning capacity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_49",
            "start": 0,
            "end": 484,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_50@0",
            "content": "For reference, we also report the results from prompt-based finetuning with null prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_50",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_50@1",
            "content": "We show the results for RoBERTa in Figure 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_50",
            "start": 90,
            "end": 133,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_50@2",
            "content": "We find that only tuning the prompt is relatively unsuccessful.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_50",
            "start": 135,
            "end": 197,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_50@3",
            "content": "First, on average it fails to match the performance of manually-designed prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_50",
            "start": 199,
            "end": 279,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_50@4",
            "content": "Second, all methods struggle to match the accuracy of prompt-based finetuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_50",
            "start": 281,
            "end": 358,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_50@5",
            "content": "In fact, for many of the datasets, prompt-only methods perform worse by a wide margin (e.g., 40% absolute difference in F 1 score on CB).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_50",
            "start": 360,
            "end": 496,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_50@6",
            "content": "This shows that finetuning masked LMs in the few-shot setting leads to substantially higher accuracy than prompt-only tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_50",
            "start": 498,
            "end": 622,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_51@0",
            "content": "Work We find that only tuning the prompt performs substantially worse than finetuning the entire LM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_51",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_51@1",
            "content": "This is in contrast to recent work, which argues that prompt-only tuning is competitive with finetuning (Lester et al., 2021;Li and Liang, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_51",
            "start": 101,
            "end": 245,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_51@2",
            "content": "We believe these are not contradictions but rather differences in the models and settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_51",
            "start": 247,
            "end": 336,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_51@3",
            "content": "Li and Liang Figure 6: Prompt-Only Tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_51",
            "start": 338,
            "end": 379,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_51@4",
            "content": "We try to simplify prompt engineering for in-context learning (i.e., using frozen models) by directly learning the prompt.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_51",
            "start": 381,
            "end": 502,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_51@5",
            "content": "The performance (accuracy/F 1 ) for prompt-only tuning is substantially lower than finetuning the LM parameters for RoBERTa-large.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_51",
            "start": 504,
            "end": 633,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_51@6",
            "content": "Thus, we recommend finetuning over in-context learning in the few-shot setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_51",
            "start": 635,
            "end": 713,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_51@7",
            "content": "(2021) focus on left-to-right LMs for generation tasks, whereas we focus on masked LMs for classification tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_51",
            "start": 715,
            "end": 826,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_51@8",
            "content": "This may explain the difference in the results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_51",
            "start": 828,
            "end": 874,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_51@9",
            "content": "Moreover, Lester et al. (2021) show that prompt-only tuning becomes less competitive as models get smaller; we use even smaller models than evaluated in their work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_51",
            "start": 876,
            "end": 1039,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_51@10",
            "content": "Consequently, although we find that finetuning a masked LM is superior to prompt-only tuning, there may be other settings in which they fair similarly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_51",
            "start": 1041,
            "end": 1191,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_52@0",
            "content": "B o o l Q C B M N L I -m M N L I -m m M R P C Q N L I Q Q P R T E S S",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_52",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_53@0",
            "content": "B o o l Q C B M N L I -m M N L I -m m M R P C Q N L I Q Q P R T E S S",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_53",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_54@0",
            "content": "Memory-Efficient Finetuning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_54",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_55@0",
            "content": "Given the inadequacies of prompt-only tuning, we next study if prompt-based finetuning can be made memory-efficient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_55",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_55@1",
            "content": "To do so, we focus on reducing the number of trainable parameters, taking inspiration from recent work in the non-few-shot setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_55",
            "start": 117,
            "end": 247,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_55@2",
            "content": "The benefits of these methods is that they (1) reduce storage costs at test time when running many tasks (one can store only the modified parameters for each task), and (2) reduce memory costs at training time, as fewer optimized parameters means much smaller statistics in optimizers like Adam.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_55",
            "start": 249,
            "end": 543,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_55@3",
            "content": "We consider four lightweight finetuning methods:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_55",
            "start": 545,
            "end": 592,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_56@0",
            "content": "\u2022 Adapters: We use Adapters (Houlsby et al., 2019), neural networks layers that are inserted between the feedforward portion of the Transformer architecture. We use the default Adapters hyperparameters from Houlsby et al. ( 2019) (\u2248 10 7 parameters per task). \u2022 BitFit: Following Ben-Zaken et al. ( 2021), we only update the bias terms inside the Transformer (\u2248 10 5 parameters per task). \u2022 LM Head Tuning: We update the embeddings in the MLM output layer that are associated with the verbalizer tokens (\u2248 10 3 parameters per task). \u2022 Calibration: Following Zhao et al. ( 2021), we learn an affine transformation on top of the logits associated with the verbalizer tokens (\u2248 10 1 parameters per task). We run prompt-based finetuning for each method with the prompts from Manual Prompts (Prior). We also report the accuracy of finetuning all of the parameters for reference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_56",
            "start": 0,
            "end": 872,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_57@0",
            "content": "We show the results in Figure 7.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_57",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_57@1",
            "content": "There are diminishing returns as the parameter count is increased.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_57",
            "start": 33,
            "end": 98,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_57@2",
            "content": "In particular, substantial gains are made when going from calibration to LM head tuning to BitFit, however, there is either a marginal improvement or even a decrease in performance when going to Adapters or All Parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_57",
            "start": 100,
            "end": 321,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_57@3",
            "content": "The BitFit method provides the best accuracy-efficiency trade-off, and even outperforms finetuning all of the parameters in terms of # Wins.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_57",
            "start": 323,
            "end": 462,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_57@4",
            "content": "This suggests that updating all of the LM's hundreds of millions of parameters on only 16 data points is suboptimal.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_57",
            "start": 464,
            "end": 579,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_58@0",
            "content": "Putting Everything Together",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_58",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_59@0",
            "content": "We finally combine null prompts and memoryefficient finetuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_59",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_59@1",
            "content": "We show the results from this method, as well as the other best few-shot methods, in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_59",
            "start": 64,
            "end": 156,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_59@2",
            "content": "Overall, we recommend finetuning with null prompts and BitFit: it achieves competitive accuracy, is simple to set up, and introduces small memory costs for each new task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_59",
            "start": 158,
            "end": 327,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_60@0",
            "content": "Conclusion and Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_60",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_61@0",
            "content": "Two high-level methods exist in few-shot prompting: using a frozen LM (in-context learning) and finetuning the LM on the few training examples (prompt-based finetuning).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_61",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_61@1",
            "content": "In this work, we demonstrate two new advantages of prompt-based finetuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_61",
            "start": 170,
            "end": 244,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_61@2",
            "content": "First, we show that it is robust to different choices of the prompt.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_61",
            "start": 246,
            "end": 313,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_61@3",
            "content": "In fact, there is a simple class of prompts-null prompts-that can be flexibly applied to different tasks without degrading performance relative to manually-written and learned prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_61",
            "start": 315,
            "end": 498,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_61@4",
            "content": "Second, we demonstrate that promptbased finetuning can be made memory efficient: finetuning only the bias terms (BitFit) achieves comparable or better accuracy than finetuning all the parameters while being 1000x more memory efficient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_61",
            "start": 500,
            "end": 734,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_61@5",
            "content": "Taken together, using null patterns with BitFit is an approach that is efficient, simple-totune, and competitive in accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_61",
            "start": 736,
            "end": 860,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_61@6",
            "content": "Code and instructions for reproducing our results is available at: omitted.link.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_61",
            "start": 862,
            "end": 941,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_61@7",
            "content": "4 Our results motivate future analysis of few-shot learning methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_61",
            "start": 943,
            "end": 1010,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_61@8",
            "content": "Concretely, we show that the success of prompt-based finetuning is not solely explained by carefully-chosen patterns or verbalizers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_61",
            "start": 1012,
            "end": 1143,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_61@9",
            "content": "This suggests that the gains from promptbased finetuning are partially due to its low-level setup, i.e., predicting on a [MASK] token with a pre-trained MLM head.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_61",
            "start": 1145,
            "end": 1306,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_61@10",
            "content": "More generally, we hope to further analyze why and how small changes to different few-shot learning methods can lead to wildly different accuracies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_61",
            "start": 1308,
            "end": 1455,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_61@11",
            "content": "We also hope to extend our findings to both very large and left-to-right LMs, as our current results are for masked LMs that are relatively small by modern standards.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_61",
            "start": 1457,
            "end": 1622,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_62@0",
            "content": "UNKNOWN, None, 2021, BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_62",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_63@0",
            "content": "UNKNOWN, None, , NeurIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_63",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_64@0",
            "content": "UNKNOWN, None, 2020, Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_64",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_65@0",
            "content": "Tianyu Gao, Adam Fisch, Danqi Chen, Making pre-trained language models better few-shot learners, 2021, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_65",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_66@0",
            "content": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, Parameter-efficient transfer learning for NLP, 2019, ICML, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_66",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_67@0",
            "content": "UNKNOWN, None, 2015, Human-level concept learning through probabilistic program induction, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_67",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_68@0",
            "content": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, ALBERT: A lite BERT for self-supervised learning of language representations, 2019, ICLR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_68",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_69@0",
            "content": "UNKNOWN, None, 2021, The power of scale for parameter-efficient prompt tuning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_69",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_70@0",
            "content": "UNKNOWN, None, 2021, Prefix-Tuning: Optimizing continuous prompts for generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_70",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_71@0",
            "content": "UNKNOWN, None, , Zhilin Yang, and Jie Tang. 2021. GPT understands, too, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_71",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_72@0",
            "content": "UNKNOWN, None, 2019, RoBERTa: A robustly optimized BERT pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_72",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_73@0",
            "content": "UNKNOWN, None, 2021, Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_73",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_74@0",
            "content": "Marius Mosbach, Maksym Andriushchenko, Dietrich Klakow, On the stability of fine-tuning BERT: Misconceptions, explanations, and strong baselines, 2021, ICLR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_74",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_75@0",
            "content": "Juri Opitz, Argumentative relation classification as plausibility ranking, 2019, Proceedings of the 15th Conference on Natural Language Processing, German Society for Computational Linguistics & Language Technology.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_75",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_76@0",
            "content": "UNKNOWN, None, 2021, True few-shot learning with language models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_76",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_77@0",
            "content": "Guanghui Qin, Jason Eisner, Learning how to ask: Querying LMs with mixtures of soft prompts, 2021, NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_77",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_78@0",
            "content": "Le Teven, Alexander Scao,  Rush, How many data points is a prompt worth, 2021, NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_78",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_79@0",
            "content": "Timo Schick, Hinrich Sch\u00fctze, Exploiting cloze questions for few-shot text classification and natural language inference, 2021, EACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_79",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_80@0",
            "content": "Timo Schick, Hinrich Sch\u00fctze, It's not just size that matters: Small language models are also few-shot learners, 2021, NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_80",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_81@0",
            "content": "Taylor Shin, Yasaman Razeghi, I Robert L Logan, Eric Wallace, Sameer Singh, AutoPrompt: Eliciting knowledge from language models with automatically generated prompts, 2020, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_81",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_82@0",
            "content": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, SuperGLUE: A stickier benchmark for general-purpose language understanding systems, 2019, NeurIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_82",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_83@0",
            "content": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, GLUE: A multi-task benchmark and analysis platform for natural language understanding, 2019, ICLR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_83",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_84@0",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R'emi Louf, Morgan Funtowicz, Jamie Brew, HuggingFace's Transformers: State-of-the-art natural language processing, 2020, EMNLP Demo Track, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_84",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_85@0",
            "content": "Z Tony, Eric Zhao, Shi Wallace, Dan Feng, Sameer Klein,  Singh, Calibrate before use: Improving few-shot performance of language models, 2021, ICML, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_85",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "405-ARR_v1_86@0",
            "content": "Zexuan Zhong, Dan Friedman, Danqi Chen, Factual probing is [MASK]: Learning vs. learning to recall, 2021, NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "405-ARR_v1_86",
            "start": 0,
            "end": 113,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "405-ARR_v1_0",
            "tgt_ix": "405-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_0",
            "tgt_ix": "405-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_1",
            "tgt_ix": "405-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_1",
            "tgt_ix": "405-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_0",
            "tgt_ix": "405-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_2",
            "tgt_ix": "405-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_4",
            "tgt_ix": "405-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_5",
            "tgt_ix": "405-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_6",
            "tgt_ix": "405-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_3",
            "tgt_ix": "405-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_3",
            "tgt_ix": "405-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_3",
            "tgt_ix": "405-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_3",
            "tgt_ix": "405-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_3",
            "tgt_ix": "405-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_0",
            "tgt_ix": "405-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_7",
            "tgt_ix": "405-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_9",
            "tgt_ix": "405-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_8",
            "tgt_ix": "405-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_8",
            "tgt_ix": "405-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_8",
            "tgt_ix": "405-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_8",
            "tgt_ix": "405-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_12",
            "tgt_ix": "405-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_11",
            "tgt_ix": "405-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_11",
            "tgt_ix": "405-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_11",
            "tgt_ix": "405-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_8",
            "tgt_ix": "405-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_13",
            "tgt_ix": "405-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_14",
            "tgt_ix": "405-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_14",
            "tgt_ix": "405-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_16",
            "tgt_ix": "405-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_17",
            "tgt_ix": "405-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_18",
            "tgt_ix": "405-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_19",
            "tgt_ix": "405-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_20",
            "tgt_ix": "405-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_21",
            "tgt_ix": "405-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_14",
            "tgt_ix": "405-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_14",
            "tgt_ix": "405-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_14",
            "tgt_ix": "405-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_14",
            "tgt_ix": "405-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_14",
            "tgt_ix": "405-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_14",
            "tgt_ix": "405-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_14",
            "tgt_ix": "405-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_15",
            "tgt_ix": "405-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_0",
            "tgt_ix": "405-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_22",
            "tgt_ix": "405-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_24",
            "tgt_ix": "405-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_25",
            "tgt_ix": "405-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_23",
            "tgt_ix": "405-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_23",
            "tgt_ix": "405-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_23",
            "tgt_ix": "405-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_23",
            "tgt_ix": "405-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_0",
            "tgt_ix": "405-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_26",
            "tgt_ix": "405-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_27",
            "tgt_ix": "405-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_27",
            "tgt_ix": "405-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_0",
            "tgt_ix": "405-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_28",
            "tgt_ix": "405-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_29",
            "tgt_ix": "405-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_29",
            "tgt_ix": "405-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_0",
            "tgt_ix": "405-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_30",
            "tgt_ix": "405-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_32",
            "tgt_ix": "405-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_34",
            "tgt_ix": "405-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_35",
            "tgt_ix": "405-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_36",
            "tgt_ix": "405-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_37",
            "tgt_ix": "405-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_38",
            "tgt_ix": "405-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_39",
            "tgt_ix": "405-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_40",
            "tgt_ix": "405-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_41",
            "tgt_ix": "405-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_31",
            "tgt_ix": "405-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_31",
            "tgt_ix": "405-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_31",
            "tgt_ix": "405-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_31",
            "tgt_ix": "405-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_31",
            "tgt_ix": "405-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_31",
            "tgt_ix": "405-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_31",
            "tgt_ix": "405-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_31",
            "tgt_ix": "405-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_31",
            "tgt_ix": "405-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_31",
            "tgt_ix": "405-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_31",
            "tgt_ix": "405-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_31",
            "tgt_ix": "405-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_43",
            "tgt_ix": "405-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_31",
            "tgt_ix": "405-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_31",
            "tgt_ix": "405-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_42",
            "tgt_ix": "405-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_0",
            "tgt_ix": "405-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_44",
            "tgt_ix": "405-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_45",
            "tgt_ix": "405-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_45",
            "tgt_ix": "405-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_45",
            "tgt_ix": "405-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_46",
            "tgt_ix": "405-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_48",
            "tgt_ix": "405-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_47",
            "tgt_ix": "405-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_47",
            "tgt_ix": "405-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_47",
            "tgt_ix": "405-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_47",
            "tgt_ix": "405-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_51",
            "tgt_ix": "405-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_52",
            "tgt_ix": "405-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_47",
            "tgt_ix": "405-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_47",
            "tgt_ix": "405-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_47",
            "tgt_ix": "405-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_50",
            "tgt_ix": "405-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_45",
            "tgt_ix": "405-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_53",
            "tgt_ix": "405-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_55",
            "tgt_ix": "405-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_54",
            "tgt_ix": "405-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_54",
            "tgt_ix": "405-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_54",
            "tgt_ix": "405-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_54",
            "tgt_ix": "405-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_45",
            "tgt_ix": "405-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_57",
            "tgt_ix": "405-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_58",
            "tgt_ix": "405-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_58",
            "tgt_ix": "405-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_0",
            "tgt_ix": "405-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_59",
            "tgt_ix": "405-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_60",
            "tgt_ix": "405-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_60",
            "tgt_ix": "405-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "405-ARR_v1_0",
            "tgt_ix": "405-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_1",
            "tgt_ix": "405-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_2",
            "tgt_ix": "405-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_2",
            "tgt_ix": "405-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_2",
            "tgt_ix": "405-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_2",
            "tgt_ix": "405-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_2",
            "tgt_ix": "405-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_3",
            "tgt_ix": "405-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_4",
            "tgt_ix": "405-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_4",
            "tgt_ix": "405-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_4",
            "tgt_ix": "405-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_4",
            "tgt_ix": "405-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_4",
            "tgt_ix": "405-ARR_v1_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_5",
            "tgt_ix": "405-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_5",
            "tgt_ix": "405-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_5",
            "tgt_ix": "405-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_5",
            "tgt_ix": "405-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_6",
            "tgt_ix": "405-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_6",
            "tgt_ix": "405-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_6",
            "tgt_ix": "405-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_6",
            "tgt_ix": "405-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_6",
            "tgt_ix": "405-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_7",
            "tgt_ix": "405-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_7",
            "tgt_ix": "405-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_7",
            "tgt_ix": "405-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_7",
            "tgt_ix": "405-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_8",
            "tgt_ix": "405-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_9",
            "tgt_ix": "405-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_9",
            "tgt_ix": "405-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_10",
            "tgt_ix": "405-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_11",
            "tgt_ix": "405-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_12",
            "tgt_ix": "405-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_12",
            "tgt_ix": "405-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_12",
            "tgt_ix": "405-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_12",
            "tgt_ix": "405-ARR_v1_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_12",
            "tgt_ix": "405-ARR_v1_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_12",
            "tgt_ix": "405-ARR_v1_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_12",
            "tgt_ix": "405-ARR_v1_12@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_13",
            "tgt_ix": "405-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_13",
            "tgt_ix": "405-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_14",
            "tgt_ix": "405-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_15",
            "tgt_ix": "405-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_15",
            "tgt_ix": "405-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_15",
            "tgt_ix": "405-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_16",
            "tgt_ix": "405-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_17",
            "tgt_ix": "405-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_18",
            "tgt_ix": "405-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_19",
            "tgt_ix": "405-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_19",
            "tgt_ix": "405-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_20",
            "tgt_ix": "405-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_20",
            "tgt_ix": "405-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_20",
            "tgt_ix": "405-ARR_v1_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_20",
            "tgt_ix": "405-ARR_v1_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_20",
            "tgt_ix": "405-ARR_v1_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_21",
            "tgt_ix": "405-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_21",
            "tgt_ix": "405-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_21",
            "tgt_ix": "405-ARR_v1_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_21",
            "tgt_ix": "405-ARR_v1_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_21",
            "tgt_ix": "405-ARR_v1_21@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_22",
            "tgt_ix": "405-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_23",
            "tgt_ix": "405-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_24",
            "tgt_ix": "405-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_24",
            "tgt_ix": "405-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_24",
            "tgt_ix": "405-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_24",
            "tgt_ix": "405-ARR_v1_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_24",
            "tgt_ix": "405-ARR_v1_24@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_24",
            "tgt_ix": "405-ARR_v1_24@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_24",
            "tgt_ix": "405-ARR_v1_24@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_25",
            "tgt_ix": "405-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_25",
            "tgt_ix": "405-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_25",
            "tgt_ix": "405-ARR_v1_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_25",
            "tgt_ix": "405-ARR_v1_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_26",
            "tgt_ix": "405-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_26",
            "tgt_ix": "405-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_26",
            "tgt_ix": "405-ARR_v1_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_26",
            "tgt_ix": "405-ARR_v1_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_27",
            "tgt_ix": "405-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_28",
            "tgt_ix": "405-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_28",
            "tgt_ix": "405-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_29",
            "tgt_ix": "405-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_30",
            "tgt_ix": "405-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_30",
            "tgt_ix": "405-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_30",
            "tgt_ix": "405-ARR_v1_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_30",
            "tgt_ix": "405-ARR_v1_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_30",
            "tgt_ix": "405-ARR_v1_30@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_30",
            "tgt_ix": "405-ARR_v1_30@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_31",
            "tgt_ix": "405-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_32",
            "tgt_ix": "405-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_32",
            "tgt_ix": "405-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_33",
            "tgt_ix": "405-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_34",
            "tgt_ix": "405-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_34",
            "tgt_ix": "405-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_35",
            "tgt_ix": "405-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_35",
            "tgt_ix": "405-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_36",
            "tgt_ix": "405-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_36",
            "tgt_ix": "405-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_37",
            "tgt_ix": "405-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_37",
            "tgt_ix": "405-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_37",
            "tgt_ix": "405-ARR_v1_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_37",
            "tgt_ix": "405-ARR_v1_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_38",
            "tgt_ix": "405-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_39",
            "tgt_ix": "405-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_39",
            "tgt_ix": "405-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_40",
            "tgt_ix": "405-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_40",
            "tgt_ix": "405-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_40",
            "tgt_ix": "405-ARR_v1_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_41",
            "tgt_ix": "405-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_41",
            "tgt_ix": "405-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_41",
            "tgt_ix": "405-ARR_v1_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_41",
            "tgt_ix": "405-ARR_v1_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_42",
            "tgt_ix": "405-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_42",
            "tgt_ix": "405-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_42",
            "tgt_ix": "405-ARR_v1_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_42",
            "tgt_ix": "405-ARR_v1_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_43",
            "tgt_ix": "405-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_43",
            "tgt_ix": "405-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_43",
            "tgt_ix": "405-ARR_v1_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_43",
            "tgt_ix": "405-ARR_v1_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_44",
            "tgt_ix": "405-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_44",
            "tgt_ix": "405-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_44",
            "tgt_ix": "405-ARR_v1_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_44",
            "tgt_ix": "405-ARR_v1_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_44",
            "tgt_ix": "405-ARR_v1_44@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_45",
            "tgt_ix": "405-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_46",
            "tgt_ix": "405-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_46",
            "tgt_ix": "405-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_46",
            "tgt_ix": "405-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_46",
            "tgt_ix": "405-ARR_v1_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_47",
            "tgt_ix": "405-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_48",
            "tgt_ix": "405-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_48",
            "tgt_ix": "405-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_48",
            "tgt_ix": "405-ARR_v1_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_49",
            "tgt_ix": "405-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_50",
            "tgt_ix": "405-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_50",
            "tgt_ix": "405-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_50",
            "tgt_ix": "405-ARR_v1_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_50",
            "tgt_ix": "405-ARR_v1_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_50",
            "tgt_ix": "405-ARR_v1_50@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_50",
            "tgt_ix": "405-ARR_v1_50@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_50",
            "tgt_ix": "405-ARR_v1_50@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_51",
            "tgt_ix": "405-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_51",
            "tgt_ix": "405-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_51",
            "tgt_ix": "405-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_51",
            "tgt_ix": "405-ARR_v1_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_51",
            "tgt_ix": "405-ARR_v1_51@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_51",
            "tgt_ix": "405-ARR_v1_51@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_51",
            "tgt_ix": "405-ARR_v1_51@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_51",
            "tgt_ix": "405-ARR_v1_51@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_51",
            "tgt_ix": "405-ARR_v1_51@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_51",
            "tgt_ix": "405-ARR_v1_51@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_51",
            "tgt_ix": "405-ARR_v1_51@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_52",
            "tgt_ix": "405-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_53",
            "tgt_ix": "405-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_54",
            "tgt_ix": "405-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_55",
            "tgt_ix": "405-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_55",
            "tgt_ix": "405-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_55",
            "tgt_ix": "405-ARR_v1_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_55",
            "tgt_ix": "405-ARR_v1_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_56",
            "tgt_ix": "405-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_57",
            "tgt_ix": "405-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_57",
            "tgt_ix": "405-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_57",
            "tgt_ix": "405-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_57",
            "tgt_ix": "405-ARR_v1_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_57",
            "tgt_ix": "405-ARR_v1_57@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_58",
            "tgt_ix": "405-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_59",
            "tgt_ix": "405-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_59",
            "tgt_ix": "405-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_59",
            "tgt_ix": "405-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_60",
            "tgt_ix": "405-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_61",
            "tgt_ix": "405-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_61",
            "tgt_ix": "405-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_61",
            "tgt_ix": "405-ARR_v1_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_61",
            "tgt_ix": "405-ARR_v1_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_61",
            "tgt_ix": "405-ARR_v1_61@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_61",
            "tgt_ix": "405-ARR_v1_61@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_61",
            "tgt_ix": "405-ARR_v1_61@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_61",
            "tgt_ix": "405-ARR_v1_61@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_61",
            "tgt_ix": "405-ARR_v1_61@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_61",
            "tgt_ix": "405-ARR_v1_61@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_61",
            "tgt_ix": "405-ARR_v1_61@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_61",
            "tgt_ix": "405-ARR_v1_61@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_62",
            "tgt_ix": "405-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_63",
            "tgt_ix": "405-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_64",
            "tgt_ix": "405-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_65",
            "tgt_ix": "405-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_66",
            "tgt_ix": "405-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_67",
            "tgt_ix": "405-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_68",
            "tgt_ix": "405-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_69",
            "tgt_ix": "405-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_70",
            "tgt_ix": "405-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_71",
            "tgt_ix": "405-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_72",
            "tgt_ix": "405-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_73",
            "tgt_ix": "405-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_74",
            "tgt_ix": "405-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_75",
            "tgt_ix": "405-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_76",
            "tgt_ix": "405-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_77",
            "tgt_ix": "405-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_78",
            "tgt_ix": "405-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_79",
            "tgt_ix": "405-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_80",
            "tgt_ix": "405-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_81",
            "tgt_ix": "405-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_82",
            "tgt_ix": "405-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_83",
            "tgt_ix": "405-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_84",
            "tgt_ix": "405-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_85",
            "tgt_ix": "405-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "405-ARR_v1_86",
            "tgt_ix": "405-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1053,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "405-ARR",
        "version": 1
    }
}