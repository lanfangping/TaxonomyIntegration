{
    "nodes": [
        {
            "ix": "120-ARR_v1_0",
            "content": "On the Limitations of Dataset Balancing: The Lost Battle Against Spurious Correlations",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_2",
            "content": "Recent work have shown that deep learning models in NLP are highly sensitive to low-level correlations between simple features and specific output labels, leading to overfitting and lack of generalization. To mitigate this problem, a common practice is to balance datasets by adding new instances or by filtering out \"easy\" instances , culminating in a recent proposal to eliminate single-word correlations altogether . In this opinion paper, we identify that despite these efforts, increasingly-powerful models keep exploiting ever-smaller spurious correlations, and as a result even balancing all single-word features is insufficient for mitigating all of these correlations. In parallel, a truly balanced dataset may be bound to \"throw the baby out with the bathwater\" and miss important signal encoding common sense and world knowledge. We highlight several alternatives to dataset balancing, focusing on enhancing datasets with richer contexts, allowing models to abstain and interact with users, and turning from large-scale fine-tuning to zero-or few-shot setups.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "120-ARR_v1_4",
            "content": "Effective human communication relies on our ability to understand extra-textual context based on common sense, world knowledge or shared cultural experiences, a property often cited as Grice's second maxim of quantity: \"Do not make your contribution more informative than is required\" (Grice, 1975(Grice, , 1989. Studies have estimated that only 12% of the information conveyed by text is mentioned explicitly (Graesser, 2013;Tandon et al., 2020). To illustrate this, consider the question \"who is the president of the U.S.?\". To answer it, a human reader is likely to presume many unstated propositions, as exemplified in Tab. 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_5",
            "content": "In contrast to humans, supervised models often fail to generalize and understand implicit context, instead resorting to low-level correlations in Who is the president of the U.S.?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_6",
            "content": "The year 2019 Donald Trump The West Wing, season 1 Josiah \"Jed\" Bartlet Table 1: Context, whether explicit or implicit, matters in textual understanding, as exemplified by the question \"who is the president of the U.S.?\". E.g., in the first line, given no other context, a QA system should provide the most sensible fallback answer (Joe Biden).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_7",
            "content": "the data, leading to amplified bias (Zhao et al., 2017; and brittle performance (Schwartz et al., 2017;Gururangan et al., 2018). To address this, recent approaches have suggested mitigating such correlations by balancing the dataset via either adding or removing certain instances (Goyal et al., 2017;Hudson and Manning, 2019;Zellers et al., 2018;. In parallel, developers keep building larger and larger pretrained models (Devlin et al., 2019;, which, when fine-tuned on these datasets, consistently manage to reach human performance. Taken together, these trends lead to an arms-race between data curation and model development (Fig. 1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_8",
            "content": "In this position paper, we question the value of mitigating spurious correlations via dataset balancing, by showing that their existence in large training sets is both inevitable and to some extent even desired, as they are an inherent property of natural language understanding. We build on a recent result by , who assumed that every single-word feature correlation is spurious, i.e., can be used to mislead a model. We extend their argument, showing that balancing single-word features is insufficient for eliminating all spurious correlations, and that balancing feature combination is needed for that purpose. On the other hand, we show that balancing too much leads to datasets that contain no learnable signal either. We conclude by questioning whether mitigating all spurious correlations via dataset balancing is practical.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_9",
            "content": "Following, we show that this practice is also undesired. We show that ignoring these correlations will hinder the learning of fallback options for both world knowledge facts (Joe Biden is the president of the U.S.) and common sense knowledge (a person is happy when receiving a gift), thus preventing models from using this knowledge in cases of uncertainty. We conclude that the existence of spurious correlations in training sets should not be solved by creating more balanced datasets. 1 We then discuss alternatives to mitigating spurious correlations. We argue that models should be trained to understand constructions emanating from an apriori theory of language, such as negation, sarcasm, humor, and metaphors. We also suggest adopting modeling approaches that identify when the context is insufficient, and the model should not fallback to default assumptions, but rather output an \"I don't know\" response (e.g., unanswerable questions, Rajpurkar et al., 2018;Sulem et al., 2021) or interact with the user to clear ambiguities. We conclude by questioning the basic procedure of large-scale fine-tuning, and suggest focusing on zero-and few-shot learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_10",
            "content": "Dataset-Model Arms Race",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "120-ARR_v1_11",
            "content": "This section provides a view of recent research in NLP as an arms race between models and datasets. Below we describe the conditions leading to this Figure 2: An example of dataset balancing (adopted from Goyal et al., 2017). For each (question, image) pair in the VQA dataset (left), VQA2.0 adds another image, for which the answer is different (right).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_12",
            "content": "arms race, and present our main research question, challenging its value for making progress in NLP.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_13",
            "content": "Dataset balancing via augmentation While pretrained models consistently perform well across multiple tasks, various studies have pointed out that this is often achieved by exploiting spurious correlations in datasets, rather than improving on the underlying task (Glockner et al., 2018;Gururangan et al., 2018;Elazar et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_14",
            "content": "Various dataset curators have tried to prevent models from learning spurious correlations by modifying their training data via a careful control for the training label distribution, effectively striving for a balanced dataset. One approach, popular in visual question answering datasets, is to add examples in order to balance the dataset (Goyal et al., 2017;Hudson and Manning, 2019). For instance, the VQA2.0 dataset (Goyal et al., 2017) is built by taking every (question q, image i, answer a) triplet in the VQA dataset (Antol et al., 2015), and adding another triplet with the same question q, but a different image i , guaranteed to lead to a different answer a . See Fig. 2 for an example.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_15",
            "content": "Filtering as balancing A complementary approach to augmentation is filtering examples out from datasets such that spurious correlations are minimized. This approach was taken in the creation of the SWAG dataset (Zellers et al., 2018), using \"adversarial filtering\" (AF). In AF, dataset instances that are easily solved by an adversarial model are filtered out. The AF approach was picked up by many follow-up datasets such as DROP (Dua et al., 2019), HellaSWAG (Zellers et al., 2019), and Wino-Grande .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_16",
            "content": "Here we argue that approaches like AF converge to removing all low-level correlations, 2 and there-fore a fully balanced dataset. As this approach relies on an external model, applying it with ever stronger models with higher capacity, will allow these models to pick up on subtler correlations. At the extreme, the remaining instances that could not be solved by a fully capable model will have no statistical signal that can be exploited by that model, i.e., a balanced dataset. We henceforth refer to both augmentation and filtering as balancing methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_17",
            "content": "Large models solve the new datasets In parallel to the efforts in dataset balancing, the leading modeling approach in recent years in NLP is pretraining large language models on raw text corpora, followed by fine-tuning them on supervised downstream applications. These models continue to grow in size (Peters et al., 2018;Devlin et al., 2019;Radford et al., 2019;, and their fine-tuning performance improves accordingly. This in turn leads to more aggressive balancing, setting in motion a kind of arms race between datasets and models (Fig. 1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_18",
            "content": "Evidently, a similar trend emerges for the previously mentioned datasets: (1) the first baselines, reflecting the state of the art at the time of dataset creation, perform poorly, e.g., 52% accuracy on SWAG, 47 F1 on DROP, 47% on Hel-laSWAG, and 53% AUC on WinoGrande; (2) model developers introduce increasingly larger and heavily-parameterized models, hill-climbing on these datasets; and eventually (3) models essentially solve the dataset within a year or two, often outperforming humans: 86% on SWAG (Devlin et al., 2019), 90 F1 on DROP , 93% on HellaSWAG (He et al., 2020), and 88% AUC on WinoGrande . (4) new large-scale datasets are collected with more aggressive pruning techniques, thus repeating the cycle.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_19",
            "content": "Based on these findings, our main research question is whether dataset balancing is the most promising method for mitigating spurious correlations. We note that an arms race between models and datasets might spur advances. Here we question a specific aspect of this arms race: the improvement of datasets by using more aggressive filtering techniques. Next we turn to present practical and conceptual limitations of this practice.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_20",
            "content": "The Lost Battle Against Spurious Correlations",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "120-ARR_v1_21",
            "content": "So far we have identified dataset balancing as a common way to mitigate spurious correlations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_22",
            "content": "Correlations between features and output labels for no reason. ungeneralizable Correlations that do not generalize to new contexts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_23",
            "content": "Correlations between every singleword feature and output label.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_24",
            "content": "Table 2: Different definitions of spurious correlations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_25",
            "content": "Next, we outline how different works define spurious correlations (Sec. 3.1), and then question whether dataset balancing is a viable way for mitigating them; we note that balancing too little is bound to leave spurious correlations in the data (Sec. 3.2), while balancing too much discards meaningful signal (Sec. 3.3). We finish by questioning whether this practice is even desired (Sec. 3.4).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_26",
            "content": "What are Spurious Correlations?",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "120-ARR_v1_27",
            "content": "Mitigating spurious correlations is frequently used as motivation for developing new balancing approaches. However, the term spurious correlations is often not clearly and consistently defined. One conceptual definition, denoted here ingenuine (e.g., Wang and Culotta, 2020;Rogers, 2021) is a feature correlated with some output label for no apparent reason. Such features often result from the annotation process (referred to as annotation artifacts; Gururangan et al., 2018). For instance, Gururangan et al. ( 2018) have shown that the words \"cat\" and \"sleeping\" are correlated with contradictions in the SNLI dataset (Bowman et al., 2015). This definition is appealing: we want our models to learn real information about the world, and not properties of a given dataset. However, it is also somewhat subjective, and could include features that might be referred to as genuine, such as the word \"not\" indicating NLI contradictions. Further, genuine features, i.e., those representing a real phenomenon in the world (e.g., \"amazing\" as a feature for positive sentiment), are also likely to lead models make to erroneous predictions in some contexts (e.g., negation or sarcasm; . Such features could thus harm generalization, so some might consider them spurious as well.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_28",
            "content": "In an alternative definition, denoted ungeneralizable, a spurious feature is one that works well for specific examples but does not hold in general (Chang et al., 2021;Yaghoobzadeh et al., 2021). This definition does not address the nature of the feature (genuine or 2021) relaxed the last constraint, and assumed that every simple correlation between single word features and output labels is spurious (henceforth every-word). They then defined a class of competent datasets, where the marginal probability for every feature is uniform over the class label, i.e., \u2200x \u2208 X , y \u2208 Y, p(y|x) = 1 |Y | , thus limiting models from picking up any correlation between single features and output labels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_29",
            "content": "We next extend the every-word approach beyond single words, showing that models that can exploit single word features can also exploit some feature interactions, and therefore these should also be considered spurious. Tab. 2 summarizes the different definitions of spurious correlations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_30",
            "content": "Balancing too Little Leaves some Spurious Features",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "120-ARR_v1_31",
            "content": "Gardner et al. ( 2021) assumed that as each word can appear in certain contexts that change its semantic meaning (e.g., negation, sarcasm), each word is potentially spurious. Here we note that the same argument can be applied to feature interactions, such as word n-grams. We start with a toy example to illustrate our argument for bigrams, and then extend it for larger values of n.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_32",
            "content": "Consider the toy dataset for the task of sentiment analysis shown in Tab. 3, with vocabulary V ={good,bad,not,very}, and label set Y ={+,\u2212 }. The Train split is balanced with respect to singleword features, i.e., \u2200w \u2208 V, y \u2208 Y : p(y|w) = 1 |Y | (a balanced or competent dataset). Assume the semantics of this dataset is that of English, while '+' means positive sentiment and '\u2212' means negative.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_33",
            "content": "A model trained on Train can achieve perfect training accuracy by learning the correct semantics. However, achieving perfect training accuracy can also be done by learning correlations between twoword features and the target label (i.e., memorizing all the training examples). In this case, the model would make the wrong prediction for the first test example in Test (as it has learned that very good is a feature that indicates positive sentiment), and similarly, will make a random prediction for the second test example, which does not contain any two-word features seen during training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_34",
            "content": "This example highlights that balancing singleword features does not guarantee resiliency to spurious correlations, and therefore in order to mitigate all spurious correlations, balancing pairs of features is also required. One can construct similar examples for larger values of n, by similarly considering multi-word expressions and common co-occurrences (e.g., \"jaw dropping\", \"worst day ever\"). These could serve as spurious correlations in the same way single words do.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_35",
            "content": "Another example is sarcasm. A model that fails to understand sarcastic contexts will misinterpret statements that appear in such contexts, even if it perfectly understands the base meaning of these statements. Thus, the entire reasoning process of such a model, whether relying on simple features, feature interactions, or other types of understanding, will result in mispredictions of certain inputs, and thus can be considered spurious.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_36",
            "content": "As a result, to truly mitigate all spurious correlations in a dataset, balancing feature combinations is required as well. Accordingly, balancing too little will leave some spurious correlations in the dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_37",
            "content": "Too much Balancing Prevents Learning Valuable Semantic Knowledge",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "120-ARR_v1_38",
            "content": "We observed that balancing too little does not allow models to fully eliminate spurious correlations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_39",
            "content": "Here we show that too much balancing can prevent models from learning valuable knowledge. Consider the training data for learning the XOR function presented in Tab. 4 (left). This dataset contains enough learnable signal when considering feature interactions despite being balanced for single words. Nonetheless, balancing this dataset for pairs of features would result in no information, and thus prevent any model from learning this function (Tab. 4,right).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_40",
            "content": "Original Train Set Augmented Samples Input Label Input Label D. Define n to be the length of the longest document in D. By definition, balancing every combination of up to n features (including) leaves no learnable signal in D. 3 We conclude that balancing too much can prevent models from learning semantic knowledge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_41",
            "content": "0 0 0 *0 0 1 0 1 1 *0 1 0 1 0 1 *1 0 0 1 1 0 *1 1 1",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_42",
            "content": "Combining the two observations, we are left with the question of the potential intersection between balancing too much and balancing too little: does a sweet spot exist for which no spurious correlations are found in the dataset, but enough learnable signal is left? And even if so, would a balancing algorithm (whether by augmentation or filtering) be able to find it? We leave these questions for future work, but note that addressing them is a prerequisite for the theoretical and practical application of dataset balancing for mitigating spurious correlations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_43",
            "content": "Dataset Balancing is Undesired",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "120-ARR_v1_44",
            "content": "Even if a sweet-spot exists between balancing too little and too much, do we really want to find it? Here we argue that perhaps not. The practice of dataset balancing is designed to prevent models from learning that some words or expressions have a common fallback meaning that can stem from dataset idiosyncrasies (e.g., \"cat\" as an indicator of contradiction) but also from cultural and historical contexts (e.g., Biden is the U.S. president). Fallback meanings are crucial for understanding language, as contexts are often underspecified (Graesser, 2013). Indeed, relying on fallback meanings might make models fail to process some inputs correctly, and might not generalize to other domains where the fallback meaning is different. Here we argue that the ability to use them is a central ability of language understanding.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_45",
            "content": "For example, substantial efforts are made to teach models world knowledge, such as that the president of the U.S. is Joe Biden, the capital of Brazil is Bras\u00edlia, and France is the soccer world champion. These efforts include building world knowledge datasets (Wang et al., 2021), developing methods for enhancing models with this information (Zhang et al., 2019;Peters et al., 2019), and evaluating how well models capture it (Rubinstein et al., 2015;. But many of these world-knowledge facts are context dependent: the capital of the Brazil has changed in 1960, the president of the U.S., as well as soccer world champions potentially change every 4 years, etc.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_46",
            "content": "Another example is common sense knowledge, such as \"people are happy when they receive a gift\", \"an elephant is taller than a zebra\", and \"a statue that doesn't fit into a suitcase is too large\". A large body of work has been carried out to create benchmarks that measure the common sense abilities of models (Liu and Singh, 2004;Levesque et al., 2012;Zellers et al., 2018;Bisk et al., 2020), as well as augmenting models with such abilities (Qin et al., 2020;Bosselut et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_47",
            "content": "Common sense reasoning is, by definition, stochastic and reliant on understanding presupposed, underspecified context. One could imagine a person unhappy to receive a gift (e.g., because it is not what they wanted), a fantastically large zebra compared to a tiny elephant, and a suitcase with multiple compartments which prevent a small statue from fitting in it.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_48",
            "content": "These examples illustrate that a model that learns these correlations and relies exclusively on them to make predictions is limited and is bound to make mistakes in some contexts. One way to avoid these mistakes is to balance these correlations out, and prevent models from knowing these assertions to begin with. We argue that this solution is not a desired solution. In essence, an interpreter's task (be it human or machine) is to infer the most probable context in which a statement is made, and as a result, it should have a fallback option for such world knowledge and common sense assertions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_49",
            "content": "We recognize that a balanced dataset may not be balanced with respect to the appearance of common-sense or world-knowledge assertions in a given context. E.g., a model might balance-out the general fact that Joe Biden is the U.S. president, but not that he is the president in 2022. As in many cases much of the context is unobserved (Graesser, 2013), the question is whether we want models to make a prediction in cases of uncertainty based on the fallback option. We argue that doing so",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_50",
            "content": "Richer contexts ( \u00a74.1) A closed label set Abstain/interact ( \u00a74.2) Large-scale fine-tuning Few-shot learning ( \u00a74.3) We want to stress that balancing methods can result in mitigating some of the spurious correlations, and therefore lead to increased generalization . Moreover, the process of filtering the data naturally results in smaller datasets, which leads to lower training costs . While such contribution is meaningful in terms of environmental concerns (Strubell et al., 2019;Schwartz et al., 2020), it is orthogonal to our research question. Overall, despite the important contributions of balancing techniques, this paper shows that even the perfect balancing method might not mitigate all spurious correlations in a satisfying way.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_51",
            "content": "So how can we make models more resilient to spurious correlations without balancing the data? Below we discuss several ideas for doing this.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_52",
            "content": "Ways to Move Forward",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "120-ARR_v1_53",
            "content": "So far, we presented limitations of dataset balancing as a means to mitigate spurious correlations. In this section we discuss several alternatives to this practice, summarized in Tab. 5. We note that none of these proposals is particularly novel. Rather, we intend to survey alternatives proposed in literature and argue that these may be promising for addressing the drawbacks of spurious correlations, and that more efforts should be put into studying them.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_54",
            "content": "Augmenting Datasets with Rich Contexts",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "120-ARR_v1_55",
            "content": "The implicit assumption of dataset balancing is that in order to mitigate spurious correlations the model has to unlearn them, that is, they should be removed altogether from the training set. We argue that instead we should be focusing on learning and modeling richer contexts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_56",
            "content": "As an example, consider negation. A model that generalizes well, should learn the meaning of words such as not, and should be able to negate new words, even those that were seen only in positive contexts at training time. For example, if a model only sees during training words like \"amazing\" or \"happy\" with positive sentiment, and thus learns that these words bear positive meaning, we would still expect it to interpret their negated appearance (e.g., not amazing) as an indicator of negative sentiment. Such generalization is crucial for language learning, and should ideally allow models to not rely exclusively on spurious correlations. Despite the immense progress in the field in the past decade, negation still poses a challenge to modern NLP tools (Hossain et al., 2020). 4 We suggest taking into account different types of contexts during dataset design. In particular, collecting training examples with contexts such as negation (Morante and Blanco, 2012), humor (Weller and Seppi, 2019;Annamoradnejad and Zoghi, 2020), sarcasm (Davidov et al., 2010;Oprea and Magdy, 2020), or metaphors (Tsvetkov et al., 2014;Mohammad et al., 2016). This recommendation applies to both supervised tasks, and perhaps more so to pretrained data. We suggest adding documents with such contexts throughout the pretraining corpus, or as a continued pretraining step to existing large-scale models. 5 To incorporate contexts from a wide range of phenomena, we can leverage the vast literature on broad-coverage semantics (Baker et al., 1998;Steedman and Baldridge, 2007;Banarescu et al., 2013;Abend and Rappoport, 2013). 6 This line of work proposes theories of language, composing inventories of linguistic constructions with an algebraic formulation of their inter-relations in terms of truth value, factuality, and more. These inventories often include the phenomena discussed above, such as negation, sarcasm, and presupposition.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_57",
            "content": "Interaction and Abstention to Cope with Underspecified Contexts",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "120-ARR_v1_58",
            "content": "Most NLP tasks are designed with a closed label set that forces models to make a concrete prediction for each test instance, without an option to abstain or interact with the user to get more information.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_59",
            "content": "Even for tasks with a large label set (e.g., language modeling), models still have to output a valid vocabulary item. Here we argue that this practice Figure 3: An example of abstention/interaction in cases of uncertainty. For the task of sentiment analysis, models currently assign a label to each input, even for ambiguous or underspecified ones (top). This may lead the model to over-rely on spurious correlations (marked in red, bottom left). Models that abstain or interact (bottom right) might learn to rely less on such correlations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_60",
            "content": "creates an inductive bias towards using spurious correlations in cases of uncertainty, as the model has \"nothing to lose\" in case of low uncertainty, and is encouraged to always make some prediction, potentially relying on spurious correlations. 7 To further illustrate this point, consider the ambiguous sentence \"To my great surprise, the movie turned out different than what I thought.\", in the context of sentiment analysis. The reader cannot infer whether the writer is pleasantly surprised (a positive review) or disappointed (a negative review). We argue that in such cases models might lean towards a positive sentiment based on the words \"great\" and \"surprise\", which are typically correlated with a positive sentiment.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_61",
            "content": "To test this, we ran a RoBERTa-large model fine-tuned on SST-2 (Socher et al., 2013) on that example. 8 As expected, the model returns a positive label, with 99.99% confidence. Interestingly, three different interpretation methods (simple gradient visualization, Simonyan et al., 2014;integrated gradient visualization, Sundararajan et al., 2017;and SmoothGrad, Smilkov et al., 2017) all find the word \"great\" to be one of the two most influential words on the model's prediction. While this example does not prove the prevalence of this problem, it does demonstrate its existence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_62",
            "content": "To address this problem, we suggest adopting ap- 7 We recognize that in some cases we do want the model to make a prediction under cases of uncertainty (see Sec. 3.4). The ability to detect when is it reasonable and when it is not to make an educated guess is an important property of an intelligent agent, and an exciting research question. 8 We used the AllenNLP demo (https://demo.allennlp.org/sentiment-analysis/). proaches that allow models to abstain and interact when they cannot make a decision with high confidence (Chow, 1957;Hellman, 1970;Laidlaw and Feizi, 2019;Balcan et al., 2020). See Fig. 3. This can be achieved by building datasets with unanswerable questions (Ray et al., 2016;Rajpurkar et al., 2018;Sulem et al., 2021), but also by designing models that abstain in cases of low certainty for all inputs, even those with an unambiguous gold label. 9 We hypothesize that encouraging the model to provide this output when it is unsure, rather than making a semi-educated guess, potentially based on spurious correlations, could reduce its dependency on such correlations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_63",
            "content": "The End of Large-Scale Fine-Tuning?",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "120-ARR_v1_64",
            "content": "This paper has demonstrated the limitations of mitigating spurious correlations via dataset balancing. A naive way to eliminate spurious correlations is to stop using large-scale datasets altogether. We argue that for supervised learning (i.e., large-scale fine-tuning), recent advances in zero-and few-shot learning might make this option possible.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_65",
            "content": "Large pretrained models such as T5 or GPT-3 (Brown et al., 2020), trained on vast amounts of data, arguably learn enough about the world to acquire many of the skills currently learned through supervised datasets. Indeed, the large increase in the size and capacity of pretrained language models has led to a new wave of few-shot and zero-shot methods Shin et al., 2020;Gu et al., 2021), which are able to reach human-level performance on certain tasks using only a few dozens of training examples . Given these impressive results, it is not clear whether there is still value in finetuning models on large-scale datasets for all tasks. In the context of this work, focusing on few-shot learning might allow models to not learn some of the correlations that result from manual annotation (Schwartz et al., 2017;Gururangan et al., 2018;Poliak et al., 2018), as they will not be exposed to many of them to begin with.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_66",
            "content": "We note that this proposal is not a perfect solution. First, some spurious correlations may be picked up by the small number of examples. This is less of a problem in the zero-shot setting, or in cases where the model parameters are not updated in few-shot settings (Brown et al., 2020), but studying the extent to which spurious correlations are picked up in other few-shot settings is an important avenue for future research. Second, some spurious correlations might be picked up during the pretraining stage (Gehman et al., 2020;Birhane et al., 2021;. Continuing to quantify this phenomenon and finding ways to mitigate it is another important line of research.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_67",
            "content": "An important question in this context is the tasks for which supervised learning is still needed. It seems possible the excelling in language modeling tasks requires mastering the skills that stand in the base of many NLP tasks, such as sentiment analysis, syntactic parsing, and NER. However, it is similarly possible that this is not the case for other tasks, e.g., summarization, simplification and dialogue. We are cautious to make concrete recommendations for which tasks to apply this principle, but suggest the following intuitive rule of thumb: for datasets or tasks for which the state of the art is close to or surpasses the human baseline, we should consider moving to few-shot setups.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_68",
            "content": "Finally, dataset creation is still a valuable and important line of research. Our recommendation to stop building large scale training sets does not make this task redundant, to both spur the design of better models, and to better test their capabilities. We suggest instead of building large training sets and small validation and tests, authors should consider building large test sets, as a means for achieving improved statistical power (Card et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_69",
            "content": "A Note on Social-Bias Correlations",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "120-ARR_v1_70",
            "content": "So far, we discussed the problems with unlearning spurious correlations, and advocated instead for more elaborate context modeling. One exception might be the case of social biases. Textual data often reflects human stereotypes such as spurious correlations between labels and protected group attributes, e.g., alignments between professions and gender or race. Unlike other types of knowledge discussed in Sec. 3.4, in this case there is an incentive to prevent models from learning this type of correlation as means for actively reducing the harms of such biases, especially in commercial and public-facing applications, such as machine translation or automated financial decision-making (Bartlett et al., 2021). As a result, methods for dataset balancing are no longer undesired for mitigating such spurious correlations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_71",
            "content": "Nonetheless, as demonstrated in Sec. 3, methods for dataset balancing are a limited solution for mitigating spurious correlations, including social-bias ones. In contrast, the methods proposed in this section for mitigating spurious correlations might also assist in mitigating social biases, or at least slow down their amplification (Zhao et al., 2017).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_72",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "120-ARR_v1_73",
            "content": "This paper discussed the arms-race between models and datasets. Previous works criticized one side of this arms race-the increasing size of pretrained models-due to ethical and environmental concerns (Schwartz et al., 2020;Bender et al., 2021), or questioning its ability to learn meaningful abstractions from raw text (Bender and Koller, 2020;. This work studies the second part of this arms race, regarding the efforts to mitigate spurious correlations through dataset balancing. The release of such datasets is often motivated by their potential to spur progress in modeling, and to help tease apart qualitative differences between models. showed that this is not necessarily the case, by observing that the ranking of reading comprehension models on small and synthetic benchmarks is similar to that of the SQuAD dataset (Rajpurkar et al., 2016).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_74",
            "content": "Finally, Raji et al. ( 2021) recently criticized the concept of benchmarks as a whole, arguing that they are only capturing specific skills and not \"general\" capabilities. Our paper raises relates concerns about training sets implicitly containing spurious correlations, and suggests reconsidering the practice of building large-scale training sets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_75",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "120-ARR_v1_76",
            "content": "Spurious correlations in large textual corpora can result in model brittleness, lack of generalization, and an inflated sense of the state of the art. Mitigating their negative side-effects is an important research goal of the NLP community. In this paper we presented practical and conceptual limitations of dataset balancing as a means for doing so. We proposed alternative ways for mitigating spurious correlations, including adding richer contexts to textual corpora, and allowing models to abstain or interact in cases of uncertainty. We concluded by suggesting to reconsider the practice of fine-tuning pretrained models on large-scale training sets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_77",
            "content": "Broader Impact and Ethical Consideration",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "120-ARR_v1_78",
            "content": "Our work did not involve any new data or annotation collection, and as such did not require crowdsourced or in-house workers, or introduces any new models and related risks. Instead, we examine existing resources and common data balancing approaches. In Section 4.4 we specifically discuss the relation between these practices and implications on social bias in models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "120-ARR_v1_79",
            "content": "Omri Abend, Ari Rappoport, Universal Conceptual Cognitive Annotation (UCCA), 2013, Proc. of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Omri Abend",
                    "Ari Rappoport"
                ],
                "title": "Universal Conceptual Cognitive Annotation (UCCA)",
                "pub_date": "2013",
                "pub_title": "Proc. of ACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_80",
            "content": "Omri Abend, Ari Rappoport, The state of the art in semantic representation, 2017, Proc. of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Omri Abend",
                    "Ari Rappoport"
                ],
                "title": "The state of the art in semantic representation",
                "pub_date": "2017",
                "pub_title": "Proc. of ACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_81",
            "content": "UNKNOWN, None, 2020, Col-BERT: Using bert sentence embedding for humor detection, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Col-BERT: Using bert sentence embedding for humor detection",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_82",
            "content": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Zitnick, Devi Parikh, VQA: visual question answering, 2015, Proc. of ICCV, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Stanislaw Antol",
                    "Aishwarya Agrawal",
                    "Jiasen Lu",
                    "Margaret Mitchell",
                    "Dhruv Batra",
                    "C Zitnick",
                    "Devi Parikh"
                ],
                "title": "VQA: visual question answering",
                "pub_date": "2015",
                "pub_title": "Proc. of ICCV",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_83",
            "content": "Collin Baker, Charles Fillmore, John Lowe, The Berkeley FrameNet project, 1998, Proc. of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Collin Baker",
                    "Charles Fillmore",
                    "John Lowe"
                ],
                "title": "The Berkeley FrameNet project",
                "pub_date": "1998",
                "pub_title": "Proc. of ACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_84",
            "content": "UNKNOWN, None, 2020, On the power of abstention and data-driven decision making for adversarial robustness, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "On the power of abstention and data-driven decision making for adversarial robustness",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_85",
            "content": "Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, Nathan Schneider, Abstract Meaning Representation for sembanking, 2013, Proc. of LAW VII & ID, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Laura Banarescu",
                    "Claire Bonial",
                    "Shu Cai",
                    "Madalina Georgescu",
                    "Kira Griffitt",
                    "Ulf Hermjakob",
                    "Kevin Knight",
                    "Philipp Koehn",
                    "Martha Palmer",
                    "Nathan Schneider"
                ],
                "title": "Abstract Meaning Representation for sembanking",
                "pub_date": "2013",
                "pub_title": "Proc. of LAW VII & ID",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_86",
            "content": "Robert Bartlett, Adair Morse, Richard Stanton, Nancy Wallace, Consumer-lending discrimination in the fintech era, 2021, Journal of Financial Economics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Robert Bartlett",
                    "Adair Morse",
                    "Richard Stanton",
                    "Nancy Wallace"
                ],
                "title": "Consumer-lending discrimination in the fintech era",
                "pub_date": "2021",
                "pub_title": "Journal of Financial Economics",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_87",
            "content": "Emily Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, On the dangers of stochastic parrots: Can language models be too big?, 2021, Proc. of FAccT, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Emily Bender",
                    "Timnit Gebru",
                    "Angelina Mcmillan-Major",
                    "Shmargaret Shmitchell"
                ],
                "title": "On the dangers of stochastic parrots: Can language models be too big?",
                "pub_date": "2021",
                "pub_title": "Proc. of FAccT",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_88",
            "content": "Emily Bender, Alexander Koller, Climbing towards NLU: On meaning, form, and understanding in the age of data, 2020, Proc. of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Emily Bender",
                    "Alexander Koller"
                ],
                "title": "Climbing towards NLU: On meaning, form, and understanding in the age of data",
                "pub_date": "2020",
                "pub_title": "Proc. of ACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_89",
            "content": "UNKNOWN, None, 2021, Multimodal datasets: misogyny, pornography, and malignant stereotypes, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Multimodal datasets: misogyny, pornography, and malignant stereotypes",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_90",
            "content": "Yonatan Bisk, Rowan Zellers, Jianfeng Ronan Le Bras, Yejin Gao,  Choi, PIQA: reasoning about physical commonsense in natural language, 2020, Proc. of AAAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Yonatan Bisk",
                    "Rowan Zellers",
                    "Jianfeng Ronan Le Bras",
                    "Yejin Gao",
                    " Choi"
                ],
                "title": "PIQA: reasoning about physical commonsense in natural language",
                "pub_date": "2020",
                "pub_title": "Proc. of AAAI",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_91",
            "content": "Antoine Bosselut, Yejin Ronan Le Bras,  Choi, Dynamic neuro-symbolic knowledge graph construction for zero-shot commonsense question answering, 2021, Proc. of AAAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Antoine Bosselut",
                    "Yejin Ronan Le Bras",
                    " Choi"
                ],
                "title": "Dynamic neuro-symbolic knowledge graph construction for zero-shot commonsense question answering",
                "pub_date": "2021",
                "pub_title": "Proc. of AAAI",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_92",
            "content": "UNKNOWN, None, 2021, When combating hype, proceed with caution, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "When combating hype, proceed with caution",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_93",
            "content": "R Samuel, Gabor Bowman, Christopher Angeli, Christopher Potts,  Manning, A large annotated corpus for learning natural language inference, 2015, Proc. of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "R Samuel",
                    "Gabor Bowman",
                    "Christopher Angeli",
                    "Christopher Potts",
                    " Manning"
                ],
                "title": "A large annotated corpus for learning natural language inference",
                "pub_date": "2015",
                "pub_title": "Proc. of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_94",
            "content": "UNKNOWN, None, , Proc. of NeurIPS, Sam Mc-Candlish.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Proc. of NeurIPS",
                "pub": "Sam Mc-Candlish"
            }
        },
        {
            "ix": "120-ARR_v1_95",
            "content": "Dallas Card, Peter Henderson, Urvashi Khandelwal, Robin Jia, Kyle Mahowald, Dan Jurafsky, With little power comes great responsibility, 2020, Proc. of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Dallas Card",
                    "Peter Henderson",
                    "Urvashi Khandelwal",
                    "Robin Jia",
                    "Kyle Mahowald",
                    "Dan Jurafsky"
                ],
                "title": "With little power comes great responsibility",
                "pub_date": "2020",
                "pub_title": "Proc. of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_96",
            "content": "Dallas Card, Noah Smith, The importance of calibration for estimating proportions from annotations, 2018, Proc. of NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Dallas Card",
                    "Noah Smith"
                ],
                "title": "The importance of calibration for estimating proportions from annotations",
                "pub_date": "2018",
                "pub_title": "Proc. of NAACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_97",
            "content": "Kai-Wei Chang, He He, Robin Jia, Sameer Singh, Robustness and adversarial examples in natural language processing, 2021, Proc. of EMNLP: Tutorial Abstracts, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Kai-Wei Chang",
                    "He He",
                    "Robin Jia",
                    "Sameer Singh"
                ],
                "title": "Robustness and adversarial examples in natural language processing",
                "pub_date": "2021",
                "pub_title": "Proc. of EMNLP: Tutorial Abstracts",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_98",
            "content": "Kunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xiaochuan, Yuyu Zhang, Le Song, Taifeng Wang, Yuan Qi, Wei Chu, Question directed graph attention network for numerical reasoning over text, 2020, Proc. of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Kunlong Chen",
                    "Weidi Xu",
                    "Xingyi Cheng",
                    "Zou Xiaochuan",
                    "Yuyu Zhang",
                    "Le Song",
                    "Taifeng Wang",
                    "Yuan Qi",
                    "Wei Chu"
                ],
                "title": "Question directed graph attention network for numerical reasoning over text",
                "pub_date": "2020",
                "pub_title": "Proc. of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_99",
            "content": "C Chow, An optimum character recognition system using decision functions, 1957, IRE Trans. Electron. Comput, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "C Chow"
                ],
                "title": "An optimum character recognition system using decision functions",
                "pub_date": "1957",
                "pub_title": "IRE Trans. Electron. Comput",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_100",
            "content": "Corinna Cortes, Giulia Desalvo, Mehryar Mohri, Boosting with abstention, 2016, Proc. of NeurIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Corinna Cortes",
                    "Giulia Desalvo",
                    "Mehryar Mohri"
                ],
                "title": "Boosting with abstention",
                "pub_date": "2016",
                "pub_title": "Proc. of NeurIPS",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_101",
            "content": "Dmitry Davidov, Oren Tsur, Ari Rappoport, Semi-supervised recognition of sarcasm in Twitter and Amazon, 2010, Proc. of CoNLL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Dmitry Davidov",
                    "Oren Tsur",
                    "Ari Rappoport"
                ],
                "title": "Semi-supervised recognition of sarcasm in Twitter and Amazon",
                "pub_date": "2010",
                "pub_title": "Proc. of CoNLL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_102",
            "content": "H Morris, Stephen Degroot,  Fienberg, The comparison and evaluation of forecasters, 1983, Journal of the Royal Statistical Society: Series D (The Statistician), .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "H Morris",
                    "Stephen Degroot",
                    " Fienberg"
                ],
                "title": "The comparison and evaluation of forecasters",
                "pub_date": "1983",
                "pub_title": "Journal of the Royal Statistical Society: Series D (The Statistician)",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_103",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proc. of NAACL-HLT, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proc. of NAACL-HLT",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_104",
            "content": "Jesse Dodge, Maarten Sap, Ana Marasovi\u0107, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner, Documenting large webtext corpora: A case study on the colossal clean crawled corpus, 2021, Proc. of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Jesse Dodge",
                    "Maarten Sap",
                    "Ana Marasovi\u0107",
                    "William Agnew",
                    "Gabriel Ilharco",
                    "Dirk Groeneveld",
                    "Margaret Mitchell",
                    "Matt Gardner"
                ],
                "title": "Documenting large webtext corpora: A case study on the colossal clean crawled corpus",
                "pub_date": "2021",
                "pub_title": "Proc. of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_105",
            "content": "Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs, 2019, Proc. of NAACL-HLT, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Dheeru Dua",
                    "Yizhong Wang",
                    "Pradeep Dasigi",
                    "Gabriel Stanovsky",
                    "Sameer Singh",
                    "Matt Gardner"
                ],
                "title": "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
                "pub_date": "2019",
                "pub_title": "Proc. of NAACL-HLT",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_106",
            "content": "Yanai Elazar, Hongming Zhang, Yoav Goldberg, Dan Roth, Back to square one: Artifact detection, training and commonsense disentanglement in the Winograd schema, 2021, Proc. of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Yanai Elazar",
                    "Hongming Zhang",
                    "Yoav Goldberg",
                    "Dan Roth"
                ],
                "title": "Back to square one: Artifact detection, training and commonsense disentanglement in the Winograd schema",
                "pub_date": "2021",
                "pub_title": "Proc. of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_107",
            "content": "Matt Gardner, William Merrill, Jesse Dodge, Matthew Peters, Alexis Ross, Sameer Singh, Noah Smith, Competency problems: On finding and removing artifacts in language data, 2021, Proc. of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Matt Gardner",
                    "William Merrill",
                    "Jesse Dodge",
                    "Matthew Peters",
                    "Alexis Ross",
                    "Sameer Singh",
                    "Noah Smith"
                ],
                "title": "Competency problems: On finding and removing artifacts in language data",
                "pub_date": "2021",
                "pub_title": "Proc. of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_108",
            "content": "Suchin Samuel Gehman, Maarten Gururangan, Yejin Sap, Noah Choi,  Smith, RealToxic-ityPrompts: Evaluating neural toxic degeneration in language models, 2020, Findings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Suchin Samuel Gehman",
                    "Maarten Gururangan",
                    "Yejin Sap",
                    "Noah Choi",
                    " Smith"
                ],
                "title": "RealToxic-ityPrompts: Evaluating neural toxic degeneration in language models",
                "pub_date": "2020",
                "pub_title": "Findings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_109",
            "content": "Max Glockner, Vered Shwartz, Yoav Goldberg, Breaking NLI systems with sentences that require simple lexical inferences, 2018, Proc. of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Max Glockner",
                    "Vered Shwartz",
                    "Yoav Goldberg"
                ],
                "title": "Breaking NLI systems with sentences that require simple lexical inferences",
                "pub_date": "2018",
                "pub_title": "Proc. of ACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_110",
            "content": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Making the V in VQA matter: Elevating the role of image understanding in visual question answering, 2017, Proc. of CVPR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Yash Goyal",
                    "Tejas Khot",
                    "Douglas Summers-Stay",
                    "Dhruv Batra",
                    "Devi Parikh"
                ],
                "title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
                "pub_date": "2017",
                "pub_title": "Proc. of CVPR",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_111",
            "content": "UNKNOWN, None, 2013, Prose comprehension beyond the word, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2013",
                "pub_title": "Prose comprehension beyond the word",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_112",
            "content": "P Herbert,  Grice, Logic and conversation, 1975, Speech acts, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "P Herbert",
                    " Grice"
                ],
                "title": "Logic and conversation",
                "pub_date": "1975",
                "pub_title": "Speech acts",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_113",
            "content": "UNKNOWN, None, 1989, Studies in the Way of Words, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": null,
                "title": null,
                "pub_date": "1989",
                "pub_title": "Studies in the Way of Words",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_114",
            "content": "UNKNOWN, None, 2021, Ppt: Pre-trained prompt tuning for few-shot learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Ppt: Pre-trained prompt tuning for few-shot learning",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_115",
            "content": "Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Weinberger, On calibration of modern neural networks, 2017, Proc. of ICML, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Chuan Guo",
                    "Geoff Pleiss",
                    "Yu Sun",
                    "Kilian Weinberger"
                ],
                "title": "On calibration of modern neural networks",
                "pub_date": "2017",
                "pub_title": "Proc. of ICML",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_116",
            "content": "Swabha Suchin Gururangan, Omer Swayamdipta, Roy Levy, Samuel Schwartz, Noah Bowman,  Smith, Annotation artifacts in natural language inference data, 2018, Proc. of NAACL-HLT, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Swabha Suchin Gururangan",
                    "Omer Swayamdipta",
                    "Roy Levy",
                    "Samuel Schwartz",
                    "Noah Bowman",
                    " Smith"
                ],
                "title": "Annotation artifacts in natural language inference data",
                "pub_date": "2018",
                "pub_title": "Proc. of NAACL-HLT",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_117",
            "content": "UNKNOWN, None, 2020, DeBERTa: Decodingenhanced bert with disentangled attention, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "DeBERTa: Decodingenhanced bert with disentangled attention",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_118",
            "content": "Martin Hellman, The nearest neighbor classification rule with a reject option, 1970, IEEE Trans. Syst. Sci. Cybern, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Martin Hellman"
                ],
                "title": "The nearest neighbor classification rule with a reject option",
                "pub_date": "1970",
                "pub_title": "IEEE Trans. Syst. Sci. Cybern",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_119",
            "content": "Antonios Md Mosharaf Hossain, Eduardo Anastasopoulos, Alexis Blanco,  Palmer, It's not a non-issue: Negation as a source of error in machine translation, 2020, Findings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Antonios Md Mosharaf Hossain",
                    "Eduardo Anastasopoulos",
                    "Alexis Blanco",
                    " Palmer"
                ],
                "title": "It's not a non-issue: Negation as a source of error in machine translation",
                "pub_date": "2020",
                "pub_title": "Findings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_120",
            "content": "A Drew, Christopher Hudson,  Manning, GQA: A new dataset for real-world visual reasoning and compositional question answering, 2019, Proc. of CVPR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "A Drew",
                    "Christopher Hudson",
                    " Manning"
                ],
                "title": "GQA: A new dataset for real-world visual reasoning and compositional question answering",
                "pub_date": "2019",
                "pub_title": "Proc. of CVPR",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_121",
            "content": "Amita Kamath, Robin Jia, Percy Liang, Selective question answering under domain shift, 2020, Proc. of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Amita Kamath",
                    "Robin Jia",
                    "Percy Liang"
                ],
                "title": "Selective question answering under domain shift",
                "pub_date": "2020",
                "pub_title": "Proc. of ACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_122",
            "content": "UNKNOWN, None, 2019, Playing it safe: Adversarial robustness with an abstain option, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Playing it safe: Adversarial robustness with an abstain option",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_123",
            "content": "Swabha Ronan Le Bras, Chandra Swayamdipta, Rowan Bhagavatula, Matthew Zellers, Ashish Peters, Yejin Sabharwal,  Choi, Adversarial filters of dataset biases, 2020, Proc. of ICML, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Swabha Ronan Le Bras",
                    "Chandra Swayamdipta",
                    "Rowan Bhagavatula",
                    "Matthew Zellers",
                    "Ashish Peters",
                    "Yejin Sabharwal",
                    " Choi"
                ],
                "title": "Adversarial filters of dataset biases",
                "pub_date": "2020",
                "pub_title": "Proc. of ICML",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_124",
            "content": "UNKNOWN, None, , Chris Callison-Burch, and Nicholas Carlini. 2021. Deduplicating training data makes language models better, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Chris Callison-Burch, and Nicholas Carlini. 2021. Deduplicating training data makes language models better",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_125",
            "content": "Hector Levesque, Ernest Davis, Leora Morgenstern, The winograd schema challenge, 2012, Proc. of KR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Hector Levesque",
                    "Ernest Davis",
                    "Leora Morgenstern"
                ],
                "title": "The winograd schema challenge",
                "pub_date": "2012",
                "pub_title": "Proc. of KR",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_126",
            "content": "H Liu, P Singh, Conceptnet: A practical commonsense reasoning toolkit, 2004, BT Technology Journal, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "H Liu",
                    "P Singh"
                ],
                "title": "Conceptnet: A practical commonsense reasoning toolkit",
                "pub_date": "2004",
                "pub_title": "BT Technology Journal",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_127",
            "content": "UNKNOWN, None, 2021, Can small and synthetic benchmarks drive modeling innovation? a retrospective study of question answering modeling approaches, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Can small and synthetic benchmarks drive modeling innovation? a retrospective study of question answering modeling approaches",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_128",
            "content": "UNKNOWN, None, 2019, RoBERTa: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "RoBERTa: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_129",
            "content": "UNKNOWN, None, 2021, Provable limitations of acquiring meaning from ungrounded form:what will future language models understand, TACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Provable limitations of acquiring meaning from ungrounded form:what will future language models understand",
                "pub": "TACL"
            }
        },
        {
            "ix": "120-ARR_v1_130",
            "content": "Saif Mohammad, Ekaterina Shutova, Peter Turney, Metaphor as a medium for emotion: An empirical study, 2016, Proc. of *SEM, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "Saif Mohammad",
                    "Ekaterina Shutova",
                    "Peter Turney"
                ],
                "title": "Metaphor as a medium for emotion: An empirical study",
                "pub_date": "2016",
                "pub_title": "Proc. of *SEM",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_131",
            "content": "Roser Morante, Eduardo Blanco, *SEM 2012 shared task: Resolving the scope and focus of negation, 2012, Proc. of *SEM, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": [
                    "Roser Morante",
                    "Eduardo Blanco"
                ],
                "title": "*SEM 2012 shared task: Resolving the scope and focus of negation",
                "pub_date": "2012",
                "pub_title": "Proc. of *SEM",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_132",
            "content": "Silviu Oprea, Walid Magdy, 2020. iSarcasm: A dataset of intended sarcasm, , Proc. of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": [
                    "Silviu Oprea",
                    "Walid Magdy"
                ],
                "title": "2020. iSarcasm: A dataset of intended sarcasm",
                "pub_date": null,
                "pub_title": "Proc. of ACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_133",
            "content": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, Deep contextualized word representations, 2018, Proc. of NAACL-HLT, .",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": [
                    "Matthew Peters",
                    "Mark Neumann",
                    "Mohit Iyyer",
                    "Matt Gardner",
                    "Christopher Clark",
                    "Kenton Lee",
                    "Luke Zettlemoyer"
                ],
                "title": "Deep contextualized word representations",
                "pub_date": "2018",
                "pub_title": "Proc. of NAACL-HLT",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_134",
            "content": "Matthew Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, Noah Smith, Knowledge enhanced contextual word representations, 2019, Proc. of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b55",
                "authors": [
                    "Matthew Peters",
                    "Mark Neumann",
                    "Robert Logan",
                    "Roy Schwartz",
                    "Vidur Joshi",
                    "Sameer Singh",
                    "Noah Smith"
                ],
                "title": "Knowledge enhanced contextual word representations",
                "pub_date": "2019",
                "pub_title": "Proc. of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_135",
            "content": "Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, Benjamin Van Durme, Hypothesis only baselines in natural language inference, 2018, Proc. of *SEM, .",
            "ntype": "ref",
            "meta": {
                "xid": "b56",
                "authors": [
                    "Adam Poliak",
                    "Jason Naradowsky",
                    "Aparajita Haldar",
                    "Rachel Rudinger",
                    "Benjamin Van Durme"
                ],
                "title": "Hypothesis only baselines in natural language inference",
                "pub_date": "2018",
                "pub_title": "Proc. of *SEM",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_136",
            "content": "Lianhui Qin, Vered Shwartz, Peter West, Chandra Bhagavatula, Jena Hwang, Ronan Bras, Antoine Bosselut, Yejin Choi, Back to the future: Unsupervised backprop-based decoding for counterfactual and abductive commonsense reasoning, 2020, Proc. of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b57",
                "authors": [
                    "Lianhui Qin",
                    "Vered Shwartz",
                    "Peter West",
                    "Chandra Bhagavatula",
                    "Jena Hwang",
                    "Ronan Bras",
                    "Antoine Bosselut",
                    "Yejin Choi"
                ],
                "title": "Back to the future: Unsupervised backprop-based decoding for counterfactual and abductive commonsense reasoning",
                "pub_date": "2020",
                "pub_title": "Proc. of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_137",
            "content": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Language models are unsupervised multitask learners, 2019, OpenAI blog, .",
            "ntype": "ref",
            "meta": {
                "xid": "b58",
                "authors": [
                    "Alec Radford",
                    "Jeffrey Wu",
                    "Rewon Child",
                    "David Luan",
                    "Dario Amodei",
                    "Ilya Sutskever"
                ],
                "title": "Language models are unsupervised multitask learners",
                "pub_date": "2019",
                "pub_title": "OpenAI blog",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_138",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, JMLR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b59",
                "authors": [
                    "Colin Raffel",
                    "Noam Shazeer",
                    "Adam Roberts",
                    "Katherine Lee",
                    "Sharan Narang",
                    "Michael Matena",
                    "Yanqi Zhou",
                    "Wei Li",
                    "Peter Liu"
                ],
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "pub_date": "2020",
                "pub_title": "JMLR",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_139",
            "content": "UNKNOWN, None, , 2021. AI and the everything in the whole wide world benchmark, .",
            "ntype": "ref",
            "meta": {
                "xid": "b60",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "2021. AI and the everything in the whole wide world benchmark",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_140",
            "content": "Pranav Rajpurkar, Robin Jia, Percy Liang, Know what you don't know: Unanswerable questions for SQuAD, 2018, Proc. of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b61",
                "authors": [
                    "Pranav Rajpurkar",
                    "Robin Jia",
                    "Percy Liang"
                ],
                "title": "Know what you don't know: Unanswerable questions for SQuAD",
                "pub_date": "2018",
                "pub_title": "Proc. of ACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_141",
            "content": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, SQuAD: 100,000+ questions for machine comprehension of text, 2016, Proc. of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b62",
                "authors": [
                    "Pranav Rajpurkar",
                    "Jian Zhang",
                    "Konstantin Lopyrev",
                    "Percy Liang"
                ],
                "title": "SQuAD: 100,000+ questions for machine comprehension of text",
                "pub_date": "2016",
                "pub_title": "Proc. of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_142",
            "content": "Arijit Ray, Gordon Christie, Mohit Bansal, Dhruv Batra, Devi Parikh, Question relevance in VQA: Identifying non-visual and false-premise questions, 2016, Proc. of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b63",
                "authors": [
                    "Arijit Ray",
                    "Gordon Christie",
                    "Mohit Bansal",
                    "Dhruv Batra",
                    "Devi Parikh"
                ],
                "title": "Question relevance in VQA: Identifying non-visual and false-premise questions",
                "pub_date": "2016",
                "pub_title": "Proc. of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_143",
            "content": "Adam Roberts, Colin Raffel, Noam Shazeer, How much knowledge can you pack into the parameters of a language model, 2020, Proc. of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b64",
                "authors": [
                    "Adam Roberts",
                    "Colin Raffel",
                    "Noam Shazeer"
                ],
                "title": "How much knowledge can you pack into the parameters of a language model",
                "pub_date": "2020",
                "pub_title": "Proc. of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_144",
            "content": "Anna Rogers, Changing the world by changing the data, 2021, Proc. of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b65",
                "authors": [
                    "Anna Rogers"
                ],
                "title": "Changing the world by changing the data",
                "pub_date": "2021",
                "pub_title": "Proc. of ACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_145",
            "content": "Dana Rubinstein, Effi Levi, Roy Schwartz, Ari Rappoport, How well do distributional models capture different types of semantic knowledge?, 2015, Proc. of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b66",
                "authors": [
                    "Dana Rubinstein",
                    "Effi Levi",
                    "Roy Schwartz",
                    "Ari Rappoport"
                ],
                "title": "How well do distributional models capture different types of semantic knowledge?",
                "pub_date": "2015",
                "pub_title": "Proc. of ACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_146",
            "content": "Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula,  Choi, WinoGrande: An adversarial winograd schema challenge at scale, 2020, Proc. of AAAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b67",
                "authors": [
                    "Keisuke Sakaguchi",
                    "Le Ronan",
                    "Chandra Bras",
                    "Yejin Bhagavatula",
                    " Choi"
                ],
                "title": "WinoGrande: An adversarial winograd schema challenge at scale",
                "pub_date": "2020",
                "pub_title": "Proc. of AAAI",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_147",
            "content": "Timo Schick, Hinrich Sch\u00fctze, It's not just size that matters: Small language models are also few-shot learners, 2021, Proc. of NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b68",
                "authors": [
                    "Timo Schick",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "It's not just size that matters: Small language models are also few-shot learners",
                "pub_date": "2021",
                "pub_title": "Proc. of NAACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_148",
            "content": "UNKNOWN, None, 2021, True fewshot learning with prompts -a real-world perspective, .",
            "ntype": "ref",
            "meta": {
                "xid": "b69",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "True fewshot learning with prompts -a real-world perspective",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_149",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b70",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_150",
            "content": "Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila Zilles, Yejin Choi, Noah Smith, The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task, 2017, Proc. of CoNLL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b71",
                "authors": [
                    "Roy Schwartz",
                    "Maarten Sap",
                    "Ioannis Konstas",
                    "Leila Zilles",
                    "Yejin Choi",
                    "Noah Smith"
                ],
                "title": "The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task",
                "pub_date": "2017",
                "pub_title": "Proc. of CoNLL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_151",
            "content": "Taylor Shin, Yasaman Razeghi, Robert Logan, I , Eric Wallace, Sameer Singh, Auto-Prompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts, 2020, Proc. of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b72",
                "authors": [
                    "Taylor Shin",
                    "Yasaman Razeghi",
                    "Robert Logan",
                    "I ",
                    "Eric Wallace",
                    "Sameer Singh"
                ],
                "title": "Auto-Prompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
                "pub_date": "2020",
                "pub_title": "Proc. of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_152",
            "content": "UNKNOWN, None, 2019, A flexible and adaptive framework for abstention under class imbalance, .",
            "ntype": "ref",
            "meta": {
                "xid": "b73",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "A flexible and adaptive framework for abstention under class imbalance",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_153",
            "content": "UNKNOWN, None, 2014, Deep inside convolutional networks: Visualising image classification models and saliency maps, .",
            "ntype": "ref",
            "meta": {
                "xid": "b74",
                "authors": null,
                "title": null,
                "pub_date": "2014",
                "pub_title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_154",
            "content": "UNKNOWN, None, 2017, SmoothGrad: removing noise by adding noise, .",
            "ntype": "ref",
            "meta": {
                "xid": "b75",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "SmoothGrad: removing noise by adding noise",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_155",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, Proc. of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b76",
                "authors": [
                    "Richard Socher",
                    "Alex Perelygin",
                    "Jean Wu",
                    "Jason Chuang",
                    "Christopher Manning",
                    "Andrew Ng",
                    "Christopher Potts"
                ],
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
                "pub_date": "2013",
                "pub_title": "Proc. of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_156",
            "content": "Gabriel Stanovsky, Noah Smith, Luke Zettlemoyer, Evaluating gender bias in machine translation, 2019, Proc. of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b77",
                "authors": [
                    "Gabriel Stanovsky",
                    "Noah Smith",
                    "Luke Zettlemoyer"
                ],
                "title": "Evaluating gender bias in machine translation",
                "pub_date": "2019",
                "pub_title": "Proc. of ACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_157",
            "content": "UNKNOWN, None, 2007, Combinatory categorial grammar, .",
            "ntype": "ref",
            "meta": {
                "xid": "b78",
                "authors": null,
                "title": null,
                "pub_date": "2007",
                "pub_title": "Combinatory categorial grammar",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_158",
            "content": "Emma Strubell, Ananya Ganesh, Andrew Mccallum, Energy and policy considerations for deep learning in NLP, 2019, Proc. of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b79",
                "authors": [
                    "Emma Strubell",
                    "Ananya Ganesh",
                    "Andrew Mccallum"
                ],
                "title": "Energy and policy considerations for deep learning in NLP",
                "pub_date": "2019",
                "pub_title": "Proc. of ACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_159",
            "content": "Elior Sulem, Jamaal Hay, Dan Roth, Do we know what we don't know? studying unanswerable questions beyond SQuAD 2.0, 2021, Findings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b80",
                "authors": [
                    "Elior Sulem",
                    "Jamaal Hay",
                    "Dan Roth"
                ],
                "title": "Do we know what we don't know? studying unanswerable questions beyond SQuAD 2.0",
                "pub_date": "2021",
                "pub_title": "Findings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_160",
            "content": "Mukund Sundararajan, Ankur Taly, Qiqi Yan, Axiomatic attribution for deep networks, 2017, Proc. of ICML, .",
            "ntype": "ref",
            "meta": {
                "xid": "b81",
                "authors": [
                    "Mukund Sundararajan",
                    "Ankur Taly",
                    "Qiqi Yan"
                ],
                "title": "Axiomatic attribution for deep networks",
                "pub_date": "2017",
                "pub_title": "Proc. of ICML",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_161",
            "content": "Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah Smith, Yejin Choi, Dataset cartography: Mapping and diagnosing datasets with training dynamics, 2020, Proc. of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b82",
                "authors": [
                    "Swabha Swayamdipta",
                    "Roy Schwartz",
                    "Nicholas Lourie",
                    "Yizhong Wang",
                    "Hannaneh Hajishirzi",
                    "Noah Smith",
                    "Yejin Choi"
                ],
                "title": "Dataset cartography: Mapping and diagnosing datasets with training dynamics",
                "pub_date": "2020",
                "pub_title": "Proc. of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_162",
            "content": "Niket Tandon, Keisuke Sakaguchi, Bhavana Dalvi, Dheeraj Rajagopal, Peter Clark, Michal Guerquin, Kyle Richardson, Eduard Hovy, A dataset for tracking entities in open domain procedural text, 2020, Proc. of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b83",
                "authors": [
                    "Niket Tandon",
                    "Keisuke Sakaguchi",
                    "Bhavana Dalvi",
                    "Dheeraj Rajagopal",
                    "Peter Clark",
                    "Michal Guerquin",
                    "Kyle Richardson",
                    "Eduard Hovy"
                ],
                "title": "A dataset for tracking entities in open domain procedural text",
                "pub_date": "2020",
                "pub_title": "Proc. of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_163",
            "content": "Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman, Eric Nyberg, Chris Dyer, Metaphor detection with cross-lingual model transfer, 2014, Proc. of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b84",
                "authors": [
                    "Yulia Tsvetkov",
                    "Leonid Boytsov",
                    "Anatole Gershman",
                    "Eric Nyberg",
                    "Chris Dyer"
                ],
                "title": "Metaphor detection with cross-lingual model transfer",
                "pub_date": "2014",
                "pub_title": "Proc. of ACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_164",
            "content": "Luyu Wang, Yujia Li, WikiGraphs: A Wikipedia text -knowledge graph paired dataset, 2021, Proc. of TextGraphs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b85",
                "authors": [
                    "Luyu Wang",
                    "Yujia Li"
                ],
                "title": "WikiGraphs: A Wikipedia text -knowledge graph paired dataset",
                "pub_date": "2021",
                "pub_title": "Proc. of TextGraphs",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_165",
            "content": "Zhao Wang, Aron Culotta, Identifying spurious correlations for robust text classification, 2020, Findings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b86",
                "authors": [
                    "Zhao Wang",
                    "Aron Culotta"
                ],
                "title": "Identifying spurious correlations for robust text classification",
                "pub_date": "2020",
                "pub_title": "Findings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_166",
            "content": "Orion Weller, Kevin Seppi, Humor detection: A transformer gets the last laugh, 2019, Proc. of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b87",
                "authors": [
                    "Orion Weller",
                    "Kevin Seppi"
                ],
                "title": "Humor detection: A transformer gets the last laugh",
                "pub_date": "2019",
                "pub_title": "Proc. of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_167",
            "content": "Yadollah Yaghoobzadeh, Soroush Mehri, Remi Tachet Des Combes, T Hazen, Alessandro Sordoni, Increasing robustness to spurious correlations using forgettable examples, 2021, Proc. of EACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b88",
                "authors": [
                    "Yadollah Yaghoobzadeh",
                    "Soroush Mehri",
                    "Remi Tachet Des Combes",
                    "T Hazen",
                    "Alessandro Sordoni"
                ],
                "title": "Increasing robustness to spurious correlations using forgettable examples",
                "pub_date": "2021",
                "pub_title": "Proc. of EACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_168",
            "content": "Rowan Zellers, Yonatan Bisk, Roy Schwartz, Yejin Choi, SWAG: A large-scale adversarial dataset for grounded commonsense inference, 2018, Proc. of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b89",
                "authors": [
                    "Rowan Zellers",
                    "Yonatan Bisk",
                    "Roy Schwartz",
                    "Yejin Choi"
                ],
                "title": "SWAG: A large-scale adversarial dataset for grounded commonsense inference",
                "pub_date": "2018",
                "pub_title": "Proc. of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_169",
            "content": "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, HellaSwag: Can a machine really finish your sentence?, 2019, Proc. of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b90",
                "authors": [
                    "Rowan Zellers",
                    "Ari Holtzman",
                    "Yonatan Bisk",
                    "Ali Farhadi",
                    "Yejin Choi"
                ],
                "title": "HellaSwag: Can a machine really finish your sentence?",
                "pub_date": "2019",
                "pub_title": "Proc. of ACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_170",
            "content": "Shujian Zhang, Chengyue Gong, Eunsol Choi, Knowing more about questions can help: Improving calibration in question answering, 2021, Findings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b91",
                "authors": [
                    "Shujian Zhang",
                    "Chengyue Gong",
                    "Eunsol Choi"
                ],
                "title": "Knowing more about questions can help: Improving calibration in question answering",
                "pub_date": "2021",
                "pub_title": "Findings of ACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_171",
            "content": "Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu, ERNIE: Enhanced language representation with informative entities, 2019, Proc. of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b92",
                "authors": [
                    "Zhengyan Zhang",
                    "Xu Han",
                    "Zhiyuan Liu",
                    "Xin Jiang",
                    "Maosong Sun",
                    "Qun Liu"
                ],
                "title": "ERNIE: Enhanced language representation with informative entities",
                "pub_date": "2019",
                "pub_title": "Proc. of ACL",
                "pub": null
            }
        },
        {
            "ix": "120-ARR_v1_172",
            "content": "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, Kai-Wei Chang, Men also like shopping: Reducing gender bias amplification using corpus-level constraints, 2017, Proc. of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b93",
                "authors": [
                    "Jieyu Zhao",
                    "Tianlu Wang",
                    "Mark Yatskar",
                    "Vicente Ordonez",
                    "Kai-Wei Chang"
                ],
                "title": "Men also like shopping: Reducing gender bias amplification using corpus-level constraints",
                "pub_date": "2017",
                "pub_title": "Proc. of EMNLP",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "120-ARR_v1_0@0",
            "content": "On the Limitations of Dataset Balancing: The Lost Battle Against Spurious Correlations",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_0",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_2@0",
            "content": "Recent work have shown that deep learning models in NLP are highly sensitive to low-level correlations between simple features and specific output labels, leading to overfitting and lack of generalization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_2",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_2@1",
            "content": "To mitigate this problem, a common practice is to balance datasets by adding new instances or by filtering out \"easy\" instances , culminating in a recent proposal to eliminate single-word correlations altogether .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_2",
            "start": 206,
            "end": 418,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_2@2",
            "content": "In this opinion paper, we identify that despite these efforts, increasingly-powerful models keep exploiting ever-smaller spurious correlations, and as a result even balancing all single-word features is insufficient for mitigating all of these correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_2",
            "start": 420,
            "end": 676,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_2@3",
            "content": "In parallel, a truly balanced dataset may be bound to \"throw the baby out with the bathwater\" and miss important signal encoding common sense and world knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_2",
            "start": 678,
            "end": 839,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_2@4",
            "content": "We highlight several alternatives to dataset balancing, focusing on enhancing datasets with richer contexts, allowing models to abstain and interact with users, and turning from large-scale fine-tuning to zero-or few-shot setups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_2",
            "start": 841,
            "end": 1069,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_4@0",
            "content": "Effective human communication relies on our ability to understand extra-textual context based on common sense, world knowledge or shared cultural experiences, a property often cited as Grice's second maxim of quantity: \"Do not make your contribution more informative than is required\" (Grice, 1975(Grice, , 1989.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_4",
            "start": 0,
            "end": 311,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_4@1",
            "content": "Studies have estimated that only 12% of the information conveyed by text is mentioned explicitly (Graesser, 2013;Tandon et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_4",
            "start": 313,
            "end": 446,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_4@2",
            "content": "To illustrate this, consider the question \"who is the president of the U.S.?\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_4",
            "start": 448,
            "end": 525,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_4@3",
            "content": "To answer it, a human reader is likely to presume many unstated propositions, as exemplified in Tab.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_4",
            "start": 527,
            "end": 626,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_4@4",
            "content": "1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_4",
            "start": 628,
            "end": 629,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_5@0",
            "content": "In contrast to humans, supervised models often fail to generalize and understand implicit context, instead resorting to low-level correlations in Who is the president of the U.S.?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_5",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_6@0",
            "content": "The year 2019 Donald Trump The West Wing, season 1 Josiah \"Jed\" Bartlet Table 1: Context, whether explicit or implicit, matters in textual understanding, as exemplified by the question \"who is the president of the U.S.?\". E.g., in the first line, given no other context, a QA system should provide the most sensible fallback answer (Joe Biden).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_6",
            "start": 0,
            "end": 343,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_7@0",
            "content": "the data, leading to amplified bias (Zhao et al., 2017; and brittle performance (Schwartz et al., 2017;Gururangan et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_7",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_7@1",
            "content": "To address this, recent approaches have suggested mitigating such correlations by balancing the dataset via either adding or removing certain instances (Goyal et al., 2017;Hudson and Manning, 2019;Zellers et al., 2018;.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_7",
            "start": 129,
            "end": 347,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_7@2",
            "content": "In parallel, developers keep building larger and larger pretrained models (Devlin et al., 2019;, which, when fine-tuned on these datasets, consistently manage to reach human performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_7",
            "start": 349,
            "end": 534,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_7@3",
            "content": "Taken together, these trends lead to an arms-race between data curation and model development (Fig. 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_7",
            "start": 536,
            "end": 638,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_8@0",
            "content": "In this position paper, we question the value of mitigating spurious correlations via dataset balancing, by showing that their existence in large training sets is both inevitable and to some extent even desired, as they are an inherent property of natural language understanding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_8",
            "start": 0,
            "end": 278,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_8@1",
            "content": "We build on a recent result by , who assumed that every single-word feature correlation is spurious, i.e., can be used to mislead a model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_8",
            "start": 280,
            "end": 417,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_8@2",
            "content": "We extend their argument, showing that balancing single-word features is insufficient for eliminating all spurious correlations, and that balancing feature combination is needed for that purpose.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_8",
            "start": 419,
            "end": 613,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_8@3",
            "content": "On the other hand, we show that balancing too much leads to datasets that contain no learnable signal either.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_8",
            "start": 615,
            "end": 723,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_8@4",
            "content": "We conclude by questioning whether mitigating all spurious correlations via dataset balancing is practical.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_8",
            "start": 725,
            "end": 831,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_9@0",
            "content": "Following, we show that this practice is also undesired.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_9",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_9@1",
            "content": "We show that ignoring these correlations will hinder the learning of fallback options for both world knowledge facts (Joe Biden is the president of the U.S.) and common sense knowledge (a person is happy when receiving a gift), thus preventing models from using this knowledge in cases of uncertainty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_9",
            "start": 57,
            "end": 357,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_9@2",
            "content": "We conclude that the existence of spurious correlations in training sets should not be solved by creating more balanced datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_9",
            "start": 359,
            "end": 487,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_9@3",
            "content": "1 We then discuss alternatives to mitigating spurious correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_9",
            "start": 489,
            "end": 555,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_9@4",
            "content": "We argue that models should be trained to understand constructions emanating from an apriori theory of language, such as negation, sarcasm, humor, and metaphors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_9",
            "start": 557,
            "end": 717,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_9@5",
            "content": "We also suggest adopting modeling approaches that identify when the context is insufficient, and the model should not fallback to default assumptions, but rather output an \"I don't know\" response (e.g., unanswerable questions, Rajpurkar et al., 2018;Sulem et al., 2021) or interact with the user to clear ambiguities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_9",
            "start": 719,
            "end": 1035,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_9@6",
            "content": "We conclude by questioning the basic procedure of large-scale fine-tuning, and suggest focusing on zero-and few-shot learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_9",
            "start": 1037,
            "end": 1162,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_10@0",
            "content": "Dataset-Model Arms Race",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_10",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_11@0",
            "content": "This section provides a view of recent research in NLP as an arms race between models and datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_11",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_11@1",
            "content": "Below we describe the conditions leading to this Figure 2: An example of dataset balancing (adopted from Goyal et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_11",
            "start": 100,
            "end": 224,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_11@2",
            "content": "For each (question, image) pair in the VQA dataset (left), VQA2.0 adds another image, for which the answer is different (right).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_11",
            "start": 226,
            "end": 353,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_12@0",
            "content": "arms race, and present our main research question, challenging its value for making progress in NLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_12",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_13@0",
            "content": "Dataset balancing via augmentation While pretrained models consistently perform well across multiple tasks, various studies have pointed out that this is often achieved by exploiting spurious correlations in datasets, rather than improving on the underlying task (Glockner et al., 2018;Gururangan et al., 2018;Elazar et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_13",
            "start": 0,
            "end": 330,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_14@0",
            "content": "Various dataset curators have tried to prevent models from learning spurious correlations by modifying their training data via a careful control for the training label distribution, effectively striving for a balanced dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_14",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_14@1",
            "content": "One approach, popular in visual question answering datasets, is to add examples in order to balance the dataset (Goyal et al., 2017;Hudson and Manning, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_14",
            "start": 227,
            "end": 384,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_14@2",
            "content": "For instance, the VQA2.0 dataset (Goyal et al., 2017) is built by taking every (question q, image i, answer a) triplet in the VQA dataset (Antol et al., 2015), and adding another triplet with the same question q, but a different image i , guaranteed to lead to a different answer a .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_14",
            "start": 386,
            "end": 668,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_14@3",
            "content": "See Fig. 2 for an example.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_14",
            "start": 670,
            "end": 695,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_15@0",
            "content": "Filtering as balancing A complementary approach to augmentation is filtering examples out from datasets such that spurious correlations are minimized.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_15",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_15@1",
            "content": "This approach was taken in the creation of the SWAG dataset (Zellers et al., 2018), using \"adversarial filtering\" (AF).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_15",
            "start": 151,
            "end": 269,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_15@2",
            "content": "In AF, dataset instances that are easily solved by an adversarial model are filtered out.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_15",
            "start": 271,
            "end": 359,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_15@3",
            "content": "The AF approach was picked up by many follow-up datasets such as DROP (Dua et al., 2019), HellaSWAG (Zellers et al., 2019), and Wino-Grande .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_15",
            "start": 361,
            "end": 501,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_16@0",
            "content": "Here we argue that approaches like AF converge to removing all low-level correlations, 2 and there-fore a fully balanced dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_16",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_16@1",
            "content": "As this approach relies on an external model, applying it with ever stronger models with higher capacity, will allow these models to pick up on subtler correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_16",
            "start": 130,
            "end": 294,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_16@2",
            "content": "At the extreme, the remaining instances that could not be solved by a fully capable model will have no statistical signal that can be exploited by that model, i.e., a balanced dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_16",
            "start": 296,
            "end": 479,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_16@3",
            "content": "We henceforth refer to both augmentation and filtering as balancing methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_16",
            "start": 481,
            "end": 556,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_17@0",
            "content": "Large models solve the new datasets In parallel to the efforts in dataset balancing, the leading modeling approach in recent years in NLP is pretraining large language models on raw text corpora, followed by fine-tuning them on supervised downstream applications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_17",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_17@1",
            "content": "These models continue to grow in size (Peters et al., 2018;Devlin et al., 2019;Radford et al., 2019;, and their fine-tuning performance improves accordingly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_17",
            "start": 264,
            "end": 420,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_17@2",
            "content": "This in turn leads to more aggressive balancing, setting in motion a kind of arms race between datasets and models (Fig. 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_17",
            "start": 422,
            "end": 545,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_18@0",
            "content": "Evidently, a similar trend emerges for the previously mentioned datasets: (1) the first baselines, reflecting the state of the art at the time of dataset creation, perform poorly, e.g., 52% accuracy on SWAG, 47 F1 on DROP, 47% on Hel-laSWAG, and 53% AUC on WinoGrande; (2) model developers introduce increasingly larger and heavily-parameterized models, hill-climbing on these datasets; and eventually (3) models essentially solve the dataset within a year or two, often outperforming humans: 86% on SWAG (Devlin et al., 2019), 90 F1 on DROP , 93% on HellaSWAG (He et al., 2020), and 88% AUC on WinoGrande .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_18",
            "start": 0,
            "end": 606,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_18@1",
            "content": "(4) new large-scale datasets are collected with more aggressive pruning techniques, thus repeating the cycle.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_18",
            "start": 608,
            "end": 716,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_19@0",
            "content": "Based on these findings, our main research question is whether dataset balancing is the most promising method for mitigating spurious correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_19",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_19@1",
            "content": "We note that an arms race between models and datasets might spur advances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_19",
            "start": 148,
            "end": 221,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_19@2",
            "content": "Here we question a specific aspect of this arms race: the improvement of datasets by using more aggressive filtering techniques.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_19",
            "start": 223,
            "end": 350,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_19@3",
            "content": "Next we turn to present practical and conceptual limitations of this practice.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_19",
            "start": 352,
            "end": 429,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_20@0",
            "content": "The Lost Battle Against Spurious Correlations",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_20",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_21@0",
            "content": "So far we have identified dataset balancing as a common way to mitigate spurious correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_21",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_22@0",
            "content": "Correlations between features and output labels for no reason.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_22",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_22@1",
            "content": "ungeneralizable Correlations that do not generalize to new contexts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_22",
            "start": 63,
            "end": 130,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_23@0",
            "content": "Correlations between every singleword feature and output label.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_23",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_24@0",
            "content": "Table 2: Different definitions of spurious correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_24",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_25@0",
            "content": "Next, we outline how different works define spurious correlations (Sec. 3.1), and then question whether dataset balancing is a viable way for mitigating them; we note that balancing too little is bound to leave spurious correlations in the data (Sec. 3.2), while balancing too much discards meaningful signal (Sec. 3.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_25",
            "start": 0,
            "end": 319,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_25@1",
            "content": "We finish by questioning whether this practice is even desired (Sec. 3.4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_25",
            "start": 321,
            "end": 394,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_26@0",
            "content": "What are Spurious Correlations?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_26",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_27@0",
            "content": "Mitigating spurious correlations is frequently used as motivation for developing new balancing approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_27",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_27@1",
            "content": "However, the term spurious correlations is often not clearly and consistently defined.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_27",
            "start": 107,
            "end": 192,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_27@2",
            "content": "One conceptual definition, denoted here ingenuine (e.g., Wang and Culotta, 2020;Rogers, 2021) is a feature correlated with some output label for no apparent reason.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_27",
            "start": 194,
            "end": 357,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_27@3",
            "content": "Such features often result from the annotation process (referred to as annotation artifacts; Gururangan et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_27",
            "start": 359,
            "end": 476,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_27@4",
            "content": "For instance, Gururangan et al. ( 2018) have shown that the words \"cat\" and \"sleeping\" are correlated with contradictions in the SNLI dataset (Bowman et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_27",
            "start": 478,
            "end": 641,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_27@5",
            "content": "This definition is appealing: we want our models to learn real information about the world, and not properties of a given dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_27",
            "start": 643,
            "end": 772,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_27@6",
            "content": "However, it is also somewhat subjective, and could include features that might be referred to as genuine, such as the word \"not\" indicating NLI contradictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_27",
            "start": 774,
            "end": 932,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_27@7",
            "content": "Further, genuine features, i.e., those representing a real phenomenon in the world (e.g., \"amazing\" as a feature for positive sentiment), are also likely to lead models make to erroneous predictions in some contexts (e.g., negation or sarcasm; .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_27",
            "start": 934,
            "end": 1178,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_27@8",
            "content": "Such features could thus harm generalization, so some might consider them spurious as well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_27",
            "start": 1180,
            "end": 1270,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_28@0",
            "content": "In an alternative definition, denoted ungeneralizable, a spurious feature is one that works well for specific examples but does not hold in general (Chang et al., 2021;Yaghoobzadeh et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_28",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_28@1",
            "content": "This definition does not address the nature of the feature (genuine or 2021) relaxed the last constraint, and assumed that every simple correlation between single word features and output labels is spurious (henceforth every-word).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_28",
            "start": 196,
            "end": 426,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_28@2",
            "content": "They then defined a class of competent datasets, where the marginal probability for every feature is uniform over the class label, i.e., \u2200x \u2208 X , y \u2208 Y, p(y|x) = 1 |Y | , thus limiting models from picking up any correlation between single features and output labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_28",
            "start": 428,
            "end": 693,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_29@0",
            "content": "We next extend the every-word approach beyond single words, showing that models that can exploit single word features can also exploit some feature interactions, and therefore these should also be considered spurious.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_29",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_29@1",
            "content": "Tab. 2 summarizes the different definitions of spurious correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_29",
            "start": 218,
            "end": 286,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_30@0",
            "content": "Balancing too Little Leaves some Spurious Features",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_30",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_31@0",
            "content": "Gardner et al. ( 2021) assumed that as each word can appear in certain contexts that change its semantic meaning (e.g., negation, sarcasm), each word is potentially spurious.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_31",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_31@1",
            "content": "Here we note that the same argument can be applied to feature interactions, such as word n-grams.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_31",
            "start": 175,
            "end": 271,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_31@2",
            "content": "We start with a toy example to illustrate our argument for bigrams, and then extend it for larger values of n.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_31",
            "start": 273,
            "end": 382,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_32@0",
            "content": "Consider the toy dataset for the task of sentiment analysis shown in Tab. 3, with vocabulary V ={good,bad,not,very}, and label set Y ={+,\u2212 }.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_32",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_32@1",
            "content": "The Train split is balanced with respect to singleword features, i.e., \u2200w \u2208 V, y \u2208 Y : p(y|w) = 1 |Y | (a balanced or competent dataset).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_32",
            "start": 142,
            "end": 278,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_32@2",
            "content": "Assume the semantics of this dataset is that of English, while '+' means positive sentiment and '\u2212' means negative.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_32",
            "start": 280,
            "end": 394,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_33@0",
            "content": "A model trained on Train can achieve perfect training accuracy by learning the correct semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_33",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_33@1",
            "content": "However, achieving perfect training accuracy can also be done by learning correlations between twoword features and the target label (i.e., memorizing all the training examples).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_33",
            "start": 98,
            "end": 275,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_33@2",
            "content": "In this case, the model would make the wrong prediction for the first test example in Test (as it has learned that very good is a feature that indicates positive sentiment), and similarly, will make a random prediction for the second test example, which does not contain any two-word features seen during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_33",
            "start": 277,
            "end": 590,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_34@0",
            "content": "This example highlights that balancing singleword features does not guarantee resiliency to spurious correlations, and therefore in order to mitigate all spurious correlations, balancing pairs of features is also required.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_34",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_34@1",
            "content": "One can construct similar examples for larger values of n, by similarly considering multi-word expressions and common co-occurrences (e.g., \"jaw dropping\", \"worst day ever\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_34",
            "start": 223,
            "end": 396,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_34@2",
            "content": "These could serve as spurious correlations in the same way single words do.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_34",
            "start": 398,
            "end": 472,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_35@0",
            "content": "Another example is sarcasm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_35",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_35@1",
            "content": "A model that fails to understand sarcastic contexts will misinterpret statements that appear in such contexts, even if it perfectly understands the base meaning of these statements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_35",
            "start": 28,
            "end": 208,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_35@2",
            "content": "Thus, the entire reasoning process of such a model, whether relying on simple features, feature interactions, or other types of understanding, will result in mispredictions of certain inputs, and thus can be considered spurious.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_35",
            "start": 210,
            "end": 437,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_36@0",
            "content": "As a result, to truly mitigate all spurious correlations in a dataset, balancing feature combinations is required as well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_36",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_36@1",
            "content": "Accordingly, balancing too little will leave some spurious correlations in the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_36",
            "start": 123,
            "end": 209,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_37@0",
            "content": "Too much Balancing Prevents Learning Valuable Semantic Knowledge",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_37",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_38@0",
            "content": "We observed that balancing too little does not allow models to fully eliminate spurious correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_38",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_39@0",
            "content": "Here we show that too much balancing can prevent models from learning valuable knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_39",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_39@1",
            "content": "Consider the training data for learning the XOR function presented in Tab.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_39",
            "start": 90,
            "end": 163,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_39@2",
            "content": "4 (left).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_39",
            "start": 165,
            "end": 173,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_39@3",
            "content": "This dataset contains enough learnable signal when considering feature interactions despite being balanced for single words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_39",
            "start": 175,
            "end": 298,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_39@4",
            "content": "Nonetheless, balancing this dataset for pairs of features would result in no information, and thus prevent any model from learning this function (Tab. 4,right).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_39",
            "start": 300,
            "end": 459,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_40@0",
            "content": "Original Train Set Augmented Samples Input Label Input Label D. Define n to be the length of the longest document in D. By definition, balancing every combination of up to n features (including) leaves no learnable signal in D. 3 We conclude that balancing too much can prevent models from learning semantic knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_40",
            "start": 0,
            "end": 317,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_41@0",
            "content": "0 0 0 *0 0 1 0 1 1 *0 1 0 1 0 1 *1 0 0 1 1 0 *1 1 1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_41",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_42@0",
            "content": "Combining the two observations, we are left with the question of the potential intersection between balancing too much and balancing too little: does a sweet spot exist for which no spurious correlations are found in the dataset, but enough learnable signal is left? And even if so, would a balancing algorithm (whether by augmentation or filtering) be able to find it? We leave these questions for future work, but note that addressing them is a prerequisite for the theoretical and practical application of dataset balancing for mitigating spurious correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_42",
            "start": 0,
            "end": 563,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_43@0",
            "content": "Dataset Balancing is Undesired",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_43",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_44@0",
            "content": "Even if a sweet-spot exists between balancing too little and too much, do we really want to find it? Here we argue that perhaps not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_44",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_44@1",
            "content": "The practice of dataset balancing is designed to prevent models from learning that some words or expressions have a common fallback meaning that can stem from dataset idiosyncrasies (e.g., \"cat\" as an indicator of contradiction) but also from cultural and historical contexts (e.g., Biden is the U.S. president).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_44",
            "start": 133,
            "end": 444,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_44@2",
            "content": "Fallback meanings are crucial for understanding language, as contexts are often underspecified (Graesser, 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_44",
            "start": 446,
            "end": 557,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_44@3",
            "content": "Indeed, relying on fallback meanings might make models fail to process some inputs correctly, and might not generalize to other domains where the fallback meaning is different.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_44",
            "start": 559,
            "end": 734,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_44@4",
            "content": "Here we argue that the ability to use them is a central ability of language understanding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_44",
            "start": 736,
            "end": 825,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_45@0",
            "content": "For example, substantial efforts are made to teach models world knowledge, such as that the president of the U.S. is Joe Biden, the capital of Brazil is Bras\u00edlia, and France is the soccer world champion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_45",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_45@1",
            "content": "These efforts include building world knowledge datasets (Wang et al., 2021), developing methods for enhancing models with this information (Zhang et al., 2019;Peters et al., 2019), and evaluating how well models capture it (Rubinstein et al., 2015;. But many of these world-knowledge facts are context dependent: the capital of the Brazil has changed in 1960, the president of the U.S., as well as soccer world champions potentially change every 4 years, etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_45",
            "start": 204,
            "end": 662,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_46@0",
            "content": "Another example is common sense knowledge, such as \"people are happy when they receive a gift\", \"an elephant is taller than a zebra\", and \"a statue that doesn't fit into a suitcase is too large\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_46",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_46@1",
            "content": "A large body of work has been carried out to create benchmarks that measure the common sense abilities of models (Liu and Singh, 2004;Levesque et al., 2012;Zellers et al., 2018;Bisk et al., 2020), as well as augmenting models with such abilities (Qin et al., 2020;Bosselut et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_46",
            "start": 196,
            "end": 482,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_47@0",
            "content": "Common sense reasoning is, by definition, stochastic and reliant on understanding presupposed, underspecified context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_47",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_47@1",
            "content": "One could imagine a person unhappy to receive a gift (e.g., because it is not what they wanted), a fantastically large zebra compared to a tiny elephant, and a suitcase with multiple compartments which prevent a small statue from fitting in it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_47",
            "start": 119,
            "end": 362,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_48@0",
            "content": "These examples illustrate that a model that learns these correlations and relies exclusively on them to make predictions is limited and is bound to make mistakes in some contexts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_48",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_48@1",
            "content": "One way to avoid these mistakes is to balance these correlations out, and prevent models from knowing these assertions to begin with.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_48",
            "start": 180,
            "end": 312,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_48@2",
            "content": "We argue that this solution is not a desired solution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_48",
            "start": 314,
            "end": 367,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_48@3",
            "content": "In essence, an interpreter's task (be it human or machine) is to infer the most probable context in which a statement is made, and as a result, it should have a fallback option for such world knowledge and common sense assertions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_48",
            "start": 369,
            "end": 598,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_49@0",
            "content": "We recognize that a balanced dataset may not be balanced with respect to the appearance of common-sense or world-knowledge assertions in a given context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_49",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_49@1",
            "content": "E.g., a model might balance-out the general fact that Joe Biden is the U.S. president, but not that he is the president in 2022.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_49",
            "start": 154,
            "end": 281,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_49@2",
            "content": "As in many cases much of the context is unobserved (Graesser, 2013), the question is whether we want models to make a prediction in cases of uncertainty based on the fallback option.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_49",
            "start": 283,
            "end": 464,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_49@3",
            "content": "We argue that doing so",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_49",
            "start": 466,
            "end": 487,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_50@0",
            "content": "Richer contexts ( \u00a74.1) A closed label set Abstain/interact ( \u00a74.2) Large-scale fine-tuning Few-shot learning ( \u00a74.3) We want to stress that balancing methods can result in mitigating some of the spurious correlations, and therefore lead to increased generalization .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_50",
            "start": 0,
            "end": 266,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_50@1",
            "content": "Moreover, the process of filtering the data naturally results in smaller datasets, which leads to lower training costs .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_50",
            "start": 268,
            "end": 387,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_50@2",
            "content": "While such contribution is meaningful in terms of environmental concerns (Strubell et al., 2019;Schwartz et al., 2020), it is orthogonal to our research question.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_50",
            "start": 389,
            "end": 550,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_50@3",
            "content": "Overall, despite the important contributions of balancing techniques, this paper shows that even the perfect balancing method might not mitigate all spurious correlations in a satisfying way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_50",
            "start": 552,
            "end": 742,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_51@0",
            "content": "So how can we make models more resilient to spurious correlations without balancing the data?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_51",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_51@1",
            "content": "Below we discuss several ideas for doing this.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_51",
            "start": 94,
            "end": 139,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_52@0",
            "content": "Ways to Move Forward",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_52",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_53@0",
            "content": "So far, we presented limitations of dataset balancing as a means to mitigate spurious correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_53",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_53@1",
            "content": "In this section we discuss several alternatives to this practice, summarized in Tab.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_53",
            "start": 100,
            "end": 183,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_53@2",
            "content": "5. We note that none of these proposals is particularly novel.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_53",
            "start": 185,
            "end": 246,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_53@3",
            "content": "Rather, we intend to survey alternatives proposed in literature and argue that these may be promising for addressing the drawbacks of spurious correlations, and that more efforts should be put into studying them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_53",
            "start": 248,
            "end": 459,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_54@0",
            "content": "Augmenting Datasets with Rich Contexts",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_54",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_55@0",
            "content": "The implicit assumption of dataset balancing is that in order to mitigate spurious correlations the model has to unlearn them, that is, they should be removed altogether from the training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_55",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_55@1",
            "content": "We argue that instead we should be focusing on learning and modeling richer contexts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_55",
            "start": 193,
            "end": 277,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_56@0",
            "content": "As an example, consider negation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_56",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_56@1",
            "content": "A model that generalizes well, should learn the meaning of words such as not, and should be able to negate new words, even those that were seen only in positive contexts at training time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_56",
            "start": 34,
            "end": 220,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_56@2",
            "content": "For example, if a model only sees during training words like \"amazing\" or \"happy\" with positive sentiment, and thus learns that these words bear positive meaning, we would still expect it to interpret their negated appearance (e.g., not amazing) as an indicator of negative sentiment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_56",
            "start": 222,
            "end": 505,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_56@3",
            "content": "Such generalization is crucial for language learning, and should ideally allow models to not rely exclusively on spurious correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_56",
            "start": 507,
            "end": 641,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_56@4",
            "content": "Despite the immense progress in the field in the past decade, negation still poses a challenge to modern NLP tools (Hossain et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_56",
            "start": 643,
            "end": 780,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_56@5",
            "content": "4 We suggest taking into account different types of contexts during dataset design.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_56",
            "start": 782,
            "end": 864,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_56@6",
            "content": "In particular, collecting training examples with contexts such as negation (Morante and Blanco, 2012), humor (Weller and Seppi, 2019;Annamoradnejad and Zoghi, 2020), sarcasm (Davidov et al., 2010;Oprea and Magdy, 2020), or metaphors (Tsvetkov et al., 2014;Mohammad et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_56",
            "start": 866,
            "end": 1144,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_56@7",
            "content": "This recommendation applies to both supervised tasks, and perhaps more so to pretrained data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_56",
            "start": 1146,
            "end": 1238,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_56@8",
            "content": "We suggest adding documents with such contexts throughout the pretraining corpus, or as a continued pretraining step to existing large-scale models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_56",
            "start": 1240,
            "end": 1387,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_56@9",
            "content": "5 To incorporate contexts from a wide range of phenomena, we can leverage the vast literature on broad-coverage semantics (Baker et al., 1998;Steedman and Baldridge, 2007;Banarescu et al., 2013;Abend and Rappoport, 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_56",
            "start": 1389,
            "end": 1609,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_56@10",
            "content": "6 This line of work proposes theories of language, composing inventories of linguistic constructions with an algebraic formulation of their inter-relations in terms of truth value, factuality, and more.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_56",
            "start": 1611,
            "end": 1812,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_56@11",
            "content": "These inventories often include the phenomena discussed above, such as negation, sarcasm, and presupposition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_56",
            "start": 1814,
            "end": 1922,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_57@0",
            "content": "Interaction and Abstention to Cope with Underspecified Contexts",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_57",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_58@0",
            "content": "Most NLP tasks are designed with a closed label set that forces models to make a concrete prediction for each test instance, without an option to abstain or interact with the user to get more information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_58",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_59@0",
            "content": "Even for tasks with a large label set (e.g., language modeling), models still have to output a valid vocabulary item.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_59",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_59@1",
            "content": "Here we argue that this practice Figure 3: An example of abstention/interaction in cases of uncertainty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_59",
            "start": 118,
            "end": 221,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_59@2",
            "content": "For the task of sentiment analysis, models currently assign a label to each input, even for ambiguous or underspecified ones (top).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_59",
            "start": 223,
            "end": 353,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_59@3",
            "content": "This may lead the model to over-rely on spurious correlations (marked in red, bottom left).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_59",
            "start": 355,
            "end": 445,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_59@4",
            "content": "Models that abstain or interact (bottom right) might learn to rely less on such correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_59",
            "start": 447,
            "end": 539,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_60@0",
            "content": "creates an inductive bias towards using spurious correlations in cases of uncertainty, as the model has \"nothing to lose\" in case of low uncertainty, and is encouraged to always make some prediction, potentially relying on spurious correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_60",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_60@1",
            "content": "7 To further illustrate this point, consider the ambiguous sentence \"To my great surprise, the movie turned out different than what I thought.\", in the context of sentiment analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_60",
            "start": 246,
            "end": 427,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_60@2",
            "content": "The reader cannot infer whether the writer is pleasantly surprised (a positive review) or disappointed (a negative review).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_60",
            "start": 429,
            "end": 551,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_60@3",
            "content": "We argue that in such cases models might lean towards a positive sentiment based on the words \"great\" and \"surprise\", which are typically correlated with a positive sentiment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_60",
            "start": 553,
            "end": 727,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_61@0",
            "content": "To test this, we ran a RoBERTa-large model fine-tuned on SST-2 (Socher et al., 2013) on that example.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_61",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_61@1",
            "content": "8 As expected, the model returns a positive label, with 99.99% confidence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_61",
            "start": 102,
            "end": 175,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_61@2",
            "content": "Interestingly, three different interpretation methods (simple gradient visualization, Simonyan et al., 2014;integrated gradient visualization, Sundararajan et al., 2017;and SmoothGrad, Smilkov et al., 2017) all find the word \"great\" to be one of the two most influential words on the model's prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_61",
            "start": 177,
            "end": 479,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_61@3",
            "content": "While this example does not prove the prevalence of this problem, it does demonstrate its existence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_61",
            "start": 481,
            "end": 580,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_62@0",
            "content": "To address this problem, we suggest adopting ap- 7 We recognize that in some cases we do want the model to make a prediction under cases of uncertainty (see Sec. 3.4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_62",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_62@1",
            "content": "The ability to detect when is it reasonable and when it is not to make an educated guess is an important property of an intelligent agent, and an exciting research question.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_62",
            "start": 168,
            "end": 340,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_62@2",
            "content": "8 We used the AllenNLP demo (https://demo.allennlp.org/sentiment-analysis/).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_62",
            "start": 342,
            "end": 417,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_62@3",
            "content": "proaches that allow models to abstain and interact when they cannot make a decision with high confidence (Chow, 1957;Hellman, 1970;Laidlaw and Feizi, 2019;Balcan et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_62",
            "start": 419,
            "end": 594,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_62@4",
            "content": "See Fig. 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_62",
            "start": 596,
            "end": 606,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_62@5",
            "content": "This can be achieved by building datasets with unanswerable questions (Ray et al., 2016;Rajpurkar et al., 2018;Sulem et al., 2021), but also by designing models that abstain in cases of low certainty for all inputs, even those with an unambiguous gold label.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_62",
            "start": 608,
            "end": 865,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_62@6",
            "content": "9 We hypothesize that encouraging the model to provide this output when it is unsure, rather than making a semi-educated guess, potentially based on spurious correlations, could reduce its dependency on such correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_62",
            "start": 867,
            "end": 1087,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_63@0",
            "content": "The End of Large-Scale Fine-Tuning?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_63",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_64@0",
            "content": "This paper has demonstrated the limitations of mitigating spurious correlations via dataset balancing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_64",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_64@1",
            "content": "A naive way to eliminate spurious correlations is to stop using large-scale datasets altogether.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_64",
            "start": 103,
            "end": 198,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_64@2",
            "content": "We argue that for supervised learning (i.e., large-scale fine-tuning), recent advances in zero-and few-shot learning might make this option possible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_64",
            "start": 200,
            "end": 348,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_65@0",
            "content": "Large pretrained models such as T5 or GPT-3 (Brown et al., 2020), trained on vast amounts of data, arguably learn enough about the world to acquire many of the skills currently learned through supervised datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_65",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_65@1",
            "content": "Indeed, the large increase in the size and capacity of pretrained language models has led to a new wave of few-shot and zero-shot methods Shin et al., 2020;Gu et al., 2021), which are able to reach human-level performance on certain tasks using only a few dozens of training examples .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_65",
            "start": 214,
            "end": 498,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_65@2",
            "content": "Given these impressive results, it is not clear whether there is still value in finetuning models on large-scale datasets for all tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_65",
            "start": 500,
            "end": 635,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_65@3",
            "content": "In the context of this work, focusing on few-shot learning might allow models to not learn some of the correlations that result from manual annotation (Schwartz et al., 2017;Gururangan et al., 2018;Poliak et al., 2018), as they will not be exposed to many of them to begin with.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_65",
            "start": 637,
            "end": 914,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_66@0",
            "content": "We note that this proposal is not a perfect solution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_66",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_66@1",
            "content": "First, some spurious correlations may be picked up by the small number of examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_66",
            "start": 54,
            "end": 136,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_66@2",
            "content": "This is less of a problem in the zero-shot setting, or in cases where the model parameters are not updated in few-shot settings (Brown et al., 2020), but studying the extent to which spurious correlations are picked up in other few-shot settings is an important avenue for future research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_66",
            "start": 138,
            "end": 426,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_66@3",
            "content": "Second, some spurious correlations might be picked up during the pretraining stage (Gehman et al., 2020;Birhane et al., 2021;. Continuing to quantify this phenomenon and finding ways to mitigate it is another important line of research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_66",
            "start": 428,
            "end": 663,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_67@0",
            "content": "An important question in this context is the tasks for which supervised learning is still needed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_67",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_67@1",
            "content": "It seems possible the excelling in language modeling tasks requires mastering the skills that stand in the base of many NLP tasks, such as sentiment analysis, syntactic parsing, and NER.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_67",
            "start": 98,
            "end": 283,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_67@2",
            "content": "However, it is similarly possible that this is not the case for other tasks, e.g., summarization, simplification and dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_67",
            "start": 285,
            "end": 410,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_67@3",
            "content": "We are cautious to make concrete recommendations for which tasks to apply this principle, but suggest the following intuitive rule of thumb: for datasets or tasks for which the state of the art is close to or surpasses the human baseline, we should consider moving to few-shot setups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_67",
            "start": 412,
            "end": 695,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_68@0",
            "content": "Finally, dataset creation is still a valuable and important line of research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_68",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_68@1",
            "content": "Our recommendation to stop building large scale training sets does not make this task redundant, to both spur the design of better models, and to better test their capabilities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_68",
            "start": 78,
            "end": 254,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_68@2",
            "content": "We suggest instead of building large training sets and small validation and tests, authors should consider building large test sets, as a means for achieving improved statistical power (Card et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_68",
            "start": 256,
            "end": 460,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_69@0",
            "content": "A Note on Social-Bias Correlations",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_69",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_70@0",
            "content": "So far, we discussed the problems with unlearning spurious correlations, and advocated instead for more elaborate context modeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_70",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_70@1",
            "content": "One exception might be the case of social biases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_70",
            "start": 132,
            "end": 180,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_70@2",
            "content": "Textual data often reflects human stereotypes such as spurious correlations between labels and protected group attributes, e.g., alignments between professions and gender or race.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_70",
            "start": 182,
            "end": 360,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_70@3",
            "content": "Unlike other types of knowledge discussed in Sec. 3.4, in this case there is an incentive to prevent models from learning this type of correlation as means for actively reducing the harms of such biases, especially in commercial and public-facing applications, such as machine translation or automated financial decision-making (Bartlett et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_70",
            "start": 362,
            "end": 713,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_70@4",
            "content": "As a result, methods for dataset balancing are no longer undesired for mitigating such spurious correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_70",
            "start": 715,
            "end": 823,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_71@0",
            "content": "Nonetheless, as demonstrated in Sec. 3, methods for dataset balancing are a limited solution for mitigating spurious correlations, including social-bias ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_71",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_71@1",
            "content": "In contrast, the methods proposed in this section for mitigating spurious correlations might also assist in mitigating social biases, or at least slow down their amplification (Zhao et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_71",
            "start": 159,
            "end": 354,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_72@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_72",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_73@0",
            "content": "This paper discussed the arms-race between models and datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_73",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_73@1",
            "content": "Previous works criticized one side of this arms race-the increasing size of pretrained models-due to ethical and environmental concerns (Schwartz et al., 2020;Bender et al., 2021), or questioning its ability to learn meaningful abstractions from raw text (Bender and Koller, 2020;. This work studies the second part of this arms race, regarding the efforts to mitigate spurious correlations through dataset balancing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_73",
            "start": 64,
            "end": 480,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_73@2",
            "content": "The release of such datasets is often motivated by their potential to spur progress in modeling, and to help tease apart qualitative differences between models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_73",
            "start": 482,
            "end": 641,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_73@3",
            "content": "showed that this is not necessarily the case, by observing that the ranking of reading comprehension models on small and synthetic benchmarks is similar to that of the SQuAD dataset (Rajpurkar et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_73",
            "start": 643,
            "end": 849,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_74@0",
            "content": "Finally, Raji et al. ( 2021) recently criticized the concept of benchmarks as a whole, arguing that they are only capturing specific skills and not \"general\" capabilities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_74",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_74@1",
            "content": "Our paper raises relates concerns about training sets implicitly containing spurious correlations, and suggests reconsidering the practice of building large-scale training sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_74",
            "start": 172,
            "end": 348,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_75@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_75",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_76@0",
            "content": "Spurious correlations in large textual corpora can result in model brittleness, lack of generalization, and an inflated sense of the state of the art.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_76",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_76@1",
            "content": "Mitigating their negative side-effects is an important research goal of the NLP community.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_76",
            "start": 151,
            "end": 240,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_76@2",
            "content": "In this paper we presented practical and conceptual limitations of dataset balancing as a means for doing so.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_76",
            "start": 242,
            "end": 350,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_76@3",
            "content": "We proposed alternative ways for mitigating spurious correlations, including adding richer contexts to textual corpora, and allowing models to abstain or interact in cases of uncertainty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_76",
            "start": 352,
            "end": 538,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_76@4",
            "content": "We concluded by suggesting to reconsider the practice of fine-tuning pretrained models on large-scale training sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_76",
            "start": 540,
            "end": 655,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_77@0",
            "content": "Broader Impact and Ethical Consideration",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_77",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_78@0",
            "content": "Our work did not involve any new data or annotation collection, and as such did not require crowdsourced or in-house workers, or introduces any new models and related risks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_78",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_78@1",
            "content": "Instead, we examine existing resources and common data balancing approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_78",
            "start": 174,
            "end": 249,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_78@2",
            "content": "In Section 4.4 we specifically discuss the relation between these practices and implications on social bias in models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_78",
            "start": 251,
            "end": 368,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_79@0",
            "content": "Omri Abend, Ari Rappoport, Universal Conceptual Cognitive Annotation (UCCA), 2013, Proc. of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_79",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_80@0",
            "content": "Omri Abend, Ari Rappoport, The state of the art in semantic representation, 2017, Proc. of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_80",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_81@0",
            "content": "UNKNOWN, None, 2020, Col-BERT: Using bert sentence embedding for humor detection, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_81",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_82@0",
            "content": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Zitnick, Devi Parikh, VQA: visual question answering, 2015, Proc. of ICCV, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_82",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_83@0",
            "content": "Collin Baker, Charles Fillmore, John Lowe, The Berkeley FrameNet project, 1998, Proc. of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_83",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_84@0",
            "content": "UNKNOWN, None, 2020, On the power of abstention and data-driven decision making for adversarial robustness, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_84",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_85@0",
            "content": "Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, Nathan Schneider, Abstract Meaning Representation for sembanking, 2013, Proc. of LAW VII & ID, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_85",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_86@0",
            "content": "Robert Bartlett, Adair Morse, Richard Stanton, Nancy Wallace, Consumer-lending discrimination in the fintech era, 2021, Journal of Financial Economics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_86",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_87@0",
            "content": "Emily Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, On the dangers of stochastic parrots: Can language models be too big?, 2021, Proc. of FAccT, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_87",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_88@0",
            "content": "Emily Bender, Alexander Koller, Climbing towards NLU: On meaning, form, and understanding in the age of data, 2020, Proc. of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_88",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_89@0",
            "content": "UNKNOWN, None, 2021, Multimodal datasets: misogyny, pornography, and malignant stereotypes, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_89",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_90@0",
            "content": "Yonatan Bisk, Rowan Zellers, Jianfeng Ronan Le Bras, Yejin Gao,  Choi, PIQA: reasoning about physical commonsense in natural language, 2020, Proc. of AAAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_90",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_91@0",
            "content": "Antoine Bosselut, Yejin Ronan Le Bras,  Choi, Dynamic neuro-symbolic knowledge graph construction for zero-shot commonsense question answering, 2021, Proc. of AAAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_91",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_92@0",
            "content": "UNKNOWN, None, 2021, When combating hype, proceed with caution, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_92",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_93@0",
            "content": "R Samuel, Gabor Bowman, Christopher Angeli, Christopher Potts,  Manning, A large annotated corpus for learning natural language inference, 2015, Proc. of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_93",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_94@0",
            "content": "UNKNOWN, None, , Proc. of NeurIPS, Sam Mc-Candlish.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_94",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_95@0",
            "content": "Dallas Card, Peter Henderson, Urvashi Khandelwal, Robin Jia, Kyle Mahowald, Dan Jurafsky, With little power comes great responsibility, 2020, Proc. of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_95",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_96@0",
            "content": "Dallas Card, Noah Smith, The importance of calibration for estimating proportions from annotations, 2018, Proc. of NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_96",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_97@0",
            "content": "Kai-Wei Chang, He He, Robin Jia, Sameer Singh, Robustness and adversarial examples in natural language processing, 2021, Proc. of EMNLP: Tutorial Abstracts, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_97",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_98@0",
            "content": "Kunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xiaochuan, Yuyu Zhang, Le Song, Taifeng Wang, Yuan Qi, Wei Chu, Question directed graph attention network for numerical reasoning over text, 2020, Proc. of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_98",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_99@0",
            "content": "C Chow, An optimum character recognition system using decision functions, 1957, IRE Trans. Electron. Comput, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_99",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_100@0",
            "content": "Corinna Cortes, Giulia Desalvo, Mehryar Mohri, Boosting with abstention, 2016, Proc. of NeurIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_100",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_101@0",
            "content": "Dmitry Davidov, Oren Tsur, Ari Rappoport, Semi-supervised recognition of sarcasm in Twitter and Amazon, 2010, Proc. of CoNLL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_101",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_102@0",
            "content": "H Morris, Stephen Degroot,  Fienberg, The comparison and evaluation of forecasters, 1983, Journal of the Royal Statistical Society: Series D (The Statistician), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_102",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_103@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proc. of NAACL-HLT, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_103",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_104@0",
            "content": "Jesse Dodge, Maarten Sap, Ana Marasovi\u0107, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner, Documenting large webtext corpora: A case study on the colossal clean crawled corpus, 2021, Proc. of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_104",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_105@0",
            "content": "Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs, 2019, Proc. of NAACL-HLT, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_105",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_106@0",
            "content": "Yanai Elazar, Hongming Zhang, Yoav Goldberg, Dan Roth, Back to square one: Artifact detection, training and commonsense disentanglement in the Winograd schema, 2021, Proc. of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_106",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_107@0",
            "content": "Matt Gardner, William Merrill, Jesse Dodge, Matthew Peters, Alexis Ross, Sameer Singh, Noah Smith, Competency problems: On finding and removing artifacts in language data, 2021, Proc. of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_107",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_108@0",
            "content": "Suchin Samuel Gehman, Maarten Gururangan, Yejin Sap, Noah Choi,  Smith, RealToxic-ityPrompts: Evaluating neural toxic degeneration in language models, 2020, Findings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_108",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_109@0",
            "content": "Max Glockner, Vered Shwartz, Yoav Goldberg, Breaking NLI systems with sentences that require simple lexical inferences, 2018, Proc. of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_109",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_110@0",
            "content": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Making the V in VQA matter: Elevating the role of image understanding in visual question answering, 2017, Proc. of CVPR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_110",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_111@0",
            "content": "UNKNOWN, None, 2013, Prose comprehension beyond the word, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_111",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_112@0",
            "content": "P Herbert,  Grice, Logic and conversation, 1975, Speech acts, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_112",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_113@0",
            "content": "UNKNOWN, None, 1989, Studies in the Way of Words, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_113",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_114@0",
            "content": "UNKNOWN, None, 2021, Ppt: Pre-trained prompt tuning for few-shot learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_114",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_115@0",
            "content": "Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Weinberger, On calibration of modern neural networks, 2017, Proc. of ICML, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_115",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_116@0",
            "content": "Swabha Suchin Gururangan, Omer Swayamdipta, Roy Levy, Samuel Schwartz, Noah Bowman,  Smith, Annotation artifacts in natural language inference data, 2018, Proc. of NAACL-HLT, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_116",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_117@0",
            "content": "UNKNOWN, None, 2020, DeBERTa: Decodingenhanced bert with disentangled attention, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_117",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_118@0",
            "content": "Martin Hellman, The nearest neighbor classification rule with a reject option, 1970, IEEE Trans. Syst. Sci. Cybern, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_118",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_119@0",
            "content": "Antonios Md Mosharaf Hossain, Eduardo Anastasopoulos, Alexis Blanco,  Palmer, It's not a non-issue: Negation as a source of error in machine translation, 2020, Findings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_119",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_120@0",
            "content": "A Drew, Christopher Hudson,  Manning, GQA: A new dataset for real-world visual reasoning and compositional question answering, 2019, Proc. of CVPR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_120",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_121@0",
            "content": "Amita Kamath, Robin Jia, Percy Liang, Selective question answering under domain shift, 2020, Proc. of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_121",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_122@0",
            "content": "UNKNOWN, None, 2019, Playing it safe: Adversarial robustness with an abstain option, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_122",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_123@0",
            "content": "Swabha Ronan Le Bras, Chandra Swayamdipta, Rowan Bhagavatula, Matthew Zellers, Ashish Peters, Yejin Sabharwal,  Choi, Adversarial filters of dataset biases, 2020, Proc. of ICML, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_123",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_124@0",
            "content": "UNKNOWN, None, , Chris Callison-Burch, and Nicholas Carlini. 2021. Deduplicating training data makes language models better, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_124",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_125@0",
            "content": "Hector Levesque, Ernest Davis, Leora Morgenstern, The winograd schema challenge, 2012, Proc. of KR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_125",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_126@0",
            "content": "H Liu, P Singh, Conceptnet: A practical commonsense reasoning toolkit, 2004, BT Technology Journal, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_126",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_127@0",
            "content": "UNKNOWN, None, 2021, Can small and synthetic benchmarks drive modeling innovation? a retrospective study of question answering modeling approaches, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_127",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_128@0",
            "content": "UNKNOWN, None, 2019, RoBERTa: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_128",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_129@0",
            "content": "UNKNOWN, None, 2021, Provable limitations of acquiring meaning from ungrounded form:what will future language models understand, TACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_129",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_130@0",
            "content": "Saif Mohammad, Ekaterina Shutova, Peter Turney, Metaphor as a medium for emotion: An empirical study, 2016, Proc. of *SEM, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_130",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_131@0",
            "content": "Roser Morante, Eduardo Blanco, *SEM 2012 shared task: Resolving the scope and focus of negation, 2012, Proc. of *SEM, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_131",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_132@0",
            "content": "Silviu Oprea, Walid Magdy, 2020. iSarcasm: A dataset of intended sarcasm, , Proc. of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_132",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_133@0",
            "content": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, Deep contextualized word representations, 2018, Proc. of NAACL-HLT, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_133",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_134@0",
            "content": "Matthew Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, Noah Smith, Knowledge enhanced contextual word representations, 2019, Proc. of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_134",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_135@0",
            "content": "Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, Benjamin Van Durme, Hypothesis only baselines in natural language inference, 2018, Proc. of *SEM, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_135",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_136@0",
            "content": "Lianhui Qin, Vered Shwartz, Peter West, Chandra Bhagavatula, Jena Hwang, Ronan Bras, Antoine Bosselut, Yejin Choi, Back to the future: Unsupervised backprop-based decoding for counterfactual and abductive commonsense reasoning, 2020, Proc. of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_136",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_137@0",
            "content": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Language models are unsupervised multitask learners, 2019, OpenAI blog, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_137",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_138@0",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, JMLR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_138",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_139@0",
            "content": "UNKNOWN, None, , 2021. AI and the everything in the whole wide world benchmark, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_139",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_140@0",
            "content": "Pranav Rajpurkar, Robin Jia, Percy Liang, Know what you don't know: Unanswerable questions for SQuAD, 2018, Proc. of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_140",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_141@0",
            "content": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, SQuAD: 100,000+ questions for machine comprehension of text, 2016, Proc. of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_141",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_142@0",
            "content": "Arijit Ray, Gordon Christie, Mohit Bansal, Dhruv Batra, Devi Parikh, Question relevance in VQA: Identifying non-visual and false-premise questions, 2016, Proc. of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_142",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_143@0",
            "content": "Adam Roberts, Colin Raffel, Noam Shazeer, How much knowledge can you pack into the parameters of a language model, 2020, Proc. of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_143",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_144@0",
            "content": "Anna Rogers, Changing the world by changing the data, 2021, Proc. of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_144",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_145@0",
            "content": "Dana Rubinstein, Effi Levi, Roy Schwartz, Ari Rappoport, How well do distributional models capture different types of semantic knowledge?, 2015, Proc. of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_145",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_146@0",
            "content": "Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula,  Choi, WinoGrande: An adversarial winograd schema challenge at scale, 2020, Proc. of AAAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_146",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_147@0",
            "content": "Timo Schick, Hinrich Sch\u00fctze, It's not just size that matters: Small language models are also few-shot learners, 2021, Proc. of NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_147",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_148@0",
            "content": "UNKNOWN, None, 2021, True fewshot learning with prompts -a real-world perspective, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_148",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_149@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_149",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_150@0",
            "content": "Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila Zilles, Yejin Choi, Noah Smith, The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task, 2017, Proc. of CoNLL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_150",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_151@0",
            "content": "Taylor Shin, Yasaman Razeghi, Robert Logan, I , Eric Wallace, Sameer Singh, Auto-Prompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts, 2020, Proc. of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_151",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_152@0",
            "content": "UNKNOWN, None, 2019, A flexible and adaptive framework for abstention under class imbalance, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_152",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_153@0",
            "content": "UNKNOWN, None, 2014, Deep inside convolutional networks: Visualising image classification models and saliency maps, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_153",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_154@0",
            "content": "UNKNOWN, None, 2017, SmoothGrad: removing noise by adding noise, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_154",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_155@0",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, Proc. of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_155",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_156@0",
            "content": "Gabriel Stanovsky, Noah Smith, Luke Zettlemoyer, Evaluating gender bias in machine translation, 2019, Proc. of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_156",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_157@0",
            "content": "UNKNOWN, None, 2007, Combinatory categorial grammar, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_157",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_158@0",
            "content": "Emma Strubell, Ananya Ganesh, Andrew Mccallum, Energy and policy considerations for deep learning in NLP, 2019, Proc. of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_158",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_159@0",
            "content": "Elior Sulem, Jamaal Hay, Dan Roth, Do we know what we don't know? studying unanswerable questions beyond SQuAD 2.0, 2021, Findings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_159",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_160@0",
            "content": "Mukund Sundararajan, Ankur Taly, Qiqi Yan, Axiomatic attribution for deep networks, 2017, Proc. of ICML, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_160",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_161@0",
            "content": "Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah Smith, Yejin Choi, Dataset cartography: Mapping and diagnosing datasets with training dynamics, 2020, Proc. of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_161",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_162@0",
            "content": "Niket Tandon, Keisuke Sakaguchi, Bhavana Dalvi, Dheeraj Rajagopal, Peter Clark, Michal Guerquin, Kyle Richardson, Eduard Hovy, A dataset for tracking entities in open domain procedural text, 2020, Proc. of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_162",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_163@0",
            "content": "Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman, Eric Nyberg, Chris Dyer, Metaphor detection with cross-lingual model transfer, 2014, Proc. of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_163",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_164@0",
            "content": "Luyu Wang, Yujia Li, WikiGraphs: A Wikipedia text -knowledge graph paired dataset, 2021, Proc. of TextGraphs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_164",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_165@0",
            "content": "Zhao Wang, Aron Culotta, Identifying spurious correlations for robust text classification, 2020, Findings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_165",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_166@0",
            "content": "Orion Weller, Kevin Seppi, Humor detection: A transformer gets the last laugh, 2019, Proc. of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_166",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_167@0",
            "content": "Yadollah Yaghoobzadeh, Soroush Mehri, Remi Tachet Des Combes, T Hazen, Alessandro Sordoni, Increasing robustness to spurious correlations using forgettable examples, 2021, Proc. of EACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_167",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_168@0",
            "content": "Rowan Zellers, Yonatan Bisk, Roy Schwartz, Yejin Choi, SWAG: A large-scale adversarial dataset for grounded commonsense inference, 2018, Proc. of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_168",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_169@0",
            "content": "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, HellaSwag: Can a machine really finish your sentence?, 2019, Proc. of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_169",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_170@0",
            "content": "Shujian Zhang, Chengyue Gong, Eunsol Choi, Knowing more about questions can help: Improving calibration in question answering, 2021, Findings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_170",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_171@0",
            "content": "Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu, ERNIE: Enhanced language representation with informative entities, 2019, Proc. of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_171",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "120-ARR_v1_172@0",
            "content": "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, Kai-Wei Chang, Men also like shopping: Reducing gender bias amplification using corpus-level constraints, 2017, Proc. of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "120-ARR_v1_172",
            "start": 0,
            "end": 184,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "120-ARR_v1_0",
            "tgt_ix": "120-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_0",
            "tgt_ix": "120-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_1",
            "tgt_ix": "120-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_1",
            "tgt_ix": "120-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_0",
            "tgt_ix": "120-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_2",
            "tgt_ix": "120-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_4",
            "tgt_ix": "120-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_3",
            "tgt_ix": "120-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_3",
            "tgt_ix": "120-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_3",
            "tgt_ix": "120-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_6",
            "tgt_ix": "120-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_7",
            "tgt_ix": "120-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_8",
            "tgt_ix": "120-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_3",
            "tgt_ix": "120-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_3",
            "tgt_ix": "120-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_3",
            "tgt_ix": "120-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_3",
            "tgt_ix": "120-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_5",
            "tgt_ix": "120-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_0",
            "tgt_ix": "120-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_9",
            "tgt_ix": "120-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_11",
            "tgt_ix": "120-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_12",
            "tgt_ix": "120-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_13",
            "tgt_ix": "120-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_14",
            "tgt_ix": "120-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_15",
            "tgt_ix": "120-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_16",
            "tgt_ix": "120-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_17",
            "tgt_ix": "120-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_18",
            "tgt_ix": "120-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_10",
            "tgt_ix": "120-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_10",
            "tgt_ix": "120-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_10",
            "tgt_ix": "120-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_10",
            "tgt_ix": "120-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_10",
            "tgt_ix": "120-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_10",
            "tgt_ix": "120-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_10",
            "tgt_ix": "120-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_10",
            "tgt_ix": "120-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_10",
            "tgt_ix": "120-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_10",
            "tgt_ix": "120-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_0",
            "tgt_ix": "120-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_19",
            "tgt_ix": "120-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_20",
            "tgt_ix": "120-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_20",
            "tgt_ix": "120-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_20",
            "tgt_ix": "120-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_21",
            "tgt_ix": "120-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_23",
            "tgt_ix": "120-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_24",
            "tgt_ix": "120-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_20",
            "tgt_ix": "120-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_20",
            "tgt_ix": "120-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_20",
            "tgt_ix": "120-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_22",
            "tgt_ix": "120-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_20",
            "tgt_ix": "120-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_25",
            "tgt_ix": "120-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_27",
            "tgt_ix": "120-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_28",
            "tgt_ix": "120-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_26",
            "tgt_ix": "120-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_26",
            "tgt_ix": "120-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_26",
            "tgt_ix": "120-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_26",
            "tgt_ix": "120-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_20",
            "tgt_ix": "120-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_29",
            "tgt_ix": "120-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_31",
            "tgt_ix": "120-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_32",
            "tgt_ix": "120-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_33",
            "tgt_ix": "120-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_34",
            "tgt_ix": "120-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_35",
            "tgt_ix": "120-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_30",
            "tgt_ix": "120-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_30",
            "tgt_ix": "120-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_30",
            "tgt_ix": "120-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_30",
            "tgt_ix": "120-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_30",
            "tgt_ix": "120-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_30",
            "tgt_ix": "120-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_30",
            "tgt_ix": "120-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_20",
            "tgt_ix": "120-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_36",
            "tgt_ix": "120-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_38",
            "tgt_ix": "120-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_37",
            "tgt_ix": "120-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_37",
            "tgt_ix": "120-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_37",
            "tgt_ix": "120-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_40",
            "tgt_ix": "120-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_41",
            "tgt_ix": "120-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_37",
            "tgt_ix": "120-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_37",
            "tgt_ix": "120-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_37",
            "tgt_ix": "120-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_39",
            "tgt_ix": "120-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_20",
            "tgt_ix": "120-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_42",
            "tgt_ix": "120-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_44",
            "tgt_ix": "120-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_45",
            "tgt_ix": "120-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_46",
            "tgt_ix": "120-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_47",
            "tgt_ix": "120-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_43",
            "tgt_ix": "120-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_43",
            "tgt_ix": "120-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_43",
            "tgt_ix": "120-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_43",
            "tgt_ix": "120-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_43",
            "tgt_ix": "120-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_43",
            "tgt_ix": "120-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_43",
            "tgt_ix": "120-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_48",
            "tgt_ix": "120-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_50",
            "tgt_ix": "120-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_43",
            "tgt_ix": "120-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_43",
            "tgt_ix": "120-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_49",
            "tgt_ix": "120-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_0",
            "tgt_ix": "120-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_51",
            "tgt_ix": "120-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_52",
            "tgt_ix": "120-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_52",
            "tgt_ix": "120-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_52",
            "tgt_ix": "120-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_53",
            "tgt_ix": "120-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_55",
            "tgt_ix": "120-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_54",
            "tgt_ix": "120-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_54",
            "tgt_ix": "120-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_54",
            "tgt_ix": "120-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_52",
            "tgt_ix": "120-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_56",
            "tgt_ix": "120-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_58",
            "tgt_ix": "120-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_59",
            "tgt_ix": "120-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_60",
            "tgt_ix": "120-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_61",
            "tgt_ix": "120-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_57",
            "tgt_ix": "120-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_57",
            "tgt_ix": "120-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_57",
            "tgt_ix": "120-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_57",
            "tgt_ix": "120-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_57",
            "tgt_ix": "120-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_57",
            "tgt_ix": "120-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_52",
            "tgt_ix": "120-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_62",
            "tgt_ix": "120-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_64",
            "tgt_ix": "120-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_65",
            "tgt_ix": "120-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_66",
            "tgt_ix": "120-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_67",
            "tgt_ix": "120-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_63",
            "tgt_ix": "120-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_63",
            "tgt_ix": "120-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_63",
            "tgt_ix": "120-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_63",
            "tgt_ix": "120-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_63",
            "tgt_ix": "120-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_63",
            "tgt_ix": "120-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_52",
            "tgt_ix": "120-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_68",
            "tgt_ix": "120-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_70",
            "tgt_ix": "120-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_69",
            "tgt_ix": "120-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_69",
            "tgt_ix": "120-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_69",
            "tgt_ix": "120-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_0",
            "tgt_ix": "120-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_71",
            "tgt_ix": "120-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_73",
            "tgt_ix": "120-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_72",
            "tgt_ix": "120-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_72",
            "tgt_ix": "120-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_72",
            "tgt_ix": "120-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_0",
            "tgt_ix": "120-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_74",
            "tgt_ix": "120-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_75",
            "tgt_ix": "120-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_75",
            "tgt_ix": "120-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_0",
            "tgt_ix": "120-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_76",
            "tgt_ix": "120-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_77",
            "tgt_ix": "120-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_77",
            "tgt_ix": "120-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "120-ARR_v1_0",
            "tgt_ix": "120-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_1",
            "tgt_ix": "120-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_2",
            "tgt_ix": "120-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_2",
            "tgt_ix": "120-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_2",
            "tgt_ix": "120-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_2",
            "tgt_ix": "120-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_2",
            "tgt_ix": "120-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_3",
            "tgt_ix": "120-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_4",
            "tgt_ix": "120-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_4",
            "tgt_ix": "120-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_4",
            "tgt_ix": "120-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_4",
            "tgt_ix": "120-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_4",
            "tgt_ix": "120-ARR_v1_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_5",
            "tgt_ix": "120-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_6",
            "tgt_ix": "120-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_7",
            "tgt_ix": "120-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_7",
            "tgt_ix": "120-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_7",
            "tgt_ix": "120-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_7",
            "tgt_ix": "120-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_8",
            "tgt_ix": "120-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_8",
            "tgt_ix": "120-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_8",
            "tgt_ix": "120-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_8",
            "tgt_ix": "120-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_8",
            "tgt_ix": "120-ARR_v1_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_9",
            "tgt_ix": "120-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_9",
            "tgt_ix": "120-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_9",
            "tgt_ix": "120-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_9",
            "tgt_ix": "120-ARR_v1_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_9",
            "tgt_ix": "120-ARR_v1_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_9",
            "tgt_ix": "120-ARR_v1_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_9",
            "tgt_ix": "120-ARR_v1_9@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_10",
            "tgt_ix": "120-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_11",
            "tgt_ix": "120-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_11",
            "tgt_ix": "120-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_11",
            "tgt_ix": "120-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_12",
            "tgt_ix": "120-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_13",
            "tgt_ix": "120-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_14",
            "tgt_ix": "120-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_14",
            "tgt_ix": "120-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_14",
            "tgt_ix": "120-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_14",
            "tgt_ix": "120-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_15",
            "tgt_ix": "120-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_15",
            "tgt_ix": "120-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_15",
            "tgt_ix": "120-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_15",
            "tgt_ix": "120-ARR_v1_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_16",
            "tgt_ix": "120-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_16",
            "tgt_ix": "120-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_16",
            "tgt_ix": "120-ARR_v1_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_16",
            "tgt_ix": "120-ARR_v1_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_17",
            "tgt_ix": "120-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_17",
            "tgt_ix": "120-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_17",
            "tgt_ix": "120-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_18",
            "tgt_ix": "120-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_18",
            "tgt_ix": "120-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_19",
            "tgt_ix": "120-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_19",
            "tgt_ix": "120-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_19",
            "tgt_ix": "120-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_19",
            "tgt_ix": "120-ARR_v1_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_20",
            "tgt_ix": "120-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_21",
            "tgt_ix": "120-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_22",
            "tgt_ix": "120-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_22",
            "tgt_ix": "120-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_23",
            "tgt_ix": "120-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_24",
            "tgt_ix": "120-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_25",
            "tgt_ix": "120-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_25",
            "tgt_ix": "120-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_26",
            "tgt_ix": "120-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_27",
            "tgt_ix": "120-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_27",
            "tgt_ix": "120-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_27",
            "tgt_ix": "120-ARR_v1_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_27",
            "tgt_ix": "120-ARR_v1_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_27",
            "tgt_ix": "120-ARR_v1_27@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_27",
            "tgt_ix": "120-ARR_v1_27@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_27",
            "tgt_ix": "120-ARR_v1_27@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_27",
            "tgt_ix": "120-ARR_v1_27@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_27",
            "tgt_ix": "120-ARR_v1_27@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_28",
            "tgt_ix": "120-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_28",
            "tgt_ix": "120-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_28",
            "tgt_ix": "120-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_29",
            "tgt_ix": "120-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_29",
            "tgt_ix": "120-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_30",
            "tgt_ix": "120-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_31",
            "tgt_ix": "120-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_31",
            "tgt_ix": "120-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_31",
            "tgt_ix": "120-ARR_v1_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_32",
            "tgt_ix": "120-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_32",
            "tgt_ix": "120-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_32",
            "tgt_ix": "120-ARR_v1_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_33",
            "tgt_ix": "120-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_33",
            "tgt_ix": "120-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_33",
            "tgt_ix": "120-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_34",
            "tgt_ix": "120-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_34",
            "tgt_ix": "120-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_34",
            "tgt_ix": "120-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_35",
            "tgt_ix": "120-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_35",
            "tgt_ix": "120-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_35",
            "tgt_ix": "120-ARR_v1_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_36",
            "tgt_ix": "120-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_36",
            "tgt_ix": "120-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_37",
            "tgt_ix": "120-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_38",
            "tgt_ix": "120-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_39",
            "tgt_ix": "120-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_39",
            "tgt_ix": "120-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_39",
            "tgt_ix": "120-ARR_v1_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_39",
            "tgt_ix": "120-ARR_v1_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_39",
            "tgt_ix": "120-ARR_v1_39@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_40",
            "tgt_ix": "120-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_41",
            "tgt_ix": "120-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_42",
            "tgt_ix": "120-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_43",
            "tgt_ix": "120-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_44",
            "tgt_ix": "120-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_44",
            "tgt_ix": "120-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_44",
            "tgt_ix": "120-ARR_v1_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_44",
            "tgt_ix": "120-ARR_v1_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_44",
            "tgt_ix": "120-ARR_v1_44@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_45",
            "tgt_ix": "120-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_45",
            "tgt_ix": "120-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_46",
            "tgt_ix": "120-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_46",
            "tgt_ix": "120-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_47",
            "tgt_ix": "120-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_47",
            "tgt_ix": "120-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_48",
            "tgt_ix": "120-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_48",
            "tgt_ix": "120-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_48",
            "tgt_ix": "120-ARR_v1_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_48",
            "tgt_ix": "120-ARR_v1_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_49",
            "tgt_ix": "120-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_49",
            "tgt_ix": "120-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_49",
            "tgt_ix": "120-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_49",
            "tgt_ix": "120-ARR_v1_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_50",
            "tgt_ix": "120-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_50",
            "tgt_ix": "120-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_50",
            "tgt_ix": "120-ARR_v1_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_50",
            "tgt_ix": "120-ARR_v1_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_51",
            "tgt_ix": "120-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_51",
            "tgt_ix": "120-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_52",
            "tgt_ix": "120-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_53",
            "tgt_ix": "120-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_53",
            "tgt_ix": "120-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_53",
            "tgt_ix": "120-ARR_v1_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_53",
            "tgt_ix": "120-ARR_v1_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_54",
            "tgt_ix": "120-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_55",
            "tgt_ix": "120-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_55",
            "tgt_ix": "120-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_56",
            "tgt_ix": "120-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_56",
            "tgt_ix": "120-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_56",
            "tgt_ix": "120-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_56",
            "tgt_ix": "120-ARR_v1_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_56",
            "tgt_ix": "120-ARR_v1_56@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_56",
            "tgt_ix": "120-ARR_v1_56@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_56",
            "tgt_ix": "120-ARR_v1_56@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_56",
            "tgt_ix": "120-ARR_v1_56@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_56",
            "tgt_ix": "120-ARR_v1_56@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_56",
            "tgt_ix": "120-ARR_v1_56@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_56",
            "tgt_ix": "120-ARR_v1_56@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_56",
            "tgt_ix": "120-ARR_v1_56@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_57",
            "tgt_ix": "120-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_58",
            "tgt_ix": "120-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_59",
            "tgt_ix": "120-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_59",
            "tgt_ix": "120-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_59",
            "tgt_ix": "120-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_59",
            "tgt_ix": "120-ARR_v1_59@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_59",
            "tgt_ix": "120-ARR_v1_59@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_60",
            "tgt_ix": "120-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_60",
            "tgt_ix": "120-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_60",
            "tgt_ix": "120-ARR_v1_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_60",
            "tgt_ix": "120-ARR_v1_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_61",
            "tgt_ix": "120-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_61",
            "tgt_ix": "120-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_61",
            "tgt_ix": "120-ARR_v1_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_61",
            "tgt_ix": "120-ARR_v1_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_62",
            "tgt_ix": "120-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_62",
            "tgt_ix": "120-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_62",
            "tgt_ix": "120-ARR_v1_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_62",
            "tgt_ix": "120-ARR_v1_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_62",
            "tgt_ix": "120-ARR_v1_62@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_62",
            "tgt_ix": "120-ARR_v1_62@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_62",
            "tgt_ix": "120-ARR_v1_62@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_63",
            "tgt_ix": "120-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_64",
            "tgt_ix": "120-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_64",
            "tgt_ix": "120-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_64",
            "tgt_ix": "120-ARR_v1_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_65",
            "tgt_ix": "120-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_65",
            "tgt_ix": "120-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_65",
            "tgt_ix": "120-ARR_v1_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_65",
            "tgt_ix": "120-ARR_v1_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_66",
            "tgt_ix": "120-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_66",
            "tgt_ix": "120-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_66",
            "tgt_ix": "120-ARR_v1_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_66",
            "tgt_ix": "120-ARR_v1_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_67",
            "tgt_ix": "120-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_67",
            "tgt_ix": "120-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_67",
            "tgt_ix": "120-ARR_v1_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_67",
            "tgt_ix": "120-ARR_v1_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_68",
            "tgt_ix": "120-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_68",
            "tgt_ix": "120-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_68",
            "tgt_ix": "120-ARR_v1_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_69",
            "tgt_ix": "120-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_70",
            "tgt_ix": "120-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_70",
            "tgt_ix": "120-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_70",
            "tgt_ix": "120-ARR_v1_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_70",
            "tgt_ix": "120-ARR_v1_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_70",
            "tgt_ix": "120-ARR_v1_70@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_71",
            "tgt_ix": "120-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_71",
            "tgt_ix": "120-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_72",
            "tgt_ix": "120-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_73",
            "tgt_ix": "120-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_73",
            "tgt_ix": "120-ARR_v1_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_73",
            "tgt_ix": "120-ARR_v1_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_73",
            "tgt_ix": "120-ARR_v1_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_74",
            "tgt_ix": "120-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_74",
            "tgt_ix": "120-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_75",
            "tgt_ix": "120-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_76",
            "tgt_ix": "120-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_76",
            "tgt_ix": "120-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_76",
            "tgt_ix": "120-ARR_v1_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_76",
            "tgt_ix": "120-ARR_v1_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_76",
            "tgt_ix": "120-ARR_v1_76@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_77",
            "tgt_ix": "120-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_78",
            "tgt_ix": "120-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_78",
            "tgt_ix": "120-ARR_v1_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_78",
            "tgt_ix": "120-ARR_v1_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_79",
            "tgt_ix": "120-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_80",
            "tgt_ix": "120-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_81",
            "tgt_ix": "120-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_82",
            "tgt_ix": "120-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_83",
            "tgt_ix": "120-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_84",
            "tgt_ix": "120-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_85",
            "tgt_ix": "120-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_86",
            "tgt_ix": "120-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_87",
            "tgt_ix": "120-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_88",
            "tgt_ix": "120-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_89",
            "tgt_ix": "120-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_90",
            "tgt_ix": "120-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_91",
            "tgt_ix": "120-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_92",
            "tgt_ix": "120-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_93",
            "tgt_ix": "120-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_94",
            "tgt_ix": "120-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_95",
            "tgt_ix": "120-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_96",
            "tgt_ix": "120-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_97",
            "tgt_ix": "120-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_98",
            "tgt_ix": "120-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_99",
            "tgt_ix": "120-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_100",
            "tgt_ix": "120-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_101",
            "tgt_ix": "120-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_102",
            "tgt_ix": "120-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_103",
            "tgt_ix": "120-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_104",
            "tgt_ix": "120-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_105",
            "tgt_ix": "120-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_106",
            "tgt_ix": "120-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_107",
            "tgt_ix": "120-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_108",
            "tgt_ix": "120-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_109",
            "tgt_ix": "120-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_110",
            "tgt_ix": "120-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_111",
            "tgt_ix": "120-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_112",
            "tgt_ix": "120-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_113",
            "tgt_ix": "120-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_114",
            "tgt_ix": "120-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_115",
            "tgt_ix": "120-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_116",
            "tgt_ix": "120-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_117",
            "tgt_ix": "120-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_118",
            "tgt_ix": "120-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_119",
            "tgt_ix": "120-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_120",
            "tgt_ix": "120-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_121",
            "tgt_ix": "120-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_122",
            "tgt_ix": "120-ARR_v1_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_123",
            "tgt_ix": "120-ARR_v1_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_124",
            "tgt_ix": "120-ARR_v1_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_125",
            "tgt_ix": "120-ARR_v1_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_126",
            "tgt_ix": "120-ARR_v1_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_127",
            "tgt_ix": "120-ARR_v1_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_128",
            "tgt_ix": "120-ARR_v1_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_129",
            "tgt_ix": "120-ARR_v1_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_130",
            "tgt_ix": "120-ARR_v1_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_131",
            "tgt_ix": "120-ARR_v1_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_132",
            "tgt_ix": "120-ARR_v1_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_133",
            "tgt_ix": "120-ARR_v1_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_134",
            "tgt_ix": "120-ARR_v1_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_135",
            "tgt_ix": "120-ARR_v1_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_136",
            "tgt_ix": "120-ARR_v1_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_137",
            "tgt_ix": "120-ARR_v1_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_138",
            "tgt_ix": "120-ARR_v1_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_139",
            "tgt_ix": "120-ARR_v1_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_140",
            "tgt_ix": "120-ARR_v1_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_141",
            "tgt_ix": "120-ARR_v1_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_142",
            "tgt_ix": "120-ARR_v1_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_143",
            "tgt_ix": "120-ARR_v1_143@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_144",
            "tgt_ix": "120-ARR_v1_144@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_145",
            "tgt_ix": "120-ARR_v1_145@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_146",
            "tgt_ix": "120-ARR_v1_146@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_147",
            "tgt_ix": "120-ARR_v1_147@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_148",
            "tgt_ix": "120-ARR_v1_148@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_149",
            "tgt_ix": "120-ARR_v1_149@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_150",
            "tgt_ix": "120-ARR_v1_150@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_151",
            "tgt_ix": "120-ARR_v1_151@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_152",
            "tgt_ix": "120-ARR_v1_152@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_153",
            "tgt_ix": "120-ARR_v1_153@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_154",
            "tgt_ix": "120-ARR_v1_154@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_155",
            "tgt_ix": "120-ARR_v1_155@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_156",
            "tgt_ix": "120-ARR_v1_156@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_157",
            "tgt_ix": "120-ARR_v1_157@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_158",
            "tgt_ix": "120-ARR_v1_158@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_159",
            "tgt_ix": "120-ARR_v1_159@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_160",
            "tgt_ix": "120-ARR_v1_160@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_161",
            "tgt_ix": "120-ARR_v1_161@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_162",
            "tgt_ix": "120-ARR_v1_162@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_163",
            "tgt_ix": "120-ARR_v1_163@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_164",
            "tgt_ix": "120-ARR_v1_164@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_165",
            "tgt_ix": "120-ARR_v1_165@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_166",
            "tgt_ix": "120-ARR_v1_166@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_167",
            "tgt_ix": "120-ARR_v1_167@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_168",
            "tgt_ix": "120-ARR_v1_168@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_169",
            "tgt_ix": "120-ARR_v1_169@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_170",
            "tgt_ix": "120-ARR_v1_170@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_171",
            "tgt_ix": "120-ARR_v1_171@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "120-ARR_v1_172",
            "tgt_ix": "120-ARR_v1_172@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1518,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "120-ARR",
        "version": 1
    }
}