{
    "nodes": [
        {
            "ix": "90-ARR_v1_0",
            "content": "Context-Aware Language Modeling for Goal-Oriented Dialogue Systems",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_2",
            "content": "Goal-oriented dialogue systems face a tradeoff between fluent language generation and task-specific control. While supervised learning with large language models is capable of producing realistic text, how to steer such responses towards completing a specific task without sacrificing language quality remains an open question. In this work, we formulate goal-oriented dialogue as a partially observed Markov decision process, interpreting the language model as a representation of both the dynamics and the policy. This view allows us to extend techniques from learning-based control, such as task relabeling, to derive a simple and effective method to finetune language models in a goal-aware way, leading to significantly improved task performance. We additionally introduce a number of training strategies that serve to better focus the model on the task at hand. We evaluate our method, Context-Aware Language Models (CALM), on a practical flightbooking task using AirDialogue. Empirically, CALM outperforms state-of-the-art method by 7% in terms of task success, matching humanlevel task performance on this dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "90-ARR_v1_4",
            "content": "Dialogue systems have typically approached the problem of generating realistic dialogue from the perspective of supervised learning (Du\u0161ek and Jurcicek, 2016;Eric and Manning, 2017;Mei et al., 2017;Wu et al., 2019a;Hosseini-Asl et al., 2020;Peng et al., 2020;Adiwardana et al., 2020). However, dialogue can also be viewed as a sequential decision making process which is well-suited to planning and reinforcement learning (RL) algorithms. A challenge with the classical RL approach to dialogue is the requirement for active interaction with humans (Ga\u0161i\u0107 et al., 2011). Training such a system with active human-in-the-loop interaction quickly becomes expensive and cumbersome, making it desirable to develop techniques for goal-directed training of dialogue systems that can effectively leverage offline data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_5",
            "content": "While many dialogue generation techniques based on RL and learned control have been proposed (Eckert et al., 1997;Levin et al., 2000;Chung, 2004;Georgila et al., 2006;Schatzmann et al., 2007;Heeman, 2009;Georgila and Traum, 2011), most such systems take a pipelined approach, where an abstract representation of states and actions is designed by hand and then combined with RL to train a \"dialogue management\" system, rather than generating dialogue end-to-end. These pipelined approaches rely on a manually designed decomposition of the dialogue task, which may be domain-specific and, more importantly, may not enjoy all of the benefits of tightly integrating lowlevel text generation with the overall goals of the task. In this work, we instead ask: how can we scalably and effectively introduce the mechanisms of goal-directed decision making into end-to-end language models, to directly steer language generation toward completing specific dialogue tasks rather than simply generating probable responses?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_6",
            "content": "To this end, rather than utilizing a pipelined approach, we aim to directly finetune language models in a task-aware manner such that they can maximize a given utility function. We observe that large language models can already be formulated within a Markov decision processes (MDP) as capturing both the dynamics and policy for a decision-making task, where dialogue history serves as state, and the agent's utterances serve as actions. We could utilize this observation by finetuning the models directly with online RL, but the need for human-inthe-loop training makes this difficult. Offline RL methods Fujimoto et al., 2019;Wu et al., 2019b;Wang et al., 2020b) provide an alternative approach, but typically require value function estimation, which is not straightforward to perform with a language model. Instead, we propose a conditional imitation learning strategy coupled with a novel task relabeling approach that can finetune language models from offline data, such that the model still represents the joint distribution over dialogues, but tilts this distribution toward dialogues with a high reward. This amounts to a task-aware finetuning strategy that integrates task information into the model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_7",
            "content": "The main contribution of our work is CALM (Context-Aware Language Modeling), a framework for end-to-end goal-directed dialogue generation. CALM unifies the traditional language modeling objective with task-specific supervision, where a language model is interpreted as a joint representation of dynamics and policies in an MDP, and the finetuning process utilizes a conditional imitation learning objective with a novel task relabeling strategy that teaches the model how to generate high-utility dialogues. Because CALM interprets the language model as both a dynamics model and a policy, it can be used as either a model-free method, where the dynamics are discarded and the policy component is used to greedily generate responses, or as a model-based method, where the dynamics component can be used to plan at test-time. We empirically evaluate CALM on AirDialogue (Wei et al., 2018), the largest dataset for goal-oriented dialogue based-on a flight-booking task. CALM improves the task success by 10% over the previous state-of-the-art method (Chen et al., 2020) following the evaluation protocol proposed by Wei et al. (2018), achieving the first-ever human-level performance on this dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_8",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "90-ARR_v1_9",
            "content": "Our goal is to enable end-to-end training of goaldirected dialogue agents. In these settings, an agent aims to complete a particular task with its utterances (Smith and Hipp, 1994). Goal-directed agents have been explored in contexts such as personal assistants (McTear, 2002;Budzianowski et al., 2018;Williams et al., 2014), recommendation systems (Liu et al., 2010;Kang et al., 2019), education (Yuan et al., 2008), and negotiation (He et al., 2018;Lewis et al., 2017). While there are multiple approaches to constructing dialogue agents, in this work we frame the problem of generating dialogue as a sequential decision making problem within a (partially observed) Markov Decision Process (MDP) (Singh et al., 1999;Young et al., 2013). Prior works that utilize such an MDP formulation typically aim to train a dialogue management system (Singh et al., 2002), in which the agent reasons about higher-level abstractions of the state of the conversation, and language generation is performed using a downstream procedure. Dialogue management systems have been trained using techniques such as online reinforcement learning via policy gradients (Ga\u0161i\u0107 et al., 2011;He et al., 2018), off-policy reinforcement learning (Pietquin et al., 2011;Yu et al., 2016) or actor-critic methods (Su et al., 2017). Our method differs from dialogue management systems in that CALM is an end-to-end system optimized for successful task completion, and performs both high-level decision making and language generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_10",
            "content": "Recent advancements in language models, such as recurrent neural networks (Sundermeyer et al., 2012;Asri et al., 2016;Su et al., 2016;Zhao et al., 2019;Wang et al., 2020a;Zhang et al., 2020) and attention-based architectures (Vaswani et al., 2017;Liu et al., 2019;Devlin et al., 2018;Brown et al., 2020), have spurred increasing interest in such endto-end dialogue systems (Hosseini-Asl et al., 2020;Peng et al., 2020;Adiwardana et al., 2020). Modelbased approaches, in which a learned agent is substituted for a human, allow learning to be done entirely within simulation without human intervention (Li et al., 2016;He et al., 2018;Kang et al., 2019;Lewis et al., 2017;Liu et al., 2018). In contrast to these approaches, CALM augments the traditional language modeling objective with taskspecific rewards in order to finetune a model that is more aware of task goals, which significantly improves performance over a na\u00efve language model without the need for simulating human responses in an interactive training loop. Jaques et al. (2019) recently proposed a model-free, offline approach to undirected dialogue, or dialogue without a specific task goal. Our method differs in that we aim to solve goal-oriented dialogue which allows us to optimize task-specific objectives, and that we take a model-based RL approach which enables us to leverage fine-tuned language models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_11",
            "content": "Preliminaries",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "90-ARR_v1_12",
            "content": "In this section, we review our notation and problem formulation for casting dialogue within a sequential decision making framework. POMDP formulation. We formulate dialogue generation as a partially observable Markov decision process (POMDP) (Kaelbling et al., 1998), with a state that consists of known and unknown context information about the task. Let c h \u2208 C (h) denote the hidden context for the task, and let c o \u2208 C (o) denote the observed context. For in- We apply Task Relabeling to our static offline dataset, by swapping out the task context -in this case a flight table -such that the attached dialogue becomes an optimal example of task completion. When fine-tuning on this relabeled dataset, we then apply a Task Specific Auxilary Loss on top of the standard language modeling objective; this helps the model learn to use the task context. Once trained, CALM can consistently solve complex tasks in dialogue.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_13",
            "content": "stance, in a flight booking task, a table of available flights might correspond to c o , while the particular flight that the human wants to book, which is unknown to the agent, corresponds to c h . Note that the reward, which requires booking the right flight, depends on both hidden and observed contexts. We can define such an environment as a POMDP M = (S, A, O, T , Z, \u00b5 0 , R, \u03b3). We denote a conversation \u03c4 as \u03c4 := {a 0 , e 0 , ..., a T }, where T denotes the number of turns in a conversation and a t and e t represent utterances (strings of tokens) from the dialogue agent (a t ) and the human (e t ) at the t-th turn, respectively. We additionally use \u03c4 <t to denote conversation history up to the t-th turn. We can represent the underlying POMDP state s t \u2208 S as the concatenation of both of the contexts and the previous conversation history s t := {c h , c o , \u03c4 <t } = {c h , c o , a 0 , e 0 , ..., a t\u22121 , e t\u22121 }. However, we only observe the last two elements of the state tuple, such that our observation o t \u2208 O at the t-th conversation turn is o t = {c o , \u03c4 <t }. An action a t \u2208 A is the agent's response to the current state s t . Given our definition of the state, the full conversation in a dialogue can be conveniently represented by the last observation and action, {o T , a T }. An agent \u03c0 : O \u2192 P(A) maps observations to sets of probability measures over the action space P(\u2022). A transition function T (\u2022|s t , a t ), represents a distribution over the human's utterances, returning s t+1 as the state at turn t + 1. We only consider the sparse reward setting with r T = R(s T , a T ) \u2208 {0, 1} denoting task completion, and r t = 0, \u2200t < T . Our final reward is therefore dependent on both the context and the dialogue: R(s T , a T ) = R(\u03c4, c h , c o ), where the context {c o , c h } is randomly sampled for each dialogue from some initial distribution \u00b5 0 . Goal-oriented dialogue. Goal-oriented dialogue systems aim to maximize the expected reward of the above POMDP",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_14",
            "content": "E {co,c h }\u223c\u00b5 0 ,\u03c0,T [ T t=0 \u03b3 t R(s t , a t )],",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_15",
            "content": "(1) where {c h , c o } is sampled from distribution \u00b5 0 . Onpolicy RL algorithms optimize this objective via environment interaction, which is represented by a real human. However, because human-in-the-loop training is expensive, we pursue an offline learning approach where we are given a fixed dataset and there is no further interaction with the human in the learning process. This dataset is composed of n trajectories with D off = {c",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_16",
            "content": "(i) h , c (i) o , \u03c4 (i) , r (i) } n i=1 with each \u03c4 (i) = {a (i) 0 , e (i) 0 , , ..., a(i)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_17",
            "content": "T } and its corresponding final reward for task completion r (i) . Our goal is to learn the policy \u03c0(a|o) which improves the dialog agent's ability in achieving the highest task reward defined in Equation 1. Language models. While conventionally a language model is seen simply as a sequence model over tokens of the form T t=1 p(x t+1 |x 1:t ), when the sequence x 1:T corresponds to a dialogue trajectory \u03c4 , we can also interpret a language model as learning the distribution over \u03c4 . This distribution can be factored into the product of the policy \u03c0(a t |\u03c4 <t ) and the dynamics T (\u03c4 <t+1 |\u03c4 <t , a t ), and so we can say that a language model also represents the policy and the dynamics. Therefore, the maximum likelihood objective for training or finetuning a language model on a dialogue dataset D off consisting of dialogue trajectories \u03c4 can be written as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_18",
            "content": "L LM (\u03b8) = max \u03b8 E \u03c4 \u223cD off T t=1 log \u03c0 \u03b8 (a t |\u03c4 <t ) + log T \u03b8 (\u03c4 <t+1 |\u03c4 <t , a t ) ,(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_19",
            "content": "where \u03c0 \u03b8 (a t |o t ) represents a policy that generates new dialogue based on the observed context and dialogue history, and T \u03b8 (\u03c4 <t+1 |\u03c4 <t , a t ) represents the observed dynamics characterizing human responses, and \u03b8 denotes parameters in \u03c0 and T . Note that \u03c4 <t consists only of the conversation history, and does not contain any task-specific context.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_20",
            "content": "A na\u00efve approach to train dialogue systems is to jointly parameterize both \u03c0 and T as one language model, and optimize Equation 2 on pre-collected conversations D off . This method corresponds to behavioral cloning (BC) (Pomerleau, 1989).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_21",
            "content": "Context conditioning. While an agent trained using Equation 2 can learn policies and dynamics that imitate human conversations, this objective does not incorporate the task goal, and may not produce a policy that is more performant than the dataset D off . While it is possible to input c o into the language model to maximize the conditional probability of P (\u03c4 |c o ) using a conditional version of the language modeling objective,",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_22",
            "content": "L CT X (\u03b8) = max \u03b8 E (\u03c4,co)\u223cD off T t=1 log \u03c0 \u03b8 (a t |\u03c4 <t , c o ) + log T \u03b8 (o t+1 |\u03c4 <t , a t , c o ) ,(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_23",
            "content": "contexts with particular task structures (e.g., a set of entries in a table) may not be simply processed as a sequence similarly to \u03c4 . Additionally, the language model is not pretrained to read structured context, and oftentimes the recent dialogue history is much more predictive of the next utterance than the task context is. As a result, language models can ignore the task context and only learn P (\u03c4 ) despite being conditioned on c o . Our approach builds on this conditional modeling approach, but makes a number of improvements that allow it to be more aware of the context information, which attains significantly better results in our experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_24",
            "content": "Context-Aware Language Modeling",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "90-ARR_v1_25",
            "content": "In this section, we present our method for goaloriented dialogue systems, Context-Aware Language Modeling (CALM). CALM interprets a language model as a combination of a policy and a dynamics model in the POMDP formulation of a dialogue task, as described in Section 3. Under this interpretation, na\u00efve supervised finetuning on the dialogue dataset can be viewed as behavioral cloning (BC) (Pomerleau, 1989). However, BC only imitates data and does not necessarily produce a good policy in terms of completing tasks. We propose to improve the policy by utilizing a task relabeling strategy (described in Section 4.1), analogous to prior task relabeling approaches (Kaelbling, 1993;Andrychowicz et al., 2017;Pong et al., 2018;Savinov et al., 2018;Ghosh et al., 2019;Lynch et al., 2020;Eysenbach et al., 2020). This relabeling procedure augments the data with examples of near-optimal utterances, making the language model more task-aware. However, we find several shortcomings with this approach alone and propose the following improvements. First, an expressive language model is liable to ignore the task context, which we address by proposing an auxiliary loss (Section 4.2) that forces the model to utilize this information. Second, learning from structured task information is difficult and can result in models that fail to capture complex task structure, so we propose a task pre-training procedure to improve the learnability (Section 4.3). Finally, to further improve performance we use a model-based planning procedure (Section 4.4) on top of the proposed method that samples multiple dialogues in parallel and selects the most promising candidates.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_26",
            "content": "Dialogue Task Relabeling",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "90-ARR_v1_27",
            "content": "L CT X (\u03b8) defines a context-conditional maximum likelihood objective for training an expert imitation policy in conjunction with a dynamics model. However, simply imitating all the dialogue data does not necessarily produce the best possible policy. We would like to learn a policy that produces dialogue that is more optimal, in the sense of better maximizing the task utility, than the average dialogue in the dataset. Task relabeling enables us to learn from optimal trajectories without simply filtering the dataset for high-reward trajectories, which would unnecessarily discard potentially informative data. In the case of dialogue, we can perform task relabeling by considering the context {c o , c h } as defining the task. While a given dialogue may be unsuccessful for the context for which it was collected, it could be considered successful under a different context. In this case, we can simply swap out {c o , c h } to create optimal task examples from the many sub-optimal examples provided by D off . Since our reward R(c h , c o , \u03c4 ) is a function of the dialogue and context, we can modify the reward for a given dialogue just by changing the given observed context c o . Using this observation, we can relabel unsuccessful dialogues with successful ones, and even for already successful dialogues there may be multiple c o corresponding to task success, allowing us to augment the number of successful (c h , c o , \u03c4 ) tuples.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_28",
            "content": "Formally, since our POMDP includes a prior distribution over contexts {c h , c o } \u223c \u00b5 0 , there exists a posterior q(c o |\u03c4, c h ) over observed contexts that correspond to optimal task completion under a given \u03c4 . We can then re-label \u03c4 to be optimal under its context by sampling a new c o from q(c o |\u03c4, c h ). In practice, this sampling is performed by rejection sampling from either \u00b5 0 or some P (c o |c h ); the latter, lower entropy distribution, can be preferred if there is a low probability of sampling valid, highreward contexts under \u00b5 0 . Now, given any \u03c4 from an offline dataset of dialogues, we can learn from the full distribution of contexts corresponding to optimal task completion under this dialogue.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_29",
            "content": "In order for this relabeling procedure not to bias our policy towards behavior that is overlyoptimistic about the user's responses, it is necessary that the distribution of these responses in our dataset does not depend on the portion of the context that is relabeled. For example, relabeling the table of available flights for a flight booking task should generally be reasonable, because the user is usually unaware of the flight table. On the other hand, relabeling the desired flight would not make sense, since the user's utterances are strongly depend on this. To provide another example, in a bargaining task (Lewis et al., 2017), the agent might fail to obtain the desired item and instead get an item of lesser value. But relabeling with a context that assigns a higher value to the item received would not lead to a reasonable example, since the agent mainly received this item as a result of the user's responses rather than as a result of their own bargaining skill.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_30",
            "content": "Methods based on similar principles have previously been proposed in the deep RL community for simple parametric tasks, such as goal-reaching or linearly-parameterized reward functions (Kaelbling, 1993;Andrychowicz et al., 2017;Eysenbach et al., 2020). However, the dialogue task relabeling that we employ is particularly effective in our setting, since there may be exponentially many contexts that are optimal for a given dialogue (e.g., many different flight tables for a flight booking task), in contrast to the simpler task parameterizations used in prior work, where for example only one goal might be optimal for a given trajectory (the one that is reached). Additionally, the structure of our specific task allows us to sample from the true posterior q(c o |\u03c4, c h ), by using the rejection sampling procedure described above, rather than using approximations as in prior work (Eysenbach et al., 2020). As a result, this technique not only allows us to turn sub-optimal task data into optimal data, but it also allows us to greatly increase the number of optimal task examples from which we can learn, which we will show leads to a large performance improvement.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_31",
            "content": "Task-Specific Auxiliary Loss",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "90-ARR_v1_32",
            "content": "Goal-oriented dialogue generation can be viewed as learning the conditional distribution P (\u03c4 |c o ), where \u03c4 represents the generated dialogue given a specific context c o . However when trained na\u00efvely, language models are liable to ignore this conditioning context, instead focusing purely on the previous utterances in the dialogue. In this case, the model is effectively only learning P (\u03c4 ) despite having both the capacity and the context to learn the lowerentropy conditional distribution P (\u03c4 |c o ).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_33",
            "content": "While dialogue tasks are by definition carried out through natural language, there is often an abstract high-level action \u03b1 h \u2208 A that essentially determines the success of the task. In the case of the information retrieval task that we consider in this paper, these high-level actions correspond to deciding which database entity to retrieve for the user (e.g. suggesting a flight to the customer that meets all of their needs). While these high-level actions are theoretically learnable from correlations between the dialogue and the given context, in general, we find that learning these correlations corresponds to a relatively small decrease in dialogue entropy under the model. As a result, the model is less incentivized to learn these correlations relevant to the task than the form of the dialogue. To address this issue, we incorporate an auxiliary objective into our training, which trains the model directly to predict the abstract high-level actions taken in the present dialogue. This objective effectively up-weights gradients relevant for learning the high-level actions, which further helps the model to utilize the context to solve the high-level task through dialogue.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_34",
            "content": "For a given dialogue-context pair (\u03c4, {c h , c o }) and high-level action, \u03b1 h , our auxiliary objective is then simply to maximize the likelihood of the high-level actions taken in the dialogue:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_35",
            "content": "C(\u03d5) = max \u03d5 E (c h ,co,\u03c4,\u03b1 h )\u223cD off log P \u03d5 (\u03b1 h |\u03c4, c o ).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_36",
            "content": "(4) Just like the language modeling objective, this classification objective is averaged over each token in the dialogue sequence. Our full training objective then becomes:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_37",
            "content": "max \u03b8,\u03d5 L CT X (\u03b8) + \u03b2 * C(\u03d5),(5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_38",
            "content": "where \u03b2 is a hyper-parameter and L CT X (\u03b8) is the standard context-conditional language modeling objective as defined in Section 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_39",
            "content": "Task Pretraining",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "90-ARR_v1_40",
            "content": "As observed by Liu et al. (2021), for some structured tasks, such as table question answering, pretraining on a simplified version of the given task with a synthetic context can help the model to focus learning on the \"skills\" that are most relevant to utilize the task context, which leads to improved downstream task performance. We instantiate this idea in our method by pre-training our model on a simplified (dialogue-free) version of the task. Instead of simultaneously modeling all the details of the raw dialogue, as is required to learn P (\u03c4 |c o ), the key observation here is that in our case the task reward only depends on the tuple {c h , c o , a T }. This enables us to effectively learn to execute the task by only modeling P (c h , a T |c o ), without any dialogue at all. By pre-training our model to first learn this simplified distribution, we effectively focus on learning the necessary skills for completing the task. It is expected that the skills learned during this pre-training phase should also generalize and transfer when we later perform training on the real dialogue. The particular instantiation of this principle in the case of AirDialogue is described in Section A.5.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_41",
            "content": "Model-Based Dialogue Rollouts",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "90-ARR_v1_42",
            "content": "While the methodology discussed so far can produce effective policies, language models also represent task dynamics, as discussed in Section 3. We can leverage this fact to further improve the performance of our fine-tuned models by performing model-based planning at test-time, using both the policy and dynamics components in concert to further maximize task reward. A full dialogue trajectory can then be formed by concatenating this sampled future trajectory \u03c4 \u2265t with the current state of the dialogue \u03c4 <t i.e., \u03c4 = {\u03c4 <t , \u03c4 \u2265t }. We perform the model-based planning by sampling k such future trajectories from the final fine-tuned model, and ranking them according to an estimated reward function R(\u03c4, c o ) (see Appendix A.7). Then, we improve upon the policy \u03c0 from which we took the samples by taking the action (i.e., the next utterance) a t which receives the highest estimated reward among the sampled trajectories. This roll-out sampling procedure is identical to the one used by Lewis et al. (2017).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_43",
            "content": "CALM for AirDialogue",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "90-ARR_v1_44",
            "content": "In this section, we instantiate our proposed method, CALM, for the AirDialogue flight booking task (Wei et al., 2018). We first give an overview of the task, and then describe how to do relabeling and context conditioning on this specific task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_45",
            "content": "AirDialogue Dataset",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "90-ARR_v1_46",
            "content": "Dataset overview. The AirDialogue dataset (Wei et al., 2018) is a recently published large-scale airline reservation dataset based on the aforementioned task. The dataset includes 402,038 conversations. The dataset involves three distinct tasks: booking, canceling, and changing flights. We describe the booking task in detail below. Flight booking task. The (human) customer is given a set of 12 trip requirements, and the flight agent (bot) is provided with a table of 30 flights. The goal of the flight agent is to book a flight from the table for the customer which meets all their requirements, or to correctly inform them that no such flight is available. To determine task success, the flight agent must predict an explicit action at the end of the dialogue indicating the flight that was booked or inform no flight available. See Figure 6 for an example conversation from the dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_47",
            "content": "Processing Tables",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "90-ARR_v1_48",
            "content": "The AirDialogue booking tasks require efficiently querying a flight table containing flight information (e.g., departing location, ticket price) given to the agent prior to the conversation. In order to successfully complete the booking task, the agent needs to be able to filter, select, and integrate information from the flight table based on the customer's preferences inferred from the dialogue.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_49",
            "content": "Instead of treating the tables as unstructured sequences (Wei et al., 2018;Jiang et al., 2021) or as SQL databases (Chen et al., 2020), CALM models tables as an observable context consisting of a set c o = {f 1 , f 2 , f 3 , ..., f N } of table rows. These rows are then input to our model as a set of embeddings (see appendix A.4 and A.9 for more details).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_50",
            "content": "Relabeling AirDialogue with CALM",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "90-ARR_v1_51",
            "content": "While the AirDialogue dataset only includes one flight table for each dialogue, there are potentially many flight tables compatible with each dialogue as each flight can appear in many tables. We hence implement our relabeling procedure as described in Section 4.1 as follows. We perform rejection sampling on the observable context (i.e., the table of flights) c o \u223c q(c o |\u03c4, c h ), sampling until we obtain a new context (c h , c o , \u03c4 ), which gives maximum reward possible R(\u03c4, c h , c o ) = max co R(\u03c4, c h , c o ). The prior distributions p(c o ) and p(c o |c h ), from which the tables in the AirDialogue dataset were sampled, are provided with the dataset. By rejection sampling from p(c o |c h ), we can effectively sample from the posterior q(c o |\u03c4, c h ) within a certain computational budget. In this setting, c o denotes tables and there are exponentially many tables which correspond to a task success under a given dialogue. Therefore, with our relabeling approach, we increase the number of near-optimal task examples exponentially, which makes it much easier for the language model to learn to query the flight table.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_52",
            "content": "Our relabeling is approximately valid according to the condition specified in Section 4.1. While the customer does not have access to the flight table and therefore is not directly affected by our relabeling, there are still some minor edge-cases in which over-optimism about the dynamics could be learned by our policy. If for example, in the dataset the customer were to occasionally reject the first flight that we suggest, our policy may learn to assign a small probability to the action of initially offering the wrong flight, relying on them subsequently rejecting it such that we can later recover and offer the correct one. However, in practice we observe that these cases are rare in AirDialogue.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_53",
            "content": "Table Selection as Auxiliary Loss",
            "ntype": "title",
            "meta": {
                "section": "5.4"
            }
        },
        {
            "ix": "90-ARR_v1_54",
            "content": "The primary high-level action involved in Air-Dialogue is the decision of which flight table entry, if any, to recommend to the user. We therefore implement our auxiliary objective as a classification head on top of the language model, trained to predict the flight table entry that meets the customer's requests. Specifically, our set of high-level actions A is the set of flight table rows {f 1 , f 2 , f 3 , ..., f N } plus an additional item f 0 , corresponding to the case in which no flights meet the customer's requirements. If f * is the flight recommended in the dialogue, then our auxiliary objective is:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_55",
            "content": "C(\u03d5) = max \u03d5 E (co,\u03c4 )\u223cD off log P \u03d5 (f * |\u03c4, c o ). (6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_56",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "90-ARR_v1_57",
            "content": "In this section, we empirically evaluate the performance of CALM on AirDialogue (Wei et al., 2018). We first show that CALM outperforms the success rate CALM (greedy) 0.88 \u00b1 2e-3 LM(GPT2-small) (greedy) 0.38 \u00b1 5e-3 AirConcierge (greedy) 0.81 \u00b1 7e-3 CALM (planning) 0.90 \u00b1 2e-3 LM(GPT2-small) (planning) 0.74 \u00b1 7e-3 Human 0.88",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_58",
            "content": "Table 1: Comparison of our method and baselines across all tasks, as well as just the booking task on AirDialogue. Using greedy decoding, our method matches human performance, greatly improving over baselines. Adding roll-outs (32 samples) further improves task completion.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_59",
            "content": "SOTA on the AirDialogue dataset by around 7% in the standard simulated evaluation protocol proposed by Chen et al. (2020), which prior work denotes as \"self-play\" (see Appendix A.6), and this matches human-level performance as reported by Wei et al. (2018). Beyond this, we also perform a comprehensive set of ablation studies to validate the necessity of each component of CALM.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_60",
            "content": "Experiment Setup and Baselines. We compare CALM on AirDialogue with two baselines. The first is AirConcierge, the previous SOTA on Air-Dialogue, which explicitly parses and executes SQL queries from the dialogue (Chen et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_61",
            "content": "The other is a standard language model (denoted as LM(GPT2-small)) trained on a dataset filtered for successful task examples, without any of our context-aware language modeling techniques (see Appendix Section A.1 for more details on dataset filtering). CALM uses the fine-tuned GPT2-small model (Radford et al., 2018) as the backbone of the policy and dynamics model. After learning the dynamics model, both CALM and the LM(GPT2small) can employ two different planning strategies: (1) a simple greedy decoding of the next utterance (equivalent to beam search with beam-width one) and (2) the rollout planning as described in Section 4.4. For AirConcierge, we only evaluate greedy decoding, as this method cannot be easily adapted for producing full rollouts as rollout planning requires a method for predicting the reward of a given dialogue. We describe our specific reward predictor for AirDialogue in Appendix Section A.7.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_62",
            "content": "Results for Task Success. In terms of task success, CALM outperforms the prior SOTA (AirConcierge) by approximately 7%, achieving 88% task success when using greedy decoding from the language model. Compared with AirConcierge, where all reasoning about the task context is done outside of the language model, CALM does all of the filtering, selecting, and responding with relevant flight table entries within the language model, in a fully endto-end manner. Meanwhile, CALM also improves over LM(GPT2-small) by 50% in terms of task success, indicating the necessity of our contextaware approach for goal-oriented tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_63",
            "content": "We further evaluate the the performance of various methods, when utilizing the rollout planning technique. As shown in Figure 3, as the number of rollout samples increases, the performance improves for all methods. Remarkably, applying the rollout planning to CALM further increases total task success by 2%, raising it to 90% and matching human performance on the AirDialogue task. The baseline LM(GPT2-small) benefits much more from rollout planning than CALM, and we suspect that at around 90% task completion, the performance becomes bottlenecked by the customer bot's mistakes, therefore we only observe less gain from rollout planning with CALM. Results for Language Quality. To quantitatively measure the generated language quality, we present perplexity and BLEU for all methods in Table 2. CALM performs similarly to LM(GPT2-small) and outperforms AirConcierge significantly. Ablation Study. To examine the effectiveness of each single component in our method, we train and evaluate four ablations of CALM. Each of these ablations remove one of the components in our approach: task relabeling (Section 4.1), auxiliary loss (Section 4.2), and table pre-training (Section 4.3).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_64",
            "content": "Beyond this, we also examine CALM without both task relabeling and pre-training. As shown in Table 3, removing any one of these components drops task success by at least 10%, and in most cases much more than that. This shows that each piece of our method plays a critical role in helping CALM to effectively learn the goal-oriented task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_65",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "90-ARR_v1_66",
            "content": "We proposed an end-to-end framework, CALM, for goal-oriented dialogue systems. Formulating end-to-end dialogue generation as a Markov decision process, CALM employs task relabeling and context-aware finetuning to steer supervised learning of language models towards specific goals, improving task performance drastically while preserving language quality. We show that this improves performance on AirDialogue over the previous state of the art, and matches previously reported human performance under the standard simulated evaluation protocol.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_67",
            "content": "Ethical Statement",
            "ntype": "title",
            "meta": {
                "section": "8"
            }
        },
        {
            "ix": "90-ARR_v1_68",
            "content": "We note that CALM is imperfect and in generating free-form dialogue can potentially produce harmful outputs. We therefore do not recommend applying this method in particularly sensitive or sufficiently wide-reaching domains without additional measures to mitigate harmful generations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_69",
            "content": "In this appendix, we provide all the details in our implementation for CALM.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_70",
            "content": "When training the LM(GPT2-small) and Customer Bot, we filter the dataset by only keeping the successful task examples. This is be achieved by simultaneously checking for successful task completion and whether a set of simple string matching heuristics are satisfied in the dialogue. Our heuristics aim to ensure that strings corresponding to each of the customer's flight requirements and the customer's goal are explicitly present in the dialogue. This combination of filtering steps reduces the size of the training set by 26%. Despite this, we find that this is still more than enough data for the model to successfully learn the task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_71",
            "content": "In Figure 4, we show the rollout planning procedure, which described in Section 4.4.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_72",
            "content": "Our customer bot is fine-tuned from GPT2-small (124M parameters), using the standard language modeling objective. We used the Huggingface Transformers library's implementation of GPT2 (Wolf et al., 2020). The customer's flight requirements are provided to the model as a prefix to the dialogue, which formatted as a comma separated list consisting of the customer's goal and flight requirements. We trained the customer bot for maximum 10 epochs with early stopping on the filtered dataset. For training, it takes around 1 day on 4 GPUs. Specifically, we trained using Adam with learning rate 1e-4 and batch size 8. Our customer bot achieves a perplexity of 1.47 on the development set and a BLEU score of 38.5.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_73",
            "content": "All our flight agent bots are fine-tuned from GPT2-small (124M parameters) using the standard language modeling objective. We used the Huggingface Transformers library's implementation of GPT2 (Wolf et al., 2020). Similar as the customer bot, we trained for maximum 10 epochs with early stopping on the filtered dataset, which takes roughly 1 day on 4 GPUs. Specifically, we trained using Adam with learning rate 1e-4 and batch size 8. We implement the final action prediction as a sequence of tokens generated at the end of each dialogue. The flight table is passed to the model as a prefix of flight embeddings, where each embedding is produced by summing embeddings corresponding to each attribute of a given flight (e.g., flight arrival/departure day/location, flight price, etc.).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_74",
            "content": "Initialized using GPT2-small (124M parameters), we further pre-train our flight-agent bots by training on simplified task sequences. Specifically, these sequences consist of our flight table followed by a comma separated list of the customer's flight requirements and a string representing the final action taken. We also apply our auxiliary loss and task-relabeling techniques during this pre-training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_75",
            "content": "We pre-train on 4 million unique samples, using batch size 64 and Adam with learning rate 1e-4, which takes around 2 days on 4 GPUs. During pre-training, we found that it took around 2 million unique samples before the model suddenly started to learn the task of querying the flight table, and it took roughly 2 million more samples before it became proficient at querying the table. Both the unusual progression of learning during this pre-training phase and the high sample complexity needed to learn the task, indicates the difficulty in learning to query the flight table. This calls for future work about further investigate the challenges in learning complex logical functions using neural networks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_76",
            "content": "Prior works primarily evaluate bots for the flight agent through \"self-play\" (Chen et al., 2020;Wei et al., 2018). We follow the same evaluation protocol in our work. Basically, we train a bot to play the role of the customer during evaluation and compute task success by simulating conversations against this bot. We run all self-play evaluations on the same subset of 1,000 dialogue scenarios, randomly selected from the validation set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_77",
            "content": "All models are evaluated against the same customer bot. including models for the baselines. We find that when running against our self-play bot, task completion success for prior methods is increased, sometimes by more than 8% (from what was reported by such prior works under the same evaluation setting). The only difference is the specific model used for customer's side of the conversation, and we conjecture that this difference is likely due to the architecture difference and the details of our dataset filtering. This significant change in evaluation performance compared with prior works, not only indicates the quality of our customer bot, but also suggests the importance of accounting for these factors in evaluating and comparing dialogue systems. To promote fair evaluations in future works that use the AirDialogue dataset, we will release the code and model weights for our customer bot upon acceptance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_78",
            "content": "Rollout Planning To execute rollout planning, we need a reward predictor which can estimate whether a given dialogue is a successful example of task completion or not. In the case of AirDialogue, we found that the most robust way to estimate this reward is the following: we first fine-tune a RoBERTa-base model (123M parameters) to predict the customer's ground-truth goal and flight requirements from the set of dialogues in the training set. We used the Huggingface Transformers library's implementation of RoBERTa (Wolf et al., 2020). We do not filter the training-set when training this model. Once this model is trained, our procedure for predicting dialogue success is the following:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_79",
            "content": "1. Given a dialogue, use our RoBERTa model to predict the customer's goal and flight requirements.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_80",
            "content": "2. We then execute this predicted information against the agent's flight table and reservation flag, to produce a set of valid final actions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_81",
            "content": "3. If the final action taken in the dialogue is within the set of predicted final actions, then predict that the current dialogue is successful, otherwise predict that it is unsuccessful.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_82",
            "content": "See Figure 5 for a visual illustration of this procedure. Our model obtains 94% accuracy in predicting the reward of the dialogues in the validation set (see Table 4 for a more extensive breakdown of the model's accuracy).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_83",
            "content": "In Figure 6, we showcase a specific example for the conversation in AirDialogue. A.9 Previous Approaches to Flight Table Processing Prior works (Wei et al., 2018;Jiang et al., 2021) typically input the table directly into a language model, expecting that the skill of querying the table will be naturally learned via the standard language modeling objective. We found this approach to under-perform in our experiments. These findings are also consistent with recent works which show that pre-training transformers for querying tables can significantly improve the transformer's performance on downstream tasks which use tables (Liu et al., 2021). AirConcierge (Chen et al., 2020) takes a different approach, and explicitly predicts and executes SQL queries based on the dialogue. This approach obtains the SOTA task success on AirDialogue, but it involves several complex components, requires the ability to preform semantic parsing on the dialogue, and of course requires additional domain knowledge about the format and structure of the flight table, which reprsents the task context. In our work, we show that applying CALM for AirDialogue can close this gap by inducing task learning from language models and achieve end-to-end learning from the flight table, without sacrificing the generated language quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "90-ARR_v1_84",
            "content": "UNKNOWN, None, 2020, Towards a human-like open-domain chatbot, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Towards a human-like open-domain chatbot",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_85",
            "content": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob Mc-Grew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba, Hindsight experience replay, 2017, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Marcin Andrychowicz",
                    "Filip Wolski",
                    "Alex Ray",
                    "Jonas Schneider",
                    "Rachel Fong",
                    "Peter Welinder",
                    "Bob Mc-Grew",
                    "Josh Tobin",
                    "Pieter Abbeel",
                    "Wojciech Zaremba"
                ],
                "title": "Hindsight experience replay",
                "pub_date": "2017",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_86",
            "content": "UNKNOWN, None, 2016, A sequence-to-sequence model for user simulation in spoken dialogue systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "A sequence-to-sequence model for user simulation in spoken dialogue systems",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_87",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_88",
            "content": "Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Milica Osman Ramadan,  Gasic, Multiwoz-a largescale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Pawe\u0142 Budzianowski",
                    "Tsung-Hsien Wen",
                    "Bo-Hsiang Tseng",
                    "I\u00f1igo Casanueva",
                    "Stefan Ultes",
                    "Milica Osman Ramadan",
                    " Gasic"
                ],
                "title": "Multiwoz-a largescale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_89",
            "content": "Chieh-Yang Chen, Pei-Hsin Wang, Shih-Chieh Chang, Da-Cheng Juan, Wei Wei, Jia-Yu Pan, Airconcierge: Generating task-oriented dialogue via efficient large-scale knowledge retrieval, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Chieh-Yang Chen",
                    "Pei-Hsin Wang",
                    "Shih-Chieh Chang",
                    "Da-Cheng Juan",
                    "Wei Wei",
                    "Jia-Yu Pan"
                ],
                "title": "Airconcierge: Generating task-oriented dialogue via efficient large-scale knowledge retrieval",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_90",
            "content": "Wenhu Chen, Jianshu Chen, Pengda Qin, Xifeng Yan, William Wang, Semantically conditioned dialog response generation via hierarchical disentangled self-attention, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Wenhu Chen",
                    "Jianshu Chen",
                    "Pengda Qin",
                    "Xifeng Yan",
                    "William Wang"
                ],
                "title": "Semantically conditioned dialog response generation via hierarchical disentangled self-attention",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_91",
            "content": "Grace Chung, Developing a flexible spoken dialog system using simulation, 2004, Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Grace Chung"
                ],
                "title": "Developing a flexible spoken dialog system using simulation",
                "pub_date": "2004",
                "pub_title": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_92",
            "content": "UNKNOWN, None, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_93",
            "content": "Ond\u0159ej Du\u0161ek, Filip Jurcicek, Sequence-tosequence generation for spoken dialogue via deep syntax trees and strings, 2016, Proceedings of the 54th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Ond\u0159ej Du\u0161ek",
                    "Filip Jurcicek"
                ],
                "title": "Sequence-tosequence generation for spoken dialogue via deep syntax trees and strings",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 54th",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_94",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "90-ARR_v1_95",
            "content": "Wieland Eckert, Esther Levin, Roberto Pieraccini, User modeling for spoken dialogue system evaluation, 1997, IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Wieland Eckert",
                    "Esther Levin",
                    "Roberto Pieraccini"
                ],
                "title": "User modeling for spoken dialogue system evaluation",
                "pub_date": "1997",
                "pub_title": "IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings",
                "pub": "IEEE"
            }
        },
        {
            "ix": "90-ARR_v1_96",
            "content": "Mihail Eric, D Christopher,  Manning, A copyaugmented sequence-to-sequence architecture gives good performance on task-oriented dialogue, 2017, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Mihail Eric",
                    "D Christopher",
                    " Manning"
                ],
                "title": "A copyaugmented sequence-to-sequence architecture gives good performance on task-oriented dialogue",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "90-ARR_v1_97",
            "content": "UNKNOWN, None, 2020, Rewriting history with inverse rl: Hindsight inference for policy improvement, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Rewriting history with inverse rl: Hindsight inference for policy improvement",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_98",
            "content": "Scott Fujimoto, David Meger, Doina Precup, Off-policy deep reinforcement learning without exploration, 2019, International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Scott Fujimoto",
                    "David Meger",
                    "Doina Precup"
                ],
                "title": "Off-policy deep reinforcement learning without exploration",
                "pub_date": "2019",
                "pub_title": "International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_99",
            "content": "Milica Ga\u0161i\u0107, Filip Jur\u010d\u00ed\u010dek, Blaise Thomson, Kai Yu, Steve Young, On-line policy optimisation of spoken dialogue systems via live interaction with human subjects, 2011, 2011 IEEE Workshop on Automatic Speech Recognition & Understanding, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Milica Ga\u0161i\u0107",
                    "Filip Jur\u010d\u00ed\u010dek",
                    "Blaise Thomson",
                    "Kai Yu",
                    "Steve Young"
                ],
                "title": "On-line policy optimisation of spoken dialogue systems via live interaction with human subjects",
                "pub_date": "2011",
                "pub_title": "2011 IEEE Workshop on Automatic Speech Recognition & Understanding",
                "pub": "IEEE"
            }
        },
        {
            "ix": "90-ARR_v1_100",
            "content": "Kallirroi Georgila, James Henderson, Oliver Lemon, User simulation for spoken dialogue systems: Learning and evaluation, 2006, Ninth International Conference on Spoken Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Kallirroi Georgila",
                    "James Henderson",
                    "Oliver Lemon"
                ],
                "title": "User simulation for spoken dialogue systems: Learning and evaluation",
                "pub_date": "2006",
                "pub_title": "Ninth International Conference on Spoken Language Processing",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_101",
            "content": "Kallirroi Georgila, David Traum, Reinforcement learning of argumentation dialogue policies in negotiation, 2011, Twelfth Annual Conference of the International Speech Communication Association, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Kallirroi Georgila",
                    "David Traum"
                ],
                "title": "Reinforcement learning of argumentation dialogue policies in negotiation",
                "pub_date": "2011",
                "pub_title": "Twelfth Annual Conference of the International Speech Communication Association",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_102",
            "content": "UNKNOWN, None, 2019, Learning to reach goals without reinforcement learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Learning to reach goals without reinforcement learning",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_103",
            "content": "He He, Derek Chen, Anusha Balakrishnan, Percy Liang, Decoupling strategy and generation in negotiation dialogues, 2018, Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "He He",
                    "Derek Chen",
                    "Anusha Balakrishnan",
                    "Percy Liang"
                ],
                "title": "Decoupling strategy and generation in negotiation dialogues",
                "pub_date": "2018",
                "pub_title": "Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_104",
            "content": "Peter A Heeman, Representing the reinforcement learning state in a negotiation dialogue, 2009, IEEE Workshop on Automatic Speech Recognition & Understanding, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    " Peter A Heeman"
                ],
                "title": "Representing the reinforcement learning state in a negotiation dialogue",
                "pub_date": "2009",
                "pub_title": "IEEE Workshop on Automatic Speech Recognition & Understanding",
                "pub": "IEEE"
            }
        },
        {
            "ix": "90-ARR_v1_105",
            "content": "UNKNOWN, None, 2020, A simple language model for task-oriented dialogue, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "A simple language model for task-oriented dialogue",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_106",
            "content": "UNKNOWN, None, 2019, Way off-policy batch deep reinforcement learning of implicit human preferences in dialog, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Way off-policy batch deep reinforcement learning of implicit human preferences in dialog",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_107",
            "content": "UNKNOWN, None, 2021, Towards automatic evaluation of dialog systems: A model-free off-policy evaluation approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Towards automatic evaluation of dialog systems: A model-free off-policy evaluation approach",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_108",
            "content": "Leslie Pack, Kaelbling , Learning to achieve goals, 1993, IJCAI, Citeseer.",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Leslie Pack",
                    "Kaelbling "
                ],
                "title": "Learning to achieve goals",
                "pub_date": "1993",
                "pub_title": "IJCAI",
                "pub": "Citeseer"
            }
        },
        {
            "ix": "90-ARR_v1_109",
            "content": "Leslie Pack Kaelbling, Anthony R Michael L Littman,  Cassandra, Planning and acting in partially observable stochastic domains, 1998, Artificial intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Leslie Pack Kaelbling",
                    "Anthony R Michael L Littman",
                    " Cassandra"
                ],
                "title": "Planning and acting in partially observable stochastic domains",
                "pub_date": "1998",
                "pub_title": "Artificial intelligence",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_110",
            "content": "Dongyeop Kang, Anusha Balakrishnan, Pararth Shah, A Paul, Y-Lan Crook, Jason Boureau,  Weston, Recommendation as a communication game: Self-supervised bot-play for goal-oriented dialogue, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Dongyeop Kang",
                    "Anusha Balakrishnan",
                    "Pararth Shah",
                    "A Paul",
                    "Y-Lan Crook",
                    "Jason Boureau",
                    " Weston"
                ],
                "title": "Recommendation as a communication game: Self-supervised bot-play for goal-oriented dialogue",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_111",
            "content": "Esther Levin, Roberto Pieraccini, Wieland Eckert, A stochastic model of human-machine interaction for learning dialog strategies, 2000, IEEE Transactions on speech and audio processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Esther Levin",
                    "Roberto Pieraccini",
                    "Wieland Eckert"
                ],
                "title": "A stochastic model of human-machine interaction for learning dialog strategies",
                "pub_date": "2000",
                "pub_title": "IEEE Transactions on speech and audio processing",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_112",
            "content": "UNKNOWN, None, 2020, Offline reinforcement learning: Tutorial, review, and perspectives on open problems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_113",
            "content": "Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh, Dhruv Batra, Deal or no deal? end-to-end learning of negotiation dialogues, 2017, Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Mike Lewis",
                    "Denis Yarats",
                    "Yann Dauphin",
                    "Devi Parikh",
                    "Dhruv Batra"
                ],
                "title": "Deal or no deal? end-to-end learning of negotiation dialogues",
                "pub_date": "2017",
                "pub_title": "Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_114",
            "content": "Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, Jianfeng Gao, Deep reinforcement learning for dialogue generation, 2016, Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Jiwei Li",
                    "Will Monroe",
                    "Alan Ritter",
                    "Dan Jurafsky",
                    "Michel Galley",
                    "Jianfeng Gao"
                ],
                "title": "Deep reinforcement learning for dialogue generation",
                "pub_date": "2016",
                "pub_title": "Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_115",
            "content": "Bing Liu, Gokhan T\u00fcr, Dilek Hakkani-T\u00fcr, Pararth Shah, Larry Heck, Dialogue learning with human teaching and feedback in end-to-end trainable task-oriented dialogue systems, 2018, Proceedings of NAACL-HLT, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Bing Liu",
                    "Gokhan T\u00fcr",
                    "Dilek Hakkani-T\u00fcr",
                    "Pararth Shah",
                    "Larry Heck"
                ],
                "title": "Dialogue learning with human teaching and feedback in end-to-end trainable task-oriented dialogue systems",
                "pub_date": "2018",
                "pub_title": "Proceedings of NAACL-HLT",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_116",
            "content": "Jingjing Liu, Stephanie Seneff, Victor Zue, Dialogue-oriented review summary generation for spoken dialogue recommendation systems, 2010, Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Jingjing Liu",
                    "Stephanie Seneff",
                    "Victor Zue"
                ],
                "title": "Dialogue-oriented review summary generation for spoken dialogue recommendation systems",
                "pub_date": "2010",
                "pub_title": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_117",
            "content": "UNKNOWN, None, 2021, Tapex: Table pre-training via learning a neural sql executor, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Tapex: Table pre-training via learning a neural sql executor",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_118",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_119",
            "content": "Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, Pierre Sermanet, Learning latent plans from play, 2020, Conference on Robot Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Corey Lynch",
                    "Mohi Khansari",
                    "Ted Xiao",
                    "Vikash Kumar",
                    "Jonathan Tompson",
                    "Sergey Levine",
                    "Pierre Sermanet"
                ],
                "title": "Learning latent plans from play",
                "pub_date": "2020",
                "pub_title": "Conference on Robot Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "90-ARR_v1_120",
            "content": "F Michael,  Mctear, Spoken dialogue technology: enabling the conversational user interface, 2002, ACM Computing Surveys (CSUR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "F Michael",
                    " Mctear"
                ],
                "title": "Spoken dialogue technology: enabling the conversational user interface",
                "pub_date": "2002",
                "pub_title": "ACM Computing Surveys (CSUR)",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_121",
            "content": "Hongyuan Mei, Mohit Bansal, Matthew R Walter, Coherent dialogue with attention-based language models, 2017, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Hongyuan Mei",
                    "Mohit Bansal",
                    "Matthew R Walter"
                ],
                "title": "Coherent dialogue with attention-based language models",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "90-ARR_v1_122",
            "content": "UNKNOWN, None, , Shahin Shayandeh, Lars Liden, and Jianfeng Gao. 2020. Soloist: Few-shot task-oriented dialog with a single pretrained auto-regressive model, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Shahin Shayandeh, Lars Liden, and Jianfeng Gao. 2020. Soloist: Few-shot task-oriented dialog with a single pretrained auto-regressive model",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_123",
            "content": "Olivier Pietquin, Matthieu Geist, Senthilkumar Chandramohan, Herv\u00e9 Frezza-Buet, Sampleefficient batch reinforcement learning for dialogue management optimization, 2011, ACM Transactions on Speech and Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Olivier Pietquin",
                    "Matthieu Geist",
                    "Senthilkumar Chandramohan",
                    "Herv\u00e9 Frezza-Buet"
                ],
                "title": "Sampleefficient batch reinforcement learning for dialogue management optimization",
                "pub_date": "2011",
                "pub_title": "ACM Transactions on Speech and Language Processing",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_124",
            "content": "UNKNOWN, None, 1989, Alvinn: An autonomous land vehicle in a neural network, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": null,
                "title": null,
                "pub_date": "1989",
                "pub_title": "Alvinn: An autonomous land vehicle in a neural network",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_125",
            "content": "UNKNOWN, None, 2018, Temporal difference models: Modelfree deep rl for model-based control, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Temporal difference models: Modelfree deep rl for model-based control",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_126",
            "content": "UNKNOWN, None, 2018, Language models are unsupervised multitask learners, OpenAI Blog.",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Language models are unsupervised multitask learners",
                "pub": "OpenAI Blog"
            }
        },
        {
            "ix": "90-ARR_v1_127",
            "content": "UNKNOWN, None, 2018, Semi-parametric topological memory for navigation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Semi-parametric topological memory for navigation",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_128",
            "content": "Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye, Steve Young, Agenda-based user simulation for bootstrapping a pomdp dialogue system, 2007, Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Jost Schatzmann",
                    "Blaise Thomson",
                    "Karl Weilhammer",
                    "Hui Ye",
                    "Steve Young"
                ],
                "title": "Agenda-based user simulation for bootstrapping a pomdp dialogue system",
                "pub_date": "2007",
                "pub_title": "Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_129",
            "content": "Satinder Singh, Michael Kearns, Diane Litman, Marilyn Walker, Reinforcement learning for spoken dialogue systems, 1999, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Satinder Singh",
                    "Michael Kearns",
                    "Diane Litman",
                    "Marilyn Walker"
                ],
                "title": "Reinforcement learning for spoken dialogue systems",
                "pub_date": "1999",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_130",
            "content": "Satinder Singh, Diane Litman, Michael Kearns, Marilyn Walker, Optimizing dialogue management with reinforcement learning: Experiments with the njfun system, 2002, Journal of Artificial Intelligence Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Satinder Singh",
                    "Diane Litman",
                    "Michael Kearns",
                    "Marilyn Walker"
                ],
                "title": "Optimizing dialogue management with reinforcement learning: Experiments with the njfun system",
                "pub_date": "2002",
                "pub_title": "Journal of Artificial Intelligence Research",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_131",
            "content": "UNKNOWN, None, 1994, Spoken natural language dialog systems: A practical approach, Oxford University Press on Demand.",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": null,
                "title": null,
                "pub_date": "1994",
                "pub_title": "Spoken natural language dialog systems: A practical approach",
                "pub": "Oxford University Press on Demand"
            }
        },
        {
            "ix": "90-ARR_v1_132",
            "content": "Pei-Hao Su, Pawe\u0142 Budzianowski, Stefan Ultes, Milica Gasic, Steve Young, Sample-efficient actor-critic reinforcement learning with supervised data for dialogue management, 2017, Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Pei-Hao Su",
                    "Pawe\u0142 Budzianowski",
                    "Stefan Ultes",
                    "Milica Gasic",
                    "Steve Young"
                ],
                "title": "Sample-efficient actor-critic reinforcement learning with supervised data for dialogue management",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_133",
            "content": "UNKNOWN, None, 2016, Continuously learning neural dialogue management, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Continuously learning neural dialogue management",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_134",
            "content": "Martin Sundermeyer, Ralf Schl\u00fcter, Hermann Ney, Lstm neural networks for language modeling, 2012, Thirteenth annual conference of the international speech communication association, .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Martin Sundermeyer",
                    "Ralf Schl\u00fcter",
                    "Hermann Ney"
                ],
                "title": "Lstm neural networks for language modeling",
                "pub_date": "2012",
                "pub_title": "Thirteenth annual conference of the international speech communication association",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_135",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_136",
            "content": "UNKNOWN, None, 2020, Modelling hierarchical structure between dialogue policy and natural language generator with option framework for task-oriented dialogue system, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Modelling hierarchical structure between dialogue policy and natural language generator with option framework for task-oriented dialogue system",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_137",
            "content": "UNKNOWN, None, , Caglar Gulcehre, Nicolas Heess, et al. 2020b. Critic regularized regression, .",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Caglar Gulcehre, Nicolas Heess, et al. 2020b. Critic regularized regression",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_138",
            "content": "Wei Wei, Quoc Le, Andrew Dai, Jia Li, Airdialogue: An environment for goal-oriented dialogue research, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": [
                    "Wei Wei",
                    "Quoc Le",
                    "Andrew Dai",
                    "Jia Li"
                ],
                "title": "Airdialogue: An environment for goal-oriented dialogue research",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_139",
            "content": "UNKNOWN, None, 2014, The dialog state tracking challenge series, .",
            "ntype": "ref",
            "meta": {
                "xid": "b55",
                "authors": null,
                "title": null,
                "pub_date": "2014",
                "pub_title": "The dialog state tracking challenge series",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_140",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b56",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_141",
            "content": "UNKNOWN, None, 2019, Alternating recurrent dialog model with large-scale pre-trained language models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b57",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Alternating recurrent dialog model with large-scale pre-trained language models",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_142",
            "content": "UNKNOWN, None, 2019, Behavior regularized offline reinforcement learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b58",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Behavior regularized offline reinforcement learning",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_143",
            "content": "Steve Young, Milica Ga\u0161i\u0107, Blaise Thomson, Jason Williams, Pomdp-based statistical spoken dialog systems: A review, 2013, Proceedings of the IEEE, .",
            "ntype": "ref",
            "meta": {
                "xid": "b59",
                "authors": [
                    "Steve Young",
                    "Milica Ga\u0161i\u0107",
                    "Blaise Thomson",
                    "Jason Williams"
                ],
                "title": "Pomdp-based statistical spoken dialog systems: A review",
                "pub_date": "2013",
                "pub_title": "Proceedings of the IEEE",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_144",
            "content": "Zhou Yu, Ziyu Xu, Alan Black, Alexander Rudnicky, Strategy and policy learning for nontask-oriented conversational systems, 2016, Proceedings of the 17th annual meeting of the special interest group on discourse and dialogue, .",
            "ntype": "ref",
            "meta": {
                "xid": "b60",
                "authors": [
                    "Zhou Yu",
                    "Ziyu Xu",
                    "Alan Black",
                    "Alexander Rudnicky"
                ],
                "title": "Strategy and policy learning for nontask-oriented conversational systems",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 17th annual meeting of the special interest group on discourse and dialogue",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_145",
            "content": "Tangming Yuan, David Moore, Alec Grierson, A human-computer dialogue system for educational debate: A computational dialectics approach, 2008, International Journal of Artificial Intelligence in Education, .",
            "ntype": "ref",
            "meta": {
                "xid": "b61",
                "authors": [
                    "Tangming Yuan",
                    "David Moore",
                    "Alec Grierson"
                ],
                "title": "A human-computer dialogue system for educational debate: A computational dialectics approach",
                "pub_date": "2008",
                "pub_title": "International Journal of Artificial Intelligence in Education",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_146",
            "content": "Yichi Zhang, Zhijian Ou, Zhou Yu, Taskoriented dialog systems that consider multiple appropriate responses under the same context, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b62",
                "authors": [
                    "Yichi Zhang",
                    "Zhijian Ou",
                    "Zhou Yu"
                ],
                "title": "Taskoriented dialog systems that consider multiple appropriate responses under the same context",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "90-ARR_v1_147",
            "content": "Tiancheng Zhao, Kaige Xie, Maxine Eskenazi, Rethinking action spaces for reinforcement learning in end-to-end dialog agents with latent variable models, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b63",
                "authors": [
                    "Tiancheng Zhao",
                    "Kaige Xie",
                    "Maxine Eskenazi"
                ],
                "title": "Rethinking action spaces for reinforcement learning in end-to-end dialog agents with latent variable models",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "90-ARR_v1_0@0",
            "content": "Context-Aware Language Modeling for Goal-Oriented Dialogue Systems",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_0",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_2@0",
            "content": "Goal-oriented dialogue systems face a tradeoff between fluent language generation and task-specific control.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_2",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_2@1",
            "content": "While supervised learning with large language models is capable of producing realistic text, how to steer such responses towards completing a specific task without sacrificing language quality remains an open question.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_2",
            "start": 109,
            "end": 326,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_2@2",
            "content": "In this work, we formulate goal-oriented dialogue as a partially observed Markov decision process, interpreting the language model as a representation of both the dynamics and the policy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_2",
            "start": 328,
            "end": 514,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_2@3",
            "content": "This view allows us to extend techniques from learning-based control, such as task relabeling, to derive a simple and effective method to finetune language models in a goal-aware way, leading to significantly improved task performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_2",
            "start": 516,
            "end": 750,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_2@4",
            "content": "We additionally introduce a number of training strategies that serve to better focus the model on the task at hand.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_2",
            "start": 752,
            "end": 866,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_2@5",
            "content": "We evaluate our method, Context-Aware Language Models (CALM), on a practical flightbooking task using AirDialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_2",
            "start": 868,
            "end": 981,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_2@6",
            "content": "Empirically, CALM outperforms state-of-the-art method by 7% in terms of task success, matching humanlevel task performance on this dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_2",
            "start": 983,
            "end": 1121,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_4@0",
            "content": "Dialogue systems have typically approached the problem of generating realistic dialogue from the perspective of supervised learning (Du\u0161ek and Jurcicek, 2016;Eric and Manning, 2017;Mei et al., 2017;Wu et al., 2019a;Hosseini-Asl et al., 2020;Peng et al., 2020;Adiwardana et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_4",
            "start": 0,
            "end": 283,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_4@1",
            "content": "However, dialogue can also be viewed as a sequential decision making process which is well-suited to planning and reinforcement learning (RL) algorithms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_4",
            "start": 285,
            "end": 437,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_4@2",
            "content": "A challenge with the classical RL approach to dialogue is the requirement for active interaction with humans (Ga\u0161i\u0107 et al., 2011).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_4",
            "start": 439,
            "end": 568,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_4@3",
            "content": "Training such a system with active human-in-the-loop interaction quickly becomes expensive and cumbersome, making it desirable to develop techniques for goal-directed training of dialogue systems that can effectively leverage offline data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_4",
            "start": 570,
            "end": 808,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_5@0",
            "content": "While many dialogue generation techniques based on RL and learned control have been proposed (Eckert et al., 1997;Levin et al., 2000;Chung, 2004;Georgila et al., 2006;Schatzmann et al., 2007;Heeman, 2009;Georgila and Traum, 2011), most such systems take a pipelined approach, where an abstract representation of states and actions is designed by hand and then combined with RL to train a \"dialogue management\" system, rather than generating dialogue end-to-end.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_5",
            "start": 0,
            "end": 460,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_5@1",
            "content": "These pipelined approaches rely on a manually designed decomposition of the dialogue task, which may be domain-specific and, more importantly, may not enjoy all of the benefits of tightly integrating lowlevel text generation with the overall goals of the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_5",
            "start": 462,
            "end": 721,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_5@2",
            "content": "In this work, we instead ask: how can we scalably and effectively introduce the mechanisms of goal-directed decision making into end-to-end language models, to directly steer language generation toward completing specific dialogue tasks rather than simply generating probable responses?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_5",
            "start": 723,
            "end": 1008,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_6@0",
            "content": "To this end, rather than utilizing a pipelined approach, we aim to directly finetune language models in a task-aware manner such that they can maximize a given utility function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_6",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_6@1",
            "content": "We observe that large language models can already be formulated within a Markov decision processes (MDP) as capturing both the dynamics and policy for a decision-making task, where dialogue history serves as state, and the agent's utterances serve as actions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_6",
            "start": 178,
            "end": 436,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_6@2",
            "content": "We could utilize this observation by finetuning the models directly with online RL, but the need for human-inthe-loop training makes this difficult.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_6",
            "start": 438,
            "end": 585,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_6@3",
            "content": "Offline RL methods Fujimoto et al., 2019;Wu et al., 2019b;Wang et al., 2020b) provide an alternative approach, but typically require value function estimation, which is not straightforward to perform with a language model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_6",
            "start": 587,
            "end": 808,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_6@4",
            "content": "Instead, we propose a conditional imitation learning strategy coupled with a novel task relabeling approach that can finetune language models from offline data, such that the model still represents the joint distribution over dialogues, but tilts this distribution toward dialogues with a high reward.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_6",
            "start": 810,
            "end": 1110,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_6@5",
            "content": "This amounts to a task-aware finetuning strategy that integrates task information into the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_6",
            "start": 1112,
            "end": 1208,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_7@0",
            "content": "The main contribution of our work is CALM (Context-Aware Language Modeling), a framework for end-to-end goal-directed dialogue generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_7",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_7@1",
            "content": "CALM unifies the traditional language modeling objective with task-specific supervision, where a language model is interpreted as a joint representation of dynamics and policies in an MDP, and the finetuning process utilizes a conditional imitation learning objective with a novel task relabeling strategy that teaches the model how to generate high-utility dialogues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_7",
            "start": 139,
            "end": 506,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_7@2",
            "content": "Because CALM interprets the language model as both a dynamics model and a policy, it can be used as either a model-free method, where the dynamics are discarded and the policy component is used to greedily generate responses, or as a model-based method, where the dynamics component can be used to plan at test-time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_7",
            "start": 508,
            "end": 823,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_7@3",
            "content": "We empirically evaluate CALM on AirDialogue (Wei et al., 2018), the largest dataset for goal-oriented dialogue based-on a flight-booking task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_7",
            "start": 825,
            "end": 966,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_7@4",
            "content": "CALM improves the task success by 10% over the previous state-of-the-art method (Chen et al., 2020) following the evaluation protocol proposed by Wei et al. (2018), achieving the first-ever human-level performance on this dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_7",
            "start": 968,
            "end": 1197,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_8@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_8",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_9@0",
            "content": "Our goal is to enable end-to-end training of goaldirected dialogue agents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_9",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_9@1",
            "content": "In these settings, an agent aims to complete a particular task with its utterances (Smith and Hipp, 1994).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_9",
            "start": 75,
            "end": 180,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_9@2",
            "content": "Goal-directed agents have been explored in contexts such as personal assistants (McTear, 2002;Budzianowski et al., 2018;Williams et al., 2014), recommendation systems (Liu et al., 2010;Kang et al., 2019), education (Yuan et al., 2008), and negotiation (He et al., 2018;Lewis et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_9",
            "start": 182,
            "end": 470,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_9@3",
            "content": "While there are multiple approaches to constructing dialogue agents, in this work we frame the problem of generating dialogue as a sequential decision making problem within a (partially observed) Markov Decision Process (MDP) (Singh et al., 1999;Young et al., 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_9",
            "start": 472,
            "end": 737,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_9@4",
            "content": "Prior works that utilize such an MDP formulation typically aim to train a dialogue management system (Singh et al., 2002), in which the agent reasons about higher-level abstractions of the state of the conversation, and language generation is performed using a downstream procedure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_9",
            "start": 739,
            "end": 1020,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_9@5",
            "content": "Dialogue management systems have been trained using techniques such as online reinforcement learning via policy gradients (Ga\u0161i\u0107 et al., 2011;He et al., 2018), off-policy reinforcement learning (Pietquin et al., 2011;Yu et al., 2016) or actor-critic methods (Su et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_9",
            "start": 1022,
            "end": 1297,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_9@6",
            "content": "Our method differs from dialogue management systems in that CALM is an end-to-end system optimized for successful task completion, and performs both high-level decision making and language generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_9",
            "start": 1299,
            "end": 1498,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_10@0",
            "content": "Recent advancements in language models, such as recurrent neural networks (Sundermeyer et al., 2012;Asri et al., 2016;Su et al., 2016;Zhao et al., 2019;Wang et al., 2020a;Zhang et al., 2020) and attention-based architectures (Vaswani et al., 2017;Liu et al., 2019;Devlin et al., 2018;Brown et al., 2020), have spurred increasing interest in such endto-end dialogue systems (Hosseini-Asl et al., 2020;Peng et al., 2020;Adiwardana et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_10",
            "start": 0,
            "end": 442,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_10@1",
            "content": "Modelbased approaches, in which a learned agent is substituted for a human, allow learning to be done entirely within simulation without human intervention (Li et al., 2016;He et al., 2018;Kang et al., 2019;Lewis et al., 2017;Liu et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_10",
            "start": 444,
            "end": 687,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_10@2",
            "content": "In contrast to these approaches, CALM augments the traditional language modeling objective with taskspecific rewards in order to finetune a model that is more aware of task goals, which significantly improves performance over a na\u00efve language model without the need for simulating human responses in an interactive training loop.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_10",
            "start": 689,
            "end": 1017,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_10@3",
            "content": "Jaques et al. (2019) recently proposed a model-free, offline approach to undirected dialogue, or dialogue without a specific task goal.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_10",
            "start": 1019,
            "end": 1153,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_10@4",
            "content": "Our method differs in that we aim to solve goal-oriented dialogue which allows us to optimize task-specific objectives, and that we take a model-based RL approach which enables us to leverage fine-tuned language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_10",
            "start": 1155,
            "end": 1373,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_11@0",
            "content": "Preliminaries",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_11",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_12@0",
            "content": "In this section, we review our notation and problem formulation for casting dialogue within a sequential decision making framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_12",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_12@1",
            "content": "POMDP formulation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_12",
            "start": 132,
            "end": 149,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_12@2",
            "content": "We formulate dialogue generation as a partially observable Markov decision process (POMDP) (Kaelbling et al., 1998), with a state that consists of known and unknown context information about the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_12",
            "start": 151,
            "end": 350,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_12@3",
            "content": "Let c h \u2208 C (h) denote the hidden context for the task, and let c o \u2208 C (o) denote the observed context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_12",
            "start": 352,
            "end": 455,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_12@4",
            "content": "For in- We apply Task Relabeling to our static offline dataset, by swapping out the task context -in this case a flight table -such that the attached dialogue becomes an optimal example of task completion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_12",
            "start": 457,
            "end": 661,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_12@5",
            "content": "When fine-tuning on this relabeled dataset, we then apply a Task Specific Auxilary Loss on top of the standard language modeling objective; this helps the model learn to use the task context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_12",
            "start": 663,
            "end": 853,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_12@6",
            "content": "Once trained, CALM can consistently solve complex tasks in dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_12",
            "start": 855,
            "end": 922,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_13@0",
            "content": "stance, in a flight booking task, a table of available flights might correspond to c o , while the particular flight that the human wants to book, which is unknown to the agent, corresponds to c h .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_13",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_13@1",
            "content": "Note that the reward, which requires booking the right flight, depends on both hidden and observed contexts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_13",
            "start": 199,
            "end": 306,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_13@2",
            "content": "We can define such an environment as a POMDP M = (S, A, O, T , Z, \u00b5 0 , R, \u03b3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_13",
            "start": 308,
            "end": 385,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_13@3",
            "content": "We denote a conversation \u03c4 as \u03c4 := {a 0 , e 0 , ..., a T }, where T denotes the number of turns in a conversation and a t and e t represent utterances (strings of tokens) from the dialogue agent (a t ) and the human (e t ) at the t-th turn, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_13",
            "start": 387,
            "end": 640,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_13@4",
            "content": "We additionally use \u03c4 <t to denote conversation history up to the t-th turn.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_13",
            "start": 642,
            "end": 717,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_13@5",
            "content": "We can represent the underlying POMDP state s t \u2208 S as the concatenation of both of the contexts and the previous conversation history s t := {c h , c o , \u03c4 <t } = {c h , c o , a 0 , e 0 , ..., a t\u22121 , e t\u22121 }.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_13",
            "start": 719,
            "end": 928,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_13@6",
            "content": "However, we only observe the last two elements of the state tuple, such that our observation o t \u2208 O at the t-th conversation turn is o t = {c o , \u03c4 <t }.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_13",
            "start": 930,
            "end": 1083,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_13@7",
            "content": "An action a t \u2208 A is the agent's response to the current state s t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_13",
            "start": 1085,
            "end": 1152,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_13@8",
            "content": "Given our definition of the state, the full conversation in a dialogue can be conveniently represented by the last observation and action, {o T , a T }.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_13",
            "start": 1154,
            "end": 1305,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_13@9",
            "content": "An agent \u03c0 : O \u2192 P(A) maps observations to sets of probability measures over the action space P(\u2022).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_13",
            "start": 1307,
            "end": 1405,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_13@10",
            "content": "A transition function T (\u2022|s t , a t ), represents a distribution over the human's utterances, returning s t+1 as the state at turn t + 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_13",
            "start": 1407,
            "end": 1544,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_13@11",
            "content": "We only consider the sparse reward setting with r T = R(s T , a T ) \u2208 {0, 1} denoting task completion, and r t = 0, \u2200t < T .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_13",
            "start": 1546,
            "end": 1669,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_13@12",
            "content": "Our final reward is therefore dependent on both the context and the dialogue: R(s T , a T ) = R(\u03c4, c h , c o ), where the context {c o , c h } is randomly sampled for each dialogue from some initial distribution \u00b5 0 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_13",
            "start": 1671,
            "end": 1887,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_13@13",
            "content": "Goal-oriented dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_13",
            "start": 1889,
            "end": 1911,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_13@14",
            "content": "Goal-oriented dialogue systems aim to maximize the expected reward of the above POMDP",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_13",
            "start": 1913,
            "end": 1997,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_14@0",
            "content": "E {co,c h }\u223c\u00b5 0 ,\u03c0,T [ T t=0 \u03b3 t R(s t , a t )],",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_14",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_15@0",
            "content": "(1) where {c h , c o } is sampled from distribution \u00b5 0 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_15",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_15@1",
            "content": "Onpolicy RL algorithms optimize this objective via environment interaction, which is represented by a real human.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_15",
            "start": 58,
            "end": 170,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_15@2",
            "content": "However, because human-in-the-loop training is expensive, we pursue an offline learning approach where we are given a fixed dataset and there is no further interaction with the human in the learning process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_15",
            "start": 172,
            "end": 378,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_15@3",
            "content": "This dataset is composed of n trajectories with D off = {c",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_15",
            "start": 380,
            "end": 437,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_16@0",
            "content": "(i) h , c (i) o , \u03c4 (i) , r (i) } n i=1 with each \u03c4 (i) = {a (i) 0 , e (i) 0 , , ..., a(i)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_16",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_17@0",
            "content": "T } and its corresponding final reward for task completion r (i) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_17",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_17@1",
            "content": "Our goal is to learn the policy \u03c0(a|o) which improves the dialog agent's ability in achieving the highest task reward defined in Equation 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_17",
            "start": 67,
            "end": 206,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_17@2",
            "content": "Language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_17",
            "start": 208,
            "end": 223,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_17@3",
            "content": "While conventionally a language model is seen simply as a sequence model over tokens of the form T t=1 p(x t+1 |x 1:t ), when the sequence x 1:T corresponds to a dialogue trajectory \u03c4 , we can also interpret a language model as learning the distribution over \u03c4 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_17",
            "start": 225,
            "end": 486,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_17@4",
            "content": "This distribution can be factored into the product of the policy \u03c0(a t |\u03c4 <t ) and the dynamics T (\u03c4 <t+1 |\u03c4 <t , a t ), and so we can say that a language model also represents the policy and the dynamics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_17",
            "start": 488,
            "end": 692,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_17@5",
            "content": "Therefore, the maximum likelihood objective for training or finetuning a language model on a dialogue dataset D off consisting of dialogue trajectories \u03c4 can be written as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_17",
            "start": 694,
            "end": 864,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_18@0",
            "content": "L LM (\u03b8) = max \u03b8 E \u03c4 \u223cD off T t=1 log \u03c0 \u03b8 (a t |\u03c4 <t ) + log T \u03b8 (\u03c4 <t+1 |\u03c4 <t , a t ) ,(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_18",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_19@0",
            "content": "where \u03c0 \u03b8 (a t |o t ) represents a policy that generates new dialogue based on the observed context and dialogue history, and T \u03b8 (\u03c4 <t+1 |\u03c4 <t , a t ) represents the observed dynamics characterizing human responses, and \u03b8 denotes parameters in \u03c0 and T .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_19",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_19@1",
            "content": "Note that \u03c4 <t consists only of the conversation history, and does not contain any task-specific context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_19",
            "start": 255,
            "end": 359,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_20@0",
            "content": "A na\u00efve approach to train dialogue systems is to jointly parameterize both \u03c0 and T as one language model, and optimize Equation 2 on pre-collected conversations D off .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_20",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_20@1",
            "content": "This method corresponds to behavioral cloning (BC) (Pomerleau, 1989).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_20",
            "start": 169,
            "end": 237,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_21@0",
            "content": "Context conditioning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_21",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_21@1",
            "content": "While an agent trained using Equation 2 can learn policies and dynamics that imitate human conversations, this objective does not incorporate the task goal, and may not produce a policy that is more performant than the dataset D off .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_21",
            "start": 22,
            "end": 255,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_21@2",
            "content": "While it is possible to input c o into the language model to maximize the conditional probability of P (\u03c4 |c o ) using a conditional version of the language modeling objective,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_21",
            "start": 257,
            "end": 432,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_22@0",
            "content": "L CT X (\u03b8) = max \u03b8 E (\u03c4,co)\u223cD off T t=1 log \u03c0 \u03b8 (a t |\u03c4 <t , c o ) + log T \u03b8 (o t+1 |\u03c4 <t , a t , c o ) ,(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_22",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_23@0",
            "content": "contexts with particular task structures (e.g., a set of entries in a table) may not be simply processed as a sequence similarly to \u03c4 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_23",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_23@1",
            "content": "Additionally, the language model is not pretrained to read structured context, and oftentimes the recent dialogue history is much more predictive of the next utterance than the task context is.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_23",
            "start": 136,
            "end": 328,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_23@2",
            "content": "As a result, language models can ignore the task context and only learn P (\u03c4 ) despite being conditioned on c o .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_23",
            "start": 330,
            "end": 442,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_23@3",
            "content": "Our approach builds on this conditional modeling approach, but makes a number of improvements that allow it to be more aware of the context information, which attains significantly better results in our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_23",
            "start": 444,
            "end": 658,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_24@0",
            "content": "Context-Aware Language Modeling",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_24",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_25@0",
            "content": "In this section, we present our method for goaloriented dialogue systems, Context-Aware Language Modeling (CALM).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_25",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_25@1",
            "content": "CALM interprets a language model as a combination of a policy and a dynamics model in the POMDP formulation of a dialogue task, as described in Section 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_25",
            "start": 114,
            "end": 267,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_25@2",
            "content": "Under this interpretation, na\u00efve supervised finetuning on the dialogue dataset can be viewed as behavioral cloning (BC) (Pomerleau, 1989).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_25",
            "start": 269,
            "end": 406,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_25@3",
            "content": "However, BC only imitates data and does not necessarily produce a good policy in terms of completing tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_25",
            "start": 408,
            "end": 514,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_25@4",
            "content": "We propose to improve the policy by utilizing a task relabeling strategy (described in Section 4.1), analogous to prior task relabeling approaches (Kaelbling, 1993;Andrychowicz et al., 2017;Pong et al., 2018;Savinov et al., 2018;Ghosh et al., 2019;Lynch et al., 2020;Eysenbach et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_25",
            "start": 516,
            "end": 806,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_25@5",
            "content": "This relabeling procedure augments the data with examples of near-optimal utterances, making the language model more task-aware.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_25",
            "start": 808,
            "end": 935,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_25@6",
            "content": "However, we find several shortcomings with this approach alone and propose the following improvements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_25",
            "start": 937,
            "end": 1038,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_25@7",
            "content": "First, an expressive language model is liable to ignore the task context, which we address by proposing an auxiliary loss (Section 4.2) that forces the model to utilize this information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_25",
            "start": 1040,
            "end": 1225,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_25@8",
            "content": "Second, learning from structured task information is difficult and can result in models that fail to capture complex task structure, so we propose a task pre-training procedure to improve the learnability (Section 4.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_25",
            "start": 1227,
            "end": 1445,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_25@9",
            "content": "Finally, to further improve performance we use a model-based planning procedure (Section 4.4) on top of the proposed method that samples multiple dialogues in parallel and selects the most promising candidates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_25",
            "start": 1447,
            "end": 1656,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_26@0",
            "content": "Dialogue Task Relabeling",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_26",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_27@0",
            "content": "L CT X (\u03b8) defines a context-conditional maximum likelihood objective for training an expert imitation policy in conjunction with a dynamics model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_27",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_27@1",
            "content": "However, simply imitating all the dialogue data does not necessarily produce the best possible policy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_27",
            "start": 148,
            "end": 249,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_27@2",
            "content": "We would like to learn a policy that produces dialogue that is more optimal, in the sense of better maximizing the task utility, than the average dialogue in the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_27",
            "start": 251,
            "end": 420,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_27@3",
            "content": "Task relabeling enables us to learn from optimal trajectories without simply filtering the dataset for high-reward trajectories, which would unnecessarily discard potentially informative data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_27",
            "start": 422,
            "end": 613,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_27@4",
            "content": "In the case of dialogue, we can perform task relabeling by considering the context {c o , c h } as defining the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_27",
            "start": 615,
            "end": 731,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_27@5",
            "content": "While a given dialogue may be unsuccessful for the context for which it was collected, it could be considered successful under a different context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_27",
            "start": 733,
            "end": 879,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_27@6",
            "content": "In this case, we can simply swap out {c o , c h } to create optimal task examples from the many sub-optimal examples provided by D off .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_27",
            "start": 881,
            "end": 1016,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_27@7",
            "content": "Since our reward R(c h , c o , \u03c4 ) is a function of the dialogue and context, we can modify the reward for a given dialogue just by changing the given observed context c o .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_27",
            "start": 1018,
            "end": 1190,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_27@8",
            "content": "Using this observation, we can relabel unsuccessful dialogues with successful ones, and even for already successful dialogues there may be multiple c o corresponding to task success, allowing us to augment the number of successful (c h , c o , \u03c4 ) tuples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_27",
            "start": 1192,
            "end": 1446,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_28@0",
            "content": "Formally, since our POMDP includes a prior distribution over contexts {c h , c o } \u223c \u00b5 0 , there exists a posterior q(c o |\u03c4, c h ) over observed contexts that correspond to optimal task completion under a given \u03c4 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_28",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_28@1",
            "content": "We can then re-label \u03c4 to be optimal under its context by sampling a new c o from q(c o |\u03c4, c h ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_28",
            "start": 216,
            "end": 313,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_28@2",
            "content": "In practice, this sampling is performed by rejection sampling from either \u00b5 0 or some P (c o |c h ); the latter, lower entropy distribution, can be preferred if there is a low probability of sampling valid, highreward contexts under \u00b5 0 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_28",
            "start": 315,
            "end": 552,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_28@3",
            "content": "Now, given any \u03c4 from an offline dataset of dialogues, we can learn from the full distribution of contexts corresponding to optimal task completion under this dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_28",
            "start": 554,
            "end": 721,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_29@0",
            "content": "In order for this relabeling procedure not to bias our policy towards behavior that is overlyoptimistic about the user's responses, it is necessary that the distribution of these responses in our dataset does not depend on the portion of the context that is relabeled.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_29",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_29@1",
            "content": "For example, relabeling the table of available flights for a flight booking task should generally be reasonable, because the user is usually unaware of the flight table.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_29",
            "start": 269,
            "end": 437,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_29@2",
            "content": "On the other hand, relabeling the desired flight would not make sense, since the user's utterances are strongly depend on this.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_29",
            "start": 439,
            "end": 565,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_29@3",
            "content": "To provide another example, in a bargaining task (Lewis et al., 2017), the agent might fail to obtain the desired item and instead get an item of lesser value. But relabeling with a context that assigns a higher value to the item received would not lead to a reasonable example, since the agent mainly received this item as a result of the user's responses rather than as a result of their own bargaining skill.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_29",
            "start": 567,
            "end": 977,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_30@0",
            "content": "Methods based on similar principles have previously been proposed in the deep RL community for simple parametric tasks, such as goal-reaching or linearly-parameterized reward functions (Kaelbling, 1993;Andrychowicz et al., 2017;Eysenbach et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_30",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_30@1",
            "content": "However, the dialogue task relabeling that we employ is particularly effective in our setting, since there may be exponentially many contexts that are optimal for a given dialogue (e.g., many different flight tables for a flight booking task), in contrast to the simpler task parameterizations used in prior work, where for example only one goal might be optimal for a given trajectory (the one that is reached).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_30",
            "start": 253,
            "end": 664,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_30@2",
            "content": "Additionally, the structure of our specific task allows us to sample from the true posterior q(c o |\u03c4, c h ), by using the rejection sampling procedure described above, rather than using approximations as in prior work (Eysenbach et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_30",
            "start": 666,
            "end": 909,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_30@3",
            "content": "As a result, this technique not only allows us to turn sub-optimal task data into optimal data, but it also allows us to greatly increase the number of optimal task examples from which we can learn, which we will show leads to a large performance improvement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_30",
            "start": 911,
            "end": 1169,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_31@0",
            "content": "Task-Specific Auxiliary Loss",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_31",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_32@0",
            "content": "Goal-oriented dialogue generation can be viewed as learning the conditional distribution P (\u03c4 |c o ), where \u03c4 represents the generated dialogue given a specific context c o .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_32",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_32@1",
            "content": "However when trained na\u00efvely, language models are liable to ignore this conditioning context, instead focusing purely on the previous utterances in the dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_32",
            "start": 175,
            "end": 335,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_32@2",
            "content": "In this case, the model is effectively only learning P (\u03c4 ) despite having both the capacity and the context to learn the lowerentropy conditional distribution P (\u03c4 |c o ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_32",
            "start": 337,
            "end": 508,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_33@0",
            "content": "While dialogue tasks are by definition carried out through natural language, there is often an abstract high-level action \u03b1 h \u2208 A that essentially determines the success of the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_33",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_33@1",
            "content": "In the case of the information retrieval task that we consider in this paper, these high-level actions correspond to deciding which database entity to retrieve for the user (e.g. suggesting a flight to the customer that meets all of their needs).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_33",
            "start": 183,
            "end": 428,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_33@2",
            "content": "While these high-level actions are theoretically learnable from correlations between the dialogue and the given context, in general, we find that learning these correlations corresponds to a relatively small decrease in dialogue entropy under the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_33",
            "start": 430,
            "end": 682,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_33@3",
            "content": "As a result, the model is less incentivized to learn these correlations relevant to the task than the form of the dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_33",
            "start": 684,
            "end": 806,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_33@4",
            "content": "To address this issue, we incorporate an auxiliary objective into our training, which trains the model directly to predict the abstract high-level actions taken in the present dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_33",
            "start": 808,
            "end": 992,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_33@5",
            "content": "This objective effectively up-weights gradients relevant for learning the high-level actions, which further helps the model to utilize the context to solve the high-level task through dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_33",
            "start": 994,
            "end": 1186,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_34@0",
            "content": "For a given dialogue-context pair (\u03c4, {c h , c o }) and high-level action, \u03b1 h , our auxiliary objective is then simply to maximize the likelihood of the high-level actions taken in the dialogue:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_34",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_35@0",
            "content": "C(\u03d5) = max \u03d5 E (c h ,co,\u03c4,\u03b1 h )\u223cD off log P \u03d5 (\u03b1 h |\u03c4, c o ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_35",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_36@0",
            "content": "(4) Just like the language modeling objective, this classification objective is averaged over each token in the dialogue sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_36",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_36@1",
            "content": "Our full training objective then becomes:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_36",
            "start": 131,
            "end": 171,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_37@0",
            "content": "max \u03b8,\u03d5 L CT X (\u03b8) + \u03b2 * C(\u03d5),(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_37",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_38@0",
            "content": "where \u03b2 is a hyper-parameter and L CT X (\u03b8) is the standard context-conditional language modeling objective as defined in Section 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_38",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_39@0",
            "content": "Task Pretraining",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_39",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_40@0",
            "content": "As observed by Liu et al. (2021), for some structured tasks, such as table question answering, pretraining on a simplified version of the given task with a synthetic context can help the model to focus learning on the \"skills\" that are most relevant to utilize the task context, which leads to improved downstream task performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_40",
            "start": 0,
            "end": 330,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_40@1",
            "content": "We instantiate this idea in our method by pre-training our model on a simplified (dialogue-free) version of the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_40",
            "start": 332,
            "end": 448,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_40@2",
            "content": "Instead of simultaneously modeling all the details of the raw dialogue, as is required to learn P (\u03c4 |c o ), the key observation here is that in our case the task reward only depends on the tuple {c h , c o , a T }.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_40",
            "start": 450,
            "end": 664,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_40@3",
            "content": "This enables us to effectively learn to execute the task by only modeling P (c h , a T |c o ), without any dialogue at all.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_40",
            "start": 666,
            "end": 788,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_40@4",
            "content": "By pre-training our model to first learn this simplified distribution, we effectively focus on learning the necessary skills for completing the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_40",
            "start": 790,
            "end": 938,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_40@5",
            "content": "It is expected that the skills learned during this pre-training phase should also generalize and transfer when we later perform training on the real dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_40",
            "start": 940,
            "end": 1097,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_40@6",
            "content": "The particular instantiation of this principle in the case of AirDialogue is described in Section A.5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_40",
            "start": 1099,
            "end": 1200,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_41@0",
            "content": "Model-Based Dialogue Rollouts",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_41",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_42@0",
            "content": "While the methodology discussed so far can produce effective policies, language models also represent task dynamics, as discussed in Section 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_42",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_42@1",
            "content": "We can leverage this fact to further improve the performance of our fine-tuned models by performing model-based planning at test-time, using both the policy and dynamics components in concert to further maximize task reward.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_42",
            "start": 144,
            "end": 367,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_42@2",
            "content": "A full dialogue trajectory can then be formed by concatenating this sampled future trajectory \u03c4 \u2265t with the current state of the dialogue \u03c4 <t i.e., \u03c4 = {\u03c4 <t , \u03c4 \u2265t }.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_42",
            "start": 369,
            "end": 536,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_42@3",
            "content": "We perform the model-based planning by sampling k such future trajectories from the final fine-tuned model, and ranking them according to an estimated reward function R(\u03c4, c o ) (see Appendix A.7).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_42",
            "start": 538,
            "end": 734,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_42@4",
            "content": "Then, we improve upon the policy \u03c0 from which we took the samples by taking the action (i.e., the next utterance) a t which receives the highest estimated reward among the sampled trajectories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_42",
            "start": 736,
            "end": 928,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_42@5",
            "content": "This roll-out sampling procedure is identical to the one used by Lewis et al. (2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_42",
            "start": 930,
            "end": 1014,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_43@0",
            "content": "CALM for AirDialogue",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_43",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_44@0",
            "content": "In this section, we instantiate our proposed method, CALM, for the AirDialogue flight booking task (Wei et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_44",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_44@1",
            "content": "We first give an overview of the task, and then describe how to do relabeling and context conditioning on this specific task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_44",
            "start": 119,
            "end": 243,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_45@0",
            "content": "AirDialogue Dataset",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_45",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_46@0",
            "content": "Dataset overview.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_46",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_46@1",
            "content": "The AirDialogue dataset (Wei et al., 2018) is a recently published large-scale airline reservation dataset based on the aforementioned task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_46",
            "start": 18,
            "end": 157,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_46@2",
            "content": "The dataset includes 402,038 conversations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_46",
            "start": 159,
            "end": 201,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_46@3",
            "content": "The dataset involves three distinct tasks: booking, canceling, and changing flights.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_46",
            "start": 203,
            "end": 286,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_46@4",
            "content": "We describe the booking task in detail below.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_46",
            "start": 288,
            "end": 332,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_46@5",
            "content": "Flight booking task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_46",
            "start": 334,
            "end": 353,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_46@6",
            "content": "The (human) customer is given a set of 12 trip requirements, and the flight agent (bot) is provided with a table of 30 flights.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_46",
            "start": 355,
            "end": 481,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_46@7",
            "content": "The goal of the flight agent is to book a flight from the table for the customer which meets all their requirements, or to correctly inform them that no such flight is available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_46",
            "start": 483,
            "end": 660,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_46@8",
            "content": "To determine task success, the flight agent must predict an explicit action at the end of the dialogue indicating the flight that was booked or inform no flight available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_46",
            "start": 662,
            "end": 832,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_46@9",
            "content": "See Figure 6 for an example conversation from the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_46",
            "start": 834,
            "end": 891,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_47@0",
            "content": "Processing Tables",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_47",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_48@0",
            "content": "The AirDialogue booking tasks require efficiently querying a flight table containing flight information (e.g., departing location, ticket price) given to the agent prior to the conversation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_48",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_48@1",
            "content": "In order to successfully complete the booking task, the agent needs to be able to filter, select, and integrate information from the flight table based on the customer's preferences inferred from the dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_48",
            "start": 191,
            "end": 399,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_49@0",
            "content": "Instead of treating the tables as unstructured sequences (Wei et al., 2018;Jiang et al., 2021) or as SQL databases (Chen et al., 2020), CALM models tables as an observable context consisting of a set c o = {f 1 , f 2 , f 3 , ..., f N } of table rows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_49",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_49@1",
            "content": "These rows are then input to our model as a set of embeddings (see appendix A.4 and A.9 for more details).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_49",
            "start": 251,
            "end": 356,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_50@0",
            "content": "Relabeling AirDialogue with CALM",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_50",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_51@0",
            "content": "While the AirDialogue dataset only includes one flight table for each dialogue, there are potentially many flight tables compatible with each dialogue as each flight can appear in many tables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_51",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_51@1",
            "content": "We hence implement our relabeling procedure as described in Section 4.1 as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_51",
            "start": 193,
            "end": 275,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_51@2",
            "content": "We perform rejection sampling on the observable context (i.e., the table of flights) c o \u223c q(c o |\u03c4, c h ), sampling until we obtain a new context (c h , c o , \u03c4 ), which gives maximum reward possible R(\u03c4, c h , c o ) = max co R(\u03c4, c h , c o ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_51",
            "start": 277,
            "end": 520,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_51@3",
            "content": "The prior distributions p(c o ) and p(c o |c h ), from which the tables in the AirDialogue dataset were sampled, are provided with the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_51",
            "start": 522,
            "end": 664,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_51@4",
            "content": "By rejection sampling from p(c o |c h ), we can effectively sample from the posterior q(c o |\u03c4, c h ) within a certain computational budget.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_51",
            "start": 666,
            "end": 805,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_51@5",
            "content": "In this setting, c o denotes tables and there are exponentially many tables which correspond to a task success under a given dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_51",
            "start": 807,
            "end": 940,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_51@6",
            "content": "Therefore, with our relabeling approach, we increase the number of near-optimal task examples exponentially, which makes it much easier for the language model to learn to query the flight table.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_51",
            "start": 942,
            "end": 1135,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_52@0",
            "content": "Our relabeling is approximately valid according to the condition specified in Section 4.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_52",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_52@1",
            "content": "While the customer does not have access to the flight table and therefore is not directly affected by our relabeling, there are still some minor edge-cases in which over-optimism about the dynamics could be learned by our policy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_52",
            "start": 91,
            "end": 319,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_52@2",
            "content": "If for example, in the dataset the customer were to occasionally reject the first flight that we suggest, our policy may learn to assign a small probability to the action of initially offering the wrong flight, relying on them subsequently rejecting it such that we can later recover and offer the correct one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_52",
            "start": 321,
            "end": 630,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_52@3",
            "content": "However, in practice we observe that these cases are rare in AirDialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_52",
            "start": 632,
            "end": 704,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_53@0",
            "content": "Table Selection as Auxiliary Loss",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_53",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_54@0",
            "content": "The primary high-level action involved in Air-Dialogue is the decision of which flight table entry, if any, to recommend to the user.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_54",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_54@1",
            "content": "We therefore implement our auxiliary objective as a classification head on top of the language model, trained to predict the flight table entry that meets the customer's requests.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_54",
            "start": 134,
            "end": 312,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_54@2",
            "content": "Specifically, our set of high-level actions A is the set of flight table rows {f 1 , f 2 , f 3 , ..., f N } plus an additional item f 0 , corresponding to the case in which no flights meet the customer's requirements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_54",
            "start": 314,
            "end": 530,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_54@3",
            "content": "If f * is the flight recommended in the dialogue, then our auxiliary objective is:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_54",
            "start": 532,
            "end": 613,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_55@0",
            "content": "C(\u03d5) = max \u03d5 E (co,\u03c4 )\u223cD off log P \u03d5 (f * |\u03c4, c o ). (6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_55",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_56@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_56",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_57@0",
            "content": "In this section, we empirically evaluate the performance of CALM on AirDialogue (Wei et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_57",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_57@1",
            "content": "We first show that CALM outperforms the success rate CALM (greedy) 0.88 \u00b1 2e-3 LM(GPT2-small) (greedy) 0.38 \u00b1 5e-3 AirConcierge (greedy) 0.81 \u00b1 7e-3 CALM (planning) 0.90 \u00b1 2e-3 LM(GPT2-small) (planning) 0.74 \u00b1 7e-3 Human 0.88",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_57",
            "start": 100,
            "end": 324,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_58@0",
            "content": "Table 1: Comparison of our method and baselines across all tasks, as well as just the booking task on AirDialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_58",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_58@1",
            "content": "Using greedy decoding, our method matches human performance, greatly improving over baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_58",
            "start": 115,
            "end": 208,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_58@2",
            "content": "Adding roll-outs (32 samples) further improves task completion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_58",
            "start": 210,
            "end": 272,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_59@0",
            "content": "SOTA on the AirDialogue dataset by around 7% in the standard simulated evaluation protocol proposed by Chen et al. (2020), which prior work denotes as \"self-play\" (see Appendix A.6), and this matches human-level performance as reported by Wei et al. (2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_59",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_59@1",
            "content": "Beyond this, we also perform a comprehensive set of ablation studies to validate the necessity of each component of CALM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_59",
            "start": 258,
            "end": 378,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_60@0",
            "content": "Experiment Setup and Baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_60",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_60@1",
            "content": "We compare CALM on AirDialogue with two baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_60",
            "start": 32,
            "end": 81,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_60@2",
            "content": "The first is AirConcierge, the previous SOTA on Air-Dialogue, which explicitly parses and executes SQL queries from the dialogue (Chen et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_60",
            "start": 83,
            "end": 231,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_61@0",
            "content": "The other is a standard language model (denoted as LM(GPT2-small)) trained on a dataset filtered for successful task examples, without any of our context-aware language modeling techniques (see Appendix Section A.1 for more details on dataset filtering).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_61",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_61@1",
            "content": "CALM uses the fine-tuned GPT2-small model (Radford et al., 2018) as the backbone of the policy and dynamics model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_61",
            "start": 255,
            "end": 368,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_61@2",
            "content": "After learning the dynamics model, both CALM and the LM(GPT2small) can employ two different planning strategies: (1) a simple greedy decoding of the next utterance (equivalent to beam search with beam-width one) and (2) the rollout planning as described in Section 4.4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_61",
            "start": 370,
            "end": 638,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_61@3",
            "content": "For AirConcierge, we only evaluate greedy decoding, as this method cannot be easily adapted for producing full rollouts as rollout planning requires a method for predicting the reward of a given dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_61",
            "start": 640,
            "end": 843,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_61@4",
            "content": "We describe our specific reward predictor for AirDialogue in Appendix Section A.7.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_61",
            "start": 845,
            "end": 926,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_62@0",
            "content": "Results for Task Success.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_62",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_62@1",
            "content": "In terms of task success, CALM outperforms the prior SOTA (AirConcierge) by approximately 7%, achieving 88% task success when using greedy decoding from the language model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_62",
            "start": 26,
            "end": 197,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_62@2",
            "content": "Compared with AirConcierge, where all reasoning about the task context is done outside of the language model, CALM does all of the filtering, selecting, and responding with relevant flight table entries within the language model, in a fully endto-end manner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_62",
            "start": 199,
            "end": 456,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_62@3",
            "content": "Meanwhile, CALM also improves over LM(GPT2-small) by 50% in terms of task success, indicating the necessity of our contextaware approach for goal-oriented tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_62",
            "start": 458,
            "end": 618,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_63@0",
            "content": "We further evaluate the the performance of various methods, when utilizing the rollout planning technique.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_63",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_63@1",
            "content": "As shown in Figure 3, as the number of rollout samples increases, the performance improves for all methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_63",
            "start": 107,
            "end": 213,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_63@2",
            "content": "Remarkably, applying the rollout planning to CALM further increases total task success by 2%, raising it to 90% and matching human performance on the AirDialogue task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_63",
            "start": 215,
            "end": 381,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_63@3",
            "content": "The baseline LM(GPT2-small) benefits much more from rollout planning than CALM, and we suspect that at around 90% task completion, the performance becomes bottlenecked by the customer bot's mistakes, therefore we only observe less gain from rollout planning with CALM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_63",
            "start": 383,
            "end": 650,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_63@4",
            "content": "Results for Language Quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_63",
            "start": 652,
            "end": 680,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_63@5",
            "content": "To quantitatively measure the generated language quality, we present perplexity and BLEU for all methods in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_63",
            "start": 682,
            "end": 797,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_63@6",
            "content": "CALM performs similarly to LM(GPT2-small) and outperforms AirConcierge significantly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_63",
            "start": 799,
            "end": 883,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_63@7",
            "content": "Ablation Study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_63",
            "start": 885,
            "end": 899,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_63@8",
            "content": "To examine the effectiveness of each single component in our method, we train and evaluate four ablations of CALM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_63",
            "start": 901,
            "end": 1014,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_63@9",
            "content": "Each of these ablations remove one of the components in our approach: task relabeling (Section 4.1), auxiliary loss (Section 4.2), and table pre-training (Section 4.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_63",
            "start": 1016,
            "end": 1183,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_64@0",
            "content": "Beyond this, we also examine CALM without both task relabeling and pre-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_64",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_64@1",
            "content": "As shown in Table 3, removing any one of these components drops task success by at least 10%, and in most cases much more than that.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_64",
            "start": 81,
            "end": 212,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_64@2",
            "content": "This shows that each piece of our method plays a critical role in helping CALM to effectively learn the goal-oriented task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_64",
            "start": 214,
            "end": 336,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_65@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_65",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_66@0",
            "content": "We proposed an end-to-end framework, CALM, for goal-oriented dialogue systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_66",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_66@1",
            "content": "Formulating end-to-end dialogue generation as a Markov decision process, CALM employs task relabeling and context-aware finetuning to steer supervised learning of language models towards specific goals, improving task performance drastically while preserving language quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_66",
            "start": 79,
            "end": 354,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_66@2",
            "content": "We show that this improves performance on AirDialogue over the previous state of the art, and matches previously reported human performance under the standard simulated evaluation protocol.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_66",
            "start": 356,
            "end": 544,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_67@0",
            "content": "Ethical Statement",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_67",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_68@0",
            "content": "We note that CALM is imperfect and in generating free-form dialogue can potentially produce harmful outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_68",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_68@1",
            "content": "We therefore do not recommend applying this method in particularly sensitive or sufficiently wide-reaching domains without additional measures to mitigate harmful generations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_68",
            "start": 109,
            "end": 283,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_69@0",
            "content": "In this appendix, we provide all the details in our implementation for CALM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_69",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_70@0",
            "content": "When training the LM(GPT2-small) and Customer Bot, we filter the dataset by only keeping the successful task examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_70",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_70@1",
            "content": "This is be achieved by simultaneously checking for successful task completion and whether a set of simple string matching heuristics are satisfied in the dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_70",
            "start": 119,
            "end": 281,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_70@2",
            "content": "Our heuristics aim to ensure that strings corresponding to each of the customer's flight requirements and the customer's goal are explicitly present in the dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_70",
            "start": 283,
            "end": 447,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_70@3",
            "content": "This combination of filtering steps reduces the size of the training set by 26%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_70",
            "start": 449,
            "end": 528,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_70@4",
            "content": "Despite this, we find that this is still more than enough data for the model to successfully learn the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_70",
            "start": 530,
            "end": 637,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_71@0",
            "content": "In Figure 4, we show the rollout planning procedure, which described in Section 4.4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_71",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_72@0",
            "content": "Our customer bot is fine-tuned from GPT2-small (124M parameters), using the standard language modeling objective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_72",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_72@1",
            "content": "We used the Huggingface Transformers library's implementation of GPT2 (Wolf et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_72",
            "start": 114,
            "end": 203,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_72@2",
            "content": "The customer's flight requirements are provided to the model as a prefix to the dialogue, which formatted as a comma separated list consisting of the customer's goal and flight requirements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_72",
            "start": 205,
            "end": 394,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_72@3",
            "content": "We trained the customer bot for maximum 10 epochs with early stopping on the filtered dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_72",
            "start": 396,
            "end": 489,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_72@4",
            "content": "For training, it takes around 1 day on 4 GPUs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_72",
            "start": 491,
            "end": 536,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_72@5",
            "content": "Specifically, we trained using Adam with learning rate 1e-4 and batch size 8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_72",
            "start": 538,
            "end": 614,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_72@6",
            "content": "Our customer bot achieves a perplexity of 1.47 on the development set and a BLEU score of 38.5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_72",
            "start": 616,
            "end": 710,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_73@0",
            "content": "All our flight agent bots are fine-tuned from GPT2-small (124M parameters) using the standard language modeling objective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_73",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_73@1",
            "content": "We used the Huggingface Transformers library's implementation of GPT2 (Wolf et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_73",
            "start": 123,
            "end": 212,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_73@2",
            "content": "Similar as the customer bot, we trained for maximum 10 epochs with early stopping on the filtered dataset, which takes roughly 1 day on 4 GPUs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_73",
            "start": 214,
            "end": 356,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_73@3",
            "content": "Specifically, we trained using Adam with learning rate 1e-4 and batch size 8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_73",
            "start": 358,
            "end": 434,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_73@4",
            "content": "We implement the final action prediction as a sequence of tokens generated at the end of each dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_73",
            "start": 436,
            "end": 538,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_73@5",
            "content": "The flight table is passed to the model as a prefix of flight embeddings, where each embedding is produced by summing embeddings corresponding to each attribute of a given flight (e.g., flight arrival/departure day/location, flight price, etc.).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_73",
            "start": 540,
            "end": 784,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_74@0",
            "content": "Initialized using GPT2-small (124M parameters), we further pre-train our flight-agent bots by training on simplified task sequences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_74",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_74@1",
            "content": "Specifically, these sequences consist of our flight table followed by a comma separated list of the customer's flight requirements and a string representing the final action taken.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_74",
            "start": 133,
            "end": 312,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_74@2",
            "content": "We also apply our auxiliary loss and task-relabeling techniques during this pre-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_74",
            "start": 314,
            "end": 402,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_75@0",
            "content": "We pre-train on 4 million unique samples, using batch size 64 and Adam with learning rate 1e-4, which takes around 2 days on 4 GPUs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_75",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_75@1",
            "content": "During pre-training, we found that it took around 2 million unique samples before the model suddenly started to learn the task of querying the flight table, and it took roughly 2 million more samples before it became proficient at querying the table.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_75",
            "start": 133,
            "end": 382,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_75@2",
            "content": "Both the unusual progression of learning during this pre-training phase and the high sample complexity needed to learn the task, indicates the difficulty in learning to query the flight table.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_75",
            "start": 384,
            "end": 575,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_75@3",
            "content": "This calls for future work about further investigate the challenges in learning complex logical functions using neural networks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_75",
            "start": 577,
            "end": 704,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_76@0",
            "content": "Prior works primarily evaluate bots for the flight agent through \"self-play\" (Chen et al., 2020;Wei et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_76",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_76@1",
            "content": "We follow the same evaluation protocol in our work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_76",
            "start": 115,
            "end": 165,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_76@2",
            "content": "Basically, we train a bot to play the role of the customer during evaluation and compute task success by simulating conversations against this bot.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_76",
            "start": 167,
            "end": 313,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_76@3",
            "content": "We run all self-play evaluations on the same subset of 1,000 dialogue scenarios, randomly selected from the validation set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_76",
            "start": 315,
            "end": 437,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_77@0",
            "content": "All models are evaluated against the same customer bot.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_77",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_77@1",
            "content": "including models for the baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_77",
            "start": 56,
            "end": 90,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_77@2",
            "content": "We find that when running against our self-play bot, task completion success for prior methods is increased, sometimes by more than 8% (from what was reported by such prior works under the same evaluation setting).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_77",
            "start": 92,
            "end": 305,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_77@3",
            "content": "The only difference is the specific model used for customer's side of the conversation, and we conjecture that this difference is likely due to the architecture difference and the details of our dataset filtering.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_77",
            "start": 307,
            "end": 519,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_77@4",
            "content": "This significant change in evaluation performance compared with prior works, not only indicates the quality of our customer bot, but also suggests the importance of accounting for these factors in evaluating and comparing dialogue systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_77",
            "start": 521,
            "end": 759,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_77@5",
            "content": "To promote fair evaluations in future works that use the AirDialogue dataset, we will release the code and model weights for our customer bot upon acceptance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_77",
            "start": 761,
            "end": 918,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_78@0",
            "content": "Rollout Planning To execute rollout planning, we need a reward predictor which can estimate whether a given dialogue is a successful example of task completion or not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_78",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_78@1",
            "content": "In the case of AirDialogue, we found that the most robust way to estimate this reward is the following: we first fine-tune a RoBERTa-base model (123M parameters) to predict the customer's ground-truth goal and flight requirements from the set of dialogues in the training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_78",
            "start": 168,
            "end": 443,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_78@2",
            "content": "We used the Huggingface Transformers library's implementation of RoBERTa (Wolf et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_78",
            "start": 445,
            "end": 537,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_78@3",
            "content": "We do not filter the training-set when training this model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_78",
            "start": 539,
            "end": 597,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_78@4",
            "content": "Once this model is trained, our procedure for predicting dialogue success is the following:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_78",
            "start": 599,
            "end": 689,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_79@0",
            "content": "1. Given a dialogue, use our RoBERTa model to predict the customer's goal and flight requirements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_79",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_80@0",
            "content": "2. We then execute this predicted information against the agent's flight table and reservation flag, to produce a set of valid final actions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_80",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_81@0",
            "content": "3. If the final action taken in the dialogue is within the set of predicted final actions, then predict that the current dialogue is successful, otherwise predict that it is unsuccessful.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_81",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_82@0",
            "content": "See Figure 5 for a visual illustration of this procedure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_82",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_82@1",
            "content": "Our model obtains 94% accuracy in predicting the reward of the dialogues in the validation set (see Table 4 for a more extensive breakdown of the model's accuracy).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_82",
            "start": 58,
            "end": 221,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_83@0",
            "content": "In Figure 6, we showcase a specific example for the conversation in AirDialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_83",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_83@1",
            "content": "A.9 Previous Approaches to Flight Table Processing Prior works (Wei et al., 2018;Jiang et al., 2021) typically input the table directly into a language model, expecting that the skill of querying the table will be naturally learned via the standard language modeling objective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_83",
            "start": 81,
            "end": 357,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_83@2",
            "content": "We found this approach to under-perform in our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_83",
            "start": 359,
            "end": 417,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_83@3",
            "content": "These findings are also consistent with recent works which show that pre-training transformers for querying tables can significantly improve the transformer's performance on downstream tasks which use tables (Liu et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_83",
            "start": 419,
            "end": 645,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_83@4",
            "content": "AirConcierge (Chen et al., 2020) takes a different approach, and explicitly predicts and executes SQL queries based on the dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_83",
            "start": 647,
            "end": 778,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_83@5",
            "content": "This approach obtains the SOTA task success on AirDialogue, but it involves several complex components, requires the ability to preform semantic parsing on the dialogue, and of course requires additional domain knowledge about the format and structure of the flight table, which reprsents the task context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_83",
            "start": 780,
            "end": 1085,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_83@6",
            "content": "In our work, we show that applying CALM for AirDialogue can close this gap by inducing task learning from language models and achieve end-to-end learning from the flight table, without sacrificing the generated language quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_83",
            "start": 1087,
            "end": 1314,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_84@0",
            "content": "UNKNOWN, None, 2020, Towards a human-like open-domain chatbot, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_84",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_85@0",
            "content": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob Mc-Grew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba, Hindsight experience replay, 2017, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_85",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_86@0",
            "content": "UNKNOWN, None, 2016, A sequence-to-sequence model for user simulation in spoken dialogue systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_86",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_87@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_87",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_88@0",
            "content": "Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Milica Osman Ramadan,  Gasic, Multiwoz-a largescale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_88",
            "start": 0,
            "end": 302,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_89@0",
            "content": "Chieh-Yang Chen, Pei-Hsin Wang, Shih-Chieh Chang, Da-Cheng Juan, Wei Wei, Jia-Yu Pan, Airconcierge: Generating task-oriented dialogue via efficient large-scale knowledge retrieval, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_89",
            "start": 0,
            "end": 285,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_90@0",
            "content": "Wenhu Chen, Jianshu Chen, Pengda Qin, Xifeng Yan, William Wang, Semantically conditioned dialog response generation via hierarchical disentangled self-attention, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_90",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_91@0",
            "content": "Grace Chung, Developing a flexible spoken dialog system using simulation, 2004, Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_91",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_92@0",
            "content": "UNKNOWN, None, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_92",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_93@0",
            "content": "Ond\u0159ej Du\u0161ek, Filip Jurcicek, Sequence-tosequence generation for spoken dialogue via deep syntax trees and strings, 2016, Proceedings of the 54th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_93",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_94@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_94",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_95@0",
            "content": "Wieland Eckert, Esther Levin, Roberto Pieraccini, User modeling for spoken dialogue system evaluation, 1997, IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_95",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_96@0",
            "content": "Mihail Eric, D Christopher,  Manning, A copyaugmented sequence-to-sequence architecture gives good performance on task-oriented dialogue, 2017, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_96",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_97@0",
            "content": "UNKNOWN, None, 2020, Rewriting history with inverse rl: Hindsight inference for policy improvement, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_97",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_98@0",
            "content": "Scott Fujimoto, David Meger, Doina Precup, Off-policy deep reinforcement learning without exploration, 2019, International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_98",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_99@0",
            "content": "Milica Ga\u0161i\u0107, Filip Jur\u010d\u00ed\u010dek, Blaise Thomson, Kai Yu, Steve Young, On-line policy optimisation of spoken dialogue systems via live interaction with human subjects, 2011, 2011 IEEE Workshop on Automatic Speech Recognition & Understanding, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_99",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_100@0",
            "content": "Kallirroi Georgila, James Henderson, Oliver Lemon, User simulation for spoken dialogue systems: Learning and evaluation, 2006, Ninth International Conference on Spoken Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_100",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_101@0",
            "content": "Kallirroi Georgila, David Traum, Reinforcement learning of argumentation dialogue policies in negotiation, 2011, Twelfth Annual Conference of the International Speech Communication Association, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_101",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_102@0",
            "content": "UNKNOWN, None, 2019, Learning to reach goals without reinforcement learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_102",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_103@0",
            "content": "He He, Derek Chen, Anusha Balakrishnan, Percy Liang, Decoupling strategy and generation in negotiation dialogues, 2018, Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_103",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_104@0",
            "content": "Peter A Heeman, Representing the reinforcement learning state in a negotiation dialogue, 2009, IEEE Workshop on Automatic Speech Recognition & Understanding, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_104",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_105@0",
            "content": "UNKNOWN, None, 2020, A simple language model for task-oriented dialogue, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_105",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_106@0",
            "content": "UNKNOWN, None, 2019, Way off-policy batch deep reinforcement learning of implicit human preferences in dialog, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_106",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_107@0",
            "content": "UNKNOWN, None, 2021, Towards automatic evaluation of dialog systems: A model-free off-policy evaluation approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_107",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_108@0",
            "content": "Leslie Pack, Kaelbling , Learning to achieve goals, 1993, IJCAI, Citeseer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_108",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_109@0",
            "content": "Leslie Pack Kaelbling, Anthony R Michael L Littman,  Cassandra, Planning and acting in partially observable stochastic domains, 1998, Artificial intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_109",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_110@0",
            "content": "Dongyeop Kang, Anusha Balakrishnan, Pararth Shah, A Paul, Y-Lan Crook, Jason Boureau,  Weston, Recommendation as a communication game: Self-supervised bot-play for goal-oriented dialogue, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_110",
            "start": 0,
            "end": 371,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_111@0",
            "content": "Esther Levin, Roberto Pieraccini, Wieland Eckert, A stochastic model of human-machine interaction for learning dialog strategies, 2000, IEEE Transactions on speech and audio processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_111",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_112@0",
            "content": "UNKNOWN, None, 2020, Offline reinforcement learning: Tutorial, review, and perspectives on open problems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_112",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_113@0",
            "content": "Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh, Dhruv Batra, Deal or no deal? end-to-end learning of negotiation dialogues, 2017, Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_113",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_114@0",
            "content": "Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, Jianfeng Gao, Deep reinforcement learning for dialogue generation, 2016, Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_114",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_115@0",
            "content": "Bing Liu, Gokhan T\u00fcr, Dilek Hakkani-T\u00fcr, Pararth Shah, Larry Heck, Dialogue learning with human teaching and feedback in end-to-end trainable task-oriented dialogue systems, 2018, Proceedings of NAACL-HLT, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_115",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_116@0",
            "content": "Jingjing Liu, Stephanie Seneff, Victor Zue, Dialogue-oriented review summary generation for spoken dialogue recommendation systems, 2010, Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_116",
            "start": 0,
            "end": 274,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_117@0",
            "content": "UNKNOWN, None, 2021, Tapex: Table pre-training via learning a neural sql executor, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_117",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_118@0",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_118",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_119@0",
            "content": "Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, Pierre Sermanet, Learning latent plans from play, 2020, Conference on Robot Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_119",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_120@0",
            "content": "F Michael,  Mctear, Spoken dialogue technology: enabling the conversational user interface, 2002, ACM Computing Surveys (CSUR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_120",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_121@0",
            "content": "Hongyuan Mei, Mohit Bansal, Matthew R Walter, Coherent dialogue with attention-based language models, 2017, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_121",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_122@0",
            "content": "UNKNOWN, None, , Shahin Shayandeh, Lars Liden, and Jianfeng Gao. 2020. Soloist: Few-shot task-oriented dialog with a single pretrained auto-regressive model, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_122",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_123@0",
            "content": "Olivier Pietquin, Matthieu Geist, Senthilkumar Chandramohan, Herv\u00e9 Frezza-Buet, Sampleefficient batch reinforcement learning for dialogue management optimization, 2011, ACM Transactions on Speech and Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_123",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_124@0",
            "content": "UNKNOWN, None, 1989, Alvinn: An autonomous land vehicle in a neural network, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_124",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_125@0",
            "content": "UNKNOWN, None, 2018, Temporal difference models: Modelfree deep rl for model-based control, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_125",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_126@0",
            "content": "UNKNOWN, None, 2018, Language models are unsupervised multitask learners, OpenAI Blog.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_126",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_127@0",
            "content": "UNKNOWN, None, 2018, Semi-parametric topological memory for navigation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_127",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_128@0",
            "content": "Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye, Steve Young, Agenda-based user simulation for bootstrapping a pomdp dialogue system, 2007, Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_128",
            "start": 0,
            "end": 310,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_129@0",
            "content": "Satinder Singh, Michael Kearns, Diane Litman, Marilyn Walker, Reinforcement learning for spoken dialogue systems, 1999, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_129",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_130@0",
            "content": "Satinder Singh, Diane Litman, Michael Kearns, Marilyn Walker, Optimizing dialogue management with reinforcement learning: Experiments with the njfun system, 2002, Journal of Artificial Intelligence Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_130",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_131@0",
            "content": "UNKNOWN, None, 1994, Spoken natural language dialog systems: A practical approach, Oxford University Press on Demand.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_131",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_132@0",
            "content": "Pei-Hao Su, Pawe\u0142 Budzianowski, Stefan Ultes, Milica Gasic, Steve Young, Sample-efficient actor-critic reinforcement learning with supervised data for dialogue management, 2017, Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_132",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_133@0",
            "content": "UNKNOWN, None, 2016, Continuously learning neural dialogue management, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_133",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_134@0",
            "content": "Martin Sundermeyer, Ralf Schl\u00fcter, Hermann Ney, Lstm neural networks for language modeling, 2012, Thirteenth annual conference of the international speech communication association, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_134",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_135@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_135",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_136@0",
            "content": "UNKNOWN, None, 2020, Modelling hierarchical structure between dialogue policy and natural language generator with option framework for task-oriented dialogue system, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_136",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_137@0",
            "content": "UNKNOWN, None, , Caglar Gulcehre, Nicolas Heess, et al. 2020b. Critic regularized regression, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_137",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_138@0",
            "content": "Wei Wei, Quoc Le, Andrew Dai, Jia Li, Airdialogue: An environment for goal-oriented dialogue research, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_138",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_139@0",
            "content": "UNKNOWN, None, 2014, The dialog state tracking challenge series, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_139",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_140@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_140",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_141@0",
            "content": "UNKNOWN, None, 2019, Alternating recurrent dialog model with large-scale pre-trained language models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_141",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_142@0",
            "content": "UNKNOWN, None, 2019, Behavior regularized offline reinforcement learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_142",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_143@0",
            "content": "Steve Young, Milica Ga\u0161i\u0107, Blaise Thomson, Jason Williams, Pomdp-based statistical spoken dialog systems: A review, 2013, Proceedings of the IEEE, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_143",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_144@0",
            "content": "Zhou Yu, Ziyu Xu, Alan Black, Alexander Rudnicky, Strategy and policy learning for nontask-oriented conversational systems, 2016, Proceedings of the 17th annual meeting of the special interest group on discourse and dialogue, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_144",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_145@0",
            "content": "Tangming Yuan, David Moore, Alec Grierson, A human-computer dialogue system for educational debate: A computational dialectics approach, 2008, International Journal of Artificial Intelligence in Education, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_145",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_146@0",
            "content": "Yichi Zhang, Zhijian Ou, Zhou Yu, Taskoriented dialog systems that consider multiple appropriate responses under the same context, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_146",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "90-ARR_v1_147@0",
            "content": "Tiancheng Zhao, Kaige Xie, Maxine Eskenazi, Rethinking action spaces for reinforcement learning in end-to-end dialog agents with latent variable models, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "90-ARR_v1_147",
            "start": 0,
            "end": 303,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "90-ARR_v1_0",
            "tgt_ix": "90-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_0",
            "tgt_ix": "90-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_1",
            "tgt_ix": "90-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_1",
            "tgt_ix": "90-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_0",
            "tgt_ix": "90-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_2",
            "tgt_ix": "90-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_4",
            "tgt_ix": "90-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_5",
            "tgt_ix": "90-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_6",
            "tgt_ix": "90-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_3",
            "tgt_ix": "90-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_3",
            "tgt_ix": "90-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_3",
            "tgt_ix": "90-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_3",
            "tgt_ix": "90-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_3",
            "tgt_ix": "90-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_0",
            "tgt_ix": "90-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_7",
            "tgt_ix": "90-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_9",
            "tgt_ix": "90-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_8",
            "tgt_ix": "90-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_8",
            "tgt_ix": "90-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_8",
            "tgt_ix": "90-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_0",
            "tgt_ix": "90-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_10",
            "tgt_ix": "90-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_12",
            "tgt_ix": "90-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_13",
            "tgt_ix": "90-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_14",
            "tgt_ix": "90-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_15",
            "tgt_ix": "90-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_16",
            "tgt_ix": "90-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_17",
            "tgt_ix": "90-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_18",
            "tgt_ix": "90-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_19",
            "tgt_ix": "90-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_20",
            "tgt_ix": "90-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_21",
            "tgt_ix": "90-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_22",
            "tgt_ix": "90-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_11",
            "tgt_ix": "90-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_11",
            "tgt_ix": "90-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_11",
            "tgt_ix": "90-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_11",
            "tgt_ix": "90-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_11",
            "tgt_ix": "90-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_11",
            "tgt_ix": "90-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_11",
            "tgt_ix": "90-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_11",
            "tgt_ix": "90-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_11",
            "tgt_ix": "90-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_11",
            "tgt_ix": "90-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_11",
            "tgt_ix": "90-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_11",
            "tgt_ix": "90-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_11",
            "tgt_ix": "90-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_0",
            "tgt_ix": "90-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_23",
            "tgt_ix": "90-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_24",
            "tgt_ix": "90-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_24",
            "tgt_ix": "90-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_24",
            "tgt_ix": "90-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_25",
            "tgt_ix": "90-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_27",
            "tgt_ix": "90-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_28",
            "tgt_ix": "90-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_29",
            "tgt_ix": "90-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_26",
            "tgt_ix": "90-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_26",
            "tgt_ix": "90-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_26",
            "tgt_ix": "90-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_26",
            "tgt_ix": "90-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_26",
            "tgt_ix": "90-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_24",
            "tgt_ix": "90-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_30",
            "tgt_ix": "90-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_32",
            "tgt_ix": "90-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_33",
            "tgt_ix": "90-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_34",
            "tgt_ix": "90-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_35",
            "tgt_ix": "90-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_36",
            "tgt_ix": "90-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_37",
            "tgt_ix": "90-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_31",
            "tgt_ix": "90-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_31",
            "tgt_ix": "90-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_31",
            "tgt_ix": "90-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_31",
            "tgt_ix": "90-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_31",
            "tgt_ix": "90-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_31",
            "tgt_ix": "90-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_31",
            "tgt_ix": "90-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_31",
            "tgt_ix": "90-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_24",
            "tgt_ix": "90-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_38",
            "tgt_ix": "90-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_39",
            "tgt_ix": "90-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_39",
            "tgt_ix": "90-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_24",
            "tgt_ix": "90-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_40",
            "tgt_ix": "90-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_41",
            "tgt_ix": "90-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_41",
            "tgt_ix": "90-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_0",
            "tgt_ix": "90-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_42",
            "tgt_ix": "90-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_43",
            "tgt_ix": "90-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_43",
            "tgt_ix": "90-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_43",
            "tgt_ix": "90-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_44",
            "tgt_ix": "90-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_45",
            "tgt_ix": "90-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_45",
            "tgt_ix": "90-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_43",
            "tgt_ix": "90-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_46",
            "tgt_ix": "90-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_48",
            "tgt_ix": "90-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_47",
            "tgt_ix": "90-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_47",
            "tgt_ix": "90-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_47",
            "tgt_ix": "90-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_43",
            "tgt_ix": "90-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_49",
            "tgt_ix": "90-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_51",
            "tgt_ix": "90-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_50",
            "tgt_ix": "90-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_50",
            "tgt_ix": "90-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_50",
            "tgt_ix": "90-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_43",
            "tgt_ix": "90-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_52",
            "tgt_ix": "90-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_54",
            "tgt_ix": "90-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_53",
            "tgt_ix": "90-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_53",
            "tgt_ix": "90-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_53",
            "tgt_ix": "90-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_0",
            "tgt_ix": "90-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_55",
            "tgt_ix": "90-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_57",
            "tgt_ix": "90-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_58",
            "tgt_ix": "90-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_59",
            "tgt_ix": "90-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_60",
            "tgt_ix": "90-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_61",
            "tgt_ix": "90-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_62",
            "tgt_ix": "90-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_63",
            "tgt_ix": "90-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_56",
            "tgt_ix": "90-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_56",
            "tgt_ix": "90-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_56",
            "tgt_ix": "90-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_56",
            "tgt_ix": "90-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_56",
            "tgt_ix": "90-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_56",
            "tgt_ix": "90-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_56",
            "tgt_ix": "90-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_56",
            "tgt_ix": "90-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_56",
            "tgt_ix": "90-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_0",
            "tgt_ix": "90-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_64",
            "tgt_ix": "90-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_65",
            "tgt_ix": "90-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_65",
            "tgt_ix": "90-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_0",
            "tgt_ix": "90-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_66",
            "tgt_ix": "90-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_67",
            "tgt_ix": "90-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_67",
            "tgt_ix": "90-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_67",
            "tgt_ix": "90-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_68",
            "tgt_ix": "90-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_67",
            "tgt_ix": "90-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_69",
            "tgt_ix": "90-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_67",
            "tgt_ix": "90-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_70",
            "tgt_ix": "90-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_67",
            "tgt_ix": "90-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_71",
            "tgt_ix": "90-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_67",
            "tgt_ix": "90-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_72",
            "tgt_ix": "90-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_74",
            "tgt_ix": "90-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_67",
            "tgt_ix": "90-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_67",
            "tgt_ix": "90-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_73",
            "tgt_ix": "90-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_76",
            "tgt_ix": "90-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_67",
            "tgt_ix": "90-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_67",
            "tgt_ix": "90-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_75",
            "tgt_ix": "90-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_78",
            "tgt_ix": "90-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_79",
            "tgt_ix": "90-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_80",
            "tgt_ix": "90-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_81",
            "tgt_ix": "90-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_67",
            "tgt_ix": "90-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_67",
            "tgt_ix": "90-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_67",
            "tgt_ix": "90-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_67",
            "tgt_ix": "90-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_67",
            "tgt_ix": "90-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_77",
            "tgt_ix": "90-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_67",
            "tgt_ix": "90-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_82",
            "tgt_ix": "90-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "90-ARR_v1_0",
            "tgt_ix": "90-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_1",
            "tgt_ix": "90-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_2",
            "tgt_ix": "90-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_2",
            "tgt_ix": "90-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_2",
            "tgt_ix": "90-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_2",
            "tgt_ix": "90-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_2",
            "tgt_ix": "90-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_2",
            "tgt_ix": "90-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_2",
            "tgt_ix": "90-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_3",
            "tgt_ix": "90-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_4",
            "tgt_ix": "90-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_4",
            "tgt_ix": "90-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_4",
            "tgt_ix": "90-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_4",
            "tgt_ix": "90-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_5",
            "tgt_ix": "90-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_5",
            "tgt_ix": "90-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_5",
            "tgt_ix": "90-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_6",
            "tgt_ix": "90-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_6",
            "tgt_ix": "90-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_6",
            "tgt_ix": "90-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_6",
            "tgt_ix": "90-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_6",
            "tgt_ix": "90-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_6",
            "tgt_ix": "90-ARR_v1_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_7",
            "tgt_ix": "90-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_7",
            "tgt_ix": "90-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_7",
            "tgt_ix": "90-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_7",
            "tgt_ix": "90-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_7",
            "tgt_ix": "90-ARR_v1_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_8",
            "tgt_ix": "90-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_9",
            "tgt_ix": "90-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_9",
            "tgt_ix": "90-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_9",
            "tgt_ix": "90-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_9",
            "tgt_ix": "90-ARR_v1_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_9",
            "tgt_ix": "90-ARR_v1_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_9",
            "tgt_ix": "90-ARR_v1_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_9",
            "tgt_ix": "90-ARR_v1_9@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_10",
            "tgt_ix": "90-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_10",
            "tgt_ix": "90-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_10",
            "tgt_ix": "90-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_10",
            "tgt_ix": "90-ARR_v1_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_10",
            "tgt_ix": "90-ARR_v1_10@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_11",
            "tgt_ix": "90-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_12",
            "tgt_ix": "90-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_12",
            "tgt_ix": "90-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_12",
            "tgt_ix": "90-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_12",
            "tgt_ix": "90-ARR_v1_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_12",
            "tgt_ix": "90-ARR_v1_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_12",
            "tgt_ix": "90-ARR_v1_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_12",
            "tgt_ix": "90-ARR_v1_12@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_13",
            "tgt_ix": "90-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_13",
            "tgt_ix": "90-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_13",
            "tgt_ix": "90-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_13",
            "tgt_ix": "90-ARR_v1_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_13",
            "tgt_ix": "90-ARR_v1_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_13",
            "tgt_ix": "90-ARR_v1_13@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_13",
            "tgt_ix": "90-ARR_v1_13@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_13",
            "tgt_ix": "90-ARR_v1_13@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_13",
            "tgt_ix": "90-ARR_v1_13@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_13",
            "tgt_ix": "90-ARR_v1_13@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_13",
            "tgt_ix": "90-ARR_v1_13@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_13",
            "tgt_ix": "90-ARR_v1_13@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_13",
            "tgt_ix": "90-ARR_v1_13@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_13",
            "tgt_ix": "90-ARR_v1_13@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_13",
            "tgt_ix": "90-ARR_v1_13@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_14",
            "tgt_ix": "90-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_15",
            "tgt_ix": "90-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_15",
            "tgt_ix": "90-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_15",
            "tgt_ix": "90-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_15",
            "tgt_ix": "90-ARR_v1_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_16",
            "tgt_ix": "90-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_17",
            "tgt_ix": "90-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_17",
            "tgt_ix": "90-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_17",
            "tgt_ix": "90-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_17",
            "tgt_ix": "90-ARR_v1_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_17",
            "tgt_ix": "90-ARR_v1_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_17",
            "tgt_ix": "90-ARR_v1_17@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_18",
            "tgt_ix": "90-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_19",
            "tgt_ix": "90-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_19",
            "tgt_ix": "90-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_20",
            "tgt_ix": "90-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_20",
            "tgt_ix": "90-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_21",
            "tgt_ix": "90-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_21",
            "tgt_ix": "90-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_21",
            "tgt_ix": "90-ARR_v1_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_22",
            "tgt_ix": "90-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_23",
            "tgt_ix": "90-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_23",
            "tgt_ix": "90-ARR_v1_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_23",
            "tgt_ix": "90-ARR_v1_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_23",
            "tgt_ix": "90-ARR_v1_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_24",
            "tgt_ix": "90-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_25",
            "tgt_ix": "90-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_25",
            "tgt_ix": "90-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_25",
            "tgt_ix": "90-ARR_v1_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_25",
            "tgt_ix": "90-ARR_v1_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_25",
            "tgt_ix": "90-ARR_v1_25@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_25",
            "tgt_ix": "90-ARR_v1_25@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_25",
            "tgt_ix": "90-ARR_v1_25@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_25",
            "tgt_ix": "90-ARR_v1_25@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_25",
            "tgt_ix": "90-ARR_v1_25@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_25",
            "tgt_ix": "90-ARR_v1_25@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_26",
            "tgt_ix": "90-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_27",
            "tgt_ix": "90-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_27",
            "tgt_ix": "90-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_27",
            "tgt_ix": "90-ARR_v1_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_27",
            "tgt_ix": "90-ARR_v1_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_27",
            "tgt_ix": "90-ARR_v1_27@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_27",
            "tgt_ix": "90-ARR_v1_27@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_27",
            "tgt_ix": "90-ARR_v1_27@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_27",
            "tgt_ix": "90-ARR_v1_27@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_27",
            "tgt_ix": "90-ARR_v1_27@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_28",
            "tgt_ix": "90-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_28",
            "tgt_ix": "90-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_28",
            "tgt_ix": "90-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_28",
            "tgt_ix": "90-ARR_v1_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_29",
            "tgt_ix": "90-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_29",
            "tgt_ix": "90-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_29",
            "tgt_ix": "90-ARR_v1_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_29",
            "tgt_ix": "90-ARR_v1_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_30",
            "tgt_ix": "90-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_30",
            "tgt_ix": "90-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_30",
            "tgt_ix": "90-ARR_v1_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_30",
            "tgt_ix": "90-ARR_v1_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_31",
            "tgt_ix": "90-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_32",
            "tgt_ix": "90-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_32",
            "tgt_ix": "90-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_32",
            "tgt_ix": "90-ARR_v1_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_33",
            "tgt_ix": "90-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_33",
            "tgt_ix": "90-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_33",
            "tgt_ix": "90-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_33",
            "tgt_ix": "90-ARR_v1_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_33",
            "tgt_ix": "90-ARR_v1_33@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_33",
            "tgt_ix": "90-ARR_v1_33@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_34",
            "tgt_ix": "90-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_35",
            "tgt_ix": "90-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_36",
            "tgt_ix": "90-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_36",
            "tgt_ix": "90-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_37",
            "tgt_ix": "90-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_38",
            "tgt_ix": "90-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_39",
            "tgt_ix": "90-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_40",
            "tgt_ix": "90-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_40",
            "tgt_ix": "90-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_40",
            "tgt_ix": "90-ARR_v1_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_40",
            "tgt_ix": "90-ARR_v1_40@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_40",
            "tgt_ix": "90-ARR_v1_40@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_40",
            "tgt_ix": "90-ARR_v1_40@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_40",
            "tgt_ix": "90-ARR_v1_40@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_41",
            "tgt_ix": "90-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_42",
            "tgt_ix": "90-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_42",
            "tgt_ix": "90-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_42",
            "tgt_ix": "90-ARR_v1_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_42",
            "tgt_ix": "90-ARR_v1_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_42",
            "tgt_ix": "90-ARR_v1_42@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_42",
            "tgt_ix": "90-ARR_v1_42@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_43",
            "tgt_ix": "90-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_44",
            "tgt_ix": "90-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_44",
            "tgt_ix": "90-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_45",
            "tgt_ix": "90-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_46",
            "tgt_ix": "90-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_46",
            "tgt_ix": "90-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_46",
            "tgt_ix": "90-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_46",
            "tgt_ix": "90-ARR_v1_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_46",
            "tgt_ix": "90-ARR_v1_46@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_46",
            "tgt_ix": "90-ARR_v1_46@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_46",
            "tgt_ix": "90-ARR_v1_46@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_46",
            "tgt_ix": "90-ARR_v1_46@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_46",
            "tgt_ix": "90-ARR_v1_46@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_46",
            "tgt_ix": "90-ARR_v1_46@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_47",
            "tgt_ix": "90-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_48",
            "tgt_ix": "90-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_48",
            "tgt_ix": "90-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_49",
            "tgt_ix": "90-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_49",
            "tgt_ix": "90-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_50",
            "tgt_ix": "90-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_51",
            "tgt_ix": "90-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_51",
            "tgt_ix": "90-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_51",
            "tgt_ix": "90-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_51",
            "tgt_ix": "90-ARR_v1_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_51",
            "tgt_ix": "90-ARR_v1_51@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_51",
            "tgt_ix": "90-ARR_v1_51@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_51",
            "tgt_ix": "90-ARR_v1_51@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_52",
            "tgt_ix": "90-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_52",
            "tgt_ix": "90-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_52",
            "tgt_ix": "90-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_52",
            "tgt_ix": "90-ARR_v1_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_53",
            "tgt_ix": "90-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_54",
            "tgt_ix": "90-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_54",
            "tgt_ix": "90-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_54",
            "tgt_ix": "90-ARR_v1_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_54",
            "tgt_ix": "90-ARR_v1_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_55",
            "tgt_ix": "90-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_56",
            "tgt_ix": "90-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_57",
            "tgt_ix": "90-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_57",
            "tgt_ix": "90-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_58",
            "tgt_ix": "90-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_58",
            "tgt_ix": "90-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_58",
            "tgt_ix": "90-ARR_v1_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_59",
            "tgt_ix": "90-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_59",
            "tgt_ix": "90-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_60",
            "tgt_ix": "90-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_60",
            "tgt_ix": "90-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_60",
            "tgt_ix": "90-ARR_v1_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_61",
            "tgt_ix": "90-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_61",
            "tgt_ix": "90-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_61",
            "tgt_ix": "90-ARR_v1_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_61",
            "tgt_ix": "90-ARR_v1_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_61",
            "tgt_ix": "90-ARR_v1_61@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_62",
            "tgt_ix": "90-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_62",
            "tgt_ix": "90-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_62",
            "tgt_ix": "90-ARR_v1_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_62",
            "tgt_ix": "90-ARR_v1_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_63",
            "tgt_ix": "90-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_63",
            "tgt_ix": "90-ARR_v1_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_63",
            "tgt_ix": "90-ARR_v1_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_63",
            "tgt_ix": "90-ARR_v1_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_63",
            "tgt_ix": "90-ARR_v1_63@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_63",
            "tgt_ix": "90-ARR_v1_63@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_63",
            "tgt_ix": "90-ARR_v1_63@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_63",
            "tgt_ix": "90-ARR_v1_63@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_63",
            "tgt_ix": "90-ARR_v1_63@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_63",
            "tgt_ix": "90-ARR_v1_63@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_64",
            "tgt_ix": "90-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_64",
            "tgt_ix": "90-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_64",
            "tgt_ix": "90-ARR_v1_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_65",
            "tgt_ix": "90-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_66",
            "tgt_ix": "90-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_66",
            "tgt_ix": "90-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_66",
            "tgt_ix": "90-ARR_v1_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_67",
            "tgt_ix": "90-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_68",
            "tgt_ix": "90-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_68",
            "tgt_ix": "90-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_69",
            "tgt_ix": "90-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_70",
            "tgt_ix": "90-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_70",
            "tgt_ix": "90-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_70",
            "tgt_ix": "90-ARR_v1_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_70",
            "tgt_ix": "90-ARR_v1_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_70",
            "tgt_ix": "90-ARR_v1_70@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_71",
            "tgt_ix": "90-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_72",
            "tgt_ix": "90-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_72",
            "tgt_ix": "90-ARR_v1_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_72",
            "tgt_ix": "90-ARR_v1_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_72",
            "tgt_ix": "90-ARR_v1_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_72",
            "tgt_ix": "90-ARR_v1_72@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_72",
            "tgt_ix": "90-ARR_v1_72@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_72",
            "tgt_ix": "90-ARR_v1_72@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_73",
            "tgt_ix": "90-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_73",
            "tgt_ix": "90-ARR_v1_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_73",
            "tgt_ix": "90-ARR_v1_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_73",
            "tgt_ix": "90-ARR_v1_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_73",
            "tgt_ix": "90-ARR_v1_73@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_73",
            "tgt_ix": "90-ARR_v1_73@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_74",
            "tgt_ix": "90-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_74",
            "tgt_ix": "90-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_74",
            "tgt_ix": "90-ARR_v1_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_75",
            "tgt_ix": "90-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_75",
            "tgt_ix": "90-ARR_v1_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_75",
            "tgt_ix": "90-ARR_v1_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_75",
            "tgt_ix": "90-ARR_v1_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_76",
            "tgt_ix": "90-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_76",
            "tgt_ix": "90-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_76",
            "tgt_ix": "90-ARR_v1_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_76",
            "tgt_ix": "90-ARR_v1_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_77",
            "tgt_ix": "90-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_77",
            "tgt_ix": "90-ARR_v1_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_77",
            "tgt_ix": "90-ARR_v1_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_77",
            "tgt_ix": "90-ARR_v1_77@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_77",
            "tgt_ix": "90-ARR_v1_77@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_77",
            "tgt_ix": "90-ARR_v1_77@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_78",
            "tgt_ix": "90-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_78",
            "tgt_ix": "90-ARR_v1_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_78",
            "tgt_ix": "90-ARR_v1_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_78",
            "tgt_ix": "90-ARR_v1_78@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_78",
            "tgt_ix": "90-ARR_v1_78@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_79",
            "tgt_ix": "90-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_80",
            "tgt_ix": "90-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_81",
            "tgt_ix": "90-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_82",
            "tgt_ix": "90-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_82",
            "tgt_ix": "90-ARR_v1_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_83",
            "tgt_ix": "90-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_83",
            "tgt_ix": "90-ARR_v1_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_83",
            "tgt_ix": "90-ARR_v1_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_83",
            "tgt_ix": "90-ARR_v1_83@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_83",
            "tgt_ix": "90-ARR_v1_83@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_83",
            "tgt_ix": "90-ARR_v1_83@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_83",
            "tgt_ix": "90-ARR_v1_83@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_84",
            "tgt_ix": "90-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_85",
            "tgt_ix": "90-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_86",
            "tgt_ix": "90-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_87",
            "tgt_ix": "90-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_88",
            "tgt_ix": "90-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_89",
            "tgt_ix": "90-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_90",
            "tgt_ix": "90-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_91",
            "tgt_ix": "90-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_92",
            "tgt_ix": "90-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_93",
            "tgt_ix": "90-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_94",
            "tgt_ix": "90-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_95",
            "tgt_ix": "90-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_96",
            "tgt_ix": "90-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_97",
            "tgt_ix": "90-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_98",
            "tgt_ix": "90-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_99",
            "tgt_ix": "90-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_100",
            "tgt_ix": "90-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_101",
            "tgt_ix": "90-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_102",
            "tgt_ix": "90-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_103",
            "tgt_ix": "90-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_104",
            "tgt_ix": "90-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_105",
            "tgt_ix": "90-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_106",
            "tgt_ix": "90-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_107",
            "tgt_ix": "90-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_108",
            "tgt_ix": "90-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_109",
            "tgt_ix": "90-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_110",
            "tgt_ix": "90-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_111",
            "tgt_ix": "90-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_112",
            "tgt_ix": "90-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_113",
            "tgt_ix": "90-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_114",
            "tgt_ix": "90-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_115",
            "tgt_ix": "90-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_116",
            "tgt_ix": "90-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_117",
            "tgt_ix": "90-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_118",
            "tgt_ix": "90-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_119",
            "tgt_ix": "90-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_120",
            "tgt_ix": "90-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_121",
            "tgt_ix": "90-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_122",
            "tgt_ix": "90-ARR_v1_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_123",
            "tgt_ix": "90-ARR_v1_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_124",
            "tgt_ix": "90-ARR_v1_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_125",
            "tgt_ix": "90-ARR_v1_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_126",
            "tgt_ix": "90-ARR_v1_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_127",
            "tgt_ix": "90-ARR_v1_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_128",
            "tgt_ix": "90-ARR_v1_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_129",
            "tgt_ix": "90-ARR_v1_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_130",
            "tgt_ix": "90-ARR_v1_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_131",
            "tgt_ix": "90-ARR_v1_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_132",
            "tgt_ix": "90-ARR_v1_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_133",
            "tgt_ix": "90-ARR_v1_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_134",
            "tgt_ix": "90-ARR_v1_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_135",
            "tgt_ix": "90-ARR_v1_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_136",
            "tgt_ix": "90-ARR_v1_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_137",
            "tgt_ix": "90-ARR_v1_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_138",
            "tgt_ix": "90-ARR_v1_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_139",
            "tgt_ix": "90-ARR_v1_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_140",
            "tgt_ix": "90-ARR_v1_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_141",
            "tgt_ix": "90-ARR_v1_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_142",
            "tgt_ix": "90-ARR_v1_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_143",
            "tgt_ix": "90-ARR_v1_143@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_144",
            "tgt_ix": "90-ARR_v1_144@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_145",
            "tgt_ix": "90-ARR_v1_145@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_146",
            "tgt_ix": "90-ARR_v1_146@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "90-ARR_v1_147",
            "tgt_ix": "90-ARR_v1_147@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1674,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "90-ARR",
        "version": 1
    }
}