{
    "nodes": [
        {
            "ix": "152-ARR_v2_0",
            "content": "Sentence-Level Resampling for Named Entity Recognition",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_2",
            "content": "As a fundamental task in natural language processing, named entity recognition (NER) aims to locate and classify named entities in unstructured text. However, named entities are always the minority among all tokens in the text. This data imbalance problem presents a challenge to machine learning models as their learning objective is usually dominated by the majority of non-entity tokens. To alleviate data imbalance, we propose a set of sentence-level resampling methods where the importance of each training sentence is computed based on its tokens and entities. We study the generalizability of these resampling methods on a wide variety of NER models (CRF, Bi-LSTM, and BERT) across corpora from diverse domains (general, social, and medical texts). Extensive experiments show that the proposed methods improve performance of the evaluated NER models especially on small corpora, frequently outperforming sub-sentence-level resampling, data augmentation, and special loss functions such as focal and Dice loss. 1",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "152-ARR_v2_4",
            "content": "In natural language processing, named entity recognition (NER) is an important task both on its own and for numerous downstream tasks such as entity linking and question answering. NER has an inherent data imbalance problem: named entities of interest are almost always the minority among irrelevant (Other type) tokens in a text corpus. Table 1 shows the prevalent imbalanced nature of NER corpora from multiple domains. As shown in Table 1, entity tokens (tokens associated with any named entity) account for 3.9-16.6% of all tokens in any of these corpora. Within entity tokens, the most frequent entity type may cover 2-200 times more tokens than the least frequent entity type. At the sentence level, 23-85% sentences contain at least one entity, suggesting that 15-77% sentences contain no entity at all.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_5",
            "content": "Data imbalance is even more severe in real-world bespoke NER tasks, which directly motivated this work. For example, given full-text articles from a medical subfield, domain experts may wish to recognize only those concepts related to specific aspects of the subfield (e.g., symptoms and medicine related to a specific disease). Compared to all tokens in the full text, extremely few tokens are annotated with any entity type. Because domain experts have limited availability, annotated corpus are usually small in such tasks. As a result, some rare entity types may have less than 10 tokens across the corpus. Such severe data imbalance and scarcity makes many NER models suffer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_6",
            "content": "Data imbalance in NER challenges machine learning-based models because their learning objective is dominated by entities of the majority type (Other), causing the model to be reluctant to predict the types of interest. Various techniques have been studied to tackle this challenge. Active learning was applied to collect a more balanced dataset at annotation time (Tomanek and Hahn, 2009). Special loss functions including focal loss (Lin et al., 2017) and Dice loss (Li et al., 2019) are proposed to deal with data imbalance. Data augmentation was shown to be effective by enriching entity-bearing sentences through methods like segment shuffling and mention replacement (Dai and Adel, 2020;Issifu and Ganiz, 2021;Wang and Henao, 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_7",
            "content": "The classical method for alleviating data imbalance is resampling (upsampling the minority class or downsampling the majority class) and its close relative, cost-sensitive learning (assigning larger weight to the minority class or smaller weight to the majority class in the learning objective) (He and Garcia, 2009). A natural question is: Can we apply resampling to address the data imbalance problem in NER? It turns out that unlike classification tasks, applying resampling to sequence (Derczynski et al., 2017); GMB subset (Bos et al., 2017); AnEM (Ohta et al., 2012); CADEC (Karimi et al., 2015); CoNLL (Sang and De Meulder, 2003); n2c2 ADE (Henry et al., 2020); OntoNotes (Ralph et al., 2013). 'Sent.' = Sentences; 'Freq.' = Frequent.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_8",
            "content": "tagging tasks like NER is not straightforward. Recent work attempted sub-sentence-level resampling -dropping tokens from the majority class either at random (Akkasi, 2018) or using heuristic rules Akkasi and Varoglu, 2019;Grancharova et al., 2020). These methods were shown to perform well with shallow NER models -conditional random fields with local n-gram and word shape features. However, sub-sentencelevel resampling inevitably destroy the structure of complete sentences and distort the contextual information around entities of interest. Complete sentences are essential for state-of-the-art NER models based on contextual word representations, e.g., deep Transformers (Devlin et al., 2018). As shown in our experiments, incomplete sentences generated by sub-sentence-level resampling often hurt the performance of deep NER models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_9",
            "content": "In this paper, we propose sentence-level resampling methods for NER, an underexplored problem in this area. As sentences are the natural units of data in NER, sentence-level resampling leaves the contextual information intact in a natural sentence needed by deep models like Transformers. Since a sentence may contain a mixture of entities whose types have different levels of rareness, traditional resampling method for imbalanced classification (e.g., inverse probability resampling) cannot be applied. Instead, we develop a set of methods for computing integer-valued importance score for a sentence based on its entity composition, and resample the sentence accordingly. Experiments show that our methods can improve performance of a variety of NER models and are especially effective on tasks with small annotated corpora, which is often seen in real-world bespoke NER tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_10",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "152-ARR_v2_11",
            "content": "Learning from Imbalanced Data",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "152-ARR_v2_12",
            "content": "Class imbalance is a long-standing problem in machine learning tasks, posing challenges to researchers and practitioners in many domains (King and Zeng, 2001;Lu and Jain, 2003;He and Garcia, 2009;Moreo et al., 2016). Classes in real-world data often have highly skewed distribution, leading to substantial gaps between majority and minority classes. While the positive (minority) class is often of interest, the lack of positive examples makes classifiers conservative, i.e., they incline to predict all example as the negative (majority) class. This often results in a low recall of the positive class. Because only a small number of examples are predicted as positive, precision of the positive class tends to be high or unstable. Such a low-recall, high-precision pattern often hurts the F1-score, the standard metric that emphasizes a balanced precision and recall (Juba and Le, 2019). This performance pattern is observed not only in classification tasks, but also in NER tasks where named entity tokens are the minority compared to non-entity tokens (Mao et al., 2007;Kuperus et al., 2013).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_13",
            "content": "Researchers have proposed various techniques for imbalanced learning, including resampling and cost-sensitive learning (He and Garcia, 2009). Both aim to re-balance the representation of different classes in the loss function, such that the classifier is less conservative in making positive predictions. In principle, by equating per-instance resampling frequency with per-instance cost, resampling can be implemented as cost-sensitive learning. How-ever, resampling can be applied to models that do not support cost-sensitive learning, making it conveniently applicable to all models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_14",
            "content": "Resampling in Sequence Tagging Tasks",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "152-ARR_v2_15",
            "content": "Resampling (and cost-sensitive learning) can be conveniently used in classification and regression tasks where a model makes pointwise predictions (a single categorical or scalar value). Each example has a clearly defined sampling rate (or cost) according to its class label. However, in sequence tagging tasks like NER (more broadly, structured prediction tasks (BakIr et al., 2007;Smith, 2011)), a model predicts multiple values for a sequence (or structured output). For sequence learning algorithms such as linear-chain conditional random fields, while the learning objective is formulated at the sequence level, the evaluation metrics are defined at the entity span level. This makes it nontrivial to determine the sampling rate (or cost) for a sequence that contains tokens from both majority and minority entity types. Simply resampling entities by stripping surrounding context is problematic as sequence tagging algorithms depend on context to make predictions. Recent works proposed to randomly or heuristically drop tokens from sentences to re-balance NER data, which had success using conditional random fields and shallow n-gram features (Akkasi, 2018;Akkasi and Varoglu, 2019;Grancharova et al., 2020). However, these methods distort the syntactic and semantic structure of complete sentences, which may generate low-quality data for models that are capable of capturing longdistance linguistic dependencies (e.g. BERT) and hurt performance of those models. In this work, we focus on resampling strategies that leaves sentences intact.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_16",
            "content": "Loss Functions for Imbalanced Data",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "152-ARR_v2_17",
            "content": "Recent literature proposed special loss functions for tackling data imbalance, including focal loss (Lin et al., 2017) and Dice loss (Li et al., 2019). They increase the cost of 'hard positives' where the correct label has low predicted probability and decrease the cost of 'easy negatives' where the correct label has high predicted probability. However, these loss functions do not fully address data imbalance in NER. First, the formulation does not always emphasize the loss of minority-class tokens -majority-class tokens can also be hard to classify, and minority-class tokens can also be easy to classify. Second, these loss functions only work on token-wise prediction outputs. They cannot work on sequence-level outputs generated by conditional random fields, which is commonly used in NER. Our resampling methods can be seen as estimating sentence-level losses with explicit emphasis on sentences containing minority-class tokens.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_18",
            "content": "Resampling Strategy Design",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "152-ARR_v2_19",
            "content": "For a sequence tagging task like NER, resampling cannot be as simple as what it is in classification and regression tasks, in which data points can be individually replicated, discarded, or synthesized. In NER, named entities cannot be resampled out of context. The surrounding context of named entities -albeit tokens from the irrelevant Other typeshould be considered as well. Resampling named entities with context is a double-edged sword: preserving context will help NER models, but too much context increases the amount of non-entity tokens and aggravates the data imbalance problem. The goal of sentence-level resampling is to find the balance between too little and too much context accompanying named entities in complete sentences.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_20",
            "content": "Sentence Importance Factors in NER",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "152-ARR_v2_21",
            "content": "Intuitively, sentences that are worth resampling are those that are more important towards constructing a balanced NER dataset. We start by proposing factors that influence the importance of a sentence in resampling. These factors share the theoretical foundation of retrieval functions in information retrieval (Fang et al., 2004). A retrieval function evaluates the utility of a document with respect to the query terms it contains. By direct analogy, sentence importance score measures the utility of a sentence with respect to the entities it contains.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_22",
            "content": "Count of entity tokens. Regardless of entity types, a sentence containing more entity tokens is more important than a sentence filled with nonentity tokens. This factor mirrors term frequency in retrieval functions (Salton and Buckley, 1988).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_23",
            "content": "Rareness of entity type. The general idea of resampling for minority classes says that the rarer an entity type is, the more times we should resample sentences containing this type of entity. This factor mirrors inverse document frequency in retrieval functions (Salton and Buckley, 1988).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_24",
            "content": "Density of tokens labeled as any entity. Including too much context can aggravate the imbalance problem. While the absolute count of entity tokens matters, the density of entity tokens in a sentence (number of entity tokens compared to the length of a sentence) should also be concerned. The higher the density, the more important a sentence. This factor mirrors document length normalization in retrieval functions (Singhal et al., 1996).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_25",
            "content": "Diminishing marginal utility. If one sentence contains twice as many tokens with a specific entity type as the other sentence with the same length, does that mean the first sentence is twice as important as the second? In reality, an entity may contain numerous tokens, or a sentence may include multiple entities of the same type. Twice as many tokens from the same entity type may not offer twice as much information (for the same reason why too many tokens from the Other type is not helpful). Therefore, as the number of tokens from the same entity type increases, they generate diminishing marginal utility to a sentence. This factor mirrors diminishing marginal gain of repeated query terms in retrieval functions (Fang et al., 2004).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_26",
            "content": "Resampling Functions",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "152-ARR_v2_27",
            "content": "Based on the above importance factors, we design a suite of functions f s \u2208 Z + to determine the number of times a sentence s should be resampled in a NER training set. These functions incorporate progressively more factors discussed previously.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_28",
            "content": "In a given corpus, let us denote the set of all entity types except for the majority type Other as T . Let c(t, s) be the count of tokens with entity type t \u2208 T in sentence s. We define the resampling function with respect to the smoothed count (sC) of all entity tokens as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_29",
            "content": "f sC s = 1 + t\u2208T c(t, s) .(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_30",
            "content": "Here, t\u2208T c(t, s) is the total number of entity tokens in sentence s. '+1' is to avoid removing entity-less sentences from the training set, in reminiscence of add-one smoothing in empirical probability estimates. It guarantees that all training sentences are resampled as least once. This smoothinglike process maintains consistency between training and test sets. If the training set contains entityless sentences, it is highly likely that the test set will contain entity-less sentences as well.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_31",
            "content": "The next function incorporates entity rareness factor. The rareness r t of an entity type t \u2208 T is measured as the self-information of the event that any token carries this type:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_32",
            "content": "r t = \u2212 log 2 s\u2208S c(t, s) N ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_33",
            "content": "where S is the set of all sentences in the training set, and therefore s\u2208S c(t, s) is the total number of tokens with entity type t. N is number of all tokens (including Other tokens) in the training set. By introducing the rareness of an entity type we propose another function called the smoothed resampling incorporating count and rareness (sCR):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_34",
            "content": "f sCR s = 1 + \uf8ee \uf8ef \uf8ef \uf8ef t\u2208T r t \u2022 c(t, s) \uf8f9 \uf8fa \uf8fa \uf8fa .(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_35",
            "content": "Ceiling function \u2308\u2022\u2309 ensures f sCR s \u2208 Z + . Square root is to slow down the increase of f sCR s when an entity type t is extremely rare (when r t is large).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_36",
            "content": "According to the density factor in the previous section, the length of sentence s plays a role in determining the density of entity tokens within a sentence. Let l s be the length of sentence s measured in number of tokens. We define the following function called the smoothed resampling incorporating count, rareness, and density (sCRD):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_37",
            "content": "f sCRD s = 1 + t\u2208T r t \u2022 c(t, s) \u221a l s .(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_38",
            "content": "We use \u221a l s instead of l s to slow down the decrease of f sCRD s when a sentence is too long. Lastly, we incorporate the diminishing marginal utility factor and propose a function called the normalized and smoothed resampling incorporating count, rareness, and density (nsCRD):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_39",
            "content": "f nsCRD s = 1 + t\u2208T r t \u2022 c(t, s) \u221a l s .(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_40",
            "content": "Here, c(t, s) applies a sublinear increasing function on c(t, s) to implement the diminishing marginal utility when a sentence contains many tokens with the same type.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_41",
            "content": "In summary, we proposed four functions for determining resampling frequencies for each sentence, representing four resampling methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_42",
            "content": "Experimental Evaluation",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "152-ARR_v2_43",
            "content": "Resampling should be a domain-and modelagnostic strategy in tackling data imbalance. Therefore, the goal of our experiments is to evaluate if the proposed resampling methods are effective in an extensive array of NER corpora and base models. Towards this goal, we apply the four resampling methods (together with baseline methods) on three representative NER models (each has two variants), 2154 and evaluate the resulting models on four corpora from diverse domains.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_44",
            "content": "Evaluation Metric",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "152-ARR_v2_45",
            "content": "We use span-level strict-match macro-averaged F1 score as our main evaluation metric. Other is not viewed as an entity type. Macro-averaged metrics emphasize a balanced treatment of all entity types, which align with our main goal. See Appendix C for micro-averaged and per-entity-type results.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_46",
            "content": "Compared Methods",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "152-ARR_v2_47",
            "content": "We compare the following baseline methods for dealing with data imbalance in NER.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_48",
            "content": "Original corpus: training data untreated.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_49",
            "content": "Balanced undersampling: We implement the algorithm proposed in as a representative of sub-sentence-level resampling.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_50",
            "content": "Data augmentation: The data augmentation techniques that includes all transformations as proposed in (Dai and Adel, 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_51",
            "content": "Focal loss(Lin et al., 2017), Dice loss (Li et al., 2019): We apply these loss functions on tokenwise predictions made by a softmax output layer. Note that they are not applicable to sequence-level predictions made by a CRF output layer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_52",
            "content": "sC, sCR, sCRD, nsCRD: the four resampling methods proposed in this work.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_53",
            "content": "NER Corpora",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "152-ARR_v2_54",
            "content": "We select four corpora from different domains. The first three are of small scale, representing bespoke NER tasks in practice where entity types are taskspecific and annotation efforts are limited.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_55",
            "content": "AnEM (Ohta et al., 2012): The Anatomical Entity Mention (AnEM) corpus consists of 500 documents selected randomly from citation abstracts and full-text papers concerning both health and pathological anatomy. With only 3.91% entity tokens and 35.38% sentences having any entity, this is a very imbalanced corpus in Table 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_56",
            "content": "WNUT (Derczynski et al., 2017): This is a social domain corpus released in the 2017 Workshop on Noisy User-generated Text (W-NUT). It contains noisy user-generated texts found in social media, online review, crowdsourcing, web forums, clinical records, and language learner essays. This is another very imbalanced corpus in Table 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_57",
            "content": "GMB subset (Bos et al., 2017;Kaggle, 2018): The Groningen Meaning Bank (GMB) corpus consists of public domain English texts with corresponding syntactic and semantic representations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_58",
            "content": "The GMB subset is extracted from the larger GMB 2.0.0 corpus which is built specially for NER.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_59",
            "content": "To test the generalizability of our methods, we also include a standard NER benchmark dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_60",
            "content": "CoNLL (Sang and De Meulder, 2003): The CoNLL-2003 English news NER corpus.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_61",
            "content": "For AnEM, WNUT, and CoNLL, we use their pre-existing training/test split. For GMB subset, we use 3:1 training/test split.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_62",
            "content": "Base NER Models and Variants",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "152-ARR_v2_63",
            "content": "To comprehensively evaluate the combinations of our upstream resampling strategies with many downstream sequence tagging models, we select the following models:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_64",
            "content": "Shallow Model. We construct shallow NER models that use pretrained word embeddings as per-word feature vectors. We consider two variants: one using a softmax output layer making tokenwise predictions; the other using a CRF (conditional random fields) output layer making sequencelevel predictions. Considering domains of the corpora, we select embeddings trained on biomedical literature (Huang et al., 2016), tweets (Glove-27Btwitter-27B), 2 and Wikipedia+news (Glove-6B), 3 for AnEM, WNUT, and datasets from general domain (GMB subset and CoNLL), respectively. All are 50-dimensional embeddings. CrfSuite 4 is applied with default hyperparameters.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_65",
            "content": "Bi-LSTM (Bidirectional Long Short-Term Memory). LSTM is a special recurrent neural network architecture in which the vanishing gradient problem can be effectively mitigated. Bi-LSTM consists of two LSTMs taking inputs in both forward and backward directions. Even though more recent models (e.g., GPT-2, BERT) are shown to outperform Bi-LSTM, it is still regarded as one of the most prevalent tools for solving sequence tagging problems. We implement two variants of Bi-LSTM: one with a softmax output layer making token-wise predictions; the other with a CRF decoding layer 5 , to ensure the local consistency of output tags. Different from the default hyperparameters, batch size and number of epochs are set to 32 and 20, respectively. Embeddings are used in the same way as in the shallow models above.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_66",
            "content": "BERT (Bidirectional Encoder Representations from Transformers). BERT is widely regarded as the most significant improvement in natural language processing. Its outstanding capability of learning contextualized word representations makes it the representative of advanced NER model in this work. Again, we implement two variants of BERT: one with a softmax output layer making token-wise predictions; the other with a CRF decoding layer 6 . Default hyperparameters are used. More implementation details are in Appendix A.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_67",
            "content": "Results and Discussion",
            "ntype": "title",
            "meta": {
                "section": "4.5"
            }
        },
        {
            "ix": "152-ARR_v2_68",
            "content": "Macro-averaged F1-scores of different methods applied to four corpora and three base NER models are reported in Tables 2-5. Our goal is not to compete with state-of-theart methods on these corpora. Instead, we aim to present an interesting and underexplored problem (sentence-level resampling for NER) and a set of simple yet promising methods. In principle, our proposed resampling methods are model-agnostic and can provide an additional performance boost for a variety of NER models. We observe the following trends in Tables 2-5.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_69",
            "content": "Overall performance of our resampling methods: Across Tables 2-5, our methods (sC, sCR, sCRD, nsCRD) generally performed well, achieving the highest or second highest F1-scores in almost every column (except for the condition 'Shallow model, Softmax' on CoNLL). Although no specific method consistently outperforms others in every condition, it is clear that sentence-level resampling is overall a promising approach to tackling the data imbalance problem in NER. The best resampling method depends on the specific base model, output layer, and corpus used. Just as the best hyperparameter values have to be empirically determined, so could be the most suitable resampling method. Fortunately, all our resampling methods are simple and straightforward, which allows for convenient experimentation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_70",
            "content": "Shallow vs. deep models: We observe a clear trend that shallow models using word embedding as features and softmax/CRF as the output layer underperform deep models such as Bi-LSTM and BERT. We view this as a sanity check. Bi-LSTMs and BERT can learn word representations that account for long-distance dependencies, and BERT should be even more powerful with contextual word representations pretrained on massive texts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_71",
            "content": "Softmax vs. CRF output layer: Using the same base model, CRF output layer often (but not always) outperforms softmax output layer. The performance gap is larger on shallow models and small corpora (AnEM, WNUT, GMB) than on deep models and large corpus (CoNLL). Indeed, Bi-LSTM and BERT are capable of learning word representations that account for long-distance word dependencies, reducing the benefit of tag dependencies offered by a CRF layer. Similar observation was made by previous work (Devlin et al., 2018). An exception is the combination (WNUT, Shallow model), where the CRF layer suffered from severe overfitting caused by noisy text and extremely imbalanced data distribution in WNUT corpus.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_72",
            "content": "Small vs. large corpus: On small corpora (AnEM, WNUT, GMB subset), our resampling methods usually outperform the original corpus baseline by a big margin. These benefits become less salient on large corpus (CoNLL). This implies that our methods are especially effective when the corpus is small and annotations are few. As corpus size gets large, even rare entity types are covered by many examples and therefore sufficiently trained.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_73",
            "content": "Sub-sentence resampling and data augmentation: Sub-sentence resampling (balanced undersampling) has large variance in its performance. In some cases it gives the highest gain (GMB subset, BERT model), and in other cases it performs worse than just using the original corpus (all corpora, Bi-LSTM models). It suggests that sub-sentence resampling is highly sensitive to the corpus and model choice. Data augmentation also shows high variance in its performance. It gives the highest gain on (GMB subset, Bi-LSTM model), and performs worse than the original corpus on (WNUT, BERT model). Sentences generated by data augmentation generally have correct syntax but garbled semantics (e.g., one entity is replaced by another same-type, out-of-context entity). The nonsensical sentences may confuse NER models. In contrast, whole-sentence resampling methods give more stable improvements over the original corpus baseline largely because they preserve the naturalness of resampled sentences.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_74",
            "content": "Focal loss and Dice loss: These loss functions are applicable only on pointwise predictions made by the softmax output layer. A major trend is that their performance tend to be unreliable across scenarios. We attribute this behavior to the difficulty in optimizing these losses. For shallow models, we optimize them by feeding gradients of either loss function (see Appendix B for their derivation) into a L-BFGS optimizer (Liu and Nocedal, 1989) in Scikit-Learn. As shown in the (Shallow model, Softmax) column of GMB subset and CoNLL corpora, the two loss functions (especially the Dice loss) performed well. For deep models (Bi-LSTM and BERT), we rely on TensorFlow's automatic differentiation and Adam gradient descent optimizer (Kingma and Ba, 2014) because manually deriving gradients for deep models is infeasible. The two loss functions sometimes give poor performance. The Bi-LSTM model with Dice loss failed completely on AnEM (F1-score: 2.31). A possible explanation is that Dice loss is non-convex and it may be difficult for first-order optimizers in current deep learning toolkits (e.g. Adam in TensorFlow) to find high-quality local minima than second-order methods like L-BFGS.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_75",
            "content": "Precision and recall: To illustrate performance changes in terms of precision and recall, Figure 1 visualizes the changes before and after resampling as displacement vectors in precision-recall plots with F1-score contour lines. Some arrows are pointing to the upper right corner of the plots, indicating the associated methods improve F1-score by improving both precision and recall. Other arrows point to the upper left, indicating the associated methods increase recall at the sacrifice of precision. In this case, most of our methods improve macroaveraged precision and recall of the BERT model on WNUT. See Appendix C.2 for more details.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_76",
            "content": "Effect on Training Corpus Size",
            "ntype": "title",
            "meta": {
                "section": "4.6"
            }
        },
        {
            "ix": "152-ARR_v2_77",
            "content": "Table 6 shows the effect of training corpus size as a result of resampling or data augmentation. These factors are the average across four corpora.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_78",
            "content": "The balanced undersampling method drops tokens from sentences, and therefore reduces training corpus size. Data augmentation method doubles the corpus size as many sentences are paraphrased into multiple versions. Our proposed methods increases corpus size by a slightly larger factor because sentences that contain rare entity types are resampled multiple times. Although increased training corpus size leads to increased training time, note that our methods are especially suitable for scenarios where the annotated corpus is small and hence the training time is still relatively short. Each NER model (Shallow, Bi-LSTM, BERT) is associated with a cluster of vectors sharing the same starting point in the space, which represents the performance on the original corpus.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_79",
            "content": "Conclusion and Future Work",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "152-ARR_v2_80",
            "content": "Our proposed sentence resampling methods generalize well across diverse NER corpora and models. They enjoy the following advantages: Model-agnostic: Since resampling only manipulates datasets and not models, the proposed methods can be directly applied to any NER model, requiring no knowledge of its functioning or any change to it. Resampling is also more convenient than cost-sensitive learning as the latter still requires changing the model training process.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_81",
            "content": "Domain-agnostic: Compared with data preprocessing methods such as data augmentation, sentence-level resampling methods are simple and do not require domain-or language-specific manipulations such as synonym replacement, saving practitioners from excessive data engineering.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_82",
            "content": "Note that data augmentation and sentence-level resampling (and resampling methods in general) are complementary methods for improving NER model training. Data augmentation improves the semantic richness of training instances by expanding the coverage of training data in the input feature space, while sentence-level resampling refines the importance weighting of training instances by bridging the gap between the training objective and evaluation metrics. Therefore, they work in orthogonal directions. This points to a promising direction for future work: to explore the two line of methods in combination rather than in competition.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_83",
            "content": "Various other avenues exist for future work. First, further theoretical and empirical research can explore more effective resampling functions that deliver consistently better performance across corpora and base NER models. Second, more corpora and models can be examined under these resampling strategies to evaluate their generalizability. Third, the variance of performance in different scenarios may potentially relate to characteristics of specific corpora. Future research may seek for corpora-level statistics that can assist practitioners in selecting the appropriate resampling methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_84",
            "content": "A.1 Software and Hardware Environment All the deep learning models are implemented in Tensorflow 1.12.0 environment.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_85",
            "content": "Softmax regression (or multinomial logistic regression) model is from Scikit-Learn package in version 0.23.2. The CRF model is implemented by the package sklearn-crfsuite in version of 0.3.6.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_86",
            "content": "Data resampling and CRF training/evaluation were performed on 2.60 GHz Intel CPUs and 8GB RAM. Bi-LSTM and BERT training/evaluation were performed on GPUs (GeForce GTX1080 8GB and Tesla V100 16GB).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_87",
            "content": "For shallow models and BERT, all hyperparameters are set by default. For details of them, please see documents of sklearn, crfsuite and BERT-NER.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_88",
            "content": "For Bi-LSTM, we adjust a few of parameters as there are some drawbacks of the default settings: 20 is not a commonly used number for batch size, and loss of Bi-LSTM model fails to converge under some circumstances. So we set them to 32 and 20, instead of default values 20 and 15. Other hyperparameters are applied according to default settings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_89",
            "content": "For the fairness in the comparison, we do not alter any hyperparameters while switching resampling methods and loss functions without changing dataset and models. We believe that it is totally appropriate in the process of comparing, despite that better performance of specific methods may be obtained after tuning hyperparameters, which beyond the scope of this exploring research.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_90",
            "content": "There are two hyperparameters in the focal loss and Dice Loss, determining converging speed and smooth degree. For focal loss, we set \u03b3 to 2, as what authors of (Lin et al., 2017) recommend. According to (Li et al., 2019), it is appropriate to set \u03b3 of Dice loss to 1 for the purpose of smoothing. In our implementation of loss function in shallow model, this setting is found effective. However, while using it in deep learning model, its effectiveness cannot be ensured. Hence, we adopt another setting of \u03b3 = 10 \u22125 in the tensor computing and obtain better results compared with those obtained with a larger \u03b3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_91",
            "content": "When using the shallow model with softmax output layer and focal/Dice loss functions, we optimize the model parameters by the quasi-Newton method L-BFGS provided by Python Scikit-Learn. This approach requires us to provide the gradients of current model parameters. Below we show our derivation of these gradients. Notations and Preliminaries. Scalar values are denoted by non-bold, lowercase letters such as x. Row vectors are denoted by bold, lowercase letters such as x. Matrices are denoted by bold, uppercase letters such as X.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_92",
            "content": "Softmax regression has the following components:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_93",
            "content": "\u2022 Feature vector: \u2022 Weight vector for the j-th class:",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_94",
            "content": "x \u2208 R m , x = [x 1 , \u2022 \u2022 \u2022 , x i , \u2022 \u2022 \u2022 , x m ]. \u2022 Label vector: y \u2208 {0, 1} k , y = [y 1 , \u2022 \u2022 \u2022 , y j , \u2022 \u2022 \u2022 , y k ]. If the ground truth is the c-th class, 1 \u2264 c \u2264 k, then y c = 1, and y j = 0 if j \u0338 = c.",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_95",
            "content": "w j \u2208 R m , w j = [w j1 , \u2022 \u2022 \u2022 , w ji , \u2022 \u2022 \u2022 , w jm ]. \u2022 Weight matrix W \u2208 R m\u00d7k , W = [w \u22a4 1 , \u2022 \u2022 \u2022 , w \u22a4 j , \u2022 \u2022 \u2022 , w \u22a4 k ]",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_96",
            "content": ". \"\u22a4\" is the transpose operation. w \u22a4 is the transpose of w, which is a column vector.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_97",
            "content": "\u2022 Bias for the the j-th class: b j \u2208 R.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_98",
            "content": ": p \u2208 [0, 1] k , p = [p 1 , \u2022 \u2022 \u2022 , p j , \u2022 \u2022 \u2022 , p k ]. p j = Pr(y j = 1|x) (5) = exp(\u27e8w j , x\u27e9 + b j ) k j \u2032 =1 exp(\u27e8w j \u2032 , x\u27e9 + b j \u2032 ) (6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_99",
            "content": "\u27e8w, x\u27e9 is the inner product of vector w and vector x.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_100",
            "content": "One can verify that the partial derivative of p c with respect to w ji , the weight of the j-th class, the i-th dimension, is the following:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_101",
            "content": "\u2202p c \u2202w ji = [1{j = c} \u2212 p j ] p c x i(7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_102",
            "content": "Suppose the ground truth is the c-th class for a given example x.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_103",
            "content": "L F L (x, y) = \u2212(1 \u2212 p c ) \u03b3 log p c (8) \u2202L F L (x, y) \u2202w ji (9) = \u2212 \u2202 \u2202p c [(1 \u2212 p c ) \u03b3 log p c ] \u2022 \u2202p c \u2202w ji (10) = \u2212 \u2212\u03b3(1 \u2212 p c ) \u03b3\u22121 log p c + (1 \u2212 p c ) \u03b3 p c \u2022 \u2202p c \u2202w ji (11) = \u2212 \u2212\u03b3(1 \u2212 p c ) \u03b3\u22121 log p c + (1 \u2212 p c ) \u03b3 p c \u2022 [1{j = c} \u2212 p j ]p c x i (12) = \u2212 [\u2212\u03b3p c (1 \u2212 p c ) \u03b3\u22121 log p c + (1 \u2212 p c ) \u03b3 ] \u2022 [1{j = c} \u2212 p j ]x i (13) =a c [p j \u2212 1{j = c}]x i(14)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_104",
            "content": "Here we set",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_105",
            "content": "a c = \u2212\u03b3p c (1 \u2212 p c ) \u03b3\u22121 log p c + (1 \u2212 p c ) \u03b3 (15)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_106",
            "content": "to reduce notational clutter. a c has nothing to do with i or j; it only has to do with c, the index of the ground truth label for the training example x. When \u03b3 = 0, a c = 1. When \u03b3 > 0, a c decreases when p c increases from 0 to 1. This means the gradient for an easy example (when p c is close to 1) have a smaller magnitude than the gradient for a hard example (when p c is close to 0). Generalizing the scalar gradient in Equation ( 14) to matrix gradient, we have",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_107",
            "content": "\u2202L F L (x, y) \u2202W = a c \u2022 x \u22a4 (p \u2212 y) .(16)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_108",
            "content": "The shape of a c \u2022 x \u22a4 (p \u2212 y) is m \u00d7 k, the same shape as W.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_109",
            "content": "An important note is that here a c is specific to that single example x, which has ground truth label y c = 1. If we have n different training examples x (1) , \u2022 \u2022 \u2022 , x (n) , then every example will have a different a c value: a",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_110",
            "content": "(1) c , \u2022 \u2022 \u2022 , a (n) c . Let's create a diagonal matrix A c \u2208 R n\u00d7n , A c = diag(a (1) c , \u2022 \u2022 \u2022 , a (n) c ).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_111",
            "content": "If we have n training examples, then the feature matrix X \u2208 R n\u00d7m , the label matrix Y \u2208 {0, 1} n\u00d7k , and the predicted probability matrix P \u2208 [0, 1] n\u00d7k . We have:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_112",
            "content": "\u2202L F L (X, Y) \u2202W = X \u22a4 A c (P \u2212 Y) .(17)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_113",
            "content": "The shape of X \u22a4 A c (P \u2212 Y) is m \u00d7 k, the same as W.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_114",
            "content": "Dice loss computes per-class F-1 scores. Suppose the ground truth is the c-th class for a given example x.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_115",
            "content": "L DL (x, y) (18)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_116",
            "content": "= k j \u2032 =1 1 \u2212 1{c = j \u2032 } \u03b3 + 2p c \u03b3 + p 2 c + 1 +1 \u2212 1{c \u0338 = j \u2032 } \u03b3 \u03b3 + p 2 j \u2032 (19) =1 \u2212 \u03b3 + 2p c \u03b3 + p 2 c + 1 + j \u2032 \u0338 =c 1 \u2212 \u03b3 \u03b3 + p 2 j \u2032 (20) =k \u2212 \u03b3 + 2p c \u03b3 + p 2 c + 1 \u2212 j \u2032 \u0338 =c \u03b3 \u03b3 + p 2 j \u2032 (21)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_117",
            "content": "Take gradient with respect to w ji , the weight of the j-th class, the i-th dimension.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_118",
            "content": "\u2202L DL (x, y) \u2202w ji (22) = \u2212 \u2202 \u2202p c \u03b3 + 2p c \u03b3 + p 2 c + 1 \u2022 \u2202p c \u2202w ji \u2212 j \u2032 \u0338 =c \u2202 \u2202p j \u2032 \u03b3 \u03b3 + p 2 j \u2032 \u2022 \u2202p j \u2032 \u2202w ji (23) = \u2212 2(\u03b3 + p 2 c + 1) \u2212 (\u03b3 + 2p c )2p c (\u03b3 + p 2 c + 1) 2 \u2022 \u2202p c \u2202w ji \u2212 j \u2032 \u0338 =c \u2212\u03b3 \u2022 2p j \u2032 (\u03b3 + p 2 j \u2032 ) 2 \u2022 \u2202p j \u2032 \u2202w ji (24) = \u2212 2(\u03b3 + p 2 c + 1) \u2212 (\u03b3 + 2p c )2p c (\u03b3 + p 2 c + 1) 2 \u2022 [1{j = c} \u2212 p j ]p c x i \u2212 j \u2032 \u0338 =c \u2212\u03b3 \u2022 2p j \u2032 (\u03b3 + p 2 j \u2032 ) 2 \u2022 [1{j = j \u2032 } \u2212 p j ]p j \u2032 x i (25) = \u2212 2(1 \u2212 p c )(1 + \u03b3 + p c )p c (\u03b3 + p 2 c + 1) 2 \u2022 [1{j = c} \u2212 p j ]x i(26)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "152-ARR_v2_119",
            "content": "A Akkasi,  Varoglu, Improvement of chemical named entity recognition through sentence-based random under-sampling and classifier combination, 2019, Journal of AI and Data Mining, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "A Akkasi",
                    " Varoglu"
                ],
                "title": "Improvement of chemical named entity recognition through sentence-based random under-sampling and classifier combination",
                "pub_date": "2019",
                "pub_title": "Journal of AI and Data Mining",
                "pub": null
            }
        },
        {
            "ix": "152-ARR_v2_120",
            "content": "Abbas Akkasi, Sentence-based undersampling for named entity recognition using genetic algorithm, 2018, Iran Journal of Computer Science, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Abbas Akkasi"
                ],
                "title": "Sentence-based undersampling for named entity recognition using genetic algorithm",
                "pub_date": "2018",
                "pub_title": "Iran Journal of Computer Science",
                "pub": null
            }
        },
        {
            "ix": "152-ARR_v2_121",
            "content": "Abbas Akkasi, Ekrem Varoglu, Nazife Dimililer, Balanced undersampling: a novel sentencebased undersampling method to improve recognition of named entities in chemical and biomedical text, 2018, Applied Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Abbas Akkasi",
                    "Ekrem Varoglu",
                    "Nazife Dimililer"
                ],
                "title": "Balanced undersampling: a novel sentencebased undersampling method to improve recognition of named entities in chemical and biomedical text",
                "pub_date": "2018",
                "pub_title": "Applied Intelligence",
                "pub": null
            }
        },
        {
            "ix": "152-ARR_v2_122",
            "content": "UNKNOWN, None, 2007, Predicting structured data, MIT press.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": "2007",
                "pub_title": "Predicting structured data",
                "pub": "MIT press"
            }
        },
        {
            "ix": "152-ARR_v2_123",
            "content": "Johan Bos, Valerio Basile, Kilian Evang, Noortje Venhuizen, Johannes Bjerva, The groningen meaning bank, 2017, Handbook of Linguistic Annotation, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Johan Bos",
                    "Valerio Basile",
                    "Kilian Evang",
                    "Noortje Venhuizen",
                    "Johannes Bjerva"
                ],
                "title": "The groningen meaning bank",
                "pub_date": "2017",
                "pub_title": "Handbook of Linguistic Annotation",
                "pub": "Springer"
            }
        },
        {
            "ix": "152-ARR_v2_124",
            "content": "UNKNOWN, None, 2020, An analysis of simple data augmentation for named entity recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "An analysis of simple data augmentation for named entity recognition",
                "pub": null
            }
        },
        {
            "ix": "152-ARR_v2_125",
            "content": "Leon Derczynski, Eric Nichols, Marieke Van Erp, Nut Limsopatham, Results of the wnut2017 shared task on novel and emerging entity recognition, 2017, Proceedings of the 3rd Workshop on Noisy Usergenerated Text, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Leon Derczynski",
                    "Eric Nichols",
                    "Marieke Van Erp",
                    "Nut Limsopatham"
                ],
                "title": "Results of the wnut2017 shared task on novel and emerging entity recognition",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 3rd Workshop on Noisy Usergenerated Text",
                "pub": null
            }
        },
        {
            "ix": "152-ARR_v2_126",
            "content": "UNKNOWN, None, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub": null
            }
        },
        {
            "ix": "152-ARR_v2_127",
            "content": "Hui Fang, Tao Tao, Chengxiang Zhai, A formal study of information retrieval heuristics, 2004, Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Hui Fang",
                    "Tao Tao",
                    "Chengxiang Zhai"
                ],
                "title": "A formal study of information retrieval heuristics",
                "pub_date": "2004",
                "pub_title": "Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval",
                "pub": null
            }
        },
        {
            "ix": "152-ARR_v2_128",
            "content": "Mila Grancharova, Hanna Berg, Hercules Dalianis, Improving named entity recognition and classification in class imbalanced swedish electronic patient records through resampling, 2020, Eighth Swedish Language Technology Conference (SLTC). F\u00f6rlag G\u00f6teborgs Universitet, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Mila Grancharova",
                    "Hanna Berg",
                    "Hercules Dalianis"
                ],
                "title": "Improving named entity recognition and classification in class imbalanced swedish electronic patient records through resampling",
                "pub_date": "2020",
                "pub_title": "Eighth Swedish Language Technology Conference (SLTC). F\u00f6rlag G\u00f6teborgs Universitet",
                "pub": null
            }
        },
        {
            "ix": "152-ARR_v2_129",
            "content": "Haibo He, A Edwardo,  Garcia, Learning from imbalanced data, 2009, IEEE Transactions on knowledge and data engineering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Haibo He",
                    "A Edwardo",
                    " Garcia"
                ],
                "title": "Learning from imbalanced data",
                "pub_date": "2009",
                "pub_title": "IEEE Transactions on knowledge and data engineering",
                "pub": null
            }
        },
        {
            "ix": "152-ARR_v2_130",
            "content": "Sam Henry, Kevin Buchan, Michele Filannino, Amber Stubbs, Ozlem Uzuner, 2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records, 2020, Journal of the American Medical Informatics Association, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Sam Henry",
                    "Kevin Buchan",
                    "Michele Filannino",
                    "Amber Stubbs",
                    "Ozlem Uzuner"
                ],
                "title": "2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records",
                "pub_date": "2020",
                "pub_title": "Journal of the American Medical Informatics Association",
                "pub": null
            }
        },
        {
            "ix": "152-ARR_v2_131",
            "content": "Jian Huang, Keyang Xu,  Vydiswaran, Analyzing multiple medical corpora using word embedding, 2016, 2016 IEEE International Conference on Healthcare Informatics (ICHI), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Jian Huang",
                    "Keyang Xu",
                    " Vydiswaran"
                ],
                "title": "Analyzing multiple medical corpora using word embedding",
                "pub_date": "2016",
                "pub_title": "2016 IEEE International Conference on Healthcare Informatics (ICHI)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "152-ARR_v2_132",
            "content": "Abdul Majeed Issifu,  Murat Can,  Ganiz, A simple data augmentation method to improve the performance of named entity recognition models in medical domain, 2021, 2021 6th International Conference on Computer Science and Engineering (UBMK), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Abdul Majeed Issifu",
                    " Murat Can",
                    " Ganiz"
                ],
                "title": "A simple data augmentation method to improve the performance of named entity recognition models in medical domain",
                "pub_date": "2021",
                "pub_title": "2021 6th International Conference on Computer Science and Engineering (UBMK)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "152-ARR_v2_133",
            "content": "Brendan Juba, S Hai,  Le, Precision-recall versus accuracy and the role of large data sets, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Brendan Juba",
                    "S Hai",
                    " Le"
                ],
                "title": "Precision-recall versus accuracy and the role of large data sets",
                "pub_date": "2019",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "152-ARR_v2_134",
            "content": "UNKNOWN, None, 2018, Annotated GMB corpus, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Annotated GMB corpus",
                "pub": null
            }
        },
        {
            "ix": "152-ARR_v2_135",
            "content": "Sarvnaz Karimi, Alejandro Metke-Jimenez, Madonna Kemp, Chen Wang, Cadec: A corpus of adverse drug event annotations, 2015, Journal of biomedical informatics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Sarvnaz Karimi",
                    "Alejandro Metke-Jimenez",
                    "Madonna Kemp",
                    "Chen Wang"
                ],
                "title": "Cadec: A corpus of adverse drug event annotations",
                "pub_date": "2015",
                "pub_title": "Journal of biomedical informatics",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "152-ARR_v2_0@0",
            "content": "Sentence-Level Resampling for Named Entity Recognition",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_0",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_2@0",
            "content": "As a fundamental task in natural language processing, named entity recognition (NER) aims to locate and classify named entities in unstructured text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_2",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_2@1",
            "content": "However, named entities are always the minority among all tokens in the text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_2",
            "start": 150,
            "end": 226,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_2@2",
            "content": "This data imbalance problem presents a challenge to machine learning models as their learning objective is usually dominated by the majority of non-entity tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_2",
            "start": 228,
            "end": 389,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_2@3",
            "content": "To alleviate data imbalance, we propose a set of sentence-level resampling methods where the importance of each training sentence is computed based on its tokens and entities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_2",
            "start": 391,
            "end": 565,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_2@4",
            "content": "We study the generalizability of these resampling methods on a wide variety of NER models (CRF, Bi-LSTM, and BERT) across corpora from diverse domains (general, social, and medical texts).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_2",
            "start": 567,
            "end": 754,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_2@5",
            "content": "Extensive experiments show that the proposed methods improve performance of the evaluated NER models especially on small corpora, frequently outperforming sub-sentence-level resampling, data augmentation, and special loss functions such as focal and Dice loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_2",
            "start": 756,
            "end": 1015,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_2@6",
            "content": "1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_2",
            "start": 1017,
            "end": 1017,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_4@0",
            "content": "In natural language processing, named entity recognition (NER) is an important task both on its own and for numerous downstream tasks such as entity linking and question answering.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_4",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_4@1",
            "content": "NER has an inherent data imbalance problem: named entities of interest are almost always the minority among irrelevant (Other type) tokens in a text corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_4",
            "start": 181,
            "end": 336,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_4@2",
            "content": "Table 1 shows the prevalent imbalanced nature of NER corpora from multiple domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_4",
            "start": 338,
            "end": 420,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_4@3",
            "content": "As shown in Table 1, entity tokens (tokens associated with any named entity) account for 3.9-16.6% of all tokens in any of these corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_4",
            "start": 422,
            "end": 558,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_4@4",
            "content": "Within entity tokens, the most frequent entity type may cover 2-200 times more tokens than the least frequent entity type.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_4",
            "start": 560,
            "end": 681,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_4@5",
            "content": "At the sentence level, 23-85% sentences contain at least one entity, suggesting that 15-77% sentences contain no entity at all.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_4",
            "start": 683,
            "end": 809,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_5@0",
            "content": "Data imbalance is even more severe in real-world bespoke NER tasks, which directly motivated this work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_5",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_5@1",
            "content": "For example, given full-text articles from a medical subfield, domain experts may wish to recognize only those concepts related to specific aspects of the subfield (e.g., symptoms and medicine related to a specific disease).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_5",
            "start": 104,
            "end": 327,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_5@2",
            "content": "Compared to all tokens in the full text, extremely few tokens are annotated with any entity type.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_5",
            "start": 329,
            "end": 425,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_5@3",
            "content": "Because domain experts have limited availability, annotated corpus are usually small in such tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_5",
            "start": 427,
            "end": 525,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_5@4",
            "content": "As a result, some rare entity types may have less than 10 tokens across the corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_5",
            "start": 527,
            "end": 609,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_5@5",
            "content": "Such severe data imbalance and scarcity makes many NER models suffer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_5",
            "start": 611,
            "end": 679,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_6@0",
            "content": "Data imbalance in NER challenges machine learning-based models because their learning objective is dominated by entities of the majority type (Other), causing the model to be reluctant to predict the types of interest.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_6",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_6@1",
            "content": "Various techniques have been studied to tackle this challenge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_6",
            "start": 219,
            "end": 280,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_6@2",
            "content": "Active learning was applied to collect a more balanced dataset at annotation time (Tomanek and Hahn, 2009).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_6",
            "start": 282,
            "end": 388,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_6@3",
            "content": "Special loss functions including focal loss (Lin et al., 2017) and Dice loss (Li et al., 2019) are proposed to deal with data imbalance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_6",
            "start": 390,
            "end": 525,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_6@4",
            "content": "Data augmentation was shown to be effective by enriching entity-bearing sentences through methods like segment shuffling and mention replacement (Dai and Adel, 2020;Issifu and Ganiz, 2021;Wang and Henao, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_6",
            "start": 527,
            "end": 736,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_7@0",
            "content": "The classical method for alleviating data imbalance is resampling (upsampling the minority class or downsampling the majority class) and its close relative, cost-sensitive learning (assigning larger weight to the minority class or smaller weight to the majority class in the learning objective) (He and Garcia, 2009).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_7",
            "start": 0,
            "end": 316,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_7@1",
            "content": "A natural question is: Can we apply resampling to address the data imbalance problem in NER? It turns out that unlike classification tasks, applying resampling to sequence (Derczynski et al., 2017); GMB subset (Bos et al., 2017); AnEM (Ohta et al., 2012); CADEC (Karimi et al., 2015); CoNLL (Sang and De Meulder, 2003); n2c2 ADE (Henry et al., 2020); OntoNotes (Ralph et al., 2013). 'Sent.' = Sentences; 'Freq.' = Frequent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_7",
            "start": 318,
            "end": 740,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_8@0",
            "content": "tagging tasks like NER is not straightforward.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_8",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_8@1",
            "content": "Recent work attempted sub-sentence-level resampling -dropping tokens from the majority class either at random (Akkasi, 2018) or using heuristic rules Akkasi and Varoglu, 2019;Grancharova et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_8",
            "start": 47,
            "end": 247,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_8@2",
            "content": "These methods were shown to perform well with shallow NER models -conditional random fields with local n-gram and word shape features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_8",
            "start": 249,
            "end": 382,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_8@3",
            "content": "However, sub-sentencelevel resampling inevitably destroy the structure of complete sentences and distort the contextual information around entities of interest.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_8",
            "start": 384,
            "end": 543,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_8@4",
            "content": "Complete sentences are essential for state-of-the-art NER models based on contextual word representations, e.g., deep Transformers (Devlin et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_8",
            "start": 545,
            "end": 697,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_8@5",
            "content": "As shown in our experiments, incomplete sentences generated by sub-sentence-level resampling often hurt the performance of deep NER models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_8",
            "start": 699,
            "end": 837,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_9@0",
            "content": "In this paper, we propose sentence-level resampling methods for NER, an underexplored problem in this area.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_9",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_9@1",
            "content": "As sentences are the natural units of data in NER, sentence-level resampling leaves the contextual information intact in a natural sentence needed by deep models like Transformers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_9",
            "start": 108,
            "end": 287,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_9@2",
            "content": "Since a sentence may contain a mixture of entities whose types have different levels of rareness, traditional resampling method for imbalanced classification (e.g., inverse probability resampling) cannot be applied.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_9",
            "start": 289,
            "end": 503,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_9@3",
            "content": "Instead, we develop a set of methods for computing integer-valued importance score for a sentence based on its entity composition, and resample the sentence accordingly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_9",
            "start": 505,
            "end": 673,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_9@4",
            "content": "Experiments show that our methods can improve performance of a variety of NER models and are especially effective on tasks with small annotated corpora, which is often seen in real-world bespoke NER tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_9",
            "start": 675,
            "end": 879,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_10@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_10",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_11@0",
            "content": "Learning from Imbalanced Data",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_11",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_12@0",
            "content": "Class imbalance is a long-standing problem in machine learning tasks, posing challenges to researchers and practitioners in many domains (King and Zeng, 2001;Lu and Jain, 2003;He and Garcia, 2009;Moreo et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_12",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_12@1",
            "content": "Classes in real-world data often have highly skewed distribution, leading to substantial gaps between majority and minority classes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_12",
            "start": 217,
            "end": 348,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_12@2",
            "content": "While the positive (minority) class is often of interest, the lack of positive examples makes classifiers conservative, i.e., they incline to predict all example as the negative (majority) class.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_12",
            "start": 350,
            "end": 544,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_12@3",
            "content": "This often results in a low recall of the positive class.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_12",
            "start": 546,
            "end": 602,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_12@4",
            "content": "Because only a small number of examples are predicted as positive, precision of the positive class tends to be high or unstable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_12",
            "start": 604,
            "end": 731,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_12@5",
            "content": "Such a low-recall, high-precision pattern often hurts the F1-score, the standard metric that emphasizes a balanced precision and recall (Juba and Le, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_12",
            "start": 733,
            "end": 888,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_12@6",
            "content": "This performance pattern is observed not only in classification tasks, but also in NER tasks where named entity tokens are the minority compared to non-entity tokens (Mao et al., 2007;Kuperus et al., 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_12",
            "start": 890,
            "end": 1095,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_13@0",
            "content": "Researchers have proposed various techniques for imbalanced learning, including resampling and cost-sensitive learning (He and Garcia, 2009).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_13",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_13@1",
            "content": "Both aim to re-balance the representation of different classes in the loss function, such that the classifier is less conservative in making positive predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_13",
            "start": 142,
            "end": 303,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_13@2",
            "content": "In principle, by equating per-instance resampling frequency with per-instance cost, resampling can be implemented as cost-sensitive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_13",
            "start": 305,
            "end": 445,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_13@3",
            "content": "How-ever, resampling can be applied to models that do not support cost-sensitive learning, making it conveniently applicable to all models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_13",
            "start": 447,
            "end": 585,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_14@0",
            "content": "Resampling in Sequence Tagging Tasks",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_14",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_15@0",
            "content": "Resampling (and cost-sensitive learning) can be conveniently used in classification and regression tasks where a model makes pointwise predictions (a single categorical or scalar value).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_15",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_15@1",
            "content": "Each example has a clearly defined sampling rate (or cost) according to its class label.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_15",
            "start": 187,
            "end": 274,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_15@2",
            "content": "However, in sequence tagging tasks like NER (more broadly, structured prediction tasks (BakIr et al., 2007;Smith, 2011)), a model predicts multiple values for a sequence (or structured output).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_15",
            "start": 276,
            "end": 468,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_15@3",
            "content": "For sequence learning algorithms such as linear-chain conditional random fields, while the learning objective is formulated at the sequence level, the evaluation metrics are defined at the entity span level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_15",
            "start": 470,
            "end": 676,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_15@4",
            "content": "This makes it nontrivial to determine the sampling rate (or cost) for a sequence that contains tokens from both majority and minority entity types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_15",
            "start": 678,
            "end": 824,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_15@5",
            "content": "Simply resampling entities by stripping surrounding context is problematic as sequence tagging algorithms depend on context to make predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_15",
            "start": 826,
            "end": 969,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_15@6",
            "content": "Recent works proposed to randomly or heuristically drop tokens from sentences to re-balance NER data, which had success using conditional random fields and shallow n-gram features (Akkasi, 2018;Akkasi and Varoglu, 2019;Grancharova et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_15",
            "start": 971,
            "end": 1215,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_15@7",
            "content": "However, these methods distort the syntactic and semantic structure of complete sentences, which may generate low-quality data for models that are capable of capturing longdistance linguistic dependencies (e.g. BERT) and hurt performance of those models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_15",
            "start": 1217,
            "end": 1470,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_15@8",
            "content": "In this work, we focus on resampling strategies that leaves sentences intact.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_15",
            "start": 1472,
            "end": 1548,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_16@0",
            "content": "Loss Functions for Imbalanced Data",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_16",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_17@0",
            "content": "Recent literature proposed special loss functions for tackling data imbalance, including focal loss (Lin et al., 2017) and Dice loss (Li et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_17",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_17@1",
            "content": "They increase the cost of 'hard positives' where the correct label has low predicted probability and decrease the cost of 'easy negatives' where the correct label has high predicted probability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_17",
            "start": 152,
            "end": 345,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_17@2",
            "content": "However, these loss functions do not fully address data imbalance in NER.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_17",
            "start": 347,
            "end": 419,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_17@3",
            "content": "First, the formulation does not always emphasize the loss of minority-class tokens -majority-class tokens can also be hard to classify, and minority-class tokens can also be easy to classify.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_17",
            "start": 421,
            "end": 611,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_17@4",
            "content": "Second, these loss functions only work on token-wise prediction outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_17",
            "start": 613,
            "end": 684,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_17@5",
            "content": "They cannot work on sequence-level outputs generated by conditional random fields, which is commonly used in NER.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_17",
            "start": 686,
            "end": 798,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_17@6",
            "content": "Our resampling methods can be seen as estimating sentence-level losses with explicit emphasis on sentences containing minority-class tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_17",
            "start": 800,
            "end": 939,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_18@0",
            "content": "Resampling Strategy Design",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_18",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_19@0",
            "content": "For a sequence tagging task like NER, resampling cannot be as simple as what it is in classification and regression tasks, in which data points can be individually replicated, discarded, or synthesized.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_19",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_19@1",
            "content": "In NER, named entities cannot be resampled out of context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_19",
            "start": 203,
            "end": 260,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_19@2",
            "content": "The surrounding context of named entities -albeit tokens from the irrelevant Other typeshould be considered as well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_19",
            "start": 262,
            "end": 377,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_19@3",
            "content": "Resampling named entities with context is a double-edged sword: preserving context will help NER models, but too much context increases the amount of non-entity tokens and aggravates the data imbalance problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_19",
            "start": 379,
            "end": 588,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_19@4",
            "content": "The goal of sentence-level resampling is to find the balance between too little and too much context accompanying named entities in complete sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_19",
            "start": 590,
            "end": 740,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_20@0",
            "content": "Sentence Importance Factors in NER",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_20",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_21@0",
            "content": "Intuitively, sentences that are worth resampling are those that are more important towards constructing a balanced NER dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_21",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_21@1",
            "content": "We start by proposing factors that influence the importance of a sentence in resampling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_21",
            "start": 128,
            "end": 215,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_21@2",
            "content": "These factors share the theoretical foundation of retrieval functions in information retrieval (Fang et al., 2004).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_21",
            "start": 217,
            "end": 331,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_21@3",
            "content": "A retrieval function evaluates the utility of a document with respect to the query terms it contains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_21",
            "start": 333,
            "end": 433,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_21@4",
            "content": "By direct analogy, sentence importance score measures the utility of a sentence with respect to the entities it contains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_21",
            "start": 435,
            "end": 555,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_22@0",
            "content": "Count of entity tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_22",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_22@1",
            "content": "Regardless of entity types, a sentence containing more entity tokens is more important than a sentence filled with nonentity tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_22",
            "start": 24,
            "end": 155,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_22@2",
            "content": "This factor mirrors term frequency in retrieval functions (Salton and Buckley, 1988).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_22",
            "start": 157,
            "end": 241,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_23@0",
            "content": "Rareness of entity type.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_23",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_23@1",
            "content": "The general idea of resampling for minority classes says that the rarer an entity type is, the more times we should resample sentences containing this type of entity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_23",
            "start": 25,
            "end": 190,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_23@2",
            "content": "This factor mirrors inverse document frequency in retrieval functions (Salton and Buckley, 1988).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_23",
            "start": 192,
            "end": 288,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_24@0",
            "content": "Density of tokens labeled as any entity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_24",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_24@1",
            "content": "Including too much context can aggravate the imbalance problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_24",
            "start": 41,
            "end": 103,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_24@2",
            "content": "While the absolute count of entity tokens matters, the density of entity tokens in a sentence (number of entity tokens compared to the length of a sentence) should also be concerned.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_24",
            "start": 105,
            "end": 286,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_24@3",
            "content": "The higher the density, the more important a sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_24",
            "start": 288,
            "end": 341,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_24@4",
            "content": "This factor mirrors document length normalization in retrieval functions (Singhal et al., 1996).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_24",
            "start": 343,
            "end": 438,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_25@0",
            "content": "Diminishing marginal utility.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_25",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_25@1",
            "content": "If one sentence contains twice as many tokens with a specific entity type as the other sentence with the same length, does that mean the first sentence is twice as important as the second? In reality, an entity may contain numerous tokens, or a sentence may include multiple entities of the same type.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_25",
            "start": 30,
            "end": 330,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_25@2",
            "content": "Twice as many tokens from the same entity type may not offer twice as much information (for the same reason why too many tokens from the Other type is not helpful).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_25",
            "start": 332,
            "end": 495,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_25@3",
            "content": "Therefore, as the number of tokens from the same entity type increases, they generate diminishing marginal utility to a sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_25",
            "start": 497,
            "end": 625,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_25@4",
            "content": "This factor mirrors diminishing marginal gain of repeated query terms in retrieval functions (Fang et al., 2004).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_25",
            "start": 627,
            "end": 739,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_26@0",
            "content": "Resampling Functions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_26",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_27@0",
            "content": "Based on the above importance factors, we design a suite of functions f s \u2208 Z + to determine the number of times a sentence s should be resampled in a NER training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_27",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_27@1",
            "content": "These functions incorporate progressively more factors discussed previously.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_27",
            "start": 169,
            "end": 244,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_28@0",
            "content": "In a given corpus, let us denote the set of all entity types except for the majority type Other as T . Let c(t, s) be the count of tokens with entity type t \u2208 T in sentence s. We define the resampling function with respect to the smoothed count (sC) of all entity tokens as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_28",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_29@0",
            "content": "f sC s = 1 + t\u2208T c(t, s) .(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_29",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_30@0",
            "content": "Here, t\u2208T c(t, s) is the total number of entity tokens in sentence s. '+1' is to avoid removing entity-less sentences from the training set, in reminiscence of add-one smoothing in empirical probability estimates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_30",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_30@1",
            "content": "It guarantees that all training sentences are resampled as least once.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_30",
            "start": 214,
            "end": 283,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_30@2",
            "content": "This smoothinglike process maintains consistency between training and test sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_30",
            "start": 285,
            "end": 364,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_30@3",
            "content": "If the training set contains entityless sentences, it is highly likely that the test set will contain entity-less sentences as well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_30",
            "start": 366,
            "end": 497,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_31@0",
            "content": "The next function incorporates entity rareness factor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_31",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_31@1",
            "content": "The rareness r t of an entity type t \u2208 T is measured as the self-information of the event that any token carries this type:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_31",
            "start": 55,
            "end": 177,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_32@0",
            "content": "r t = \u2212 log 2 s\u2208S c(t, s) N ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_32",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_33@0",
            "content": "where S is the set of all sentences in the training set, and therefore s\u2208S c(t, s) is the total number of tokens with entity type t. N is number of all tokens (including Other tokens) in the training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_33",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_33@1",
            "content": "By introducing the rareness of an entity type we propose another function called the smoothed resampling incorporating count and rareness (sCR):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_33",
            "start": 205,
            "end": 348,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_34@0",
            "content": "f sCR s = 1 + \uf8ee \uf8ef \uf8ef \uf8ef t\u2208T r t \u2022 c(t, s) \uf8f9 \uf8fa \uf8fa \uf8fa .(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_34",
            "start": 0,
            "end": 51,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_35@0",
            "content": "Ceiling function \u2308\u2022\u2309 ensures f sCR s \u2208 Z + .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_35",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_35@1",
            "content": "Square root is to slow down the increase of f sCR s when an entity type t is extremely rare (when r t is large).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_35",
            "start": 45,
            "end": 156,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_36@0",
            "content": "According to the density factor in the previous section, the length of sentence s plays a role in determining the density of entity tokens within a sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_36",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_36@1",
            "content": "Let l s be the length of sentence s measured in number of tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_36",
            "start": 158,
            "end": 222,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_36@2",
            "content": "We define the following function called the smoothed resampling incorporating count, rareness, and density (sCRD):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_36",
            "start": 224,
            "end": 337,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_37@0",
            "content": "f sCRD s = 1 + t\u2208T r t \u2022 c(t, s) \u221a l s .(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_37",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_38@0",
            "content": "We use \u221a l s instead of l s to slow down the decrease of f sCRD s when a sentence is too long.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_38",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_38@1",
            "content": "Lastly, we incorporate the diminishing marginal utility factor and propose a function called the normalized and smoothed resampling incorporating count, rareness, and density (nsCRD):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_38",
            "start": 95,
            "end": 277,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_39@0",
            "content": "f nsCRD s = 1 + t\u2208T r t \u2022 c(t, s) \u221a l s .(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_39",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_40@0",
            "content": "Here, c(t, s) applies a sublinear increasing function on c(t, s) to implement the diminishing marginal utility when a sentence contains many tokens with the same type.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_40",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_41@0",
            "content": "In summary, we proposed four functions for determining resampling frequencies for each sentence, representing four resampling methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_41",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_42@0",
            "content": "Experimental Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_42",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_43@0",
            "content": "Resampling should be a domain-and modelagnostic strategy in tackling data imbalance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_43",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_43@1",
            "content": "Therefore, the goal of our experiments is to evaluate if the proposed resampling methods are effective in an extensive array of NER corpora and base models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_43",
            "start": 85,
            "end": 240,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_43@2",
            "content": "Towards this goal, we apply the four resampling methods (together with baseline methods) on three representative NER models (each has two variants), 2154 and evaluate the resulting models on four corpora from diverse domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_43",
            "start": 242,
            "end": 466,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_44@0",
            "content": "Evaluation Metric",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_44",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_45@0",
            "content": "We use span-level strict-match macro-averaged F1 score as our main evaluation metric.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_45",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_45@1",
            "content": "Other is not viewed as an entity type.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_45",
            "start": 86,
            "end": 123,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_45@2",
            "content": "Macro-averaged metrics emphasize a balanced treatment of all entity types, which align with our main goal.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_45",
            "start": 125,
            "end": 230,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_45@3",
            "content": "See Appendix C for micro-averaged and per-entity-type results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_45",
            "start": 232,
            "end": 293,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_46@0",
            "content": "Compared Methods",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_46",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_47@0",
            "content": "We compare the following baseline methods for dealing with data imbalance in NER.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_47",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_48@0",
            "content": "Original corpus: training data untreated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_48",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_49@0",
            "content": "Balanced undersampling: We implement the algorithm proposed in as a representative of sub-sentence-level resampling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_49",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_50@0",
            "content": "Data augmentation: The data augmentation techniques that includes all transformations as proposed in (Dai and Adel, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_50",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_51@0",
            "content": "Focal loss(Lin et al., 2017), Dice loss (Li et al., 2019): We apply these loss functions on tokenwise predictions made by a softmax output layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_51",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_51@1",
            "content": "Note that they are not applicable to sequence-level predictions made by a CRF output layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_51",
            "start": 146,
            "end": 236,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_52@0",
            "content": "sC, sCR, sCRD, nsCRD: the four resampling methods proposed in this work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_52",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_53@0",
            "content": "NER Corpora",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_53",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_54@0",
            "content": "We select four corpora from different domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_54",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_54@1",
            "content": "The first three are of small scale, representing bespoke NER tasks in practice where entity types are taskspecific and annotation efforts are limited.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_54",
            "start": 47,
            "end": 196,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_55@0",
            "content": "AnEM (Ohta et al., 2012): The Anatomical Entity Mention (AnEM) corpus consists of 500 documents selected randomly from citation abstracts and full-text papers concerning both health and pathological anatomy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_55",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_55@1",
            "content": "With only 3.91% entity tokens and 35.38% sentences having any entity, this is a very imbalanced corpus in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_55",
            "start": 208,
            "end": 321,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_56@0",
            "content": "WNUT (Derczynski et al., 2017): This is a social domain corpus released in the 2017 Workshop on Noisy User-generated Text (W-NUT).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_56",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_56@1",
            "content": "It contains noisy user-generated texts found in social media, online review, crowdsourcing, web forums, clinical records, and language learner essays.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_56",
            "start": 131,
            "end": 280,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_56@2",
            "content": "This is another very imbalanced corpus in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_56",
            "start": 282,
            "end": 331,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_57@0",
            "content": "GMB subset (Bos et al., 2017;Kaggle, 2018): The Groningen Meaning Bank (GMB) corpus consists of public domain English texts with corresponding syntactic and semantic representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_57",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_58@0",
            "content": "The GMB subset is extracted from the larger GMB 2.0.0 corpus which is built specially for NER.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_58",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_59@0",
            "content": "To test the generalizability of our methods, we also include a standard NER benchmark dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_59",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_60@0",
            "content": "CoNLL (Sang and De Meulder, 2003): The CoNLL-2003 English news NER corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_60",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_61@0",
            "content": "For AnEM, WNUT, and CoNLL, we use their pre-existing training/test split.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_61",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_61@1",
            "content": "For GMB subset, we use 3:1 training/test split.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_61",
            "start": 74,
            "end": 120,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_62@0",
            "content": "Base NER Models and Variants",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_62",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_63@0",
            "content": "To comprehensively evaluate the combinations of our upstream resampling strategies with many downstream sequence tagging models, we select the following models:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_63",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_64@0",
            "content": "Shallow Model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_64",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_64@1",
            "content": "We construct shallow NER models that use pretrained word embeddings as per-word feature vectors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_64",
            "start": 15,
            "end": 110,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_64@2",
            "content": "We consider two variants: one using a softmax output layer making tokenwise predictions; the other using a CRF (conditional random fields) output layer making sequencelevel predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_64",
            "start": 112,
            "end": 296,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_64@3",
            "content": "Considering domains of the corpora, we select embeddings trained on biomedical literature (Huang et al., 2016), tweets (Glove-27Btwitter-27B), 2 and Wikipedia+news (Glove-6B), 3 for AnEM, WNUT, and datasets from general domain (GMB subset and CoNLL), respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_64",
            "start": 298,
            "end": 561,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_64@4",
            "content": "All are 50-dimensional embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_64",
            "start": 563,
            "end": 596,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_64@5",
            "content": "CrfSuite 4 is applied with default hyperparameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_64",
            "start": 598,
            "end": 648,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_65@0",
            "content": "Bi-LSTM (Bidirectional Long Short-Term Memory).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_65",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_65@1",
            "content": "LSTM is a special recurrent neural network architecture in which the vanishing gradient problem can be effectively mitigated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_65",
            "start": 48,
            "end": 172,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_65@2",
            "content": "Bi-LSTM consists of two LSTMs taking inputs in both forward and backward directions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_65",
            "start": 174,
            "end": 257,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_65@3",
            "content": "Even though more recent models (e.g., GPT-2, BERT) are shown to outperform Bi-LSTM, it is still regarded as one of the most prevalent tools for solving sequence tagging problems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_65",
            "start": 259,
            "end": 436,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_65@4",
            "content": "We implement two variants of Bi-LSTM: one with a softmax output layer making token-wise predictions; the other with a CRF decoding layer 5 , to ensure the local consistency of output tags.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_65",
            "start": 438,
            "end": 625,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_65@5",
            "content": "Different from the default hyperparameters, batch size and number of epochs are set to 32 and 20, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_65",
            "start": 627,
            "end": 737,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_65@6",
            "content": "Embeddings are used in the same way as in the shallow models above.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_65",
            "start": 739,
            "end": 805,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_66@0",
            "content": "BERT (Bidirectional Encoder Representations from Transformers).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_66",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_66@1",
            "content": "BERT is widely regarded as the most significant improvement in natural language processing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_66",
            "start": 64,
            "end": 154,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_66@2",
            "content": "Its outstanding capability of learning contextualized word representations makes it the representative of advanced NER model in this work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_66",
            "start": 156,
            "end": 293,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_66@3",
            "content": "Again, we implement two variants of BERT: one with a softmax output layer making token-wise predictions; the other with a CRF decoding layer 6 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_66",
            "start": 295,
            "end": 438,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_66@4",
            "content": "Default hyperparameters are used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_66",
            "start": 440,
            "end": 472,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_66@5",
            "content": "More implementation details are in Appendix A.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_66",
            "start": 474,
            "end": 519,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_67@0",
            "content": "Results and Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_67",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_68@0",
            "content": "Macro-averaged F1-scores of different methods applied to four corpora and three base NER models are reported in Tables 2-5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_68",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_68@1",
            "content": "Our goal is not to compete with state-of-theart methods on these corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_68",
            "start": 124,
            "end": 196,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_68@2",
            "content": "Instead, we aim to present an interesting and underexplored problem (sentence-level resampling for NER) and a set of simple yet promising methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_68",
            "start": 198,
            "end": 343,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_68@3",
            "content": "In principle, our proposed resampling methods are model-agnostic and can provide an additional performance boost for a variety of NER models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_68",
            "start": 345,
            "end": 485,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_68@4",
            "content": "We observe the following trends in Tables 2-5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_68",
            "start": 487,
            "end": 532,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_69@0",
            "content": "Overall performance of our resampling methods: Across Tables 2-5, our methods (sC, sCR, sCRD, nsCRD) generally performed well, achieving the highest or second highest F1-scores in almost every column (except for the condition 'Shallow model, Softmax' on CoNLL).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_69",
            "start": 0,
            "end": 260,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_69@1",
            "content": "Although no specific method consistently outperforms others in every condition, it is clear that sentence-level resampling is overall a promising approach to tackling the data imbalance problem in NER.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_69",
            "start": 262,
            "end": 462,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_69@2",
            "content": "The best resampling method depends on the specific base model, output layer, and corpus used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_69",
            "start": 464,
            "end": 556,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_69@3",
            "content": "Just as the best hyperparameter values have to be empirically determined, so could be the most suitable resampling method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_69",
            "start": 558,
            "end": 679,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_69@4",
            "content": "Fortunately, all our resampling methods are simple and straightforward, which allows for convenient experimentation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_69",
            "start": 681,
            "end": 796,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_70@0",
            "content": "Shallow vs. deep models: We observe a clear trend that shallow models using word embedding as features and softmax/CRF as the output layer underperform deep models such as Bi-LSTM and BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_70",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_70@1",
            "content": "We view this as a sanity check.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_70",
            "start": 190,
            "end": 220,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_70@2",
            "content": "Bi-LSTMs and BERT can learn word representations that account for long-distance dependencies, and BERT should be even more powerful with contextual word representations pretrained on massive texts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_70",
            "start": 222,
            "end": 418,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_71@0",
            "content": "Softmax vs. CRF output layer: Using the same base model, CRF output layer often (but not always) outperforms softmax output layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_71",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_71@1",
            "content": "The performance gap is larger on shallow models and small corpora (AnEM, WNUT, GMB) than on deep models and large corpus (CoNLL).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_71",
            "start": 131,
            "end": 259,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_71@2",
            "content": "Indeed, Bi-LSTM and BERT are capable of learning word representations that account for long-distance word dependencies, reducing the benefit of tag dependencies offered by a CRF layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_71",
            "start": 261,
            "end": 444,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_71@3",
            "content": "Similar observation was made by previous work (Devlin et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_71",
            "start": 446,
            "end": 513,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_71@4",
            "content": "An exception is the combination (WNUT, Shallow model), where the CRF layer suffered from severe overfitting caused by noisy text and extremely imbalanced data distribution in WNUT corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_71",
            "start": 515,
            "end": 701,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_72@0",
            "content": "Small vs. large corpus: On small corpora (AnEM, WNUT, GMB subset), our resampling methods usually outperform the original corpus baseline by a big margin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_72",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_72@1",
            "content": "These benefits become less salient on large corpus (CoNLL).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_72",
            "start": 155,
            "end": 213,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_72@2",
            "content": "This implies that our methods are especially effective when the corpus is small and annotations are few.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_72",
            "start": 215,
            "end": 318,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_72@3",
            "content": "As corpus size gets large, even rare entity types are covered by many examples and therefore sufficiently trained.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_72",
            "start": 320,
            "end": 433,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_73@0",
            "content": "Sub-sentence resampling and data augmentation: Sub-sentence resampling (balanced undersampling) has large variance in its performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_73",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_73@1",
            "content": "In some cases it gives the highest gain (GMB subset, BERT model), and in other cases it performs worse than just using the original corpus (all corpora, Bi-LSTM models).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_73",
            "start": 135,
            "end": 303,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_73@2",
            "content": "It suggests that sub-sentence resampling is highly sensitive to the corpus and model choice.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_73",
            "start": 305,
            "end": 396,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_73@3",
            "content": "Data augmentation also shows high variance in its performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_73",
            "start": 398,
            "end": 459,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_73@4",
            "content": "It gives the highest gain on (GMB subset, Bi-LSTM model), and performs worse than the original corpus on (WNUT, BERT model).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_73",
            "start": 461,
            "end": 584,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_73@5",
            "content": "Sentences generated by data augmentation generally have correct syntax but garbled semantics (e.g., one entity is replaced by another same-type, out-of-context entity).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_73",
            "start": 586,
            "end": 753,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_73@6",
            "content": "The nonsensical sentences may confuse NER models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_73",
            "start": 755,
            "end": 803,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_73@7",
            "content": "In contrast, whole-sentence resampling methods give more stable improvements over the original corpus baseline largely because they preserve the naturalness of resampled sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_73",
            "start": 805,
            "end": 984,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_74@0",
            "content": "Focal loss and Dice loss: These loss functions are applicable only on pointwise predictions made by the softmax output layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_74",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_74@1",
            "content": "A major trend is that their performance tend to be unreliable across scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_74",
            "start": 126,
            "end": 204,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_74@2",
            "content": "We attribute this behavior to the difficulty in optimizing these losses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_74",
            "start": 206,
            "end": 277,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_74@3",
            "content": "For shallow models, we optimize them by feeding gradients of either loss function (see Appendix B for their derivation) into a L-BFGS optimizer (Liu and Nocedal, 1989) in Scikit-Learn.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_74",
            "start": 279,
            "end": 462,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_74@4",
            "content": "As shown in the (Shallow model, Softmax) column of GMB subset and CoNLL corpora, the two loss functions (especially the Dice loss) performed well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_74",
            "start": 464,
            "end": 609,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_74@5",
            "content": "For deep models (Bi-LSTM and BERT), we rely on TensorFlow's automatic differentiation and Adam gradient descent optimizer (Kingma and Ba, 2014) because manually deriving gradients for deep models is infeasible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_74",
            "start": 611,
            "end": 820,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_74@6",
            "content": "The two loss functions sometimes give poor performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_74",
            "start": 822,
            "end": 876,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_74@7",
            "content": "The Bi-LSTM model with Dice loss failed completely on AnEM (F1-score: 2.31).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_74",
            "start": 878,
            "end": 953,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_74@8",
            "content": "A possible explanation is that Dice loss is non-convex and it may be difficult for first-order optimizers in current deep learning toolkits (e.g. Adam in TensorFlow) to find high-quality local minima than second-order methods like L-BFGS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_74",
            "start": 955,
            "end": 1192,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_75@0",
            "content": "Precision and recall: To illustrate performance changes in terms of precision and recall, Figure 1 visualizes the changes before and after resampling as displacement vectors in precision-recall plots with F1-score contour lines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_75",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_75@1",
            "content": "Some arrows are pointing to the upper right corner of the plots, indicating the associated methods improve F1-score by improving both precision and recall.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_75",
            "start": 229,
            "end": 383,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_75@2",
            "content": "Other arrows point to the upper left, indicating the associated methods increase recall at the sacrifice of precision.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_75",
            "start": 385,
            "end": 502,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_75@3",
            "content": "In this case, most of our methods improve macroaveraged precision and recall of the BERT model on WNUT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_75",
            "start": 504,
            "end": 606,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_75@4",
            "content": "See Appendix C.2 for more details.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_75",
            "start": 608,
            "end": 641,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_76@0",
            "content": "Effect on Training Corpus Size",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_76",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_77@0",
            "content": "Table 6 shows the effect of training corpus size as a result of resampling or data augmentation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_77",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_77@1",
            "content": "These factors are the average across four corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_77",
            "start": 97,
            "end": 146,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_78@0",
            "content": "The balanced undersampling method drops tokens from sentences, and therefore reduces training corpus size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_78",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_78@1",
            "content": "Data augmentation method doubles the corpus size as many sentences are paraphrased into multiple versions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_78",
            "start": 107,
            "end": 212,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_78@2",
            "content": "Our proposed methods increases corpus size by a slightly larger factor because sentences that contain rare entity types are resampled multiple times.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_78",
            "start": 214,
            "end": 362,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_78@3",
            "content": "Although increased training corpus size leads to increased training time, note that our methods are especially suitable for scenarios where the annotated corpus is small and hence the training time is still relatively short.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_78",
            "start": 364,
            "end": 587,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_78@4",
            "content": "Each NER model (Shallow, Bi-LSTM, BERT) is associated with a cluster of vectors sharing the same starting point in the space, which represents the performance on the original corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_78",
            "start": 589,
            "end": 770,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_79@0",
            "content": "Conclusion and Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_79",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_80@0",
            "content": "Our proposed sentence resampling methods generalize well across diverse NER corpora and models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_80",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_80@1",
            "content": "They enjoy the following advantages: Model-agnostic: Since resampling only manipulates datasets and not models, the proposed methods can be directly applied to any NER model, requiring no knowledge of its functioning or any change to it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_80",
            "start": 96,
            "end": 332,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_80@2",
            "content": "Resampling is also more convenient than cost-sensitive learning as the latter still requires changing the model training process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_80",
            "start": 334,
            "end": 462,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_81@0",
            "content": "Domain-agnostic: Compared with data preprocessing methods such as data augmentation, sentence-level resampling methods are simple and do not require domain-or language-specific manipulations such as synonym replacement, saving practitioners from excessive data engineering.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_81",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_82@0",
            "content": "Note that data augmentation and sentence-level resampling (and resampling methods in general) are complementary methods for improving NER model training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_82",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_82@1",
            "content": "Data augmentation improves the semantic richness of training instances by expanding the coverage of training data in the input feature space, while sentence-level resampling refines the importance weighting of training instances by bridging the gap between the training objective and evaluation metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_82",
            "start": 154,
            "end": 456,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_82@2",
            "content": "Therefore, they work in orthogonal directions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_82",
            "start": 458,
            "end": 503,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_82@3",
            "content": "This points to a promising direction for future work: to explore the two line of methods in combination rather than in competition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_82",
            "start": 505,
            "end": 635,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_83@0",
            "content": "Various other avenues exist for future work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_83",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_83@1",
            "content": "First, further theoretical and empirical research can explore more effective resampling functions that deliver consistently better performance across corpora and base NER models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_83",
            "start": 45,
            "end": 222,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_83@2",
            "content": "Second, more corpora and models can be examined under these resampling strategies to evaluate their generalizability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_83",
            "start": 224,
            "end": 340,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_83@3",
            "content": "Third, the variance of performance in different scenarios may potentially relate to characteristics of specific corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_83",
            "start": 342,
            "end": 461,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_83@4",
            "content": "Future research may seek for corpora-level statistics that can assist practitioners in selecting the appropriate resampling methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_83",
            "start": 463,
            "end": 594,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_84@0",
            "content": "A.1 Software and Hardware Environment All the deep learning models are implemented in Tensorflow 1.12.0 environment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_84",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_85@0",
            "content": "Softmax regression (or multinomial logistic regression) model is from Scikit-Learn package in version 0.23.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_85",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_85@1",
            "content": "The CRF model is implemented by the package sklearn-crfsuite in version of 0.3.6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_85",
            "start": 110,
            "end": 190,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_86@0",
            "content": "Data resampling and CRF training/evaluation were performed on 2.60 GHz Intel CPUs and 8GB RAM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_86",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_86@1",
            "content": "Bi-LSTM and BERT training/evaluation were performed on GPUs (GeForce GTX1080 8GB and Tesla V100 16GB).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_86",
            "start": 95,
            "end": 196,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_87@0",
            "content": "For shallow models and BERT, all hyperparameters are set by default.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_87",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_87@1",
            "content": "For details of them, please see documents of sklearn, crfsuite and BERT-NER.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_87",
            "start": 69,
            "end": 144,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_88@0",
            "content": "For Bi-LSTM, we adjust a few of parameters as there are some drawbacks of the default settings: 20 is not a commonly used number for batch size, and loss of Bi-LSTM model fails to converge under some circumstances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_88",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_88@1",
            "content": "So we set them to 32 and 20, instead of default values 20 and 15.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_88",
            "start": 215,
            "end": 279,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_88@2",
            "content": "Other hyperparameters are applied according to default settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_88",
            "start": 281,
            "end": 344,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_89@0",
            "content": "For the fairness in the comparison, we do not alter any hyperparameters while switching resampling methods and loss functions without changing dataset and models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_89",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_89@1",
            "content": "We believe that it is totally appropriate in the process of comparing, despite that better performance of specific methods may be obtained after tuning hyperparameters, which beyond the scope of this exploring research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_89",
            "start": 163,
            "end": 381,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_90@0",
            "content": "There are two hyperparameters in the focal loss and Dice Loss, determining converging speed and smooth degree.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_90",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_90@1",
            "content": "For focal loss, we set \u03b3 to 2, as what authors of (Lin et al., 2017) recommend.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_90",
            "start": 111,
            "end": 189,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_90@2",
            "content": "According to (Li et al., 2019), it is appropriate to set \u03b3 of Dice loss to 1 for the purpose of smoothing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_90",
            "start": 191,
            "end": 296,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_90@3",
            "content": "In our implementation of loss function in shallow model, this setting is found effective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_90",
            "start": 298,
            "end": 386,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_90@4",
            "content": "However, while using it in deep learning model, its effectiveness cannot be ensured.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_90",
            "start": 388,
            "end": 471,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_90@5",
            "content": "Hence, we adopt another setting of \u03b3 = 10 \u22125 in the tensor computing and obtain better results compared with those obtained with a larger \u03b3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_90",
            "start": 473,
            "end": 612,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_91@0",
            "content": "When using the shallow model with softmax output layer and focal/Dice loss functions, we optimize the model parameters by the quasi-Newton method L-BFGS provided by Python Scikit-Learn.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_91",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_91@1",
            "content": "This approach requires us to provide the gradients of current model parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_91",
            "start": 186,
            "end": 264,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_91@2",
            "content": "Below we show our derivation of these gradients.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_91",
            "start": 266,
            "end": 313,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_91@3",
            "content": "Notations and Preliminaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_91",
            "start": 315,
            "end": 342,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_91@4",
            "content": "Scalar values are denoted by non-bold, lowercase letters such as x. Row vectors are denoted by bold, lowercase letters such as x. Matrices are denoted by bold, uppercase letters such as X.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_91",
            "start": 344,
            "end": 531,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_92@0",
            "content": "Softmax regression has the following components:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_92",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_93@0",
            "content": "\u2022 Feature vector: \u2022 Weight vector for the j-th class:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_93",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_94@0",
            "content": "x \u2208 R m , x = [x 1 , \u2022 \u2022 \u2022 , x i , \u2022 \u2022 \u2022 , x m ]. \u2022 Label vector: y \u2208 {0, 1} k , y = [y 1 , \u2022 \u2022 \u2022 , y j , \u2022 \u2022 \u2022 , y k ]. If the ground truth is the c-th class, 1 \u2264 c \u2264 k, then y c = 1, and y j = 0 if j \u0338 = c.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_94",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_95@0",
            "content": "w j \u2208 R m , w j = [w j1 , \u2022 \u2022 \u2022 , w ji , \u2022 \u2022 \u2022 , w jm ]. \u2022 Weight matrix W \u2208 R m\u00d7k , W = [w \u22a4 1 , \u2022 \u2022 \u2022 , w \u22a4 j , \u2022 \u2022 \u2022 , w \u22a4 k ]",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_95",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_96@0",
            "content": ". \"\u22a4\" is the transpose operation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_96",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_96@1",
            "content": "w \u22a4 is the transpose of w, which is a column vector.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_96",
            "start": 34,
            "end": 85,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_97@0",
            "content": "\u2022 Bias for the the j-th class: b j \u2208 R.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_97",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_98@0",
            "content": ": p \u2208 [0, 1] k , p = [p 1 , \u2022 \u2022 \u2022 , p j , \u2022 \u2022 \u2022 , p k ]. p j = Pr(y j = 1|x) (5) = exp(\u27e8w j , x\u27e9 + b j ) k j \u2032 =1 exp(\u27e8w j \u2032 , x\u27e9 + b j \u2032 ) (6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_98",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_99@0",
            "content": "\u27e8w, x\u27e9 is the inner product of vector w and vector x.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_99",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_100@0",
            "content": "One can verify that the partial derivative of p c with respect to w ji , the weight of the j-th class, the i-th dimension, is the following:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_100",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_101@0",
            "content": "\u2202p c \u2202w ji = [1{j = c} \u2212 p j ] p c x i(7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_101",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_102@0",
            "content": "Suppose the ground truth is the c-th class for a given example x.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_102",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_103@0",
            "content": "L F L (x, y) = \u2212(1 \u2212 p c ) \u03b3 log p c (8) \u2202L F L (x, y) \u2202w ji (9) = \u2212 \u2202 \u2202p c [(1 \u2212 p c ) \u03b3 log p c ] \u2022 \u2202p c \u2202w ji (10) = \u2212 \u2212\u03b3(1 \u2212 p c ) \u03b3\u22121 log p c + (1 \u2212 p c ) \u03b3 p c \u2022 \u2202p c \u2202w ji (11) = \u2212 \u2212\u03b3(1 \u2212 p c ) \u03b3\u22121 log p c + (1 \u2212 p c ) \u03b3 p c \u2022 [1{j = c} \u2212 p j ]p c x i (12) = \u2212 [\u2212\u03b3p c (1 \u2212 p c ) \u03b3\u22121 log p c + (1 \u2212 p c ) \u03b3 ] \u2022 [1{j = c} \u2212 p j ]x i (13) =a c [p j \u2212 1{j = c}]x i(14)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_103",
            "start": 0,
            "end": 370,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_104@0",
            "content": "Here we set",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_104",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_105@0",
            "content": "a c = \u2212\u03b3p c (1 \u2212 p c ) \u03b3\u22121 log p c + (1 \u2212 p c ) \u03b3 (15)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_105",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_106@0",
            "content": "to reduce notational clutter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_106",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_106@1",
            "content": "a c has nothing to do with i or j; it only has to do with c, the index of the ground truth label for the training example x. When \u03b3 = 0, a c = 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_106",
            "start": 30,
            "end": 174,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_106@2",
            "content": "When \u03b3 > 0, a c decreases when p c increases from 0 to 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_106",
            "start": 176,
            "end": 232,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_106@3",
            "content": "This means the gradient for an easy example (when p c is close to 1) have a smaller magnitude than the gradient for a hard example (when p c is close to 0).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_106",
            "start": 234,
            "end": 389,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_106@4",
            "content": "Generalizing the scalar gradient in Equation ( 14) to matrix gradient, we have",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_106",
            "start": 391,
            "end": 468,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_107@0",
            "content": "\u2202L F L (x, y) \u2202W = a c \u2022 x \u22a4 (p \u2212 y) .(16)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_107",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_108@0",
            "content": "The shape of a c \u2022 x \u22a4 (p \u2212 y) is m \u00d7 k, the same shape as W.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_108",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_109@0",
            "content": "An important note is that here a c is specific to that single example x, which has ground truth label y c = 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_109",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_109@1",
            "content": "If we have n different training examples x (1) , \u2022 \u2022 \u2022 , x (n) , then every example will have a different a c value: a",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_109",
            "start": 111,
            "end": 228,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_110@0",
            "content": "(1) c , \u2022 \u2022 \u2022 , a (n) c . Let's create a diagonal matrix A c \u2208 R n\u00d7n , A c = diag(a (1) c , \u2022 \u2022 \u2022 , a (n) c ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_110",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_111@0",
            "content": "If we have n training examples, then the feature matrix X \u2208 R n\u00d7m , the label matrix Y \u2208 {0, 1} n\u00d7k , and the predicted probability matrix P \u2208 [0, 1] n\u00d7k .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_111",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_111@1",
            "content": "We have:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_111",
            "start": 156,
            "end": 163,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_112@0",
            "content": "\u2202L F L (X, Y) \u2202W = X \u22a4 A c (P \u2212 Y) .(17)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_112",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_113@0",
            "content": "The shape of X \u22a4 A c (P \u2212 Y) is m \u00d7 k, the same as W.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_113",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_114@0",
            "content": "Dice loss computes per-class F-1 scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_114",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_114@1",
            "content": "Suppose the ground truth is the c-th class for a given example x.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_114",
            "start": 41,
            "end": 105,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_115@0",
            "content": "L DL (x, y) (18)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_115",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_116@0",
            "content": "= k j \u2032 =1 1 \u2212 1{c = j \u2032 } \u03b3 + 2p c \u03b3 + p 2 c + 1 +1 \u2212 1{c \u0338 = j \u2032 } \u03b3 \u03b3 + p 2 j \u2032 (19) =1 \u2212 \u03b3 + 2p c \u03b3 + p 2 c + 1 + j \u2032 \u0338 =c 1 \u2212 \u03b3 \u03b3 + p 2 j \u2032 (20) =k \u2212 \u03b3 + 2p c \u03b3 + p 2 c + 1 \u2212 j \u2032 \u0338 =c \u03b3 \u03b3 + p 2 j \u2032 (21)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_116",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_117@0",
            "content": "Take gradient with respect to w ji , the weight of the j-th class, the i-th dimension.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_117",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_118@0",
            "content": "\u2202L DL (x, y) \u2202w ji (22) = \u2212 \u2202 \u2202p c \u03b3 + 2p c \u03b3 + p 2 c + 1 \u2022 \u2202p c \u2202w ji \u2212 j \u2032 \u0338 =c \u2202 \u2202p j \u2032 \u03b3 \u03b3 + p 2 j \u2032 \u2022 \u2202p j \u2032 \u2202w ji (23) = \u2212 2(\u03b3 + p 2 c + 1) \u2212 (\u03b3 + 2p c )2p c (\u03b3 + p 2 c + 1) 2 \u2022 \u2202p c \u2202w ji \u2212 j \u2032 \u0338 =c \u2212\u03b3 \u2022 2p j \u2032 (\u03b3 + p 2 j \u2032 ) 2 \u2022 \u2202p j \u2032 \u2202w ji (24) = \u2212 2(\u03b3 + p 2 c + 1) \u2212 (\u03b3 + 2p c )2p c (\u03b3 + p 2 c + 1) 2 \u2022 [1{j = c} \u2212 p j ]p c x i \u2212 j \u2032 \u0338 =c \u2212\u03b3 \u2022 2p j \u2032 (\u03b3 + p 2 j \u2032 ) 2 \u2022 [1{j = j \u2032 } \u2212 p j ]p j \u2032 x i (25) = \u2212 2(1 \u2212 p c )(1 + \u03b3 + p c )p c (\u03b3 + p 2 c + 1) 2 \u2022 [1{j = c} \u2212 p j ]x i(26)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_118",
            "start": 0,
            "end": 492,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_119@0",
            "content": "A Akkasi,  Varoglu, Improvement of chemical named entity recognition through sentence-based random under-sampling and classifier combination, 2019, Journal of AI and Data Mining, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_119",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_120@0",
            "content": "Abbas Akkasi, Sentence-based undersampling for named entity recognition using genetic algorithm, 2018, Iran Journal of Computer Science, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_120",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_121@0",
            "content": "Abbas Akkasi, Ekrem Varoglu, Nazife Dimililer, Balanced undersampling: a novel sentencebased undersampling method to improve recognition of named entities in chemical and biomedical text, 2018, Applied Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_121",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_122@0",
            "content": "UNKNOWN, None, 2007, Predicting structured data, MIT press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_122",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_123@0",
            "content": "Johan Bos, Valerio Basile, Kilian Evang, Noortje Venhuizen, Johannes Bjerva, The groningen meaning bank, 2017, Handbook of Linguistic Annotation, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_123",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_124@0",
            "content": "UNKNOWN, None, 2020, An analysis of simple data augmentation for named entity recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_124",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_125@0",
            "content": "Leon Derczynski, Eric Nichols, Marieke Van Erp, Nut Limsopatham, Results of the wnut2017 shared task on novel and emerging entity recognition, 2017, Proceedings of the 3rd Workshop on Noisy Usergenerated Text, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_125",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_126@0",
            "content": "UNKNOWN, None, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_126",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_127@0",
            "content": "Hui Fang, Tao Tao, Chengxiang Zhai, A formal study of information retrieval heuristics, 2004, Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_127",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_128@0",
            "content": "Mila Grancharova, Hanna Berg, Hercules Dalianis, Improving named entity recognition and classification in class imbalanced swedish electronic patient records through resampling, 2020, Eighth Swedish Language Technology Conference (SLTC). F\u00f6rlag G\u00f6teborgs Universitet, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_128",
            "start": 0,
            "end": 268,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_129@0",
            "content": "Haibo He, A Edwardo,  Garcia, Learning from imbalanced data, 2009, IEEE Transactions on knowledge and data engineering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_129",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_130@0",
            "content": "Sam Henry, Kevin Buchan, Michele Filannino, Amber Stubbs, Ozlem Uzuner, 2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records, 2020, Journal of the American Medical Informatics Association, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_130",
            "start": 0,
            "end": 236,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_131@0",
            "content": "Jian Huang, Keyang Xu,  Vydiswaran, Analyzing multiple medical corpora using word embedding, 2016, 2016 IEEE International Conference on Healthcare Informatics (ICHI), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_131",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_132@0",
            "content": "Abdul Majeed Issifu,  Murat Can,  Ganiz, A simple data augmentation method to improve the performance of named entity recognition models in medical domain, 2021, 2021 6th International Conference on Computer Science and Engineering (UBMK), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_132",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_133@0",
            "content": "Brendan Juba, S Hai,  Le, Precision-recall versus accuracy and the role of large data sets, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_133",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_134@0",
            "content": "UNKNOWN, None, 2018, Annotated GMB corpus, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_134",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "152-ARR_v2_135@0",
            "content": "Sarvnaz Karimi, Alejandro Metke-Jimenez, Madonna Kemp, Chen Wang, Cadec: A corpus of adverse drug event annotations, 2015, Journal of biomedical informatics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "152-ARR_v2_135",
            "start": 0,
            "end": 158,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "152-ARR_v2_0",
            "tgt_ix": "152-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_0",
            "tgt_ix": "152-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_1",
            "tgt_ix": "152-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_1",
            "tgt_ix": "152-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_0",
            "tgt_ix": "152-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_2",
            "tgt_ix": "152-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_4",
            "tgt_ix": "152-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_5",
            "tgt_ix": "152-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_6",
            "tgt_ix": "152-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_7",
            "tgt_ix": "152-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_8",
            "tgt_ix": "152-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_3",
            "tgt_ix": "152-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_3",
            "tgt_ix": "152-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_3",
            "tgt_ix": "152-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_3",
            "tgt_ix": "152-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_3",
            "tgt_ix": "152-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_3",
            "tgt_ix": "152-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_3",
            "tgt_ix": "152-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_0",
            "tgt_ix": "152-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_9",
            "tgt_ix": "152-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_10",
            "tgt_ix": "152-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_10",
            "tgt_ix": "152-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_12",
            "tgt_ix": "152-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_11",
            "tgt_ix": "152-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_11",
            "tgt_ix": "152-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_11",
            "tgt_ix": "152-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_10",
            "tgt_ix": "152-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_13",
            "tgt_ix": "152-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_14",
            "tgt_ix": "152-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_14",
            "tgt_ix": "152-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_10",
            "tgt_ix": "152-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_15",
            "tgt_ix": "152-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_16",
            "tgt_ix": "152-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_16",
            "tgt_ix": "152-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_0",
            "tgt_ix": "152-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_17",
            "tgt_ix": "152-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_18",
            "tgt_ix": "152-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_18",
            "tgt_ix": "152-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_18",
            "tgt_ix": "152-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_19",
            "tgt_ix": "152-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_21",
            "tgt_ix": "152-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_22",
            "tgt_ix": "152-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_23",
            "tgt_ix": "152-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_24",
            "tgt_ix": "152-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_20",
            "tgt_ix": "152-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_20",
            "tgt_ix": "152-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_20",
            "tgt_ix": "152-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_20",
            "tgt_ix": "152-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_20",
            "tgt_ix": "152-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_20",
            "tgt_ix": "152-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_18",
            "tgt_ix": "152-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_25",
            "tgt_ix": "152-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_27",
            "tgt_ix": "152-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_28",
            "tgt_ix": "152-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_29",
            "tgt_ix": "152-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_30",
            "tgt_ix": "152-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_31",
            "tgt_ix": "152-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_32",
            "tgt_ix": "152-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_33",
            "tgt_ix": "152-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_34",
            "tgt_ix": "152-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_35",
            "tgt_ix": "152-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_36",
            "tgt_ix": "152-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_37",
            "tgt_ix": "152-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_38",
            "tgt_ix": "152-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_39",
            "tgt_ix": "152-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_40",
            "tgt_ix": "152-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_26",
            "tgt_ix": "152-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_26",
            "tgt_ix": "152-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_26",
            "tgt_ix": "152-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_26",
            "tgt_ix": "152-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_26",
            "tgt_ix": "152-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_26",
            "tgt_ix": "152-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_26",
            "tgt_ix": "152-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_26",
            "tgt_ix": "152-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_26",
            "tgt_ix": "152-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_26",
            "tgt_ix": "152-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_26",
            "tgt_ix": "152-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_26",
            "tgt_ix": "152-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_26",
            "tgt_ix": "152-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_26",
            "tgt_ix": "152-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_26",
            "tgt_ix": "152-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_26",
            "tgt_ix": "152-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_0",
            "tgt_ix": "152-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_41",
            "tgt_ix": "152-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_42",
            "tgt_ix": "152-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_42",
            "tgt_ix": "152-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_42",
            "tgt_ix": "152-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_43",
            "tgt_ix": "152-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_44",
            "tgt_ix": "152-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_44",
            "tgt_ix": "152-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_42",
            "tgt_ix": "152-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_45",
            "tgt_ix": "152-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_47",
            "tgt_ix": "152-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_48",
            "tgt_ix": "152-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_49",
            "tgt_ix": "152-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_50",
            "tgt_ix": "152-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_51",
            "tgt_ix": "152-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_46",
            "tgt_ix": "152-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_46",
            "tgt_ix": "152-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_46",
            "tgt_ix": "152-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_46",
            "tgt_ix": "152-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_46",
            "tgt_ix": "152-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_46",
            "tgt_ix": "152-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_46",
            "tgt_ix": "152-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_42",
            "tgt_ix": "152-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_52",
            "tgt_ix": "152-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_54",
            "tgt_ix": "152-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_55",
            "tgt_ix": "152-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_56",
            "tgt_ix": "152-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_57",
            "tgt_ix": "152-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_58",
            "tgt_ix": "152-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_59",
            "tgt_ix": "152-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_60",
            "tgt_ix": "152-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_53",
            "tgt_ix": "152-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_53",
            "tgt_ix": "152-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_53",
            "tgt_ix": "152-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_53",
            "tgt_ix": "152-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_53",
            "tgt_ix": "152-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_53",
            "tgt_ix": "152-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_53",
            "tgt_ix": "152-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_53",
            "tgt_ix": "152-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_53",
            "tgt_ix": "152-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_42",
            "tgt_ix": "152-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_61",
            "tgt_ix": "152-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_63",
            "tgt_ix": "152-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_64",
            "tgt_ix": "152-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_65",
            "tgt_ix": "152-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_62",
            "tgt_ix": "152-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_62",
            "tgt_ix": "152-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_62",
            "tgt_ix": "152-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_62",
            "tgt_ix": "152-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_62",
            "tgt_ix": "152-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_42",
            "tgt_ix": "152-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_66",
            "tgt_ix": "152-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_68",
            "tgt_ix": "152-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_69",
            "tgt_ix": "152-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_70",
            "tgt_ix": "152-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_71",
            "tgt_ix": "152-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_72",
            "tgt_ix": "152-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_73",
            "tgt_ix": "152-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_74",
            "tgt_ix": "152-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_67",
            "tgt_ix": "152-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_67",
            "tgt_ix": "152-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_67",
            "tgt_ix": "152-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_67",
            "tgt_ix": "152-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_67",
            "tgt_ix": "152-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_67",
            "tgt_ix": "152-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_67",
            "tgt_ix": "152-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_67",
            "tgt_ix": "152-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_67",
            "tgt_ix": "152-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_42",
            "tgt_ix": "152-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_75",
            "tgt_ix": "152-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_77",
            "tgt_ix": "152-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_76",
            "tgt_ix": "152-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_76",
            "tgt_ix": "152-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_76",
            "tgt_ix": "152-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_0",
            "tgt_ix": "152-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_78",
            "tgt_ix": "152-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_80",
            "tgt_ix": "152-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_81",
            "tgt_ix": "152-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_82",
            "tgt_ix": "152-ARR_v2_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_84",
            "tgt_ix": "152-ARR_v2_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_85",
            "tgt_ix": "152-ARR_v2_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_83",
            "tgt_ix": "152-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_87",
            "tgt_ix": "152-ARR_v2_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_88",
            "tgt_ix": "152-ARR_v2_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_86",
            "tgt_ix": "152-ARR_v2_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_89",
            "tgt_ix": "152-ARR_v2_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_91",
            "tgt_ix": "152-ARR_v2_92",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_92",
            "tgt_ix": "152-ARR_v2_93",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_95",
            "tgt_ix": "152-ARR_v2_96",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_96",
            "tgt_ix": "152-ARR_v2_97",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_92",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_93",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_94",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_95",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_96",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_97",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_90",
            "tgt_ix": "152-ARR_v2_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_98",
            "tgt_ix": "152-ARR_v2_99",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_99",
            "tgt_ix": "152-ARR_v2_100",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_100",
            "tgt_ix": "152-ARR_v2_101",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_98",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_99",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_100",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_101",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_102",
            "tgt_ix": "152-ARR_v2_103",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_103",
            "tgt_ix": "152-ARR_v2_104",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_104",
            "tgt_ix": "152-ARR_v2_105",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_105",
            "tgt_ix": "152-ARR_v2_106",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_106",
            "tgt_ix": "152-ARR_v2_107",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_107",
            "tgt_ix": "152-ARR_v2_108",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_108",
            "tgt_ix": "152-ARR_v2_109",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_109",
            "tgt_ix": "152-ARR_v2_110",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_110",
            "tgt_ix": "152-ARR_v2_111",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_111",
            "tgt_ix": "152-ARR_v2_112",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_112",
            "tgt_ix": "152-ARR_v2_113",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_102",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_103",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_104",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_105",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_106",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_107",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_108",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_109",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_110",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_111",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_112",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_113",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_101",
            "tgt_ix": "152-ARR_v2_102",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_114",
            "tgt_ix": "152-ARR_v2_115",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_115",
            "tgt_ix": "152-ARR_v2_116",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_116",
            "tgt_ix": "152-ARR_v2_117",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_117",
            "tgt_ix": "152-ARR_v2_118",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_114",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_115",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_116",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_117",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_118",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_113",
            "tgt_ix": "152-ARR_v2_114",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "152-ARR_v2_0",
            "tgt_ix": "152-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_1",
            "tgt_ix": "152-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_2",
            "tgt_ix": "152-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_2",
            "tgt_ix": "152-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_2",
            "tgt_ix": "152-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_2",
            "tgt_ix": "152-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_2",
            "tgt_ix": "152-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_2",
            "tgt_ix": "152-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_2",
            "tgt_ix": "152-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_3",
            "tgt_ix": "152-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_4",
            "tgt_ix": "152-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_4",
            "tgt_ix": "152-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_4",
            "tgt_ix": "152-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_4",
            "tgt_ix": "152-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_4",
            "tgt_ix": "152-ARR_v2_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_4",
            "tgt_ix": "152-ARR_v2_4@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_5",
            "tgt_ix": "152-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_5",
            "tgt_ix": "152-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_5",
            "tgt_ix": "152-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_5",
            "tgt_ix": "152-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_5",
            "tgt_ix": "152-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_5",
            "tgt_ix": "152-ARR_v2_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_6",
            "tgt_ix": "152-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_6",
            "tgt_ix": "152-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_6",
            "tgt_ix": "152-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_6",
            "tgt_ix": "152-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_6",
            "tgt_ix": "152-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_7",
            "tgt_ix": "152-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_7",
            "tgt_ix": "152-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_8",
            "tgt_ix": "152-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_8",
            "tgt_ix": "152-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_8",
            "tgt_ix": "152-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_8",
            "tgt_ix": "152-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_8",
            "tgt_ix": "152-ARR_v2_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_8",
            "tgt_ix": "152-ARR_v2_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_9",
            "tgt_ix": "152-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_9",
            "tgt_ix": "152-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_9",
            "tgt_ix": "152-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_9",
            "tgt_ix": "152-ARR_v2_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_9",
            "tgt_ix": "152-ARR_v2_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_10",
            "tgt_ix": "152-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_11",
            "tgt_ix": "152-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_12",
            "tgt_ix": "152-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_12",
            "tgt_ix": "152-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_12",
            "tgt_ix": "152-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_12",
            "tgt_ix": "152-ARR_v2_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_12",
            "tgt_ix": "152-ARR_v2_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_12",
            "tgt_ix": "152-ARR_v2_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_12",
            "tgt_ix": "152-ARR_v2_12@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_13",
            "tgt_ix": "152-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_13",
            "tgt_ix": "152-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_13",
            "tgt_ix": "152-ARR_v2_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_13",
            "tgt_ix": "152-ARR_v2_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_14",
            "tgt_ix": "152-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_15",
            "tgt_ix": "152-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_15",
            "tgt_ix": "152-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_15",
            "tgt_ix": "152-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_15",
            "tgt_ix": "152-ARR_v2_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_15",
            "tgt_ix": "152-ARR_v2_15@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_15",
            "tgt_ix": "152-ARR_v2_15@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_15",
            "tgt_ix": "152-ARR_v2_15@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_15",
            "tgt_ix": "152-ARR_v2_15@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_15",
            "tgt_ix": "152-ARR_v2_15@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_16",
            "tgt_ix": "152-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_17",
            "tgt_ix": "152-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_17",
            "tgt_ix": "152-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_17",
            "tgt_ix": "152-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_17",
            "tgt_ix": "152-ARR_v2_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_17",
            "tgt_ix": "152-ARR_v2_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_17",
            "tgt_ix": "152-ARR_v2_17@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_17",
            "tgt_ix": "152-ARR_v2_17@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_18",
            "tgt_ix": "152-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_19",
            "tgt_ix": "152-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_19",
            "tgt_ix": "152-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_19",
            "tgt_ix": "152-ARR_v2_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_19",
            "tgt_ix": "152-ARR_v2_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_19",
            "tgt_ix": "152-ARR_v2_19@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_20",
            "tgt_ix": "152-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_21",
            "tgt_ix": "152-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_21",
            "tgt_ix": "152-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_21",
            "tgt_ix": "152-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_21",
            "tgt_ix": "152-ARR_v2_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_21",
            "tgt_ix": "152-ARR_v2_21@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_22",
            "tgt_ix": "152-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_22",
            "tgt_ix": "152-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_22",
            "tgt_ix": "152-ARR_v2_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_23",
            "tgt_ix": "152-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_23",
            "tgt_ix": "152-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_23",
            "tgt_ix": "152-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_24",
            "tgt_ix": "152-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_24",
            "tgt_ix": "152-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_24",
            "tgt_ix": "152-ARR_v2_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_24",
            "tgt_ix": "152-ARR_v2_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_24",
            "tgt_ix": "152-ARR_v2_24@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_25",
            "tgt_ix": "152-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_25",
            "tgt_ix": "152-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_25",
            "tgt_ix": "152-ARR_v2_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_25",
            "tgt_ix": "152-ARR_v2_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_25",
            "tgt_ix": "152-ARR_v2_25@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_26",
            "tgt_ix": "152-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_27",
            "tgt_ix": "152-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_27",
            "tgt_ix": "152-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_28",
            "tgt_ix": "152-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_29",
            "tgt_ix": "152-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_30",
            "tgt_ix": "152-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_30",
            "tgt_ix": "152-ARR_v2_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_30",
            "tgt_ix": "152-ARR_v2_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_30",
            "tgt_ix": "152-ARR_v2_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_31",
            "tgt_ix": "152-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_31",
            "tgt_ix": "152-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_32",
            "tgt_ix": "152-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_33",
            "tgt_ix": "152-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_33",
            "tgt_ix": "152-ARR_v2_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_34",
            "tgt_ix": "152-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_35",
            "tgt_ix": "152-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_35",
            "tgt_ix": "152-ARR_v2_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_36",
            "tgt_ix": "152-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_36",
            "tgt_ix": "152-ARR_v2_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_36",
            "tgt_ix": "152-ARR_v2_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_37",
            "tgt_ix": "152-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_38",
            "tgt_ix": "152-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_38",
            "tgt_ix": "152-ARR_v2_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_39",
            "tgt_ix": "152-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_40",
            "tgt_ix": "152-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_41",
            "tgt_ix": "152-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_42",
            "tgt_ix": "152-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_43",
            "tgt_ix": "152-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_43",
            "tgt_ix": "152-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_43",
            "tgt_ix": "152-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_44",
            "tgt_ix": "152-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_45",
            "tgt_ix": "152-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_45",
            "tgt_ix": "152-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_45",
            "tgt_ix": "152-ARR_v2_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_45",
            "tgt_ix": "152-ARR_v2_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_46",
            "tgt_ix": "152-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_47",
            "tgt_ix": "152-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_48",
            "tgt_ix": "152-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_49",
            "tgt_ix": "152-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_50",
            "tgt_ix": "152-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_51",
            "tgt_ix": "152-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_51",
            "tgt_ix": "152-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_52",
            "tgt_ix": "152-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_53",
            "tgt_ix": "152-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_54",
            "tgt_ix": "152-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_54",
            "tgt_ix": "152-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_55",
            "tgt_ix": "152-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_55",
            "tgt_ix": "152-ARR_v2_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_56",
            "tgt_ix": "152-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_56",
            "tgt_ix": "152-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_56",
            "tgt_ix": "152-ARR_v2_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_57",
            "tgt_ix": "152-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_58",
            "tgt_ix": "152-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_59",
            "tgt_ix": "152-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_60",
            "tgt_ix": "152-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_61",
            "tgt_ix": "152-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_61",
            "tgt_ix": "152-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_62",
            "tgt_ix": "152-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_63",
            "tgt_ix": "152-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_64",
            "tgt_ix": "152-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_64",
            "tgt_ix": "152-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_64",
            "tgt_ix": "152-ARR_v2_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_64",
            "tgt_ix": "152-ARR_v2_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_64",
            "tgt_ix": "152-ARR_v2_64@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_64",
            "tgt_ix": "152-ARR_v2_64@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_65",
            "tgt_ix": "152-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_65",
            "tgt_ix": "152-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_65",
            "tgt_ix": "152-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_65",
            "tgt_ix": "152-ARR_v2_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_65",
            "tgt_ix": "152-ARR_v2_65@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_65",
            "tgt_ix": "152-ARR_v2_65@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_65",
            "tgt_ix": "152-ARR_v2_65@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_66",
            "tgt_ix": "152-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_66",
            "tgt_ix": "152-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_66",
            "tgt_ix": "152-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_66",
            "tgt_ix": "152-ARR_v2_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_66",
            "tgt_ix": "152-ARR_v2_66@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_66",
            "tgt_ix": "152-ARR_v2_66@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_67",
            "tgt_ix": "152-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_68",
            "tgt_ix": "152-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_68",
            "tgt_ix": "152-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_68",
            "tgt_ix": "152-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_68",
            "tgt_ix": "152-ARR_v2_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_68",
            "tgt_ix": "152-ARR_v2_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_69",
            "tgt_ix": "152-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_69",
            "tgt_ix": "152-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_69",
            "tgt_ix": "152-ARR_v2_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_69",
            "tgt_ix": "152-ARR_v2_69@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_69",
            "tgt_ix": "152-ARR_v2_69@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_70",
            "tgt_ix": "152-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_70",
            "tgt_ix": "152-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_70",
            "tgt_ix": "152-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_71",
            "tgt_ix": "152-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_71",
            "tgt_ix": "152-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_71",
            "tgt_ix": "152-ARR_v2_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_71",
            "tgt_ix": "152-ARR_v2_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_71",
            "tgt_ix": "152-ARR_v2_71@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_72",
            "tgt_ix": "152-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_72",
            "tgt_ix": "152-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_72",
            "tgt_ix": "152-ARR_v2_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_72",
            "tgt_ix": "152-ARR_v2_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_73",
            "tgt_ix": "152-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_73",
            "tgt_ix": "152-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_73",
            "tgt_ix": "152-ARR_v2_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_73",
            "tgt_ix": "152-ARR_v2_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_73",
            "tgt_ix": "152-ARR_v2_73@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_73",
            "tgt_ix": "152-ARR_v2_73@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_73",
            "tgt_ix": "152-ARR_v2_73@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_73",
            "tgt_ix": "152-ARR_v2_73@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_74",
            "tgt_ix": "152-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_74",
            "tgt_ix": "152-ARR_v2_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_74",
            "tgt_ix": "152-ARR_v2_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_74",
            "tgt_ix": "152-ARR_v2_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_74",
            "tgt_ix": "152-ARR_v2_74@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_74",
            "tgt_ix": "152-ARR_v2_74@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_74",
            "tgt_ix": "152-ARR_v2_74@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_74",
            "tgt_ix": "152-ARR_v2_74@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_74",
            "tgt_ix": "152-ARR_v2_74@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_75",
            "tgt_ix": "152-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_75",
            "tgt_ix": "152-ARR_v2_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_75",
            "tgt_ix": "152-ARR_v2_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_75",
            "tgt_ix": "152-ARR_v2_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_75",
            "tgt_ix": "152-ARR_v2_75@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_76",
            "tgt_ix": "152-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_77",
            "tgt_ix": "152-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_77",
            "tgt_ix": "152-ARR_v2_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_78",
            "tgt_ix": "152-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_78",
            "tgt_ix": "152-ARR_v2_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_78",
            "tgt_ix": "152-ARR_v2_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_78",
            "tgt_ix": "152-ARR_v2_78@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_78",
            "tgt_ix": "152-ARR_v2_78@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_79",
            "tgt_ix": "152-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_80",
            "tgt_ix": "152-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_80",
            "tgt_ix": "152-ARR_v2_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_80",
            "tgt_ix": "152-ARR_v2_80@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_81",
            "tgt_ix": "152-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_82",
            "tgt_ix": "152-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_82",
            "tgt_ix": "152-ARR_v2_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_82",
            "tgt_ix": "152-ARR_v2_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_82",
            "tgt_ix": "152-ARR_v2_82@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_83",
            "tgt_ix": "152-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_83",
            "tgt_ix": "152-ARR_v2_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_83",
            "tgt_ix": "152-ARR_v2_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_83",
            "tgt_ix": "152-ARR_v2_83@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_83",
            "tgt_ix": "152-ARR_v2_83@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_84",
            "tgt_ix": "152-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_85",
            "tgt_ix": "152-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_85",
            "tgt_ix": "152-ARR_v2_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_86",
            "tgt_ix": "152-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_86",
            "tgt_ix": "152-ARR_v2_86@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_87",
            "tgt_ix": "152-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_87",
            "tgt_ix": "152-ARR_v2_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_88",
            "tgt_ix": "152-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_88",
            "tgt_ix": "152-ARR_v2_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_88",
            "tgt_ix": "152-ARR_v2_88@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_89",
            "tgt_ix": "152-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_89",
            "tgt_ix": "152-ARR_v2_89@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_90",
            "tgt_ix": "152-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_90",
            "tgt_ix": "152-ARR_v2_90@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_90",
            "tgt_ix": "152-ARR_v2_90@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_90",
            "tgt_ix": "152-ARR_v2_90@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_90",
            "tgt_ix": "152-ARR_v2_90@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_90",
            "tgt_ix": "152-ARR_v2_90@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_91",
            "tgt_ix": "152-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_91",
            "tgt_ix": "152-ARR_v2_91@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_91",
            "tgt_ix": "152-ARR_v2_91@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_91",
            "tgt_ix": "152-ARR_v2_91@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_91",
            "tgt_ix": "152-ARR_v2_91@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_92",
            "tgt_ix": "152-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_93",
            "tgt_ix": "152-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_94",
            "tgt_ix": "152-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_95",
            "tgt_ix": "152-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_96",
            "tgt_ix": "152-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_96",
            "tgt_ix": "152-ARR_v2_96@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_97",
            "tgt_ix": "152-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_98",
            "tgt_ix": "152-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_99",
            "tgt_ix": "152-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_100",
            "tgt_ix": "152-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_101",
            "tgt_ix": "152-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_102",
            "tgt_ix": "152-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_103",
            "tgt_ix": "152-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_104",
            "tgt_ix": "152-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_105",
            "tgt_ix": "152-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_106",
            "tgt_ix": "152-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_106",
            "tgt_ix": "152-ARR_v2_106@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_106",
            "tgt_ix": "152-ARR_v2_106@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_106",
            "tgt_ix": "152-ARR_v2_106@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_106",
            "tgt_ix": "152-ARR_v2_106@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_107",
            "tgt_ix": "152-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_108",
            "tgt_ix": "152-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_109",
            "tgt_ix": "152-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_109",
            "tgt_ix": "152-ARR_v2_109@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_110",
            "tgt_ix": "152-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_111",
            "tgt_ix": "152-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_111",
            "tgt_ix": "152-ARR_v2_111@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_112",
            "tgt_ix": "152-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_113",
            "tgt_ix": "152-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_114",
            "tgt_ix": "152-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_114",
            "tgt_ix": "152-ARR_v2_114@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_115",
            "tgt_ix": "152-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_116",
            "tgt_ix": "152-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_117",
            "tgt_ix": "152-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_118",
            "tgt_ix": "152-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_119",
            "tgt_ix": "152-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_120",
            "tgt_ix": "152-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_121",
            "tgt_ix": "152-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_122",
            "tgt_ix": "152-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_123",
            "tgt_ix": "152-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_124",
            "tgt_ix": "152-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_125",
            "tgt_ix": "152-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_126",
            "tgt_ix": "152-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_127",
            "tgt_ix": "152-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_128",
            "tgt_ix": "152-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_129",
            "tgt_ix": "152-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_130",
            "tgt_ix": "152-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_131",
            "tgt_ix": "152-ARR_v2_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_132",
            "tgt_ix": "152-ARR_v2_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_133",
            "tgt_ix": "152-ARR_v2_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_134",
            "tgt_ix": "152-ARR_v2_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "152-ARR_v2_135",
            "tgt_ix": "152-ARR_v2_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 965,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "152-ARR",
        "version": 2
    }
}