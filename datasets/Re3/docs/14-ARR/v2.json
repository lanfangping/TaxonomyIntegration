{
    "nodes": [
        {
            "ix": "14-ARR_v2_0",
            "content": "Learning to Rank Visual Stories from Human Ranking Data",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_2",
            "content": "Visual storytelling (VIST) is a typical vision and language task that has seen extensive development in the natural language generation research domain. However, it remains unclear whether conventional automatic evaluation metrics for text generation are applicable on VIST. In this paper, we present the VHED (VIST Human Evaluation Data) dataset, which first re-purposes human evaluation results for automatic evaluation; hence we develop Vrank (VIST ranker), a novel reference-free VIST metric for story evaluation. 1 We first show that the results from commonly adopted automatic metrics for text generation have little correlation with those obtained from human evaluation, which motivates us to directly utilize human evaluation results to learn the automatic evaluation model. In the experiments, we evaluate the generated texts to predict story ranks using our model as well as other reference-based and reference-free metrics. Results show that Vrank prediction is significantly more aligned to human evaluation than other metrics with almost 30% higher accuracy when ranking story pairs. Moreover, we demonstrate that only Vrank shows human-like behavior in its strong ability to find better stories when the quality gap between two stories is high. Finally, we show the superiority of Vrank by its generalizability to pure textual stories, and conclude that this reuse of human evaluation results puts Vrank in a strong position for continued future advances. * * denotes equal contribution 1 Dataset VHED and metric Vrank can be found on GitHub: https://github.com/AcademiaSinicaNLPLab/ VHED.git the city was very busy. there were many different kinds of bikes. some were very unique. they were all very fast. i had a great time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_3",
            "content": "i went to the park station. it was a train trip to the museum. the train was very long. we had to go on our way out of the trains. this dog is so happy to see us.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_4",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "14-ARR_v2_5",
            "content": "In visual storytelling (VIST) (Huang et al., 2016), a generation model tells a short story to describe the given five images. Automatic generation of visual stories is challenging because it has the complexity of cross-modal understanding with the diversity Reference: i decided my dog would like a train ride. off to the train station we go. this is the train we will be taking our short trip on. my friend is the conductor. he is getting ready to attach the cars. here is the train all together. as you can see, my dog had a fantastic time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_6",
            "content": "Model 1 (BLEU-1: 0.605, Human Rankers: ) Model 2 (BLEU-1: 0.354, Human Rankers: ) Figure 1: Ranking of two stories generated by Model 1 and 2, by human rankers versus BLEU-1 score. BLEU-1 mispredicts due to unreasonable matches, correlating poorly with human ranking judgment. and sophistication of creative writing (Zhu et al., 2020). Extensive efforts in model developments have decreased the distance between machinegenerated and human-written stories, but research on VIST evaluation remains stagnant.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_7",
            "content": "Automatic metrics and human evaluation are widely used to examine natural language generation. Traditional n-gram-based or referencebased autometrics such as BLEU (Papineni et al., 2002), CIDEr (Vedantam et al., 2015), and ME-TEOR (Banerjee and Lavie, 2005) are common for VIST evaluation. However, preliminary findings have shown that these metrics have many drawbacks and hence are incompatible with VIST (Wang et al., 2018b). In particular, they assume that human-written stories are always better than machine-generated stories, limiting the advance of models yet not conforming to our observation on human judgment. Rethinking this postulation in evaluation, we believe the dependence on references should be minimized and human evaluation results should be fully utilized instead, because human judgements contain more meaningful signals. Recent hybrid and referencefree metrics such as BLEURT (Sellam et al., 2020) and UNION (Guan and Huang, 2020) have not yet been implemented or studied in VIST. Nevertheless, BLEURT utilizes few human results in finetuning, and UNION still regards human references as gold labels, which results in poor correlation to human judgment. On the other hand, human evaluations are relatively reliable for performance reports, and recent studies often include them to provide more convincing experimental results (Hsu et al., 2020(Hsu et al., , 2021a. However, human evaluations are expensive, time-consuming, and difficult to reproduce. Therefore, results should be recycled to benefit future evaluations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_8",
            "content": "Accordingly, we re-collected the human evaluation results from multiple published papers and organized the data into story pairs (Wei and Jia, 2021) as the VHED (VIST Human Evaluation Data) dataset. We then re-purposed VHED to create a better metric Vrank for VIST to rank visual stories. Vrank is a reference-free SIMCSE (Gao et al., 2021) based metric trained on VHED to learn to rank visual stories. We believe a storytelling metric should be independent of the references because stories are highly diverse by nature (Zhu et al., 2020), and it is reasonable for them to be dissimilar to the references (Guan and Huang, 2020) As shown in Fig. 1, the story generated by Model 1 is assigned a higher BLEU score because larger portions of text overlap with the reference. However, human rankers recognize description in isolation and object detection error in Model 1, and instead rank Model 2 better. We conduct experiments to show that Vrank is superior to existing metrics, many of which lack properties essential to evaluating stories in a human-like fashion.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_9",
            "content": "Therefore, we utilize VHED to understand and analyze human judgment in evaluating visual stories, and to provide additional metric assessments to reveal the shortcomings of existing metrics. The metric assessment experiments are conducted as the story-pair ranking task in which two stories are ranked based on their story quality. We observe three characteristics and design corresponding assessments to demonstrate Vrank's merits. First, larger rank differences in story quality are easier for people to differentiate. We measure the performance of metrics in story pairs with large gaps versus small gaps to determine whether all metrics have this property. Our assessment indicates this property is exclusively hold by Vrank. Second, human-written stories are not always better than machine-written stories. Indeed, 38% of machine-generated stories are better than the references, which suggests that the afore-mentioned assumption may need to be revisited . We examine the ability of metrics to rank such human-machine pairs, which Vrank performs relatively well. Finally, most generated stories still contain many errors, which serve as signals for human rankers (Modi and Parde, 2019). Hence we evaluate the ability of metrics to detect errors and show that Vrank is a better indicator of errors. Also, we show that Vrank is able to generalize to other datasets without bias to VHED. In conclusion, Vrank excels in the above assessments and able to follow human behaviors in ranking. Moreover, Vrank can rank machine and human stories decently and is better at detecting story errors. Specifically, we make three major contributions:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_10",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "14-ARR_v2_11",
            "content": "Visual Storytelling (VIST) Visual storytelling was introduced by Huang et al. (2016) as the task of generating a coherent story given five images. They provided a dataset, Sequential Images Narrative Dataset (SIND), containing images and references in which references are human-written short stories describing images. For every image prompt (one sequence of photos), there are 2 to 5 references. VIST requires deeper understanding of the photo events to prevent descriptions in isolation (i.e., image captions). Researchers have proposed various methods for this task. Knowledge graphs are often integrated in models to encourage diversity of terms and plots in the stories (Hsu et al., 2020(Hsu et al., , 2021a. Some studies use reinforcement learning to reward models that generate stories that contain fewer errors and are more topically-focused Hu et al., 2020a). However, existing evaluation methods are unable to capture the true quality of the generated stories. Thus we examine automatic metrics to devise a better way for machines to evaluate stories.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_12",
            "content": "VIST-Human Evaluation Several VIST generation models use human evaluation to evaluate model performance. Recent studies apply aspectbased rating evaluation. Hu et al. (2020b) and Wang et al. (2020b) ask workers to rate stories based on pre-defined aspects. 2 However, it is difficult to normalize these aspects as the definition of aspect varies from paper to paper. Also, these aspects are not mutually independent, making it difficult to analyze results based on these ratings. Therefore, we consider the ranking method as it is commonly used (Hsu et al., 2020;Wang et al., 2020b;Hsu et al., 2021a) Automatic Metrics Automatic evaluation metrics are widely used in language generation tasks. Most reference-based metrics (e.g., BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004)) evaluate the ngram similarity between a generated text and the reference. However, referenced metrics correlate poorly with human judgment (Wang et al., 2018b;Hsu et al., 2019;Modi and Parde, 2019) in dialog generation and story generation tasks: the generated text is given unreasonable scores due to incongruity with the reference. To account for this, several reference-free metrics (Sinha et al., 2020;Guan and Huang, 2020) have been designed to measure generated texts without any reference. BERT-Score (Zhang et al., 2020), for instance, uses contextual embedding to calculate the similarity between candidates and references, and BLEURT (Sellam et al., 2020) ) uses pre-defined negative samples to train a model in an attempt to provide a metric that specializes in story generation. In our analysis, current metrics remain unable to mimic human judgment to discern quality differences in story pairs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_13",
            "content": "VHED",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "14-ARR_v2_14",
            "content": "Dataset Description",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "14-ARR_v2_15",
            "content": "The VHED dataset is a collection of human evaluation results from three VIST studies: KG-Story (Hsu et al., 2020), PR-VIST (Hsu et al., 2021a), and Stretch-VST (Hsu et al., 2021b). All papers followed Hsu et al. (2020)'s human evaluation method using Amazon Mechanical Turk. For each task, the workers were to rank the story by overall quality, from the best story to the worst story. Specifically, each task displayed N stories, and each worker ranked each story from 1 to N . Details about each paper are listed in Table 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_16",
            "content": "The construction of VHED is shown in Figure 2. Collected from the aforementioned papers, we obtained 4,500 task results. Further, we grouped N stories into story pairs, where the number of story pairs per task is C N 2 . The resulting story pairs (x 1 , x 2 ) are either two machine-generated stories from two different models or one reference and one machine-generated story. For each story pair, there are five attributes:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_17",
            "content": "\u2022 Stories: A story pair consists of a better-ranked story and worse-ranked story. The story pair is either a reference with a machine-generated story, or two machine-generated stories. \u2022 Image Sequence IDs: A list of IDs for each of the five images from the SIND dataset (Huang et al., 2016). \u2022 Average Rank: The average of the five workers' story rankings is divided by N for normalization. N varies from paper to paper (Table 1). (Hsu et al., 2020), PRVIST (Hsu et al., 2021a), and Stretch-VST (Hsu et al., 2021b) ranking of x 2 . The ranking gap distribution is shown in the appendix (Table 6). \u2022 Human Agreement: Human agreement is when k workers agree that the better-ranked stories are better than the worse-ranked stories.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_18",
            "content": "Note that human agreement = 2 is equivalent to human agreement = 3, because 1 person agreeing that story A is better than B is equivalent to 4 people agreeing that story B is better than A. Therefore, we kept human agreements = 3,4,5 for simple notation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_19",
            "content": "For quality control, we remove story pairs with zero ranking gap. This yields 13,875 story pairs in total. The train-test-validation sets were split at a ratio of 8:1:1 to 11,208, 1,351, and 1,316 story pairs. The descriptions of VIST models' generated stories are included in the appendix.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_20",
            "content": "Data Analysis and Findings",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "14-ARR_v2_21",
            "content": "As we acquired data about human preferences in story pairs, we conducted analyses to understand the potential patterns for workers when assigning story ranks, the quality gap between machinegenerated and human-written stories, and the errors in the stories. The results of this observation are crucial for assessing the performance of a metric.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_22",
            "content": "Worker Ranking Analysis Story pairs are grouped by the same human agreement. \u2126 k denotes a sub-dataset containing story pairs with human agreement = k. In Table 2, we calculate the number of story pairs as well as the averaged ranking gap of each sub-dataset. For story pairs, we note that story pairs with k = 3 account for 53% of the dataset, meaning that half of the tasks have inconsistent annotations. Regardless, this paper evaluates the story pairs with k \u2265 4 to filter out inconsistent human annotations. We also note that the ranking gap increases as human agreement increases. The ranking gap indicates the quality difference between a better-ranked and a worseranked story. That is, the difference between a ranked 1 story and a ranked 5 story should be larger than that between a ranked 2 story and a ranked 3 story. From Table 2, we find that story-pairs with lower agreement are closer in ranking. In other words, a story pair with a marginal quality difference easily leads to inconsistent worker annotations, because it is harder to rank two similar stories. Essentially, we expect the metrics to exhibit similar behavior: the larger the ranking gap, the easier it is to rank.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_23",
            "content": "Who Wins? Machine vs. Human Stories Next we revisit the assertion that references are always superior. We select story pairs with a reference and a machine-generated story. We analyze the number and percentage of references that are ranked better than the generated stories on three human agreements. From Table 2, we observe that when more humans agree on the ranking results, the percentage of the reference being better also increases. In addition, further analysis shows that, on average, 38% of the machine-generated stories are in fact better than the references, showing that references are not always better than machine-generated stories.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_24",
            "content": "Error Analysis To understand the difference between better-and worse-ranked stories, deeper analysis into the story content is necessary. We randomly sampled 200 stories from VHED (67 human and 134 machine generated) and manually labeled the stories according to the following error aspects:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_25",
            "content": "\u2022 Grammatical error (Gram): Erroneous usage of past/current tense and mistakes in misplaced modifiers. \u2022 Event mismatch (Event): Stories that are offtopic, which present events that are not relevant to the image stream. \u2022 Object mismatch (Obj): Irrelevant nouns that do not appear in the images and are not semantically related.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_26",
            "content": "We first labeled stories based on all 11 error aspects defined in (Modi and Parde, 2019) and we select the most occurring errors, which are grammar, repetition, description in isolation, and absurdity. These four error aspects focus primarily on story coherence and within-story consistency. However, visual storytelling requires generated stories to fit the given story images. Rohrbach et al. (2019) show that humans are aware of the correctness of image descriptions. Also, Wang et al. (2020a) show that mismatched events in stories can lead to poor story quality. Therefore, we added event and object mismatch into our analysis. The error examples and correlation between the error are illustrated in the appendix (Table 9 and Figure 5). From our observation, 79.8% of the sampled machine generated stories contained at least one of the errors in the categories, meaning most VIST models are unable to generate perfect stories. In Table 3, the high percentage of object and event mismatch errors also show that current VIST models do not capture visual groundings accurately. This can lead to humans assigning higher scores to human-written stories since they are most likely to be relevant to the given images. Grammatical errors and absurdities are also common in generated text, which can lead to ambiguous stories that humans are unable to comprehend. The prevalence of errors makes it essential for evaluation metrics to automatically detect these errors.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_27",
            "content": "Vrank",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "14-ARR_v2_28",
            "content": "We propose Vrank, a reference-free automatic metric that inputs story pairs to predict human preferences between the two stories. We utilize SIM-CSE (Gao et al., 2021) to leverage better sentence representations. SIMCSE uses contrastive learning with dropout as augmentation, then trained on natural langauge inference datasets to obtain better sentence embeddings from BERT (Devlin et al., 2019). First, we pre-trained the SIMCSE model using SIND reference stories with the Masked Language Model objective. Then, we input two stories with a [SEP] token in between through the pre-trained model. We use the acquired sentence embeddings and feed it through a regression layer to predict a ranking gap. We used mean squared error to calculate the loss between the predicted ranking gap and true ranking gap. After obtaining the ranking gap, we predict which story is better according to the sign of the predicted ranking gap. Although Vrank is a simple model fine-tuned solely on human judgment, it still outperforms current existing metrics in our assessments. This suggests further potential for use with VHED; more studies can be conducted to replace Vrank with stronger neural network models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_29",
            "content": "During model training, since the number of positives and negatives were not balanced in the original dataset, we augmented the data to create a symmetric dataset of VHED to minimize dataset bias. 3 The ranking gap in the resulting dataset was close to normally distributed. We hypothesize that utilizing this feature makes it possible to extract more information, making it easier for the model to learn human judgment. However, due to the small amount of data available, high variance is likely (Mosbach et al., 2020) to occur during inference. Hence, we used all data from VHED, including human agreement=3 to increase the stability of our model following Mosbach et al. (2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_30",
            "content": "Metric Assessment",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "14-ARR_v2_31",
            "content": "In this section, we describe a series of assessments conducted on existing metrics on VHED, in which the assessment methods are based on the analyses in VHED. The objective is to examine whether Vrank is superior to other metrics based on our analysis of VHED.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_32",
            "content": "Experimental Settings",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "14-ARR_v2_33",
            "content": "Story-Pair Ranking A recent study (Wei and Jia, 2021) illustrates that pairwise accuracy reflects metric performance better than using correlation with human evaluation. Hence, we propose simple story-pair ranking to evaluate automatic evaluation metrics for visual storytelling. The task is to determine the correct ranking order of the stories in a story pair based on the story quality scores predicted by the automatic evaluation metrics being assessed. Given the story pair (x 1 , x 2 ), the automatic metric being assessed predicts the corresponding story quality scores (s 1 , s 2 ) which we compare to the averaged ranks y 1 and y 2 of x 1 and x 1 from human evaluation. The performance of the evaluation metric on the i-th story pair is formulated as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_34",
            "content": "ranking_acc i = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1, if s 1 > s 2 and y 1 < y 2 1, if s 1 < s 2 and y 1 > y 2 0, otherwise,(1",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_35",
            "content": ") where ranking_acc i = 1 indicates correct (incorrect) prediction. Note that low scores indicate high rank. The overall metric performance over M story pairs is defined as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_36",
            "content": "avg_ranking_acc = 1 M M i=1 ranking_acc i .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_37",
            "content": "(",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_38",
            "content": ")2",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_39",
            "content": "Datasets In addition to VHED, we also collected VIST-Edit 4 (Hsu et al., 2019) for story-pair ranking. VIST-Edit includes 2,981 visual stories generated by AREL (Wang et al., 2018a) and GLAC (Kim et al., 2018), and 14,905 human-edited visual stories, that is, AREL and GLAC-generated stories edited by workers. Their paper shows that the crowd workers' edits systematically increased the lexical diversity of the stories. Since the purpose of the editing was to improve the machine-4 VIST-Edit: https://github.com/tingyaohsu/ VIST-Edit generated stories, we paired up human-edited stories and machine-generated stories as better-ranked and worse-ranked samples (labeled as 1 and 2), resulting in 14,905 story pairs. Comparing VHED to VIST-Edit, VHED contains reference and multiple models' generated stories, but VIST-Edit has only human-machine story pairs. Additionally, VIST-Edit is not in Vrank's training data. VIST-Edit is utilized only for metric performance reports, serving as an unseen dataset for Vrank.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_40",
            "content": "We first consider traditional n-gram-based reference-based metrics, BLEU, ROUGE-L, METEOR, and Sacre-BLEU (Keenan, 2017). We also implement the more recent BERT-Score, BLEURT, and UNION as baseline metrics. In addition to the above automatic metrics, we also include a random baseline, denoted as Random in Table 4, to provide a random score for each story as the lower bound.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_41",
            "content": "A common practice for reference-based metrics: a candidate story is scored against each reference r j in a gold reference set R = {r i } n i=1 ; the highest score was used. However, applying this method on a reference-machine story pair would always result in reference having a full score, because of the exact match between reference and the gold reference set. To ensure a fair evaluation and avoid meaningless matching, we first check that the gold references do not include the reference. To this end, we propose the Reference Absent Algorithm for evaluating story pairs containing the reference story (or stories) as in Eq. 3, which removes the r j from R when any of the candidate stories in a story pair (x = {x 1 , x 2 }) is identical to r j .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_42",
            "content": "s j = max(metric(x j , R \u2212 x)), j = {1, 2}, (3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_43",
            "content": "where metric(\u2022) can be any reference-based metric and s j is the story quality scores for the j-th story in a story pair. This algorithm only applies when evaluating story pairs containing references, i.e., reference-machine pairs in this paper.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_44",
            "content": "Results and Discussion",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "14-ARR_v2_45",
            "content": "Pairwise Story Evaluation Accuracy: Metric's ability to determine the correct ranking order in story pairs. The average ranking accuracy of each automatic metric on VHED and VIST-Edit are presented in Table 4 (left). Around 50% corresponds to random guessing, as shown as Random in the table. Vrank shows superior performance in VHED and VIST-Edit, which VIST-Edit is the unknown dataset to Vrank. High performance on VIST-Edit and VHED indicates Vrank has the ability to distinguish diverse story pairs. In contrast, we observe unexpectedly low performance for most baseline metrics, as they perform no better than the Random baseline. BLEU-4 especially struggles to rank the stories in both datasets. Further analysis suggests that BLEU-4 marked \u223c80% of the stories as 0, and Equation 1 coincidentally treated them as incorrect prediction because it discourages ties. BLEURT, in turn, also performed poorly because it relies on reference-based metrics as signals for training. Reference-free metrics, especially UNION, perform well on VIST-Edit. However, its design is not generalizeable to VHED.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_46",
            "content": "Worker Ranking Behavior on Metrics: The larger the ranking gap, the easier is it to rank. The ranking gap is the difference between a betterranked and worse-ranked sample's average ranks. VHED is categorized into four sub-datasets with different ranking gaps. This assessment tests each metrics' ability to mimic worker ranking behavior observed in the analysis. Story pairs with larger gaps suggest stronger linguistic differences and are likely easier to rank, whereas those with smaller gaps are likely more difficult. In Fig. 3, all baseline automatic metrics, including metrics not reported in the figure, show randomly distributed scores, most of which remain around 50%, thus failing to exhibit such behavior. On the contrary, Vrank yields an ideal decrease. Starting with ranking gaps over 0.3, the accuracy reaches \u223c0.85 and a gradual decrease afterward. We believe such behavior reveals Vrank to be a more preferable metric for visual story evaluation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_47",
            "content": "Machine and Human on Metrics: Machines are sometimes better than humans. Two aspects are studied in this section. First, we evaluate the ability of Vrank and reference-based metrics to rank reference-machine (R&M) pairs. Although some machine texts have progressed to humanlevel, to our knowledge, there has been little investigation of metrics' ability to evaluate references and machines. We apply reference-based metrics with Eq. 3. This results in poor performance for reference-based metrics as shown in R&M in Table 4 5 . An explanation is that since the reference is removed from the reference set by Eq. 3, the reference needs to match with the remaining references in the reference set. Although most references are on topic, the stories are highly diverse (Zhu et al., 2020). These metrics are unable to calculate the similarity to semantic levels; thus, they result in poor performance. On the contrary, Vrank is a deep learning model, trained on VHED and thus learned to rate based on story quality rather than similarity. We also find that Vrank ranks correctly when machine is better than reference, showing that Vrank yields 26.5% recall when the other metrics have 0 recall without Eq. 3 and \u223c18% with Eq. 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_48",
            "content": "Second, we observe the performance of metrics on M&M (machine-machine pairs). M&M ranking gaps are smaller than those of R&M pairs (0.18 v.s. 0.21), making them harder to rank because their story qualities are closer. However, Vrank still shows promising performance when ranking Table 5: This table shows the correlation of human rankings, automatic metric scores with the corresponding error categories. An ideal correlation should be closer to Human. Negative correlation illustrates that higher rankings (average ranking closer to 1) co-occur with few errors in the story. Hence, an high error detection rate is a correlation coefficient closer to -1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_49",
            "content": "such story pairs, outperforming existing metrics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_50",
            "content": "Errors in Metrics: Metric's ability to detect errors. Current generated stories often contain errors which prompt human evaluators to assign lower scores. It is crucial for automatic metrics to also recognize errors to judge generated text.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_51",
            "content": "To do this, we adapted the point-biserial correlation coefficient to analyze the correlation between binary annotated errors and metric scores.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_52",
            "content": "The correlation between metrics and errors is presented in Table 5: existing metrics are not able to detect errors as the correlation coefficients are low. From the correlation coefficients between the human ranking score and each error aspect, we observe that human evaluation for stories may be influenced by error aspects, especially absurdity and description in isolation. In general, Vrank performs best in detecting absurdity and description in isolation. UNION-WP performs best in correlation with repetition, which is reasonable since UNION is trained to discriminate erroneous stories that are repetitive in structure. In summary, current metrics remain unable to detect errors to evaluate coherency efficiently. Metrics ability to detect errors may give clearer indications of the quality of generated texts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_53",
            "content": "Dataset Generalization",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "14-ARR_v2_54",
            "content": "In addition to VIST, we expect Vrank to reasonably evaluate the quality of text as well. To determine whether Vrank generalizes to textual stories, we selected MANS dataset (Guan et al., 2021), an imagefree storytelling dataset in which the stories are derived from the ROCStories corpus . MANS includes 200 story prompts, where each prompt includes five model-generated stories and a reference. However, it does not contain human story rankings. Thus, for each story prompt, we asked five workers from Amazon Mechanical Turk to rank the five stories to obtain ranking scores. Following the VHED construction procedure, the ranked stories were converted into story pairs, making for 1,112 story pairs for which 3 workers agreed on the ranking, 605 story pairs for which 4 workers agreed, and 132 story pairs for which 5 workers agreed. Likewise, we evaluate story pairs with k \u2265 4.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_55",
            "content": "Subset \u2126 4 \u2126 5 \u2126 {4,5}",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_56",
            "content": "The results of Vrank and the baseline automatic metrics when ranking MANS are shown in Table 6. We find that Vrank outperforms baseline metrics in story pairs with k \u2265 4, whereas the latter still show limited abilities to rank the MANS dataset. In general, the accuracy of automatic evaluation on MANS is lower than that on VHED. This may be due to the comparably unconstrained writing styles of pure textual stories. An example of the evaluation on stories is given in the appendix (Table 8).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_57",
            "content": "Conclusion and Discussion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "14-ARR_v2_58",
            "content": "We present VHED and Vrank, the first dataset of human evaluation results and evaluation metric for VIST. We show that Vrank performs significantly better in three assessment tasks and generalizes to other datasets. Also, recent automatic metrics are ill-suited to evaluating visual stories, especially human-level written stories. We welcome researchers to share their human evaluation results to the community to broaden the data domain to obtain more knowledge about human judgment and improve the performance of Vrank. As the gap between machines and humans continues to decrease, stronger metrics will be needed to evaluate machine and human stories. Improving Vrank performance to replace reference-based metrics is our future goal.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_59",
            "content": "Appendix",
            "ntype": "title",
            "meta": {
                "section": "9"
            }
        },
        {
            "ix": "14-ARR_v2_60",
            "content": "Application In this section, we introduce an application for Vrank and other reference-free metrics. Our assessment indicates that Vrank's predictions strongly agree with human judgment. We quantify the distance between humans and machines by pairing up reference and generated stories and calculating the ratio of generated stories that outmatch the references. Unlike human evaluation, which can be conducted only on a portion of the testing data, this method allows researchers to evaluate the proposed model over the entire testing dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_61",
            "content": "After applying Vrank to assess five recent VIST models, we present the results in Figure 4: the models are gradually approaching human-level writing, outlining an exciting development of NLG in VIST. Error Type Examples and Correlation In Table 9, we show examples of error types mentioned in our error analysis. We also show the correlation between different error types in Figure 5. As the error types are mutually independent, there is the potential to construct tools to automatically detect each error, since they do not overlap with each other. Ranking Gap Distribution The ranking gap distribution is shown in Figure 6, in which both the ranking gaps and the number of stories are normalized. Also, since the ranking gaps contain both negative and positive values, we took the absolute value of the gap for the histogram. We observe that the machine-machine pairs are centered closer to zero. However, the human-machine pairs are distributed more evenly than the M&M pairs, which indicates that human-machine pairs are easier to distinguish than machine-machine pairs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_62",
            "content": "Without Reference Absent Algorithm Here, we show the results of automatic metric accuracy in story-pair ranking without the proposed Reference Absent Algorithm. As expected, the accuracies for H&M pairs are the same. Since all references are regarded as ground truth for reference-based automatic metrics, the accuracy is shown as the percentage of the human-written stories that are better than machine-generated stories. Hence, these metrics are unable to identify any machine-generated stories that are better than human-written stories. This demonstrates the importance of our proposed algorithm in the experiment results.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_63",
            "content": "We sampled 250 to 500 image prompts from SIND's testing dataset and hired crowd workers from Amazon Mechanical Turk to evaluate the visual stories that were generated based of these image prompts. The workers were adult workers in the US with 98% assignments approved and who had completed at least 3,000 HITs. A user interface for workers to complete was called a task. A task displayed one image prompt on the top with several stories at the bottom, and five workers were recruited to rank the stories.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_64",
            "content": "The stories usually included a reference, stories generated using the proposed model, and several baseline stories. The compensation was USD 0.10 per task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_65",
            "content": "Training Details We use the pre-trained base model from Huggingface (Wolf et al., 2020) and fine-tune it to our regression objective. We utilized Adam as optimizer with learning rate 2e-5 and trained for 30 epochs. The batch size is set as 32 and the random seed for training can be set as 7,777 for reproduction. Checkpoints are stored for every 500 steps and we also utilized mixed precision training for more efficient training. The environment of our operating system is Ubuntu 20.0.4.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_66",
            "content": "Training was completed on two NVidia RTX 3090 GPUs, each of which contains 24 GB of memory.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_67",
            "content": "Model Design Before we came up with the final model using SIMCSE, we tried several settings. Formulating the task as a binary classification task didn't achieve good accuracy, we speculate that this is because the boundaries for a good and bad story is hard to find. Also, we tried to augment the story-pairs with agreement=5. We found out that it didn't improve the performance. Moreover, we tested using CLIP (Radford et al., 2021) to extract image features for additional features and visionlanguage models also did not improve performance. Hence, we picked a simple model architecture to demonstrate our performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_68",
            "content": "\u2022 GLAC (Kim et al., 2018): combines global and local attention to construct image-dependent sentences. A context cascading mechanism is incorporated to improve story coherency. \u2022 AREL (Wang et al., 2018a): uses a policy model and reward model to associate reward learning.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_69",
            "content": "The policy model is used to generate stories, and the reward model learns from human demonstrations. \u2022 KGStory (Hsu et al., 2020): a three-stage framework which distills a set of representive words from the input text and utilizes knowledge graphs to enrich the content. It generates stories from the enriched word set. \u2022 PRVIST (Hsu et al., 2021a): a two-stage framework that finds an optimal path through the constructed story graph which forms the best storyline. This path is then used to generate the story. \u2022 Stretch-VST (Hsu et al., 2021b): a modification of KGStory that produces more sentences in the story while maintaining quality. Appropriate knowledge added to the story results in a more detailed story.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v2_70",
            "content": "Satanjeev Banerjee, Alon Lavie, METEOR: An automatic metric for MT evaluation with improved correlation with human judgments, 2005, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Satanjeev Banerjee",
                    "Alon Lavie"
                ],
                "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
                "pub_date": "2005",
                "pub_title": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "14-ARR_v2_71",
            "content": "UNKNOWN, None, 2021, Commonsense knowledge aware concept selection for diverse and informative visual storytelling, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Commonsense knowledge aware concept selection for diverse and informative visual storytelling",
                "pub": "CoRR"
            }
        },
        {
            "ix": "14-ARR_v2_72",
            "content": "Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, Noah Smith, All that's 'human' is not gold: Evaluating human evaluation of generated text, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Elizabeth Clark",
                    "Tal August",
                    "Sofia Serrano",
                    "Nikita Haduong",
                    "Suchin Gururangan",
                    "Noah Smith"
                ],
                "title": "All that's 'human' is not gold: Evaluating human evaluation of generated text",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "14-ARR_v2_73",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "14-ARR_v2_74",
            "content": "Tianyu Gao, Xingcheng Yao, Danqi Chen, SimCSE: Simple contrastive learning of sentence embeddings, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Tianyu Gao",
                    "Xingcheng Yao",
                    "Danqi Chen"
                ],
                "title": "SimCSE: Simple contrastive learning of sentence embeddings",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "14-ARR_v2_75",
            "content": "Jian Guan, Minlie Huang, UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Jian Guan",
                    "Minlie Huang"
                ],
                "title": "UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_76",
            "content": "Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, Minlie Huang, OpenMEVA: A benchmark for evaluating open-ended story generation metrics, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Jian Guan",
                    "Zhexin Zhang",
                    "Zhuoer Feng",
                    "Zitao Liu",
                    "Wenbiao Ding",
                    "Xiaoxi Mao",
                    "Changjie Fan",
                    "Minlie Huang"
                ],
                "title": "OpenMEVA: A benchmark for evaluating open-ended story generation metrics",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "14-ARR_v2_77",
            "content": "Chao-Chun Hsu, Zi-Yuan Chen, Chi-Yang Hsu, Chih-Chia Li, Tzu-Yuan Lin, Ting-Hao ; Huang, Lun-Wei Ku, Knowledge-enriched visual storytelling, 2020, Proceedings of Thirty-Fourth AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Chao-Chun Hsu",
                    "Zi-Yuan Chen",
                    "Chi-Yang Hsu",
                    "Chih-Chia Li",
                    "Tzu-Yuan Lin",
                    "Ting-Hao ; Huang",
                    "Lun-Wei Ku"
                ],
                "title": "Knowledge-enriched visual storytelling",
                "pub_date": "2020",
                "pub_title": "Proceedings of Thirty-Fourth AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_78",
            "content": "Chi-Yang Hsu, Yun-Wei Chu, Ting-Hao ; Kenneth, ) Huang, Lun-Wei Ku, Plot and rework: Modeling storylines for visual storytelling, 2021-08-01, Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Chi-Yang Hsu",
                    "Yun-Wei Chu",
                    "Ting-Hao ; Kenneth",
                    ") Huang",
                    "Lun-Wei Ku"
                ],
                "title": "Plot and rework: Modeling storylines for visual storytelling",
                "pub_date": "2021-08-01",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "14-ARR_v2_79",
            "content": "Chi-Yang Hsu, Yun-Wei Chu, Tsai-Lun Yang, Ting-Hao Huang, Lun-Wei Ku, Stretch-VST: Getting flexible with visual stories, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Chi-Yang Hsu",
                    "Yun-Wei Chu",
                    "Tsai-Lun Yang",
                    "Ting-Hao Huang",
                    "Lun-Wei Ku"
                ],
                "title": "Stretch-VST: Getting flexible with visual stories",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "14-ARR_v2_80",
            "content": "Ting-Yao Hsu, Chieh-Yang Huang, Yen-Chia Hsu, Ting-Hao Huang, Visual story post-editing, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Ting-Yao Hsu",
                    "Chieh-Yang Huang",
                    "Yen-Chia Hsu",
                    "Ting-Hao Huang"
                ],
                "title": "Visual story post-editing",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "14-ARR_v2_81",
            "content": "Junjie Hu, Yu Cheng, Zhe Gan, Jingjing Liu, Jianfeng Gao, Graham Neubig, What makes a good story? designing composite rewards for visual storytelling, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Junjie Hu",
                    "Yu Cheng",
                    "Zhe Gan",
                    "Jingjing Liu",
                    "Jianfeng Gao",
                    "Graham Neubig"
                ],
                "title": "What makes a good story? designing composite rewards for visual storytelling",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_82",
            "content": "Junjie Hu, Yu Cheng, Zhe Gan, Jingjing Liu, Jianfeng Gao, Graham Neubig, What makes a good story? Designing composite rewards for visual storytelling, 2020, Thirty-Fourth AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Junjie Hu",
                    "Yu Cheng",
                    "Zhe Gan",
                    "Jingjing Liu",
                    "Jianfeng Gao",
                    "Graham Neubig"
                ],
                "title": "What makes a good story? Designing composite rewards for visual storytelling",
                "pub_date": "2020",
                "pub_title": "Thirty-Fourth AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_83",
            "content": "Qiuyuan Huang, Zhe Gan, Asli Celikyilmaz, Dapeng Wu, Hierarchically structured reinforcement learning for topically coherent visual story generation, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Qiuyuan Huang",
                    "Zhe Gan",
                    "Asli Celikyilmaz",
                    "Dapeng Wu"
                ],
                "title": "Hierarchically structured reinforcement learning for topically coherent visual story generation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_84",
            "content": "Ting-Hao Kenneth Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, Visual storytelling, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Ting-Hao Kenneth Huang",
                    "Francis Ferraro",
                    "Nasrin Mostafazadeh",
                    "Ishan Misra",
                    "Aishwarya Agrawal",
                    "Jacob Devlin",
                    "Ross Girshick",
                    "Xiaodong He",
                    "Pushmeet Kohli",
                    "Dhruv Batra"
                ],
                "title": "Visual storytelling",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_85",
            "content": "J William,  Keenan, Sacre Bleu: Faith, fashion and freedom: Marist foundation garments 1817-1862, 2017, Materializing Religion, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "J William",
                    " Keenan"
                ],
                "title": "Sacre Bleu: Faith, fashion and freedom: Marist foundation garments 1817-1862",
                "pub_date": "2017",
                "pub_title": "Materializing Religion",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_86",
            "content": "UNKNOWN, None, 2018, GLAC Net: GLocal Attention Cascading Networks for multi-image cued story generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "GLAC Net: GLocal Attention Cascading Networks for multi-image cued story generation",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_87",
            "content": "Chin-Yew Lin, ROUGE: A package for automatic evaluation of summaries, 2004, Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Chin-Yew Lin"
                ],
                "title": "ROUGE: A package for automatic evaluation of summaries",
                "pub_date": "2004",
                "pub_title": "Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_88",
            "content": "Yatri Modi, Natalie Parde, The steep road to happily ever after: An analysis of current visual storytelling models, 2019, Proceedings of the Second Workshop on Shortcomings in Vision and Language, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Yatri Modi",
                    "Natalie Parde"
                ],
                "title": "The steep road to happily ever after: An analysis of current visual storytelling models",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Second Workshop on Shortcomings in Vision and Language",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_89",
            "content": "UNKNOWN, None, 2006, On the stability of fine-tuning BERT: misconceptions, explanations, and strong baselines. CoRR, abs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": null,
                "title": null,
                "pub_date": "2006",
                "pub_title": "On the stability of fine-tuning BERT: misconceptions, explanations, and strong baselines. CoRR, abs",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_90",
            "content": "Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, James Allen, A corpus and cloze evaluation for deeper understanding of commonsense stories, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Nasrin Mostafazadeh",
                    "Nathanael Chambers",
                    "Xiaodong He",
                    "Devi Parikh",
                    "Dhruv Batra",
                    "Lucy Vanderwende",
                    "Pushmeet Kohli",
                    "James Allen"
                ],
                "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_91",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, BLEU: A method for automatic evaluation of machine translation, 2002, Proceedings of the 40th annual meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "BLEU: A method for automatic evaluation of machine translation",
                "pub_date": "2002",
                "pub_title": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_92",
            "content": "Alec Radford, Jong Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Learning transferable visual models from natural language supervision, 2021, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Alec Radford",
                    "Jong Kim",
                    "Chris Hallacy",
                    "Aditya Ramesh",
                    "Gabriel Goh",
                    "Sandhini Agarwal",
                    "Girish Sastry",
                    "Amanda Askell",
                    "Pamela Mishkin",
                    "Jack Clark"
                ],
                "title": "Learning transferable visual models from natural language supervision",
                "pub_date": "2021",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "14-ARR_v2_93",
            "content": "Anna Rohrbach, Lisa Hendricks, Kaylee Burns, Trevor Darrell, Kate Saenko, Object hallucination in image captioning, 2019, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Anna Rohrbach",
                    "Lisa Hendricks",
                    "Kaylee Burns",
                    "Trevor Darrell",
                    "Kate Saenko"
                ],
                "title": "Object hallucination in image captioning",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_94",
            "content": "Thibault Sellam, Dipanjan Das, Ankur Parikh, BLEURT: Learning robust metrics for text generation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Thibault Sellam",
                    "Dipanjan Das",
                    "Ankur Parikh"
                ],
                "title": "BLEURT: Learning robust metrics for text generation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_95",
            "content": "Koustuv Sinha, Prasanna Parthasarathi, Jasmine Wang, Ryan Lowe, William Hamilton, Joelle Pineau, Learning an unreferenced metric for online dialogue evaluation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Koustuv Sinha",
                    "Prasanna Parthasarathi",
                    "Jasmine Wang",
                    "Ryan Lowe",
                    "William Hamilton",
                    "Joelle Pineau"
                ],
                "title": "Learning an unreferenced metric for online dialogue evaluation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_96",
            "content": "C Ramakrishna Vedantam, Devi Zitnick,  Parikh, Cider: Consensus-based image description evaluation, 2015, IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "C Ramakrishna Vedantam",
                    "Devi Zitnick",
                    " Parikh"
                ],
                "title": "Cider: Consensus-based image description evaluation",
                "pub_date": "2015",
                "pub_title": "IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_97",
            "content": "Jing Wang, Jianlong Fu, Jinhui Tang, Zechao Li, Tao Mei, Show, reward and tell: Automatic generation of narrative paragraph from photo stream by adversarial training, 2018, Thirty-Second AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Jing Wang",
                    "Jianlong Fu",
                    "Jinhui Tang",
                    "Zechao Li",
                    "Tao Mei"
                ],
                "title": "Show, reward and tell: Automatic generation of narrative paragraph from photo stream by adversarial training",
                "pub_date": "2018",
                "pub_title": "Thirty-Second AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_98",
            "content": "Ruize Wang, Zhongyu Wei, Ying Cheng, Piji Li, Haijun Shan, Ji Zhang, Qi Zhang, Xuanjing Huang, Keep it consistent: Topic-aware storytelling from an image stream via iterative multi-agent communication, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Ruize Wang",
                    "Zhongyu Wei",
                    "Ying Cheng",
                    "Piji Li",
                    "Haijun Shan",
                    "Ji Zhang",
                    "Qi Zhang",
                    "Xuanjing Huang"
                ],
                "title": "Keep it consistent: Topic-aware storytelling from an image stream via iterative multi-agent communication",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_99",
            "content": "UNKNOWN, None, 2020, Storytelling from an image stream using scene graphs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Storytelling from an image stream using scene graphs",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_100",
            "content": "Xin Wang, Wenhu Chen, Yuan-Fang Wang, William Wang, No metrics are perfect: Adversarial reward learning for visual storytelling, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Xin Wang",
                    "Wenhu Chen",
                    "Yuan-Fang Wang",
                    "William Wang"
                ],
                "title": "No metrics are perfect: Adversarial reward learning for visual storytelling",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "14-ARR_v2_101",
            "content": "Johnny Wei, Robin Jia, The statistical advantage of automatic NLG metrics at the system level, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Johnny Wei",
                    "Robin Jia"
                ],
                "title": "The statistical advantage of automatic NLG metrics at the system level",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "14-ARR_v2_102",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_103",
            "content": "Alexander Lhoest,  Rush, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Alexander Lhoest",
                    " Rush"
                ],
                "title": "Transformers: State-of-the-art natural language processing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_104",
            "content": "Tianyi Zhang, Varsha Kishore, Felix Wu, Q Kilian, Yoav Weinberger,  Artzi, BERTScore: Evaluating text generation with BERT, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Tianyi Zhang",
                    "Varsha Kishore",
                    "Felix Wu",
                    "Q Kilian",
                    "Yoav Weinberger",
                    " Artzi"
                ],
                "title": "BERTScore: Evaluating text generation with BERT",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "14-ARR_v2_105",
            "content": "Wanrong Zhu, Xin Wang, Pradyumna Narayana, Kazoo Sone, Sugato Basu, William Wang, Towards understanding sample variance in visually grounded language generation: Evaluations and observations, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Wanrong Zhu",
                    "Xin Wang",
                    "Pradyumna Narayana",
                    "Kazoo Sone",
                    "Sugato Basu",
                    "William Wang"
                ],
                "title": "Towards understanding sample variance in visually grounded language generation: Evaluations and observations",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "14-ARR_v2_0@0",
            "content": "Learning to Rank Visual Stories from Human Ranking Data",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_0",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_2@0",
            "content": "Visual storytelling (VIST) is a typical vision and language task that has seen extensive development in the natural language generation research domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_2",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_2@1",
            "content": "However, it remains unclear whether conventional automatic evaluation metrics for text generation are applicable on VIST.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_2",
            "start": 153,
            "end": 273,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_2@2",
            "content": "In this paper, we present the VHED (VIST Human Evaluation Data) dataset, which first re-purposes human evaluation results for automatic evaluation; hence we develop Vrank (VIST ranker), a novel reference-free VIST metric for story evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_2",
            "start": 275,
            "end": 516,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_2@3",
            "content": "1 We first show that the results from commonly adopted automatic metrics for text generation have little correlation with those obtained from human evaluation, which motivates us to directly utilize human evaluation results to learn the automatic evaluation model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_2",
            "start": 518,
            "end": 781,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_2@4",
            "content": "In the experiments, we evaluate the generated texts to predict story ranks using our model as well as other reference-based and reference-free metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_2",
            "start": 783,
            "end": 933,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_2@5",
            "content": "Results show that Vrank prediction is significantly more aligned to human evaluation than other metrics with almost 30% higher accuracy when ranking story pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_2",
            "start": 935,
            "end": 1095,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_2@6",
            "content": "Moreover, we demonstrate that only Vrank shows human-like behavior in its strong ability to find better stories when the quality gap between two stories is high.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_2",
            "start": 1097,
            "end": 1257,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_2@7",
            "content": "Finally, we show the superiority of Vrank by its generalizability to pure textual stories, and conclude that this reuse of human evaluation results puts Vrank in a strong position for continued future advances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_2",
            "start": 1259,
            "end": 1468,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_2@8",
            "content": "* * denotes equal contribution 1 Dataset VHED and metric Vrank can be found on GitHub: https://github.com/AcademiaSinicaNLPLab/ VHED.git the city was very busy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_2",
            "start": 1470,
            "end": 1629,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_2@9",
            "content": "there were many different kinds of bikes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_2",
            "start": 1631,
            "end": 1671,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_2@10",
            "content": "some were very unique.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_2",
            "start": 1673,
            "end": 1694,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_2@11",
            "content": "they were all very fast.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_2",
            "start": 1696,
            "end": 1719,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_2@12",
            "content": "i had a great time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_2",
            "start": 1721,
            "end": 1739,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_3@0",
            "content": "i went to the park station.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_3",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_3@1",
            "content": "it was a train trip to the museum.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_3",
            "start": 28,
            "end": 61,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_3@2",
            "content": "the train was very long.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_3",
            "start": 63,
            "end": 86,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_3@3",
            "content": "we had to go on our way out of the trains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_3",
            "start": 88,
            "end": 129,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_3@4",
            "content": "this dog is so happy to see us.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_3",
            "start": 131,
            "end": 161,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_4@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_4",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_5@0",
            "content": "In visual storytelling (VIST) (Huang et al., 2016), a generation model tells a short story to describe the given five images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_5",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_5@1",
            "content": "Automatic generation of visual stories is challenging because it has the complexity of cross-modal understanding with the diversity Reference: i decided my dog would like a train ride.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_5",
            "start": 126,
            "end": 309,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_5@2",
            "content": "off to the train station we go.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_5",
            "start": 311,
            "end": 341,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_5@3",
            "content": "this is the train we will be taking our short trip on.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_5",
            "start": 343,
            "end": 396,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_5@4",
            "content": "my friend is the conductor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_5",
            "start": 398,
            "end": 424,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_5@5",
            "content": "he is getting ready to attach the cars.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_5",
            "start": 426,
            "end": 464,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_5@6",
            "content": "here is the train all together. as you can see, my dog had a fantastic time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_5",
            "start": 466,
            "end": 541,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_6@0",
            "content": "Model 1 (BLEU-1: 0.605, Human Rankers: ) Model 2 (BLEU-1: 0.354, Human Rankers: ) Figure 1: Ranking of two stories generated by Model 1 and 2, by human rankers versus BLEU-1 score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_6",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_6@1",
            "content": "BLEU-1 mispredicts due to unreasonable matches, correlating poorly with human ranking judgment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_6",
            "start": 181,
            "end": 275,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_6@2",
            "content": "and sophistication of creative writing (Zhu et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_6",
            "start": 277,
            "end": 334,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_6@3",
            "content": "Extensive efforts in model developments have decreased the distance between machinegenerated and human-written stories, but research on VIST evaluation remains stagnant.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_6",
            "start": 336,
            "end": 504,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_7@0",
            "content": "Automatic metrics and human evaluation are widely used to examine natural language generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_7",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_7@1",
            "content": "Traditional n-gram-based or referencebased autometrics such as BLEU (Papineni et al., 2002), CIDEr (Vedantam et al., 2015), and ME-TEOR (Banerjee and Lavie, 2005) are common for VIST evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_7",
            "start": 95,
            "end": 288,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_7@2",
            "content": "However, preliminary findings have shown that these metrics have many drawbacks and hence are incompatible with VIST (Wang et al., 2018b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_7",
            "start": 290,
            "end": 427,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_7@3",
            "content": "In particular, they assume that human-written stories are always better than machine-generated stories, limiting the advance of models yet not conforming to our observation on human judgment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_7",
            "start": 429,
            "end": 619,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_7@4",
            "content": "Rethinking this postulation in evaluation, we believe the dependence on references should be minimized and human evaluation results should be fully utilized instead, because human judgements contain more meaningful signals.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_7",
            "start": 621,
            "end": 843,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_7@5",
            "content": "Recent hybrid and referencefree metrics such as BLEURT (Sellam et al., 2020) and UNION (Guan and Huang, 2020) have not yet been implemented or studied in VIST.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_7",
            "start": 845,
            "end": 1003,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_7@6",
            "content": "Nevertheless, BLEURT utilizes few human results in finetuning, and UNION still regards human references as gold labels, which results in poor correlation to human judgment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_7",
            "start": 1005,
            "end": 1176,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_7@7",
            "content": "On the other hand, human evaluations are relatively reliable for performance reports, and recent studies often include them to provide more convincing experimental results (Hsu et al., 2020(Hsu et al., , 2021a.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_7",
            "start": 1178,
            "end": 1387,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_7@8",
            "content": "However, human evaluations are expensive, time-consuming, and difficult to reproduce.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_7",
            "start": 1389,
            "end": 1473,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_7@9",
            "content": "Therefore, results should be recycled to benefit future evaluations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_7",
            "start": 1475,
            "end": 1542,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_8@0",
            "content": "Accordingly, we re-collected the human evaluation results from multiple published papers and organized the data into story pairs (Wei and Jia, 2021) as the VHED (VIST Human Evaluation Data) dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_8",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_8@1",
            "content": "We then re-purposed VHED to create a better metric Vrank for VIST to rank visual stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_8",
            "start": 199,
            "end": 287,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_8@2",
            "content": "Vrank is a reference-free SIMCSE (Gao et al., 2021) based metric trained on VHED to learn to rank visual stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_8",
            "start": 289,
            "end": 401,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_8@3",
            "content": "We believe a storytelling metric should be independent of the references because stories are highly diverse by nature (Zhu et al., 2020), and it is reasonable for them to be dissimilar to the references (Guan and Huang, 2020) As shown in Fig. 1, the story generated by Model 1 is assigned a higher BLEU score because larger portions of text overlap with the reference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_8",
            "start": 403,
            "end": 770,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_8@4",
            "content": "However, human rankers recognize description in isolation and object detection error in Model 1, and instead rank Model 2 better.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_8",
            "start": 772,
            "end": 900,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_8@5",
            "content": "We conduct experiments to show that Vrank is superior to existing metrics, many of which lack properties essential to evaluating stories in a human-like fashion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_8",
            "start": 902,
            "end": 1062,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_9@0",
            "content": "Therefore, we utilize VHED to understand and analyze human judgment in evaluating visual stories, and to provide additional metric assessments to reveal the shortcomings of existing metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_9",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_9@1",
            "content": "The metric assessment experiments are conducted as the story-pair ranking task in which two stories are ranked based on their story quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_9",
            "start": 191,
            "end": 330,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_9@2",
            "content": "We observe three characteristics and design corresponding assessments to demonstrate Vrank's merits.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_9",
            "start": 332,
            "end": 431,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_9@3",
            "content": "First, larger rank differences in story quality are easier for people to differentiate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_9",
            "start": 433,
            "end": 519,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_9@4",
            "content": "We measure the performance of metrics in story pairs with large gaps versus small gaps to determine whether all metrics have this property.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_9",
            "start": 521,
            "end": 659,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_9@5",
            "content": "Our assessment indicates this property is exclusively hold by Vrank.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_9",
            "start": 661,
            "end": 728,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_9@6",
            "content": "Second, human-written stories are not always better than machine-written stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_9",
            "start": 730,
            "end": 810,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_9@7",
            "content": "Indeed, 38% of machine-generated stories are better than the references, which suggests that the afore-mentioned assumption may need to be revisited .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_9",
            "start": 812,
            "end": 961,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_9@8",
            "content": "We examine the ability of metrics to rank such human-machine pairs, which Vrank performs relatively well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_9",
            "start": 963,
            "end": 1067,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_9@9",
            "content": "Finally, most generated stories still contain many errors, which serve as signals for human rankers (Modi and Parde, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_9",
            "start": 1069,
            "end": 1191,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_9@10",
            "content": "Hence we evaluate the ability of metrics to detect errors and show that Vrank is a better indicator of errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_9",
            "start": 1193,
            "end": 1302,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_9@11",
            "content": "Also, we show that Vrank is able to generalize to other datasets without bias to VHED.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_9",
            "start": 1304,
            "end": 1389,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_9@12",
            "content": "In conclusion, Vrank excels in the above assessments and able to follow human behaviors in ranking.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_9",
            "start": 1391,
            "end": 1489,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_9@13",
            "content": "Moreover, Vrank can rank machine and human stories decently and is better at detecting story errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_9",
            "start": 1491,
            "end": 1590,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_9@14",
            "content": "Specifically, we make three major contributions:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_9",
            "start": 1592,
            "end": 1639,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_10@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_10",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_11@0",
            "content": "Visual Storytelling (VIST) Visual storytelling was introduced by Huang et al. (2016) as the task of generating a coherent story given five images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_11",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_11@1",
            "content": "They provided a dataset, Sequential Images Narrative Dataset (SIND), containing images and references in which references are human-written short stories describing images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_11",
            "start": 147,
            "end": 318,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_11@2",
            "content": "For every image prompt (one sequence of photos), there are 2 to 5 references.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_11",
            "start": 320,
            "end": 396,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_11@3",
            "content": "VIST requires deeper understanding of the photo events to prevent descriptions in isolation (i.e., image captions).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_11",
            "start": 398,
            "end": 512,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_11@4",
            "content": "Researchers have proposed various methods for this task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_11",
            "start": 514,
            "end": 569,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_11@5",
            "content": "Knowledge graphs are often integrated in models to encourage diversity of terms and plots in the stories (Hsu et al., 2020(Hsu et al., , 2021a. Some studies use reinforcement learning to reward models that generate stories that contain fewer errors and are more topically-focused Hu et al., 2020a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_11",
            "start": 571,
            "end": 868,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_11@6",
            "content": "However, existing evaluation methods are unable to capture the true quality of the generated stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_11",
            "start": 870,
            "end": 970,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_11@7",
            "content": "Thus we examine automatic metrics to devise a better way for machines to evaluate stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_11",
            "start": 972,
            "end": 1061,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_12@0",
            "content": "VIST-Human Evaluation Several VIST generation models use human evaluation to evaluate model performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_12",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_12@1",
            "content": "Recent studies apply aspectbased rating evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_12",
            "start": 105,
            "end": 155,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_12@2",
            "content": "Hu et al. (2020b) and Wang et al. (2020b) ask workers to rate stories based on pre-defined aspects.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_12",
            "start": 157,
            "end": 255,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_12@3",
            "content": "2 However, it is difficult to normalize these aspects as the definition of aspect varies from paper to paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_12",
            "start": 257,
            "end": 365,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_12@4",
            "content": "Also, these aspects are not mutually independent, making it difficult to analyze results based on these ratings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_12",
            "start": 367,
            "end": 478,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_12@5",
            "content": "Therefore, we consider the ranking method as it is commonly used (Hsu et al., 2020;Wang et al., 2020b;Hsu et al., 2021a) Automatic Metrics Automatic evaluation metrics are widely used in language generation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_12",
            "start": 480,
            "end": 692,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_12@6",
            "content": "Most reference-based metrics (e.g., BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004)) evaluate the ngram similarity between a generated text and the reference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_12",
            "start": 694,
            "end": 890,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_12@7",
            "content": "However, referenced metrics correlate poorly with human judgment (Wang et al., 2018b;Hsu et al., 2019;Modi and Parde, 2019) in dialog generation and story generation tasks: the generated text is given unreasonable scores due to incongruity with the reference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_12",
            "start": 892,
            "end": 1150,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_12@8",
            "content": "To account for this, several reference-free metrics (Sinha et al., 2020;Guan and Huang, 2020) have been designed to measure generated texts without any reference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_12",
            "start": 1152,
            "end": 1313,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_12@9",
            "content": "BERT-Score (Zhang et al., 2020), for instance, uses contextual embedding to calculate the similarity between candidates and references, and BLEURT (Sellam et al., 2020) ) uses pre-defined negative samples to train a model in an attempt to provide a metric that specializes in story generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_12",
            "start": 1315,
            "end": 1607,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_12@10",
            "content": "In our analysis, current metrics remain unable to mimic human judgment to discern quality differences in story pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_12",
            "start": 1609,
            "end": 1725,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_13@0",
            "content": "VHED",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_13",
            "start": 0,
            "end": 3,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_14@0",
            "content": "Dataset Description",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_14",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_15@0",
            "content": "The VHED dataset is a collection of human evaluation results from three VIST studies: KG-Story (Hsu et al., 2020), PR-VIST (Hsu et al., 2021a), and Stretch-VST (Hsu et al., 2021b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_15",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_15@1",
            "content": "All papers followed Hsu et al. (2020)'s human evaluation method using Amazon Mechanical Turk.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_15",
            "start": 181,
            "end": 273,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_15@2",
            "content": "For each task, the workers were to rank the story by overall quality, from the best story to the worst story.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_15",
            "start": 275,
            "end": 383,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_15@3",
            "content": "Specifically, each task displayed N stories, and each worker ranked each story from 1 to N .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_15",
            "start": 385,
            "end": 476,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_15@4",
            "content": "Details about each paper are listed in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_15",
            "start": 478,
            "end": 524,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_16@0",
            "content": "The construction of VHED is shown in Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_16",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_16@1",
            "content": "Collected from the aforementioned papers, we obtained 4,500 task results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_16",
            "start": 47,
            "end": 119,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_16@2",
            "content": "Further, we grouped N stories into story pairs, where the number of story pairs per task is C N 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_16",
            "start": 121,
            "end": 219,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_16@3",
            "content": "The resulting story pairs (x 1 , x 2 ) are either two machine-generated stories from two different models or one reference and one machine-generated story.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_16",
            "start": 221,
            "end": 375,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_16@4",
            "content": "For each story pair, there are five attributes:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_16",
            "start": 377,
            "end": 423,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_17@0",
            "content": "\u2022 Stories: A story pair consists of a better-ranked story and worse-ranked story. The story pair is either a reference with a machine-generated story, or two machine-generated stories. \u2022 Image Sequence IDs: A list of IDs for each of the five images from the SIND dataset (Huang et al., 2016). \u2022 Average Rank: The average of the five workers' story rankings is divided by N for normalization. N varies from paper to paper (Table 1). (Hsu et al., 2020), PRVIST (Hsu et al., 2021a), and Stretch-VST (Hsu et al., 2021b) ranking of x 2 . The ranking gap distribution is shown in the appendix (Table 6). \u2022 Human Agreement: Human agreement is when k workers agree that the better-ranked stories are better than the worse-ranked stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_17",
            "start": 0,
            "end": 728,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_18@0",
            "content": "Note that human agreement = 2 is equivalent to human agreement = 3, because 1 person agreeing that story A is better than B is equivalent to 4 people agreeing that story B is better than A. Therefore, we kept human agreements = 3,4,5 for simple notation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_18",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_19@0",
            "content": "For quality control, we remove story pairs with zero ranking gap.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_19",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_19@1",
            "content": "This yields 13,875 story pairs in total.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_19",
            "start": 66,
            "end": 105,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_19@2",
            "content": "The train-test-validation sets were split at a ratio of 8:1:1 to 11,208, 1,351, and 1,316 story pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_19",
            "start": 107,
            "end": 208,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_19@3",
            "content": "The descriptions of VIST models' generated stories are included in the appendix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_19",
            "start": 210,
            "end": 289,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_20@0",
            "content": "Data Analysis and Findings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_20",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_21@0",
            "content": "As we acquired data about human preferences in story pairs, we conducted analyses to understand the potential patterns for workers when assigning story ranks, the quality gap between machinegenerated and human-written stories, and the errors in the stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_21",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_21@1",
            "content": "The results of this observation are crucial for assessing the performance of a metric.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_21",
            "start": 258,
            "end": 343,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_22@0",
            "content": "Worker Ranking Analysis Story pairs are grouped by the same human agreement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_22",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_22@1",
            "content": "\u2126 k denotes a sub-dataset containing story pairs with human agreement = k. In Table 2, we calculate the number of story pairs as well as the averaged ranking gap of each sub-dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_22",
            "start": 77,
            "end": 258,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_22@2",
            "content": "For story pairs, we note that story pairs with k = 3 account for 53% of the dataset, meaning that half of the tasks have inconsistent annotations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_22",
            "start": 260,
            "end": 405,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_22@3",
            "content": "Regardless, this paper evaluates the story pairs with k \u2265 4 to filter out inconsistent human annotations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_22",
            "start": 407,
            "end": 511,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_22@4",
            "content": "We also note that the ranking gap increases as human agreement increases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_22",
            "start": 513,
            "end": 585,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_22@5",
            "content": "The ranking gap indicates the quality difference between a better-ranked and a worseranked story.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_22",
            "start": 587,
            "end": 683,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_22@6",
            "content": "That is, the difference between a ranked 1 story and a ranked 5 story should be larger than that between a ranked 2 story and a ranked 3 story.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_22",
            "start": 685,
            "end": 827,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_22@7",
            "content": "From Table 2, we find that story-pairs with lower agreement are closer in ranking.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_22",
            "start": 829,
            "end": 910,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_22@8",
            "content": "In other words, a story pair with a marginal quality difference easily leads to inconsistent worker annotations, because it is harder to rank two similar stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_22",
            "start": 912,
            "end": 1073,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_22@9",
            "content": "Essentially, we expect the metrics to exhibit similar behavior: the larger the ranking gap, the easier it is to rank.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_22",
            "start": 1075,
            "end": 1191,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_23@0",
            "content": "Who Wins?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_23",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_23@1",
            "content": "Machine vs. Human Stories Next we revisit the assertion that references are always superior.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_23",
            "start": 10,
            "end": 101,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_23@2",
            "content": "We select story pairs with a reference and a machine-generated story.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_23",
            "start": 103,
            "end": 171,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_23@3",
            "content": "We analyze the number and percentage of references that are ranked better than the generated stories on three human agreements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_23",
            "start": 173,
            "end": 299,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_23@4",
            "content": "From Table 2, we observe that when more humans agree on the ranking results, the percentage of the reference being better also increases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_23",
            "start": 301,
            "end": 437,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_23@5",
            "content": "In addition, further analysis shows that, on average, 38% of the machine-generated stories are in fact better than the references, showing that references are not always better than machine-generated stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_23",
            "start": 439,
            "end": 646,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_24@0",
            "content": "Error Analysis To understand the difference between better-and worse-ranked stories, deeper analysis into the story content is necessary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_24",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_24@1",
            "content": "We randomly sampled 200 stories from VHED (67 human and 134 machine generated) and manually labeled the stories according to the following error aspects:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_24",
            "start": 138,
            "end": 290,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_25@0",
            "content": "\u2022 Grammatical error (Gram): Erroneous usage of past/current tense and mistakes in misplaced modifiers. \u2022 Event mismatch (Event): Stories that are offtopic, which present events that are not relevant to the image stream. \u2022 Object mismatch (Obj): Irrelevant nouns that do not appear in the images and are not semantically related.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_25",
            "start": 0,
            "end": 327,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_26@0",
            "content": "We first labeled stories based on all 11 error aspects defined in (Modi and Parde, 2019) and we select the most occurring errors, which are grammar, repetition, description in isolation, and absurdity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_26",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_26@1",
            "content": "These four error aspects focus primarily on story coherence and within-story consistency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_26",
            "start": 202,
            "end": 290,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_26@2",
            "content": "However, visual storytelling requires generated stories to fit the given story images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_26",
            "start": 292,
            "end": 377,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_26@3",
            "content": "Rohrbach et al. (2019) show that humans are aware of the correctness of image descriptions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_26",
            "start": 379,
            "end": 469,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_26@4",
            "content": "Also, Wang et al. (2020a) show that mismatched events in stories can lead to poor story quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_26",
            "start": 471,
            "end": 566,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_26@5",
            "content": "Therefore, we added event and object mismatch into our analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_26",
            "start": 568,
            "end": 631,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_26@6",
            "content": "The error examples and correlation between the error are illustrated in the appendix (Table 9 and Figure 5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_26",
            "start": 633,
            "end": 740,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_26@7",
            "content": "From our observation, 79.8% of the sampled machine generated stories contained at least one of the errors in the categories, meaning most VIST models are unable to generate perfect stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_26",
            "start": 742,
            "end": 930,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_26@8",
            "content": "In Table 3, the high percentage of object and event mismatch errors also show that current VIST models do not capture visual groundings accurately.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_26",
            "start": 932,
            "end": 1078,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_26@9",
            "content": "This can lead to humans assigning higher scores to human-written stories since they are most likely to be relevant to the given images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_26",
            "start": 1080,
            "end": 1214,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_26@10",
            "content": "Grammatical errors and absurdities are also common in generated text, which can lead to ambiguous stories that humans are unable to comprehend.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_26",
            "start": 1216,
            "end": 1358,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_26@11",
            "content": "The prevalence of errors makes it essential for evaluation metrics to automatically detect these errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_26",
            "start": 1360,
            "end": 1463,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_27@0",
            "content": "Vrank",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_27",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_28@0",
            "content": "We propose Vrank, a reference-free automatic metric that inputs story pairs to predict human preferences between the two stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_28",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_28@1",
            "content": "We utilize SIM-CSE (Gao et al., 2021) to leverage better sentence representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_28",
            "start": 130,
            "end": 211,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_28@2",
            "content": "SIMCSE uses contrastive learning with dropout as augmentation, then trained on natural langauge inference datasets to obtain better sentence embeddings from BERT (Devlin et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_28",
            "start": 213,
            "end": 396,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_28@3",
            "content": "First, we pre-trained the SIMCSE model using SIND reference stories with the Masked Language Model objective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_28",
            "start": 398,
            "end": 506,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_28@4",
            "content": "Then, we input two stories with a [SEP] token in between through the pre-trained model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_28",
            "start": 508,
            "end": 594,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_28@5",
            "content": "We use the acquired sentence embeddings and feed it through a regression layer to predict a ranking gap.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_28",
            "start": 596,
            "end": 699,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_28@6",
            "content": "We used mean squared error to calculate the loss between the predicted ranking gap and true ranking gap. After obtaining the ranking gap, we predict which story is better according to the sign of the predicted ranking gap.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_28",
            "start": 701,
            "end": 922,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_28@7",
            "content": "Although Vrank is a simple model fine-tuned solely on human judgment, it still outperforms current existing metrics in our assessments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_28",
            "start": 924,
            "end": 1058,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_28@8",
            "content": "This suggests further potential for use with VHED; more studies can be conducted to replace Vrank with stronger neural network models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_28",
            "start": 1060,
            "end": 1193,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_29@0",
            "content": "During model training, since the number of positives and negatives were not balanced in the original dataset, we augmented the data to create a symmetric dataset of VHED to minimize dataset bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_29",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_29@1",
            "content": "3 The ranking gap in the resulting dataset was close to normally distributed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_29",
            "start": 196,
            "end": 272,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_29@2",
            "content": "We hypothesize that utilizing this feature makes it possible to extract more information, making it easier for the model to learn human judgment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_29",
            "start": 274,
            "end": 418,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_29@3",
            "content": "However, due to the small amount of data available, high variance is likely (Mosbach et al., 2020) to occur during inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_29",
            "start": 420,
            "end": 544,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_29@4",
            "content": "Hence, we used all data from VHED, including human agreement=3 to increase the stability of our model following Mosbach et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_29",
            "start": 546,
            "end": 679,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_30@0",
            "content": "Metric Assessment",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_30",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_31@0",
            "content": "In this section, we describe a series of assessments conducted on existing metrics on VHED, in which the assessment methods are based on the analyses in VHED.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_31",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_31@1",
            "content": "The objective is to examine whether Vrank is superior to other metrics based on our analysis of VHED.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_31",
            "start": 159,
            "end": 259,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_32@0",
            "content": "Experimental Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_32",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_33@0",
            "content": "Story-Pair Ranking A recent study (Wei and Jia, 2021) illustrates that pairwise accuracy reflects metric performance better than using correlation with human evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_33",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_33@1",
            "content": "Hence, we propose simple story-pair ranking to evaluate automatic evaluation metrics for visual storytelling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_33",
            "start": 170,
            "end": 278,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_33@2",
            "content": "The task is to determine the correct ranking order of the stories in a story pair based on the story quality scores predicted by the automatic evaluation metrics being assessed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_33",
            "start": 280,
            "end": 456,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_33@3",
            "content": "Given the story pair (x 1 , x 2 ), the automatic metric being assessed predicts the corresponding story quality scores (s 1 , s 2 ) which we compare to the averaged ranks y 1 and y 2 of x 1 and x 1 from human evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_33",
            "start": 458,
            "end": 677,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_33@4",
            "content": "The performance of the evaluation metric on the i-th story pair is formulated as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_33",
            "start": 679,
            "end": 758,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_34@0",
            "content": "ranking_acc i = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1, if s 1 > s 2 and y 1 < y 2 1, if s 1 < s 2 and y 1 > y 2 0, otherwise,(1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_34",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_35@0",
            "content": ") where ranking_acc i = 1 indicates correct (incorrect) prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_35",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_35@1",
            "content": "Note that low scores indicate high rank.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_35",
            "start": 68,
            "end": 107,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_35@2",
            "content": "The overall metric performance over M story pairs is defined as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_35",
            "start": 109,
            "end": 172,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_36@0",
            "content": "avg_ranking_acc = 1 M M i=1 ranking_acc i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_36",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_37@0",
            "content": "(",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_37",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_38@0",
            "content": ")2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_38",
            "start": 0,
            "end": 1,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_39@0",
            "content": "Datasets In addition to VHED, we also collected VIST-Edit 4 (Hsu et al., 2019) for story-pair ranking.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_39",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_39@1",
            "content": "VIST-Edit includes 2,981 visual stories generated by AREL (Wang et al., 2018a) and GLAC (Kim et al., 2018), and 14,905 human-edited visual stories, that is, AREL and GLAC-generated stories edited by workers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_39",
            "start": 103,
            "end": 309,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_39@2",
            "content": "Their paper shows that the crowd workers' edits systematically increased the lexical diversity of the stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_39",
            "start": 311,
            "end": 420,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_39@3",
            "content": "Since the purpose of the editing was to improve the machine-4 VIST-Edit: https://github.com/tingyaohsu/ VIST-Edit generated stories, we paired up human-edited stories and machine-generated stories as better-ranked and worse-ranked samples (labeled as 1 and 2), resulting in 14,905 story pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_39",
            "start": 422,
            "end": 714,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_39@4",
            "content": "Comparing VHED to VIST-Edit, VHED contains reference and multiple models' generated stories, but VIST-Edit has only human-machine story pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_39",
            "start": 716,
            "end": 857,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_39@5",
            "content": "Additionally, VIST-Edit is not in Vrank's training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_39",
            "start": 859,
            "end": 914,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_39@6",
            "content": "VIST-Edit is utilized only for metric performance reports, serving as an unseen dataset for Vrank.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_39",
            "start": 916,
            "end": 1013,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_40@0",
            "content": "We first consider traditional n-gram-based reference-based metrics, BLEU, ROUGE-L, METEOR, and Sacre-BLEU (Keenan, 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_40",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_40@1",
            "content": "We also implement the more recent BERT-Score, BLEURT, and UNION as baseline metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_40",
            "start": 122,
            "end": 205,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_40@2",
            "content": "In addition to the above automatic metrics, we also include a random baseline, denoted as Random in Table 4, to provide a random score for each story as the lower bound.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_40",
            "start": 207,
            "end": 375,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_41@0",
            "content": "A common practice for reference-based metrics: a candidate story is scored against each reference r j in a gold reference set R = {r i } n i=1 ; the highest score was used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_41",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_41@1",
            "content": "However, applying this method on a reference-machine story pair would always result in reference having a full score, because of the exact match between reference and the gold reference set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_41",
            "start": 173,
            "end": 362,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_41@2",
            "content": "To ensure a fair evaluation and avoid meaningless matching, we first check that the gold references do not include the reference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_41",
            "start": 364,
            "end": 492,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_41@3",
            "content": "To this end, we propose the Reference Absent Algorithm for evaluating story pairs containing the reference story (or stories) as in Eq. 3, which removes the r j from R when any of the candidate stories in a story pair (x = {x 1 , x 2 }) is identical to r j .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_41",
            "start": 494,
            "end": 751,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_42@0",
            "content": "s j = max(metric(x j , R \u2212 x)), j = {1, 2}, (3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_42",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_43@0",
            "content": "where metric(\u2022) can be any reference-based metric and s j is the story quality scores for the j-th story in a story pair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_43",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_43@1",
            "content": "This algorithm only applies when evaluating story pairs containing references, i.e., reference-machine pairs in this paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_43",
            "start": 122,
            "end": 244,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_44@0",
            "content": "Results and Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_44",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_45@0",
            "content": "Pairwise Story Evaluation Accuracy: Metric's ability to determine the correct ranking order in story pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_45",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_45@1",
            "content": "The average ranking accuracy of each automatic metric on VHED and VIST-Edit are presented in Table 4 (left).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_45",
            "start": 108,
            "end": 215,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_45@2",
            "content": "Around 50% corresponds to random guessing, as shown as Random in the table.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_45",
            "start": 217,
            "end": 291,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_45@3",
            "content": "Vrank shows superior performance in VHED and VIST-Edit, which VIST-Edit is the unknown dataset to Vrank.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_45",
            "start": 293,
            "end": 396,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_45@4",
            "content": "High performance on VIST-Edit and VHED indicates Vrank has the ability to distinguish diverse story pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_45",
            "start": 398,
            "end": 503,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_45@5",
            "content": "In contrast, we observe unexpectedly low performance for most baseline metrics, as they perform no better than the Random baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_45",
            "start": 505,
            "end": 635,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_45@6",
            "content": "BLEU-4 especially struggles to rank the stories in both datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_45",
            "start": 637,
            "end": 701,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_45@7",
            "content": "Further analysis suggests that BLEU-4 marked \u223c80% of the stories as 0, and Equation 1 coincidentally treated them as incorrect prediction because it discourages ties.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_45",
            "start": 703,
            "end": 868,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_45@8",
            "content": "BLEURT, in turn, also performed poorly because it relies on reference-based metrics as signals for training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_45",
            "start": 870,
            "end": 977,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_45@9",
            "content": "Reference-free metrics, especially UNION, perform well on VIST-Edit.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_45",
            "start": 979,
            "end": 1046,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_45@10",
            "content": "However, its design is not generalizeable to VHED.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_45",
            "start": 1048,
            "end": 1097,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_46@0",
            "content": "Worker Ranking Behavior on Metrics: The larger the ranking gap, the easier is it to rank.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_46",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_46@1",
            "content": "The ranking gap is the difference between a betterranked and worse-ranked sample's average ranks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_46",
            "start": 90,
            "end": 186,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_46@2",
            "content": "VHED is categorized into four sub-datasets with different ranking gaps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_46",
            "start": 188,
            "end": 258,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_46@3",
            "content": "This assessment tests each metrics' ability to mimic worker ranking behavior observed in the analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_46",
            "start": 260,
            "end": 361,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_46@4",
            "content": "Story pairs with larger gaps suggest stronger linguistic differences and are likely easier to rank, whereas those with smaller gaps are likely more difficult.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_46",
            "start": 363,
            "end": 520,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_46@5",
            "content": "In Fig. 3, all baseline automatic metrics, including metrics not reported in the figure, show randomly distributed scores, most of which remain around 50%, thus failing to exhibit such behavior.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_46",
            "start": 522,
            "end": 715,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_46@6",
            "content": "On the contrary, Vrank yields an ideal decrease.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_46",
            "start": 717,
            "end": 764,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_46@7",
            "content": "Starting with ranking gaps over 0.3, the accuracy reaches \u223c0.85 and a gradual decrease afterward.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_46",
            "start": 766,
            "end": 862,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_46@8",
            "content": "We believe such behavior reveals Vrank to be a more preferable metric for visual story evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_46",
            "start": 864,
            "end": 961,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_47@0",
            "content": "Machine and Human on Metrics: Machines are sometimes better than humans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_47",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_47@1",
            "content": "Two aspects are studied in this section.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_47",
            "start": 73,
            "end": 112,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_47@2",
            "content": "First, we evaluate the ability of Vrank and reference-based metrics to rank reference-machine (R&M) pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_47",
            "start": 114,
            "end": 219,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_47@3",
            "content": "Although some machine texts have progressed to humanlevel, to our knowledge, there has been little investigation of metrics' ability to evaluate references and machines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_47",
            "start": 221,
            "end": 389,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_47@4",
            "content": "We apply reference-based metrics with Eq. 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_47",
            "start": 391,
            "end": 434,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_47@5",
            "content": "This results in poor performance for reference-based metrics as shown in R&M in Table 4 5 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_47",
            "start": 436,
            "end": 526,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_47@6",
            "content": "An explanation is that since the reference is removed from the reference set by Eq. 3, the reference needs to match with the remaining references in the reference set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_47",
            "start": 528,
            "end": 694,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_47@7",
            "content": "Although most references are on topic, the stories are highly diverse (Zhu et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_47",
            "start": 696,
            "end": 784,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_47@8",
            "content": "These metrics are unable to calculate the similarity to semantic levels; thus, they result in poor performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_47",
            "start": 786,
            "end": 896,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_47@9",
            "content": "On the contrary, Vrank is a deep learning model, trained on VHED and thus learned to rate based on story quality rather than similarity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_47",
            "start": 898,
            "end": 1033,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_47@10",
            "content": "We also find that Vrank ranks correctly when machine is better than reference, showing that Vrank yields 26.5% recall when the other metrics have 0 recall without Eq. 3 and \u223c18% with Eq. 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_47",
            "start": 1035,
            "end": 1223,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_48@0",
            "content": "Second, we observe the performance of metrics on M&M (machine-machine pairs).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_48",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_48@1",
            "content": "M&M ranking gaps are smaller than those of R&M pairs (0.18 v.s. 0.21), making them harder to rank because their story qualities are closer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_48",
            "start": 78,
            "end": 216,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_48@2",
            "content": "However, Vrank still shows promising performance when ranking Table 5: This table shows the correlation of human rankings, automatic metric scores with the corresponding error categories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_48",
            "start": 218,
            "end": 404,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_48@3",
            "content": "An ideal correlation should be closer to Human.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_48",
            "start": 406,
            "end": 452,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_48@4",
            "content": "Negative correlation illustrates that higher rankings (average ranking closer to 1) co-occur with few errors in the story.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_48",
            "start": 454,
            "end": 575,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_48@5",
            "content": "Hence, an high error detection rate is a correlation coefficient closer to -1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_48",
            "start": 577,
            "end": 654,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_49@0",
            "content": "such story pairs, outperforming existing metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_49",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_50@0",
            "content": "Errors in Metrics: Metric's ability to detect errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_50",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_50@1",
            "content": "Current generated stories often contain errors which prompt human evaluators to assign lower scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_50",
            "start": 54,
            "end": 153,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_50@2",
            "content": "It is crucial for automatic metrics to also recognize errors to judge generated text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_50",
            "start": 155,
            "end": 239,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_51@0",
            "content": "To do this, we adapted the point-biserial correlation coefficient to analyze the correlation between binary annotated errors and metric scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_51",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_52@0",
            "content": "The correlation between metrics and errors is presented in Table 5: existing metrics are not able to detect errors as the correlation coefficients are low.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_52",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_52@1",
            "content": "From the correlation coefficients between the human ranking score and each error aspect, we observe that human evaluation for stories may be influenced by error aspects, especially absurdity and description in isolation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_52",
            "start": 156,
            "end": 375,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_52@2",
            "content": "In general, Vrank performs best in detecting absurdity and description in isolation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_52",
            "start": 377,
            "end": 460,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_52@3",
            "content": "UNION-WP performs best in correlation with repetition, which is reasonable since UNION is trained to discriminate erroneous stories that are repetitive in structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_52",
            "start": 462,
            "end": 626,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_52@4",
            "content": "In summary, current metrics remain unable to detect errors to evaluate coherency efficiently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_52",
            "start": 628,
            "end": 720,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_52@5",
            "content": "Metrics ability to detect errors may give clearer indications of the quality of generated texts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_52",
            "start": 722,
            "end": 817,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_53@0",
            "content": "Dataset Generalization",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_53",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_54@0",
            "content": "In addition to VIST, we expect Vrank to reasonably evaluate the quality of text as well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_54",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_54@1",
            "content": "To determine whether Vrank generalizes to textual stories, we selected MANS dataset (Guan et al., 2021), an imagefree storytelling dataset in which the stories are derived from the ROCStories corpus .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_54",
            "start": 89,
            "end": 288,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_54@2",
            "content": "MANS includes 200 story prompts, where each prompt includes five model-generated stories and a reference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_54",
            "start": 290,
            "end": 394,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_54@3",
            "content": "However, it does not contain human story rankings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_54",
            "start": 396,
            "end": 445,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_54@4",
            "content": "Thus, for each story prompt, we asked five workers from Amazon Mechanical Turk to rank the five stories to obtain ranking scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_54",
            "start": 447,
            "end": 575,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_54@5",
            "content": "Following the VHED construction procedure, the ranked stories were converted into story pairs, making for 1,112 story pairs for which 3 workers agreed on the ranking, 605 story pairs for which 4 workers agreed, and 132 story pairs for which 5 workers agreed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_54",
            "start": 577,
            "end": 834,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_54@6",
            "content": "Likewise, we evaluate story pairs with k \u2265 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_54",
            "start": 836,
            "end": 880,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_55@0",
            "content": "Subset \u2126 4 \u2126 5 \u2126 {4,5}",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_55",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_56@0",
            "content": "The results of Vrank and the baseline automatic metrics when ranking MANS are shown in Table 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_56",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_56@1",
            "content": "We find that Vrank outperforms baseline metrics in story pairs with k \u2265 4, whereas the latter still show limited abilities to rank the MANS dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_56",
            "start": 96,
            "end": 243,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_56@2",
            "content": "In general, the accuracy of automatic evaluation on MANS is lower than that on VHED.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_56",
            "start": 245,
            "end": 328,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_56@3",
            "content": "This may be due to the comparably unconstrained writing styles of pure textual stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_56",
            "start": 330,
            "end": 416,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_56@4",
            "content": "An example of the evaluation on stories is given in the appendix (Table 8).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_56",
            "start": 418,
            "end": 492,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_57@0",
            "content": "Conclusion and Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_57",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_58@0",
            "content": "We present VHED and Vrank, the first dataset of human evaluation results and evaluation metric for VIST.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_58",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_58@1",
            "content": "We show that Vrank performs significantly better in three assessment tasks and generalizes to other datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_58",
            "start": 105,
            "end": 213,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_58@2",
            "content": "Also, recent automatic metrics are ill-suited to evaluating visual stories, especially human-level written stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_58",
            "start": 215,
            "end": 329,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_58@3",
            "content": "We welcome researchers to share their human evaluation results to the community to broaden the data domain to obtain more knowledge about human judgment and improve the performance of Vrank.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_58",
            "start": 331,
            "end": 520,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_58@4",
            "content": "As the gap between machines and humans continues to decrease, stronger metrics will be needed to evaluate machine and human stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_58",
            "start": 522,
            "end": 653,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_58@5",
            "content": "Improving Vrank performance to replace reference-based metrics is our future goal.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_58",
            "start": 655,
            "end": 736,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_59@0",
            "content": "Appendix",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_59",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_60@0",
            "content": "Application In this section, we introduce an application for Vrank and other reference-free metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_60",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_60@1",
            "content": "Our assessment indicates that Vrank's predictions strongly agree with human judgment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_60",
            "start": 101,
            "end": 185,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_60@2",
            "content": "We quantify the distance between humans and machines by pairing up reference and generated stories and calculating the ratio of generated stories that outmatch the references.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_60",
            "start": 187,
            "end": 361,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_60@3",
            "content": "Unlike human evaluation, which can be conducted only on a portion of the testing data, this method allows researchers to evaluate the proposed model over the entire testing dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_60",
            "start": 363,
            "end": 543,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_61@0",
            "content": "After applying Vrank to assess five recent VIST models, we present the results in Figure 4: the models are gradually approaching human-level writing, outlining an exciting development of NLG in VIST.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_61",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_61@1",
            "content": "Error Type Examples and Correlation In Table 9, we show examples of error types mentioned in our error analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_61",
            "start": 200,
            "end": 311,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_61@2",
            "content": "We also show the correlation between different error types in Figure 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_61",
            "start": 313,
            "end": 383,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_61@3",
            "content": "As the error types are mutually independent, there is the potential to construct tools to automatically detect each error, since they do not overlap with each other.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_61",
            "start": 385,
            "end": 549,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_61@4",
            "content": "Ranking Gap Distribution The ranking gap distribution is shown in Figure 6, in which both the ranking gaps and the number of stories are normalized.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_61",
            "start": 551,
            "end": 698,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_61@5",
            "content": "Also, since the ranking gaps contain both negative and positive values, we took the absolute value of the gap for the histogram.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_61",
            "start": 700,
            "end": 827,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_61@6",
            "content": "We observe that the machine-machine pairs are centered closer to zero.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_61",
            "start": 829,
            "end": 898,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_61@7",
            "content": "However, the human-machine pairs are distributed more evenly than the M&M pairs, which indicates that human-machine pairs are easier to distinguish than machine-machine pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_61",
            "start": 900,
            "end": 1074,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_62@0",
            "content": "Without Reference Absent Algorithm Here, we show the results of automatic metric accuracy in story-pair ranking without the proposed Reference Absent Algorithm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_62",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_62@1",
            "content": "As expected, the accuracies for H&M pairs are the same.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_62",
            "start": 161,
            "end": 215,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_62@2",
            "content": "Since all references are regarded as ground truth for reference-based automatic metrics, the accuracy is shown as the percentage of the human-written stories that are better than machine-generated stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_62",
            "start": 217,
            "end": 421,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_62@3",
            "content": "Hence, these metrics are unable to identify any machine-generated stories that are better than human-written stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_62",
            "start": 423,
            "end": 539,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_62@4",
            "content": "This demonstrates the importance of our proposed algorithm in the experiment results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_62",
            "start": 541,
            "end": 625,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_63@0",
            "content": "We sampled 250 to 500 image prompts from SIND's testing dataset and hired crowd workers from Amazon Mechanical Turk to evaluate the visual stories that were generated based of these image prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_63",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_63@1",
            "content": "The workers were adult workers in the US with 98% assignments approved and who had completed at least 3,000 HITs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_63",
            "start": 197,
            "end": 309,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_63@2",
            "content": "A user interface for workers to complete was called a task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_63",
            "start": 311,
            "end": 369,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_63@3",
            "content": "A task displayed one image prompt on the top with several stories at the bottom, and five workers were recruited to rank the stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_63",
            "start": 371,
            "end": 503,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_64@0",
            "content": "The stories usually included a reference, stories generated using the proposed model, and several baseline stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_64",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_64@1",
            "content": "The compensation was USD 0.10 per task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_64",
            "start": 116,
            "end": 154,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_65@0",
            "content": "Training Details We use the pre-trained base model from Huggingface (Wolf et al., 2020) and fine-tune it to our regression objective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_65",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_65@1",
            "content": "We utilized Adam as optimizer with learning rate 2e-5 and trained for 30 epochs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_65",
            "start": 134,
            "end": 213,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_65@2",
            "content": "The batch size is set as 32 and the random seed for training can be set as 7,777 for reproduction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_65",
            "start": 215,
            "end": 312,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_65@3",
            "content": "Checkpoints are stored for every 500 steps and we also utilized mixed precision training for more efficient training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_65",
            "start": 314,
            "end": 430,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_65@4",
            "content": "The environment of our operating system is Ubuntu 20.0.4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_65",
            "start": 432,
            "end": 488,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_66@0",
            "content": "Training was completed on two NVidia RTX 3090 GPUs, each of which contains 24 GB of memory.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_66",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_67@0",
            "content": "Model Design Before we came up with the final model using SIMCSE, we tried several settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_67",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_67@1",
            "content": "Formulating the task as a binary classification task didn't achieve good accuracy, we speculate that this is because the boundaries for a good and bad story is hard to find.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_67",
            "start": 93,
            "end": 265,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_67@2",
            "content": "Also, we tried to augment the story-pairs with agreement=5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_67",
            "start": 267,
            "end": 325,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_67@3",
            "content": "We found out that it didn't improve the performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_67",
            "start": 327,
            "end": 378,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_67@4",
            "content": "Moreover, we tested using CLIP (Radford et al., 2021) to extract image features for additional features and visionlanguage models also did not improve performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_67",
            "start": 380,
            "end": 542,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_67@5",
            "content": "Hence, we picked a simple model architecture to demonstrate our performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_67",
            "start": 544,
            "end": 619,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_68@0",
            "content": "\u2022 GLAC (Kim et al., 2018): combines global and local attention to construct image-dependent sentences. A context cascading mechanism is incorporated to improve story coherency. \u2022 AREL (Wang et al., 2018a): uses a policy model and reward model to associate reward learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_68",
            "start": 0,
            "end": 271,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_69@0",
            "content": "The policy model is used to generate stories, and the reward model learns from human demonstrations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_69",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_69@1",
            "content": "\u2022 KGStory (Hsu et al., 2020): a three-stage framework which distills a set of representive words from the input text and utilizes knowledge graphs to enrich the content.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_69",
            "start": 101,
            "end": 269,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_69@2",
            "content": "It generates stories from the enriched word set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_69",
            "start": 271,
            "end": 318,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_69@3",
            "content": "\u2022 PRVIST (Hsu et al., 2021a): a two-stage framework that finds an optimal path through the constructed story graph which forms the best storyline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_69",
            "start": 320,
            "end": 465,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_69@4",
            "content": "This path is then used to generate the story.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_69",
            "start": 467,
            "end": 511,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_69@5",
            "content": "\u2022 Stretch-VST (Hsu et al., 2021b): a modification of KGStory that produces more sentences in the story while maintaining quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_69",
            "start": 513,
            "end": 641,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_69@6",
            "content": "Appropriate knowledge added to the story results in a more detailed story.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_69",
            "start": 643,
            "end": 716,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_70@0",
            "content": "Satanjeev Banerjee, Alon Lavie, METEOR: An automatic metric for MT evaluation with improved correlation with human judgments, 2005, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_70",
            "start": 0,
            "end": 298,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_71@0",
            "content": "UNKNOWN, None, 2021, Commonsense knowledge aware concept selection for diverse and informative visual storytelling, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_71",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_72@0",
            "content": "Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, Noah Smith, All that's 'human' is not gold: Evaluating human evaluation of generated text, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_72",
            "start": 0,
            "end": 381,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_73@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_73",
            "start": 0,
            "end": 335,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_74@0",
            "content": "Tianyu Gao, Xingcheng Yao, Danqi Chen, SimCSE: Simple contrastive learning of sentence embeddings, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_74",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_75@0",
            "content": "Jian Guan, Minlie Huang, UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_75",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_76@0",
            "content": "Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, Minlie Huang, OpenMEVA: A benchmark for evaluating open-ended story generation metrics, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_76",
            "start": 0,
            "end": 388,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_77@0",
            "content": "Chao-Chun Hsu, Zi-Yuan Chen, Chi-Yang Hsu, Chih-Chia Li, Tzu-Yuan Lin, Ting-Hao ; Huang, Lun-Wei Ku, Knowledge-enriched visual storytelling, 2020, Proceedings of Thirty-Fourth AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_77",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_78@0",
            "content": "Chi-Yang Hsu, Yun-Wei Chu, Ting-Hao ; Kenneth, ) Huang, Lun-Wei Ku, Plot and rework: Modeling storylines for visual storytelling, 2021-08-01, Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_78",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_79@0",
            "content": "Chi-Yang Hsu, Yun-Wei Chu, Tsai-Lun Yang, Ting-Hao Huang, Lun-Wei Ku, Stretch-VST: Getting flexible with visual stories, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_79",
            "start": 0,
            "end": 355,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_80@0",
            "content": "Ting-Yao Hsu, Chieh-Yang Huang, Yen-Chia Hsu, Ting-Hao Huang, Visual story post-editing, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_80",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_81@0",
            "content": "Junjie Hu, Yu Cheng, Zhe Gan, Jingjing Liu, Jianfeng Gao, Graham Neubig, What makes a good story? designing composite rewards for visual storytelling, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_81",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_82@0",
            "content": "Junjie Hu, Yu Cheng, Zhe Gan, Jingjing Liu, Jianfeng Gao, Graham Neubig, What makes a good story? Designing composite rewards for visual storytelling, 2020, Thirty-Fourth AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_82",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_83@0",
            "content": "Qiuyuan Huang, Zhe Gan, Asli Celikyilmaz, Dapeng Wu, Hierarchically structured reinforcement learning for topically coherent visual story generation, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_83",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_84@0",
            "content": "Ting-Hao Kenneth Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, Visual storytelling, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_84",
            "start": 0,
            "end": 336,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_85@0",
            "content": "J William,  Keenan, Sacre Bleu: Faith, fashion and freedom: Marist foundation garments 1817-1862, 2017, Materializing Religion, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_85",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_86@0",
            "content": "UNKNOWN, None, 2018, GLAC Net: GLocal Attention Cascading Networks for multi-image cued story generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_86",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_87@0",
            "content": "Chin-Yew Lin, ROUGE: A package for automatic evaluation of summaries, 2004, Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_87",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_88@0",
            "content": "Yatri Modi, Natalie Parde, The steep road to happily ever after: An analysis of current visual storytelling models, 2019, Proceedings of the Second Workshop on Shortcomings in Vision and Language, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_88",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_89@0",
            "content": "UNKNOWN, None, 2006, On the stability of fine-tuning BERT: misconceptions, explanations, and strong baselines. CoRR, abs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_89",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_90@0",
            "content": "Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, James Allen, A corpus and cloze evaluation for deeper understanding of commonsense stories, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_90",
            "start": 0,
            "end": 356,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_91@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, BLEU: A method for automatic evaluation of machine translation, 2002, Proceedings of the 40th annual meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_91",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_92@0",
            "content": "Alec Radford, Jong Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Learning transferable visual models from natural language supervision, 2021, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_92",
            "start": 0,
            "end": 270,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_93@0",
            "content": "Anna Rohrbach, Lisa Hendricks, Kaylee Burns, Trevor Darrell, Kate Saenko, Object hallucination in image captioning, 2019, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_93",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_94@0",
            "content": "Thibault Sellam, Dipanjan Das, Ankur Parikh, BLEURT: Learning robust metrics for text generation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_94",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_95@0",
            "content": "Koustuv Sinha, Prasanna Parthasarathi, Jasmine Wang, Ryan Lowe, William Hamilton, Joelle Pineau, Learning an unreferenced metric for online dialogue evaluation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_95",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_96@0",
            "content": "C Ramakrishna Vedantam, Devi Zitnick,  Parikh, Cider: Consensus-based image description evaluation, 2015, IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_96",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_97@0",
            "content": "Jing Wang, Jianlong Fu, Jinhui Tang, Zechao Li, Tao Mei, Show, reward and tell: Automatic generation of narrative paragraph from photo stream by adversarial training, 2018, Thirty-Second AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_97",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_98@0",
            "content": "Ruize Wang, Zhongyu Wei, Ying Cheng, Piji Li, Haijun Shan, Ji Zhang, Qi Zhang, Xuanjing Huang, Keep it consistent: Topic-aware storytelling from an image stream via iterative multi-agent communication, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_98",
            "start": 0,
            "end": 287,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_99@0",
            "content": "UNKNOWN, None, 2020, Storytelling from an image stream using scene graphs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_99",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_100@0",
            "content": "Xin Wang, Wenhu Chen, Yuan-Fang Wang, William Wang, No metrics are perfect: Adversarial reward learning for visual storytelling, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_100",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_101@0",
            "content": "Johnny Wei, Robin Jia, The statistical advantage of automatic NLG metrics at the system level, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_101",
            "start": 0,
            "end": 314,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_102@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_102",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_103@0",
            "content": "Alexander Lhoest,  Rush, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_103",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_104@0",
            "content": "Tianyi Zhang, Varsha Kishore, Felix Wu, Q Kilian, Yoav Weinberger,  Artzi, BERTScore: Evaluating text generation with BERT, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_104",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "14-ARR_v2_105@0",
            "content": "Wanrong Zhu, Xin Wang, Pradyumna Narayana, Kazoo Sone, Sugato Basu, William Wang, Towards understanding sample variance in visually grounded language generation: Evaluations and observations, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v2_105",
            "start": 0,
            "end": 327,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "14-ARR_v2_0",
            "tgt_ix": "14-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_0",
            "tgt_ix": "14-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_1",
            "tgt_ix": "14-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_1",
            "tgt_ix": "14-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_1",
            "tgt_ix": "14-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_2",
            "tgt_ix": "14-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_0",
            "tgt_ix": "14-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_3",
            "tgt_ix": "14-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_5",
            "tgt_ix": "14-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_6",
            "tgt_ix": "14-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_7",
            "tgt_ix": "14-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_8",
            "tgt_ix": "14-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_4",
            "tgt_ix": "14-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_4",
            "tgt_ix": "14-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_4",
            "tgt_ix": "14-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_4",
            "tgt_ix": "14-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_4",
            "tgt_ix": "14-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_4",
            "tgt_ix": "14-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_0",
            "tgt_ix": "14-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_9",
            "tgt_ix": "14-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_11",
            "tgt_ix": "14-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_10",
            "tgt_ix": "14-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_10",
            "tgt_ix": "14-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_10",
            "tgt_ix": "14-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_0",
            "tgt_ix": "14-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_12",
            "tgt_ix": "14-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_13",
            "tgt_ix": "14-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_13",
            "tgt_ix": "14-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_15",
            "tgt_ix": "14-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_16",
            "tgt_ix": "14-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_18",
            "tgt_ix": "14-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_14",
            "tgt_ix": "14-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_14",
            "tgt_ix": "14-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_14",
            "tgt_ix": "14-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_14",
            "tgt_ix": "14-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_14",
            "tgt_ix": "14-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_14",
            "tgt_ix": "14-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_13",
            "tgt_ix": "14-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_19",
            "tgt_ix": "14-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_21",
            "tgt_ix": "14-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_22",
            "tgt_ix": "14-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_23",
            "tgt_ix": "14-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_24",
            "tgt_ix": "14-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_20",
            "tgt_ix": "14-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_20",
            "tgt_ix": "14-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_20",
            "tgt_ix": "14-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_20",
            "tgt_ix": "14-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_20",
            "tgt_ix": "14-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_20",
            "tgt_ix": "14-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_20",
            "tgt_ix": "14-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_0",
            "tgt_ix": "14-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_26",
            "tgt_ix": "14-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_28",
            "tgt_ix": "14-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_27",
            "tgt_ix": "14-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_27",
            "tgt_ix": "14-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_27",
            "tgt_ix": "14-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_0",
            "tgt_ix": "14-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_29",
            "tgt_ix": "14-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_30",
            "tgt_ix": "14-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_30",
            "tgt_ix": "14-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_30",
            "tgt_ix": "14-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_31",
            "tgt_ix": "14-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_33",
            "tgt_ix": "14-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_34",
            "tgt_ix": "14-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_35",
            "tgt_ix": "14-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_36",
            "tgt_ix": "14-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_37",
            "tgt_ix": "14-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_38",
            "tgt_ix": "14-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_32",
            "tgt_ix": "14-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_32",
            "tgt_ix": "14-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_32",
            "tgt_ix": "14-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_32",
            "tgt_ix": "14-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_32",
            "tgt_ix": "14-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_32",
            "tgt_ix": "14-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_32",
            "tgt_ix": "14-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_32",
            "tgt_ix": "14-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_40",
            "tgt_ix": "14-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_41",
            "tgt_ix": "14-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_42",
            "tgt_ix": "14-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_32",
            "tgt_ix": "14-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_32",
            "tgt_ix": "14-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_32",
            "tgt_ix": "14-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_32",
            "tgt_ix": "14-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_39",
            "tgt_ix": "14-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_30",
            "tgt_ix": "14-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_43",
            "tgt_ix": "14-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_45",
            "tgt_ix": "14-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_46",
            "tgt_ix": "14-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_47",
            "tgt_ix": "14-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_48",
            "tgt_ix": "14-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_49",
            "tgt_ix": "14-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_50",
            "tgt_ix": "14-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_51",
            "tgt_ix": "14-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_44",
            "tgt_ix": "14-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_44",
            "tgt_ix": "14-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_44",
            "tgt_ix": "14-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_44",
            "tgt_ix": "14-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_44",
            "tgt_ix": "14-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_44",
            "tgt_ix": "14-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_44",
            "tgt_ix": "14-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_44",
            "tgt_ix": "14-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_44",
            "tgt_ix": "14-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_0",
            "tgt_ix": "14-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_52",
            "tgt_ix": "14-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_53",
            "tgt_ix": "14-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_53",
            "tgt_ix": "14-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_55",
            "tgt_ix": "14-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_53",
            "tgt_ix": "14-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_53",
            "tgt_ix": "14-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_54",
            "tgt_ix": "14-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_0",
            "tgt_ix": "14-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_56",
            "tgt_ix": "14-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_57",
            "tgt_ix": "14-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_57",
            "tgt_ix": "14-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_0",
            "tgt_ix": "14-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_58",
            "tgt_ix": "14-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_60",
            "tgt_ix": "14-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_61",
            "tgt_ix": "14-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_59",
            "tgt_ix": "14-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_59",
            "tgt_ix": "14-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_59",
            "tgt_ix": "14-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_59",
            "tgt_ix": "14-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_63",
            "tgt_ix": "14-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_64",
            "tgt_ix": "14-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_65",
            "tgt_ix": "14-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_66",
            "tgt_ix": "14-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_59",
            "tgt_ix": "14-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_59",
            "tgt_ix": "14-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_59",
            "tgt_ix": "14-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_59",
            "tgt_ix": "14-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_59",
            "tgt_ix": "14-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_62",
            "tgt_ix": "14-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_59",
            "tgt_ix": "14-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_59",
            "tgt_ix": "14-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_67",
            "tgt_ix": "14-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v2_0",
            "tgt_ix": "14-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_1",
            "tgt_ix": "14-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_2",
            "tgt_ix": "14-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_2",
            "tgt_ix": "14-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_2",
            "tgt_ix": "14-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_2",
            "tgt_ix": "14-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_2",
            "tgt_ix": "14-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_2",
            "tgt_ix": "14-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_2",
            "tgt_ix": "14-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_2",
            "tgt_ix": "14-ARR_v2_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_2",
            "tgt_ix": "14-ARR_v2_2@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_2",
            "tgt_ix": "14-ARR_v2_2@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_2",
            "tgt_ix": "14-ARR_v2_2@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_2",
            "tgt_ix": "14-ARR_v2_2@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_2",
            "tgt_ix": "14-ARR_v2_2@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_3",
            "tgt_ix": "14-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_3",
            "tgt_ix": "14-ARR_v2_3@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_3",
            "tgt_ix": "14-ARR_v2_3@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_3",
            "tgt_ix": "14-ARR_v2_3@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_3",
            "tgt_ix": "14-ARR_v2_3@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_4",
            "tgt_ix": "14-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_5",
            "tgt_ix": "14-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_5",
            "tgt_ix": "14-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_5",
            "tgt_ix": "14-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_5",
            "tgt_ix": "14-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_5",
            "tgt_ix": "14-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_5",
            "tgt_ix": "14-ARR_v2_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_5",
            "tgt_ix": "14-ARR_v2_5@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_6",
            "tgt_ix": "14-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_6",
            "tgt_ix": "14-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_6",
            "tgt_ix": "14-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_6",
            "tgt_ix": "14-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_7",
            "tgt_ix": "14-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_7",
            "tgt_ix": "14-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_7",
            "tgt_ix": "14-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_7",
            "tgt_ix": "14-ARR_v2_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_7",
            "tgt_ix": "14-ARR_v2_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_7",
            "tgt_ix": "14-ARR_v2_7@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_7",
            "tgt_ix": "14-ARR_v2_7@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_7",
            "tgt_ix": "14-ARR_v2_7@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_7",
            "tgt_ix": "14-ARR_v2_7@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_7",
            "tgt_ix": "14-ARR_v2_7@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_8",
            "tgt_ix": "14-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_8",
            "tgt_ix": "14-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_8",
            "tgt_ix": "14-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_8",
            "tgt_ix": "14-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_8",
            "tgt_ix": "14-ARR_v2_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_8",
            "tgt_ix": "14-ARR_v2_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_9",
            "tgt_ix": "14-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_9",
            "tgt_ix": "14-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_9",
            "tgt_ix": "14-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_9",
            "tgt_ix": "14-ARR_v2_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_9",
            "tgt_ix": "14-ARR_v2_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_9",
            "tgt_ix": "14-ARR_v2_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_9",
            "tgt_ix": "14-ARR_v2_9@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_9",
            "tgt_ix": "14-ARR_v2_9@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_9",
            "tgt_ix": "14-ARR_v2_9@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_9",
            "tgt_ix": "14-ARR_v2_9@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_9",
            "tgt_ix": "14-ARR_v2_9@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_9",
            "tgt_ix": "14-ARR_v2_9@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_9",
            "tgt_ix": "14-ARR_v2_9@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_9",
            "tgt_ix": "14-ARR_v2_9@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_9",
            "tgt_ix": "14-ARR_v2_9@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_10",
            "tgt_ix": "14-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_11",
            "tgt_ix": "14-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_11",
            "tgt_ix": "14-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_11",
            "tgt_ix": "14-ARR_v2_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_11",
            "tgt_ix": "14-ARR_v2_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_11",
            "tgt_ix": "14-ARR_v2_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_11",
            "tgt_ix": "14-ARR_v2_11@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_11",
            "tgt_ix": "14-ARR_v2_11@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_11",
            "tgt_ix": "14-ARR_v2_11@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_12",
            "tgt_ix": "14-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_12",
            "tgt_ix": "14-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_12",
            "tgt_ix": "14-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_12",
            "tgt_ix": "14-ARR_v2_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_12",
            "tgt_ix": "14-ARR_v2_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_12",
            "tgt_ix": "14-ARR_v2_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_12",
            "tgt_ix": "14-ARR_v2_12@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_12",
            "tgt_ix": "14-ARR_v2_12@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_12",
            "tgt_ix": "14-ARR_v2_12@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_12",
            "tgt_ix": "14-ARR_v2_12@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_12",
            "tgt_ix": "14-ARR_v2_12@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_13",
            "tgt_ix": "14-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_14",
            "tgt_ix": "14-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_15",
            "tgt_ix": "14-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_15",
            "tgt_ix": "14-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_15",
            "tgt_ix": "14-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_15",
            "tgt_ix": "14-ARR_v2_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_15",
            "tgt_ix": "14-ARR_v2_15@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_16",
            "tgt_ix": "14-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_16",
            "tgt_ix": "14-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_16",
            "tgt_ix": "14-ARR_v2_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_16",
            "tgt_ix": "14-ARR_v2_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_16",
            "tgt_ix": "14-ARR_v2_16@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_17",
            "tgt_ix": "14-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_18",
            "tgt_ix": "14-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_19",
            "tgt_ix": "14-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_19",
            "tgt_ix": "14-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_19",
            "tgt_ix": "14-ARR_v2_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_19",
            "tgt_ix": "14-ARR_v2_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_20",
            "tgt_ix": "14-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_21",
            "tgt_ix": "14-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_21",
            "tgt_ix": "14-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_22",
            "tgt_ix": "14-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_22",
            "tgt_ix": "14-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_22",
            "tgt_ix": "14-ARR_v2_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_22",
            "tgt_ix": "14-ARR_v2_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_22",
            "tgt_ix": "14-ARR_v2_22@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_22",
            "tgt_ix": "14-ARR_v2_22@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_22",
            "tgt_ix": "14-ARR_v2_22@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_22",
            "tgt_ix": "14-ARR_v2_22@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_22",
            "tgt_ix": "14-ARR_v2_22@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_22",
            "tgt_ix": "14-ARR_v2_22@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_23",
            "tgt_ix": "14-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_23",
            "tgt_ix": "14-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_23",
            "tgt_ix": "14-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_23",
            "tgt_ix": "14-ARR_v2_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_23",
            "tgt_ix": "14-ARR_v2_23@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_23",
            "tgt_ix": "14-ARR_v2_23@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_24",
            "tgt_ix": "14-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_24",
            "tgt_ix": "14-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_25",
            "tgt_ix": "14-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_26",
            "tgt_ix": "14-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_26",
            "tgt_ix": "14-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_26",
            "tgt_ix": "14-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_26",
            "tgt_ix": "14-ARR_v2_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_26",
            "tgt_ix": "14-ARR_v2_26@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_26",
            "tgt_ix": "14-ARR_v2_26@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_26",
            "tgt_ix": "14-ARR_v2_26@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_26",
            "tgt_ix": "14-ARR_v2_26@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_26",
            "tgt_ix": "14-ARR_v2_26@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_26",
            "tgt_ix": "14-ARR_v2_26@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_26",
            "tgt_ix": "14-ARR_v2_26@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_26",
            "tgt_ix": "14-ARR_v2_26@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_27",
            "tgt_ix": "14-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_28",
            "tgt_ix": "14-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_28",
            "tgt_ix": "14-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_28",
            "tgt_ix": "14-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_28",
            "tgt_ix": "14-ARR_v2_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_28",
            "tgt_ix": "14-ARR_v2_28@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_28",
            "tgt_ix": "14-ARR_v2_28@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_28",
            "tgt_ix": "14-ARR_v2_28@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_28",
            "tgt_ix": "14-ARR_v2_28@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_28",
            "tgt_ix": "14-ARR_v2_28@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_29",
            "tgt_ix": "14-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_29",
            "tgt_ix": "14-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_29",
            "tgt_ix": "14-ARR_v2_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_29",
            "tgt_ix": "14-ARR_v2_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_29",
            "tgt_ix": "14-ARR_v2_29@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_30",
            "tgt_ix": "14-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_31",
            "tgt_ix": "14-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_31",
            "tgt_ix": "14-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_32",
            "tgt_ix": "14-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_33",
            "tgt_ix": "14-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_33",
            "tgt_ix": "14-ARR_v2_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_33",
            "tgt_ix": "14-ARR_v2_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_33",
            "tgt_ix": "14-ARR_v2_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_33",
            "tgt_ix": "14-ARR_v2_33@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_34",
            "tgt_ix": "14-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_35",
            "tgt_ix": "14-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_35",
            "tgt_ix": "14-ARR_v2_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_35",
            "tgt_ix": "14-ARR_v2_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_36",
            "tgt_ix": "14-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_37",
            "tgt_ix": "14-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_38",
            "tgt_ix": "14-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_39",
            "tgt_ix": "14-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_39",
            "tgt_ix": "14-ARR_v2_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_39",
            "tgt_ix": "14-ARR_v2_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_39",
            "tgt_ix": "14-ARR_v2_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_39",
            "tgt_ix": "14-ARR_v2_39@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_39",
            "tgt_ix": "14-ARR_v2_39@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_39",
            "tgt_ix": "14-ARR_v2_39@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_40",
            "tgt_ix": "14-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_40",
            "tgt_ix": "14-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_40",
            "tgt_ix": "14-ARR_v2_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_41",
            "tgt_ix": "14-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_41",
            "tgt_ix": "14-ARR_v2_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_41",
            "tgt_ix": "14-ARR_v2_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_41",
            "tgt_ix": "14-ARR_v2_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_42",
            "tgt_ix": "14-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_43",
            "tgt_ix": "14-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_43",
            "tgt_ix": "14-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_44",
            "tgt_ix": "14-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_45",
            "tgt_ix": "14-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_45",
            "tgt_ix": "14-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_45",
            "tgt_ix": "14-ARR_v2_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_45",
            "tgt_ix": "14-ARR_v2_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_45",
            "tgt_ix": "14-ARR_v2_45@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_45",
            "tgt_ix": "14-ARR_v2_45@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_45",
            "tgt_ix": "14-ARR_v2_45@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_45",
            "tgt_ix": "14-ARR_v2_45@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_45",
            "tgt_ix": "14-ARR_v2_45@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_45",
            "tgt_ix": "14-ARR_v2_45@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_45",
            "tgt_ix": "14-ARR_v2_45@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_46",
            "tgt_ix": "14-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_46",
            "tgt_ix": "14-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_46",
            "tgt_ix": "14-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_46",
            "tgt_ix": "14-ARR_v2_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_46",
            "tgt_ix": "14-ARR_v2_46@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_46",
            "tgt_ix": "14-ARR_v2_46@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_46",
            "tgt_ix": "14-ARR_v2_46@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_46",
            "tgt_ix": "14-ARR_v2_46@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_46",
            "tgt_ix": "14-ARR_v2_46@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_47",
            "tgt_ix": "14-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_47",
            "tgt_ix": "14-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_47",
            "tgt_ix": "14-ARR_v2_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_47",
            "tgt_ix": "14-ARR_v2_47@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_47",
            "tgt_ix": "14-ARR_v2_47@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_47",
            "tgt_ix": "14-ARR_v2_47@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_47",
            "tgt_ix": "14-ARR_v2_47@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_47",
            "tgt_ix": "14-ARR_v2_47@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_47",
            "tgt_ix": "14-ARR_v2_47@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_47",
            "tgt_ix": "14-ARR_v2_47@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_47",
            "tgt_ix": "14-ARR_v2_47@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_48",
            "tgt_ix": "14-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_48",
            "tgt_ix": "14-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_48",
            "tgt_ix": "14-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_48",
            "tgt_ix": "14-ARR_v2_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_48",
            "tgt_ix": "14-ARR_v2_48@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_48",
            "tgt_ix": "14-ARR_v2_48@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_49",
            "tgt_ix": "14-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_50",
            "tgt_ix": "14-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_50",
            "tgt_ix": "14-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_50",
            "tgt_ix": "14-ARR_v2_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_51",
            "tgt_ix": "14-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_52",
            "tgt_ix": "14-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_52",
            "tgt_ix": "14-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_52",
            "tgt_ix": "14-ARR_v2_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_52",
            "tgt_ix": "14-ARR_v2_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_52",
            "tgt_ix": "14-ARR_v2_52@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_52",
            "tgt_ix": "14-ARR_v2_52@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_53",
            "tgt_ix": "14-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_54",
            "tgt_ix": "14-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_54",
            "tgt_ix": "14-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_54",
            "tgt_ix": "14-ARR_v2_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_54",
            "tgt_ix": "14-ARR_v2_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_54",
            "tgt_ix": "14-ARR_v2_54@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_54",
            "tgt_ix": "14-ARR_v2_54@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_54",
            "tgt_ix": "14-ARR_v2_54@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_55",
            "tgt_ix": "14-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_56",
            "tgt_ix": "14-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_56",
            "tgt_ix": "14-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_56",
            "tgt_ix": "14-ARR_v2_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_56",
            "tgt_ix": "14-ARR_v2_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_56",
            "tgt_ix": "14-ARR_v2_56@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_57",
            "tgt_ix": "14-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_58",
            "tgt_ix": "14-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_58",
            "tgt_ix": "14-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_58",
            "tgt_ix": "14-ARR_v2_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_58",
            "tgt_ix": "14-ARR_v2_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_58",
            "tgt_ix": "14-ARR_v2_58@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_58",
            "tgt_ix": "14-ARR_v2_58@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_59",
            "tgt_ix": "14-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_60",
            "tgt_ix": "14-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_60",
            "tgt_ix": "14-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_60",
            "tgt_ix": "14-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_60",
            "tgt_ix": "14-ARR_v2_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_61",
            "tgt_ix": "14-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_61",
            "tgt_ix": "14-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_61",
            "tgt_ix": "14-ARR_v2_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_61",
            "tgt_ix": "14-ARR_v2_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_61",
            "tgt_ix": "14-ARR_v2_61@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_61",
            "tgt_ix": "14-ARR_v2_61@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_61",
            "tgt_ix": "14-ARR_v2_61@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_61",
            "tgt_ix": "14-ARR_v2_61@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_62",
            "tgt_ix": "14-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_62",
            "tgt_ix": "14-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_62",
            "tgt_ix": "14-ARR_v2_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_62",
            "tgt_ix": "14-ARR_v2_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_62",
            "tgt_ix": "14-ARR_v2_62@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_63",
            "tgt_ix": "14-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_63",
            "tgt_ix": "14-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_63",
            "tgt_ix": "14-ARR_v2_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_63",
            "tgt_ix": "14-ARR_v2_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_64",
            "tgt_ix": "14-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_64",
            "tgt_ix": "14-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_65",
            "tgt_ix": "14-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_65",
            "tgt_ix": "14-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_65",
            "tgt_ix": "14-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_65",
            "tgt_ix": "14-ARR_v2_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_65",
            "tgt_ix": "14-ARR_v2_65@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_66",
            "tgt_ix": "14-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_67",
            "tgt_ix": "14-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_67",
            "tgt_ix": "14-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_67",
            "tgt_ix": "14-ARR_v2_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_67",
            "tgt_ix": "14-ARR_v2_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_67",
            "tgt_ix": "14-ARR_v2_67@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_67",
            "tgt_ix": "14-ARR_v2_67@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_68",
            "tgt_ix": "14-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_69",
            "tgt_ix": "14-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_69",
            "tgt_ix": "14-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_69",
            "tgt_ix": "14-ARR_v2_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_69",
            "tgt_ix": "14-ARR_v2_69@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_69",
            "tgt_ix": "14-ARR_v2_69@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_69",
            "tgt_ix": "14-ARR_v2_69@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_69",
            "tgt_ix": "14-ARR_v2_69@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_70",
            "tgt_ix": "14-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_71",
            "tgt_ix": "14-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_72",
            "tgt_ix": "14-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_73",
            "tgt_ix": "14-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_74",
            "tgt_ix": "14-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_75",
            "tgt_ix": "14-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_76",
            "tgt_ix": "14-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_77",
            "tgt_ix": "14-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_78",
            "tgt_ix": "14-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_79",
            "tgt_ix": "14-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_80",
            "tgt_ix": "14-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_81",
            "tgt_ix": "14-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_82",
            "tgt_ix": "14-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_83",
            "tgt_ix": "14-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_84",
            "tgt_ix": "14-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_85",
            "tgt_ix": "14-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_86",
            "tgt_ix": "14-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_87",
            "tgt_ix": "14-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_88",
            "tgt_ix": "14-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_89",
            "tgt_ix": "14-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_90",
            "tgt_ix": "14-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_91",
            "tgt_ix": "14-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_92",
            "tgt_ix": "14-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_93",
            "tgt_ix": "14-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_94",
            "tgt_ix": "14-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_95",
            "tgt_ix": "14-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_96",
            "tgt_ix": "14-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_97",
            "tgt_ix": "14-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_98",
            "tgt_ix": "14-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_99",
            "tgt_ix": "14-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_100",
            "tgt_ix": "14-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_101",
            "tgt_ix": "14-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_102",
            "tgt_ix": "14-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_103",
            "tgt_ix": "14-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_104",
            "tgt_ix": "14-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v2_105",
            "tgt_ix": "14-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 971,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "14-ARR",
        "version": 2
    }
}