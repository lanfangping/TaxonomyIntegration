{
    "nodes": [
        {
            "ix": "14-ARR_v1_review3_0",
            "content": "14-ARR_v1_review3",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "14-ARR_v1_review3_1",
            "content": "paper_summary. The authors propose a new learned metric for ranking the quality of generated visual stories. Their model/metric, VRANK, is trained directly on pairwise human judgments. Compared to metrics traditionally used for caption generation evaluation like METEOR and ROUGE-L, the authors new metric performs better. They also evaluate on a non-visual corpus, and demonstrate that their model also works to evaluate in that case.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v1_review3_2",
            "content": "summary_of_strengths. I liked this work! I suspect that some controversy because the author's metric is i) model-based; and ii) trained directly on human ratings, but the authors make a compelling case that our current metrics simply are not working for this setting. In some sense, the results are not surprising: of course the learned metric does better in the evaluation setting that it was trained in. But, the fact that it also does well in a unimodal setting suggests that the model has not overfit to particular spurious factors in the corpus, which is promising. Overall --- I think the authors make a convincing case for their model-based metric. And, the fine-grained evaluations in table 5 provide some perspective on where the metric could be improved, e.g., accounting more for grammatically and irrelevant nouns.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v1_review3_3",
            "content": "summary_of_weaknesses. Of course, training on human judgments directly is not a panacea. There's always the concern that whichever pairwise comparisons were originally collected won't generalize to future corpora, e.g., if the models improve in the future, will the particular model generations that Vrank was trained on become \"stale\"? \nThat being said, because the current evaluation metrics are essentially performing at random for the story generation setting, even a stale Vrank couldn't possibly be worse.\nThe author's contribution is in visual story generation evaluation; the task itself is somewhat niche, and so, while I think folks who evaluate on this dataset/setup should use this dataset, I suspect that the work's works impact will mostly be focused on that specific community.\nOne small technical concern: For Table 4, The BLEU-4 score is suspiciously low, especially in comparison to SacreBLEU, which also computes BLEU-4. My suspicion is that BLEU-4 is often zero for some stories, which leads to many ties of 0 vs 0. I think that ties should be broken randomly, or that the authors should mention this is the cause (if indeed my suspicions are correct).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "14-ARR_v1_review3_4",
            "content": "comments,_suggestions_and_typos. Overall: the authors make a compelling case that current generation evaluation metrics perform poorly for ranking the quality of visual story generation. While a somewhat niche task, their model clearly performs better than the other metrics, and given that the other metrics are essentially performing at random, the normal critiques of \"staleness\"/\"neural nets are uninterpretable as metrics\" that could apply in this setting hold less weight.",
            "ntype": "p",
            "meta": null
        }
    ],
    "span_nodes": [
        {
            "ix": "14-ARR_v1_review3_0@0",
            "content": "14-ARR_v1_review3",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_0",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "14-ARR_v1_review3_1@0",
            "content": "paper_summary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_1",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "14-ARR_v1_review3_1@1",
            "content": "The authors propose a new learned metric for ranking the quality of generated visual stories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_1",
            "start": 15,
            "end": 107,
            "label": {}
        },
        {
            "ix": "14-ARR_v1_review3_1@2",
            "content": "Their model/metric, VRANK, is trained directly on pairwise human judgments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_1",
            "start": 109,
            "end": 183,
            "label": {}
        },
        {
            "ix": "14-ARR_v1_review3_1@3",
            "content": "Compared to metrics traditionally used for caption generation evaluation like METEOR and ROUGE-L, the authors new metric performs better.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_1",
            "start": 185,
            "end": 321,
            "label": {}
        },
        {
            "ix": "14-ARR_v1_review3_1@4",
            "content": "They also evaluate on a non-visual corpus, and demonstrate that their model also works to evaluate in that case.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_1",
            "start": 323,
            "end": 434,
            "label": {}
        },
        {
            "ix": "14-ARR_v1_review3_2@0",
            "content": "summary_of_strengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_2",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "14-ARR_v1_review3_2@1",
            "content": "I liked this work! I suspect that some controversy because the author's metric is i) model-based; and ii) trained directly on human ratings, but the authors make a compelling case that our current metrics simply are not working for this setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_2",
            "start": 22,
            "end": 266,
            "label": {}
        },
        {
            "ix": "14-ARR_v1_review3_2@2",
            "content": "In some sense, the results are not surprising: of course the learned metric does better in the evaluation setting that it was trained in. But, the fact that it also does well in a unimodal setting suggests that the model has not overfit to particular spurious factors in the corpus, which is promising.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_2",
            "start": 268,
            "end": 569,
            "label": {}
        },
        {
            "ix": "14-ARR_v1_review3_2@3",
            "content": "Overall --- I think the authors make a convincing case for their model-based metric. And, the fine-grained evaluations in table 5 provide some perspective on where the metric could be improved, e.g., accounting more for grammatically and irrelevant nouns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_2",
            "start": 571,
            "end": 825,
            "label": {}
        },
        {
            "ix": "14-ARR_v1_review3_3@0",
            "content": "summary_of_weaknesses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_3",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "14-ARR_v1_review3_3@1",
            "content": "Of course, training on human judgments directly is not a panacea.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_3",
            "start": 23,
            "end": 87,
            "label": {}
        },
        {
            "ix": "14-ARR_v1_review3_3@2",
            "content": "There's always the concern that whichever pairwise comparisons were originally collected won't generalize to future corpora, e.g., if the models improve in the future, will the particular model generations that Vrank was trained on become \"stale\"? \nThat being said, because the current evaluation metrics are essentially performing at random for the story generation setting, even a stale Vrank couldn't possibly be worse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_3",
            "start": 89,
            "end": 510,
            "label": {}
        },
        {
            "ix": "14-ARR_v1_review3_3@3",
            "content": "\nThe author's contribution is in visual story generation evaluation; the task itself is somewhat niche, and so, while I think folks who evaluate on this dataset/setup should use this dataset, I suspect that the work's works impact will mostly be focused on that specific community.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_3",
            "start": 511,
            "end": 791,
            "label": {}
        },
        {
            "ix": "14-ARR_v1_review3_3@4",
            "content": "\nOne small technical concern: For Table 4, The BLEU-4 score is suspiciously low, especially in comparison to SacreBLEU, which also computes BLEU-4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_3",
            "start": 792,
            "end": 938,
            "label": {}
        },
        {
            "ix": "14-ARR_v1_review3_3@5",
            "content": "My suspicion is that BLEU-4 is often zero for some stories, which leads to many ties of 0 vs 0. I think that ties should be broken randomly, or that the authors should mention this is the cause (if indeed my suspicions are correct).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_3",
            "start": 940,
            "end": 1171,
            "label": {}
        },
        {
            "ix": "14-ARR_v1_review3_4@0",
            "content": "comments,_suggestions_and_typos.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_4",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "14-ARR_v1_review3_4@1",
            "content": "Overall: the authors make a compelling case that current generation evaluation metrics perform poorly for ranking the quality of visual story generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_4",
            "start": 33,
            "end": 185,
            "label": {}
        },
        {
            "ix": "14-ARR_v1_review3_4@2",
            "content": "While a somewhat niche task, their model clearly performs better than the other metrics, and given that the other metrics are essentially performing at random, the normal critiques of \"staleness\"/\"neural nets are uninterpretable as metrics\" that could apply in this setting hold less weight.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "14-ARR_v1_review3_4",
            "start": 187,
            "end": 477,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "14-ARR_v1_review3_0",
            "tgt_ix": "14-ARR_v1_review3_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v1_review3_0",
            "tgt_ix": "14-ARR_v1_review3_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v1_review3_0",
            "tgt_ix": "14-ARR_v1_review3_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v1_review3_0",
            "tgt_ix": "14-ARR_v1_review3_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v1_review3_0",
            "tgt_ix": "14-ARR_v1_review3_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v1_review3_1",
            "tgt_ix": "14-ARR_v1_review3_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v1_review3_2",
            "tgt_ix": "14-ARR_v1_review3_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v1_review3_3",
            "tgt_ix": "14-ARR_v1_review3_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "14-ARR_v1_review3_0",
            "tgt_ix": "14-ARR_v1_review3_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v1_review3_1",
            "tgt_ix": "14-ARR_v1_review3_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v1_review3_1",
            "tgt_ix": "14-ARR_v1_review3_1@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v1_review3_1",
            "tgt_ix": "14-ARR_v1_review3_1@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v1_review3_1",
            "tgt_ix": "14-ARR_v1_review3_1@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v1_review3_1",
            "tgt_ix": "14-ARR_v1_review3_1@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v1_review3_2",
            "tgt_ix": "14-ARR_v1_review3_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v1_review3_2",
            "tgt_ix": "14-ARR_v1_review3_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v1_review3_2",
            "tgt_ix": "14-ARR_v1_review3_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v1_review3_2",
            "tgt_ix": "14-ARR_v1_review3_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v1_review3_3",
            "tgt_ix": "14-ARR_v1_review3_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v1_review3_3",
            "tgt_ix": "14-ARR_v1_review3_3@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v1_review3_3",
            "tgt_ix": "14-ARR_v1_review3_3@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v1_review3_3",
            "tgt_ix": "14-ARR_v1_review3_3@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v1_review3_3",
            "tgt_ix": "14-ARR_v1_review3_3@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v1_review3_3",
            "tgt_ix": "14-ARR_v1_review3_3@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v1_review3_4",
            "tgt_ix": "14-ARR_v1_review3_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v1_review3_4",
            "tgt_ix": "14-ARR_v1_review3_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "14-ARR_v1_review3_4",
            "tgt_ix": "14-ARR_v1_review3_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "14-ARR_v1_review3",
    "meta": {
        "ix_counter": 23,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy"
    }
}