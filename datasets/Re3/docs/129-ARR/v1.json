{
    "nodes": [
        {
            "ix": "129-ARR_v1_0",
            "content": "ACTUNE: Uncertainty-Aware Active Self-Training for Active Fine-Tuning of Pretrained Language Models",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_2",
            "content": "Although fine-tuning pre-trained language models (PLMs) renders strong performance in many NLP tasks, it relies on excessive labeled data. Recently, researchers have resorted to active fine-tuning for enhancing the label efficiency of PLM fine-tuning, but existing methods of this type usually ignore the potential of unlabeled data. We develop ACTUNE, a new framework that improves the label efficiency of active PLM fine-tuning by unleashing the power of unlabeled data via self training. AC-TUNE switches between data annotation and model self-training based on uncertainty: the unlabeled samples of high-uncertainty are selected for annotation, while the ones from lowuncertainty regions are used for model selftraining. Additionally, we design (1) a regionaware sampling strategy to avoid redundant samples when querying annotations and (2) a momentum-based memory bank to dynamically aggregate the model's pseudo labels to suppress label noise in self-training. Experiments on 6 text classification datasets show that ACTUNE outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM fine-tuning by 56.2% on average.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "129-ARR_v1_4",
            "content": "Fine-tuning pre-trained language models (PLMs) has achieved enormous success in natural language processing (NLP) (Devlin et al., 2019;Liu et al., 2019;Brown et al., 2020), one of which is the competitive performance it offers when consuming only a few labeled data (Bansal et al., 2020;Gao et al., 2021). However, there are still significant gaps between few-shot and fully-supervised PLM fine-tuning in many classification tasks. Besides, the performance of few-shot PLM fine-tuning can vary substantially with different sets of training data (Bragg et al., 2021). Therefore, there is a crucial need for PLM fine-tuning approaches with better label-efficiency and being robust to selection of training data, especially for applications where labeled data are scarce and expensive to obtain.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_5",
            "content": "Towards this goal, researchers have resorted to active fine-tuning of PLMs and achieved comparable performance to fully-supervised methods with much less annotated samples (Ein-Dor et al., 2020;Margatina et al., 2021a,b;Yuan et al., 2020). Nevertheless, they usually neglect unlabeled data, which can be useful for improving label efficiency for PLM fine-tuning (Du et al., 2021). To leverage those unlabeled data to improve label efficiency of active learning, efforts have been made in the semi-supervised active learning literature (Wang et al., 2016;Rottmann et al., 2018;Sim\u00e9oni et al., 2020), but the proposed query strategies can return highly redundant samples due to limited representation power, resulting in suboptimal label efficiency. Moreover, they usually rely on pseudo-labeling to utilize unlabeled data, which requires greater (yet often absent) care to denoise the pseudo labels, otherwise the errors could accumulate and deteriorate the model performance. This phenomenon can be even more severe for PLMs, as the finetuning process often suffers from the instability issue caused by different weight initialization and data orders (Dodge et al., 2020). Thus, it still remains open and challenging to design robust and label efficient method for active PLM fine-tuning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_6",
            "content": "To tackle above challenges, we propose AC-TUNE, a new method that improves the label efficiency and robustness of active PLM fine-tuning with self-training. Based on the estimated uncertainty of data, ACTUNE chooses from one of the following cases in each learning round: (1) when the average uncertainty of a region is low, we trust the model's prediction and select most certain predictions within the region for self-training;",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_7",
            "content": "(2) when the average uncertainty of a region is high, indicating inadequate observations for parameter learning, we actively annotate most uncertain samples within the region to improve the model. Different from existing AL methods that only leverage uncertainty for querying labels, our uncertaintydriven self-training paradigm gradually unleash the data with low uncertainty via self-training, while reducing the chance of error propagation triggered by highly-uncertain mis-labeled data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_8",
            "content": "To further boost the performance on downstream tasks, we design two techniques, namely regionaware sampling (RS) and momentum-based memory bank (MMB) to improve the query strategies and suppress label noise for ACTUNE. Inspired by the fact that existing uncertainty-based AL methods often end up choosing uncertain yet repetitive data (Ein-Dor et al., 2020;Margatina et al., 2021b), we design a region-aware sampling technique to promote both diversity and representativeness by leveraging the representation power of PLMs. Specifically, we first estimate the uncertainties of the unlabeled data with PLMs, then cluster the data using their PLM representations and weigh the data by the corresponding uncertainty. Such a clustering scheme partitions the embedding space into small sub-regions with an emphasis on highlyuncertain samples. Finally, by sampling over multiple high-uncertainty regions, our strategy selects data with high uncertainty and low redundancy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_9",
            "content": "To rectify the erroneous pseudo labels derived by self-training, we design a simple but effective way to select low-uncertainty data for self-training. Our method is motivated by the fact that fine-tuning PLMs suffer from instability issues -distinct initializations and data orders can result in a large variance of the task performance (Dodge et al., 2020;Mosbach et al., 2021). However, previous approaches only select pseudo-labeled data based on the prediction of the current round and therefore are less reliable. In contrast, we maintain a dynamic memory bank to save the predictions of unlabeled samples for later use. we propose a momentum updating method to dynamically aggregate the predictions from preceding rounds (Laine and Aila, 2016) and select low-uncertainty samples based on aggregated prediction. As a consequence, only the samples with high prediction confidence over multiple rounds will be used for self-training, which mitigates the issue of label noise. We highlight that our active self-training approach is an efficient substitution to existing AL methods, requiring ignorable extra computational cost.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_10",
            "content": "Our key contributions are: (1) an active selftraining paradigm ACTUNE that integrates the benefit of self-training and active learning in a principled way to minimize the labeling cost for finetuning PLMs; (2) a region-aware querying strategy to enforce both the informativeness and the diversity of queried samples during AL; (3) a simple and effective momentum-based method to harness the predictions for preceding rounds to alleviate the label noise in self-training and (4) experiments on 6 benchmarks demonstrating ACTUNE improves the label efficiency over existing self-training and active learning baselines by 56.2%.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_11",
            "content": "2 Uncertainty-aware Active Self-training",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_12",
            "content": "Problem Formulation",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "129-ARR_v1_13",
            "content": "We study active fine-tuning of pre-trained language models for text classification, formulated as follows: Given a small number of labeled samples X l = {(x i , y i )} L i=1 and unlabeled samples",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_14",
            "content": "X u = {x j } U j=1 (|X l | |X u |)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_15",
            "content": ", we aim to fine-tune a pre-trained language model f (x; \u03b8) : X \u2192 Y in an interactive way: we perform active self-training for T rounds with the total labeling budget b. In each round, we aim to query B = b/T samples denoted as B from X u to fine-tune a pre-trained language model f (x; \u03b8) with both X l , B and X u to maximize the performance on downstream text classification tasks. Here X = X l \u222a X u denotes all samples and Y = {1, 2, \u2022 \u2022 \u2022 , C} is the label set, where C is the number of classes.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_16",
            "content": "Overview of ACTUNE Framework",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "129-ARR_v1_17",
            "content": "We now present our active self-training paradigm ACTUNE underpinned by estimated uncertainty. We begin the active self-training loop by finetuning a BERT f (\u03b8 (0) ) on the initial labeled data X L . Formally, we solve the following optimization problem",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_18",
            "content": "min \u03b8 1 |X L | (x i ,y i )\u2208X L CE f (x i ; \u03b8 (0) ), y i , (1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_19",
            "content": "In round t (1 \u2264 t \u2264 T ) of the active self-training procedure, we first calculate the uncertainty score based on a given function a (t) i = a(x i , \u03b8 (t) ) 1 for all x i \u2208 X u . Then, we query labeled samples and generate pseudo-labels for unlabeled data X u simultaneously to facilitate self-training. For each sample x i , the pseudo-label y is calculated based on the current model's output: 3), ( 4)) until convergence. 2. Select sample set Q (t) for AL and S (t) for self-training from Xu based on Eq. ( 11) or (13). 3. Augment the labeled set XL = XL \u222a Q (t) . 4. Obtain \u03b8 (t) by finetuning f (\u2022; \u03b8 t ) with LST ( Eq. ( 14)) using AdamW. 5. Update memory bank g(x; \u03b8 t ) with Eq. ( 10) or ( 12).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_20",
            "content": "y = argmax j\u2208Y f (x; \u03b8 (t) ) j ,(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_21",
            "content": "Output: The final fine-tuned model f (\u2022; \u03b8 T ).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_22",
            "content": "where f (x; \u03b8 (t) ) \u2208 R C is a probability simplex and [f (x; \u03b8 (t) )] j is the j-th entry. The procedure of ACTUNE is summarized in Algorithm 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_23",
            "content": "Region-aware Sampling for Active",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "129-ARR_v1_24",
            "content": "Learning on High-uncertainty Data After obtaining the uncertainty for unlabeled data, we aim to query annotation for high-uncertainty samples. However, directly sampling the most uncertain samples gives suboptimal result since uncertainty-based sampling tends to query repetitive data (Ein-Dor et al., 2020) and results in poor representativeness of the overall data distribution.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_25",
            "content": "To tackle this issue, we propose region-aware sampling to capture both uncertainty and diversity during active self-training. Specifically, in the tth round, we first conduct the weighted K-means clustering (Huang et al., 2005), which weights samples based on their uncertainty. Denote K the number of clusters and v (t) i = BERT(x i ) the representation of x i from the penultimate layer of BERT. The weighted K-means first initializes the center of each each cluster \u00b5 i (1 \u2264 i \u2264 K) via K-Means++ (Arthur and Vassilvitskii, 2007). Then, it jointly updates the centroid of each cluster and assigns each sample to cluster c i as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_26",
            "content": "c (t) i = argmin k=1,...,K vi \u2212 \u00b5 k 2 ,(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_27",
            "content": "\u00b5 (t) k = x i \u2208C (t) k a(xi, \u03b8 (t) ) \u2022 v (t) i x\u2208C (t) k a(xi, \u03b8 (t) )(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_28",
            "content": "where",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_29",
            "content": "C (t) k = {x (t) i |c (t) i",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_30",
            "content": "= k}(k = 1, . . . , K) stands for the k-th cluster. The above two steps in Eq. ( 3), (4) are repeated until convergence. Compared with vanilla K-Means method, the weighting scheme increases the density of the samples with high uncertainty, thus enabling the K-Means methods to discover clusters with high uncertainty. After obtaining K regions with the corresponding data C (t) k , we calculate the uncertainty of each region as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_31",
            "content": "u (t) k = U (C (t) k ) + \u03b2I(C (t) k )(5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_32",
            "content": "where",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_33",
            "content": "U (C (t) k ) = 1 |C (t) k | x i \u2208C (t) k a(xi, \u03b8 (t) )(6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_34",
            "content": "stands for the average uncertainty of samples and",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_35",
            "content": "I(C (t) k ) = \u2212 j\u2208C f (t) j log f (t) j (7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_36",
            "content": "stands for the inter-class diversity within cluster k and f",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_37",
            "content": "(t) j = i 1{ y i =j} |C (t) k |",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_38",
            "content": "represents the frequency of class j on cluster k. Notably, the term U (C",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_39",
            "content": "(t)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_40",
            "content": "k ) assigns higher score for clusters with more uncertain samples, and I(C (t) k ) grants higher scores for clusters containing samples with more diverse predicted classes from pseudo labels since such clusters would be closer to the decision boundary.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_41",
            "content": "Then, we rank the clusters in an ascending order according to u (t) k . A high score indicates high uncertainty of the model in these regions, and we need to actively annotate the associated instances to reduce uncertainty and improve the model's performance. We adopt a hierarchical sampling strategy: we first select the M clusters with the highest uncertainty, and then sample b = B M data with the highest uncertainty to form the batch Q (t) . 2",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_42",
            "content": "K (t) a = top-M k\u2208{1,...,K} u (t) k , Q (t) = k\u2208K (t) a C (t) a,k where C (t) a,k = Top-b x i \u2208C (t) k a(xi, \u03b8 (t) ).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_43",
            "content": "(8) We remark that such a hierarchical sampling strategy queries most uncertain samples from different regions, thus the uncertainty and diversity of queried samples can be both achieved.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_44",
            "content": "Self-training for Most Confident Data from Low-uncertainty Regions",
            "ntype": "title",
            "meta": {
                "section": "2.4"
            }
        },
        {
            "ix": "129-ARR_v1_45",
            "content": "For self-training, we aim to select unlabeled samples which are most likely to have been correctly classified by the current model. This requires the sample to have low uncertainty. Therefore, we select the top k samples from the M lowest uncertainty regions to form the acquired batch S (t) :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_46",
            "content": "C (t) s = k\u2208K (t) s C (t) k where K (t) s = bottom-M k\u2208{1,...,K} u (t) k , S (t) = bottom-k x i \u2208C (t) s a(xi, \u03b8 (t) ),(9)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_47",
            "content": "Momentum-based Memory Bank for Selftraining. As PLMs are sensitive to the stochasticity involved in fine-tuning, the model suffers from the instability issue -different weight initialization and data orders may result in different predictions on the same dataset (Dodge et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_48",
            "content": "Additionally, if the model gives inconsistent predictions in different rounds for a specific sample, then it is potentially uncertain about the sample, and adding it to the training set may harm the active self-training process. For example, for a twoclass classification problem, suppose we obtain f (x; \u03b8 (t\u22121) ) = [0.65, 0.35] for sample x the round (t\u22121) and f (x; \u03b8 (t) ) = [0.05, 0.95] for the round t.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_49",
            "content": "Although the model is quite 'confident' on the class of x when we only consider the result of the round t, it gives contradictory predictions over these two consecutive rounds, which indicates that the model is still uncertain to which class x belongs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_50",
            "content": "To effectively mitigate the noise and stabilize the active self-training process, we maintain a dynamic memory bank to save the results from previous rounds, and use momentum update (He et al., 2020;Laine and Aila, 2016) to aggregate the results from both the previous and current rounds. Then, during active self-training, we will select samples with the highest aggregated score. In this way, only those samples that the model is certain about over all previous rounds will be selected for self-training. We design two variants for the memory bank, namely prediction-based and value-based aggregation. Prediction based Momentum Update. We adopt an exponential moving average approach to aggregate the prediction g(x; \u03b8 (t) ) on round t as g(x; \u03b8",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_51",
            "content": "(t) ) = m t \u00d7f (x; \u03b8 (t) )+(1\u2212m t )\u00d7g(x; \u03b8 (t\u22121) ), (10",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_52",
            "content": ") where m t = (1 \u2212 t T )m L + t T m H (0 < m L \u2264 m H \u2264 1",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_53",
            "content": ") is a momentum coefficient. We gradually increase the weight for models on later rounds, since they are trained with more labeled data thus being able to provide more reliable predictions. Then, we calculate the uncertainty based on g(x; \u03b8 (t) ) and rewrite Eq. ( 9) and (2) as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_54",
            "content": "S (t) = bottom-k x i \u2208C (t) s a x i , g(x; \u03b8 (t) ), \u03b8 (t) y = argmax j\u2208Y g(x; \u03b8 (t) ) j ,(11)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_55",
            "content": "Value-based Momentum Update. For methods that do not directly use prediction for uncertainty estimation, we aggregate the uncertainty value as g(x; \u03b8 (t) ) = m t \u00d7a(x; \u03b8 (t) )+(1\u2212m t )\u00d7g(x; \u03b8 (t\u22121) ).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_56",
            "content": "(12) Then, we use Eq. ( 12) to sample low-uncertainty data for self-training as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_57",
            "content": "S (t) = bottom-k x i \u2208C (t) s g(x i , \u03b8 (t) ), y = argmax j\u2208Y f (x; \u03b8 (t) ) j . (13",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_58",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_59",
            "content": "By aggregating the prediction results over previous rounds, we filter the sample with inconsistent predictions to suppress noisy labels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_60",
            "content": "Model Learning and Update",
            "ntype": "title",
            "meta": {
                "section": "2.5"
            }
        },
        {
            "ix": "129-ARR_v1_61",
            "content": "After obtaining both the labeled data and pseudolabeled data, we fine-tune a new pre-trained BERT model \u03b8 (t+1) on them. Although we only include low-uncertainty samples during self-training, it is difficult to eliminate all the wrong pseudo-labels, and such mislabeled samples can still hurt model performance. To suppress such label noise, we use a threshold-based strategy to further remove noisy labels by selecting samples that agree with the corresponding pseudo labels. The loss objective of optimizing \u03b8 (t+1) is",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_62",
            "content": "LST = 1 |XL \u222a Q (t) | x i \u2208X L \u222aQ (t) CE f (xi; \u03b8 (t+1) ), yi + \u03bb |S (t) | x i \u2208S (t) \u03c9i CE f ( xi; \u03b8 (t+1) ), yi , (14)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_63",
            "content": "where \u03bb is a hyper-parameter balancing the weight between clean and pseudo labels, and \u03c9 i = 1{ f (x i ; \u03b8 (t+1) ) y i > \u03b3} stands for the thresholding function. Complexity Analysis. The running time of AC-TUNE is mainly consisted of two parts: the inference time O(|X u |) and the time for K-Means clustering O(dK|X u |), where d is the dimension of the BERT feature v. Note that the clustering can be efficiently implemented with FAISS (Johnson et al., 2019), and will not excessively increase the total running time. For self-training, the size of the memory bank g(x; \u03b8) is proportional to |X u |, while the extra computation of maintaining this dictionary is ignorable since we do not inference over the unlabeled data for multiple times in each round as BALD (Gal et al., 2017) does. The running time of ACTUNE will be shown in section C. Note that when compared with active learning baselines, we do not augment the train set with pseudolabeled data (Eq. ( 9)) to ensure fair comparisons.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_64",
            "content": "Main Result",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "129-ARR_v1_65",
            "content": "Figure 1 reports the performance of ACTUNE and the baselines on 4 benchmarks. From the results, we have the following observations:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_66",
            "content": "\u2022 ACTUNE consistently outperforms baselines in most of the cases. Different from studies in the computer vision (CV) domain (Sim\u00e9oni et al., 2020) where the model does not perform well in the low-data regime, pre-trained LM has achieved competitive performance with only a few labeled data, which makes further improvements to the vanilla fine-tuning challenging. Nevertheless, AC-TUNE surpasses baselines in more than 90% of the rounds and achieves 0.4%-0.7% and 0.3%-1.5% absolute gain at the end of AL and SSAL respectively. Figure 2 quantitatively measures the number of labels needed for the most advanced active learning model and self-training model (UST) to outperform ACTUNE with 1000 labels. These baselines need >2000 clean labeled samples to reach the performance as ours. ACTUNE saves on average 56.2% and 57.0% of the labeled samples than most advanced active learning and selftraining baselines respectively, which justifies its promising performance under low-resource scenarios. Such improvements show the merits of two key designs under our active self-training framework: the region-aware sampling for active learning and the momentum-based memory bank for robust selftraining, which will be discussed in the section 3.5. \u2022 Compared with the previous AL baselines, AC-TUNE can bring consistent performance gain, while previous semi-supervised active learning methods cannot. For instance, BASS is based on BALD for active learning, but sometimes it performs even worse than BALD with the same number of labeled data (see Fig. 5(b) and Fig. 1(f)). This is mainly because previous methods simply combine noisy pseudo labels with clean labels for training without explicitly rectifying the wrongly-labeled data, which will cause the LM to overfit these hazardous labels. Moreover, previous methods do not exploit momentum updates to stabilize the learning process, as there are oscillations in the beginning rounds. In contrast, ACTUNE achieves a more stable learning process and enables an active selftraining process to benefit from more labeled data. \u2022 The self-training methods (ST & UST) achieve superior performance with limited labels. However, they mainly focus on leveraging unlabeled data for improving the performance, while our results demonstrate that adaptive selecting the most useful data for fine-tuning is also important for improving the performance. With a powerful querying policy, ACTUNE can improve these self-training baselines by 1.05% in terms of accuracy on average.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_67",
            "content": "Extension to Weakly-supervised Learning",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "129-ARR_v1_68",
            "content": "ACTUNE can be naturally extended to weaklysupervised classification, where X l is a set of data annotated by linguistic patterns or rules. Since the initial label set is noisy, then the model trained with Eq. ( 1) will overfit to the label noise, and we can actively query labeled data to refine the model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_69",
            "content": "We conduct experiments on the TREC and Chemprot dataset 4 , where we first use Snorkel (Ratner et al., 2017) to obtain weak label set X l , then fine-tune the pre-trained LM f (\u03b8 (0) ) on X l . After that, we adopt ACTUNE for active self-training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_70",
            "content": "Fig. 5 shows the results of these two datasets 5 . When combining ACTUNE with CAL, the performance is unsatisfactory. We argue it is because CAL requires clean labels to calculate uncertainties. When labels are inaccurate, it will prevent AC-TUNE from querying informative samples. In contrast, ACTUNE achieves the best performance over baselines when using Entropy as the uncertainty measure. The performance gain is more notable on the TREC dataset, where we achieve 96.68% accuracy, close to the fully supervised performance (96.80%) with only \u223c6% of clean labels. self-training, ACTUNE can further boost the performance. This indicates that ACTUNE is a general active self-training approach, as it can serve as an efficient plug-in module for existing AL methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_71",
            "content": "Combination with Other AL Methods",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "129-ARR_v1_72",
            "content": "Ablation and Hyperparameter Study",
            "ntype": "title",
            "meta": {
                "section": "3.5"
            }
        },
        {
            "ix": "129-ARR_v1_73",
            "content": "The Effect of Different Components in AC-TUNE. We inspect different components of ACTUNE, including the region-sampling (RS), momentum-based memory bank (MMB), and weighted clustering (WClus) 6 . Experimental results (Fig. 4(b)) shows that all the three components contribute to the final performance, as removing any of them hurts the classification accuracy. Also, we find that when removing MMB, the performance hurts most in the beginning rounds, which indicates that MMB effectively suppresses label noise when the model's capacity is weak. Con-versely, removing WClus hurts the performance on later rounds, as it enables the model to select most informative samples.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_74",
            "content": "Hyperparameter Study. We study two hyperparameters, namely \u03b2 and K used in querying labels. Figure 6(e) and 6(f) show the results. In general, the model is insensitive to \u03b2 as the performance difference is less than 0.6%. The model cannot perform well with smaller K since it cannot pinpoint to high-uncertainty regions. For larger K, the performance also drops as some of the high-uncertainty regions can be outliers and sampling from them would hurt the model performance (Karamcheti et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_75",
            "content": "A Closer Look at the Momentum-based Memory Bank. To examine the role of MMB, we show the overall accuracy of pseudo-labels on AG News dataset in Fig. 6(g). From the result, it is clear that the momentum-based memory bank can stabilize the active self-training process, as the accuracy of pseudo labels increases around 1%, especially in later rounds. Fig 6(h) and 3(e) illustrates the model performance with different m L and m H . Overall, we find that our model is robust to different choices as ACTUNE outperform the baseline without momentum update consistently. Moreover, we find that the larger m H will generally lead to better performance in later rounds. This is mainly because in later rounds, the model's prediction is more reliable. Conversely, at the beginning of the training, the model's prediction might be oscillating on unlabeled data. In this case, using a smaller m L will favor samples with consistent predictions Value, mL=0.7, mH=0.9 Value, mL=0.9, mH=0.9 Prob, m=0.7, mH=0.9 Prob, m=0.9, mH=0.9 No Momentum (d) Entropy Value, mL=0.7, mH=0.9 Value, mL=0.9, mH=0.9 Prob, m=0.7, mH=0.9 Prob, m=0.9, mH=0.9",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_76",
            "content": "No Momentum (e) CAL to improve the robustness of active self-training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_77",
            "content": "Another finding is that for different AL methods, the optimal memory bank can be different. For Entropy, probability-based memory bank leads to a better result, while for CAL, simple aggregating over uncertainty score achieves better performance. This is mainly because the method used in CAL is more complicated, and using probability-based memory bank may hurt the uncertainty calculation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_78",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "129-ARR_v1_79",
            "content": "Active Learning. Active learning has been widely applied to various NLP tasks (Yuan et al., 2020;Zhao et al., 2020;Shelmanov et al., 2021;Karamcheti et al., 2021). So far, AL methods can be categorized into uncertainty-based methods (Gal et al., 2017;Margatina et al., 2021a,b), diversitybased methods (Ru et al., 2020;Sener and Savarese, 2018) and hybrid methods (Yuan et al., 2020;Ash et al., 2020;Kirsch et al., 2019). Ein-Dor et al. (2020) offer an empirical study of active learning with PLMs. In our study, we leverage the power of unlabeled instances via self-training to further promote the performance of AL. Semi-supervised Active Learning (SSAL). Gao et al. (2020); Song et al. (2019); Guo et al. (2021) design query strategies for specific semi-supervised methods, Tomanek and Hahn (2009); Rottmann et al. (2018); Sim\u00e9oni et al. (2020) exploit the mostcertain samples from the unlabeled with pseudolabeling to augment the training set. So far, most of the SSAL approaches are designed for CV domain and it remains unknown how this paradigm performs with PLMs on NLP tasks. In contrast, we propose ACTUNE to effectively leverage unlabeled data during finetuing PLMs for NLP tasks. Self-training. Self-training first generates pseudo labels for high-confidence samples, then fits a new model on pseudo labeled data to improve the generalization ability (Rosenberg et al., 2005;Lee, 2013). However, it is known to be vulnerable to error propagation (Arazo et al., 2020;Rizve et al., 2021). To alleviate this, we adopt a simple momentumbased method to select high confidence samples, effectively reducing the pseudo labels noise for active learning. Note that although Mukherjee and Awadallah (2020); Rizve et al. ( 2021) also leverage uncertainty information for self-training, their focus is on developing better self-training methods, while we aim to jointly query high-uncertainty samples and generate pseudo-labels for low-uncertainty samples. The experiments in Sec. 3 show that with appropriate querying methods, ACTUNE can further improve the performance of self-training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_80",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "129-ARR_v1_81",
            "content": "In this paper, we develop ACTUNE, a general active self-training framework for enhancing both label efficiency and model performance in fine-tuning pre-trained language models (PLMs). We propose a region-aware sampling approach to guarantee both the uncertainty the diversity for querying labels. To combat the label noise propagation issue, we design a momentum-based memory bank to effectively utilize the model predictions for preceding AL rounds. Empirical results on 6 public text classification benchmarks suggest the superiority of ACTUNE to conventional active learning and semi-supervised active learning methods for fine-tuning PLMs with limited resources. (Holub et al., 2008) 461s 646s BALD (Gal et al., 2017) 4595s 6451s ALPS (Yuan et al., 2020) 488s 677s BADGE (Ash et al., 2020) 554s 1140s CAL (Margatina et al., 2021b) 493s 688s REVIVAL (Guo et al., Our implementation of ACTUNE will be published upon acceptance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_82",
            "content": "We use AdamW (Loshchilov and Hutter, 2019) 2. Specifically, we search T 1 from 10 to 2000, T 2 from 1000 to 5000, T 3 from 10 to 500, \u03be from 0 to 1, and \u03bb from 0 to 0.5. All results are reported as the average over three runs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_83",
            "content": "In our experiments, we keep \u03b2 = 0.5, \u03bb = 1 for all datasets. For other parameters, we use a grid search to find the optimal setting for each datasets. Specifically, we search \u03b3 from [0.5, 0.6, 0.7], m L from [0.6, 0.7, 0.8], m H from [0.8, 0.9, 1]. For AC-TUNE with Entropy, we use probability based aggregation and for ACTUNE with CAL, we use value based aggregation by default.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_84",
            "content": "For other SSAL methods, we mainly tune their key hyperparameters. Note that Entropy (Holub et al., 2008), BALD (Gal et al., 2017), ALPS (Yuan et al., 2020), BADGE (Ash et al., 2020) do not introduce any new hyperparameters. For CAL (Margatina et al., 2021b), we tune the number for KNN k from [5,10,20] and report the best performance. For ST (Lee, 2013), CEAL (Wang et al., 2016) & BASS (Rottmann et al., 2018), it uses a threshold \u03b4 for selecting high-confidence data. We tune \u03b4 from [0.6, 0.7, 0.8, 0.9] to report the best performance. For UST (Mukherjee and Awadallah, 2020), we tune the number of lowuncertainty samples used in the next round from [1024,2048,4096]. For COSINE (Yu et al., 2021), we set the weight for confidence regularization \u03bb as 0.1, the threshold \u03c4 for selecting high-confidence data from [0.7, 0.9] and the update period of selftraining from [50,100,150]. For REVIVAL (Guo et al., 2021), it calculates uncertainty with adversarial perturbation, we tune the size of the perturbation from [1e \u2212 3, 1e \u2212 4, 1e \u2212 5].",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_85",
            "content": "C Runtime Analysis.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_86",
            "content": "Table 3 shows the time in one active learning round of ACTUNE and baselines. Here we highlight that the additional time for region-aware sampling and momentum-based memory bank is rather small compared with the inference time. Among all baselines, we find that the running time of clustering-based method is faster than the original reported time in the paper. This is because we use FAISS (Johnson et al., 2019) instead of SKLearn (Pedregosa et al., 2011) for clustering, which accelerates the clustering step significantly. Also, we find that BALD and REVIVAL are not so efficient. For BALD, it needs to infer the uncertainty of the model by passing the data to model with multitple times. Such an operation will make the total inference time for PLMs very long. For REVIVAL, we find that calculating the adversarial gradient needs extra forward passes and backward passes, which could be time-consuming for PLMs with millions of parameters 7 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_87",
            "content": "First, since our focus is on fine-tuning pre-trained language models, we use the representation of [CLS] token for classification. In the future work, we can consider using prompt tuning (Gao et al., 2021;Schick and Sch\u00fctze, 2021), a more dataefficient method for adopting pre-trained language models on classification tasks to further promote the efficiency. Also, due to the computational resource constraints, we do not use larger pre-trained language models such as RoBERTa-large (Liu et al.,7 The original model is proposed with CV tasks and they use ResNet-18 as the backbone which only contains 11M parameters (around 10% of the parameters of Roberta-base). 2019) which shown even better performance with only a few labels (Du et al., 2021). Last, apart from the text classification task, we can also extend our work into other tasks such as sequence labeling and natural language inference.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_88",
            "content": "Here we give an example of our querying strategy on AG News and Pubmed dataset for the 1st round of active self-training process in figure 6. Note that we use t-SNE algorithm (Van der Maaten and Hinton, 2008) for dimension reduction, and the black triangle stands for the queried samples while other circles stands for the unlabeled data. Different colors stands for different classes. From the comparision, we can see that the existing uncertainty based methods such as Entropy and CAL, are suffered from the issue of limited diversity. However, when combined with ACTUNE, the diversity is much improved. Such results, compared with the main results in figure 1, demonstrate the efficacy of ACTUNE empirically.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v1_89",
            "content": "Eric Arazo, Diego Ortego, Paul Albert, E O' Noel, Kevin Connor,  Mcguinness, Pseudolabeling and confirmation bias in deep semisupervised learning, 2020, 2020 International Joint Conference on Neural Networks (IJCNN), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Eric Arazo",
                    "Diego Ortego",
                    "Paul Albert",
                    "E O' Noel",
                    "Kevin Connor",
                    " Mcguinness"
                ],
                "title": "Pseudolabeling and confirmation bias in deep semisupervised learning",
                "pub_date": "2020",
                "pub_title": "2020 International Joint Conference on Neural Networks (IJCNN)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "129-ARR_v1_90",
            "content": "David Arthur, Sergei Vassilvitskii, K-means++: The advantages of careful seeding, 2007, Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "David Arthur",
                    "Sergei Vassilvitskii"
                ],
                "title": "K-means++: The advantages of careful seeding",
                "pub_date": "2007",
                "pub_title": "Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_91",
            "content": "Jordan Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, Alekh Agarwal, Deep batch active learning by diverse, uncertain gradient lower bounds, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Jordan Ash",
                    "Chicheng Zhang",
                    "Akshay Krishnamurthy",
                    "John Langford",
                    "Alekh Agarwal"
                ],
                "title": "Deep batch active learning by diverse, uncertain gradient lower bounds",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_92",
            "content": "Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai, Andrew Mccallum, Self-supervised meta-learning for few-shot natural language classification tasks, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Trapit Bansal",
                    "Rishikesh Jha",
                    "Tsendsuren Munkhdalai",
                    "Andrew Mccallum"
                ],
                "title": "Self-supervised meta-learning for few-shot natural language classification tasks",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "129-ARR_v1_93",
            "content": "Iz Beltagy, Kyle Lo, Arman Cohan, SciB-ERT: A pretrained language model for scientific text, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Iz Beltagy",
                    "Kyle Lo",
                    "Arman Cohan"
                ],
                "title": "SciB-ERT: A pretrained language model for scientific text",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_94",
            "content": "Jonathan Bragg, Arman Cohan, Kyle Lo, Iz Beltagy, Flex: Unifying evaluation for few-shot nlp, 2021, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Jonathan Bragg",
                    "Arman Cohan",
                    "Kyle Lo",
                    "Iz Beltagy"
                ],
                "title": "Flex: Unifying evaluation for few-shot nlp",
                "pub_date": "2021",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_95",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Language models are fewshot learners, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Tom Brown",
                    "Benjamin Mann",
                    "Nick Ryder",
                    "Melanie Subbiah",
                    "Jared Kaplan",
                    "Prafulla Dhariwal",
                    "Arvind Neelakantan"
                ],
                "title": "Language models are fewshot learners",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_96",
            "content": "Franck Dernoncourt, Ji Lee, PubMed 200k RCT: a dataset for sequential sentence classification in medical abstracts, 2017, Proceedings of the Eighth International Joint Conference on Natural Language Processing, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Franck Dernoncourt",
                    "Ji Lee"
                ],
                "title": "PubMed 200k RCT: a dataset for sequential sentence classification in medical abstracts",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Eighth International Joint Conference on Natural Language Processing",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "129-ARR_v1_97",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "129-ARR_v1_98",
            "content": "UNKNOWN, None, 2002, 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": null,
                "title": null,
                "pub_date": "2002",
                "pub_title": "2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_99",
            "content": "Rotem Dror, Gili Baumer, Segev Shlomov, Roi Reichart, The hitchhiker's guide to testing statistical significance in natural language processing, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Rotem Dror",
                    "Gili Baumer",
                    "Segev Shlomov",
                    "Roi Reichart"
                ],
                "title": "The hitchhiker's guide to testing statistical significance in natural language processing",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "129-ARR_v1_100",
            "content": "Jingfei Du, Edouard Grave, Beliz Gunel, Vishrav Chaudhary, Onur Celebi, Michael Auli, Veselin Stoyanov, Alexis Conneau, Self-training improves pre-training for natural language understanding, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Jingfei Du",
                    "Edouard Grave",
                    "Beliz Gunel",
                    "Vishrav Chaudhary",
                    "Onur Celebi",
                    "Michael Auli",
                    "Veselin Stoyanov",
                    "Alexis Conneau"
                ],
                "title": "Self-training improves pre-training for natural language understanding",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_101",
            "content": "Alon Liat Ein-Dor, Ariel Halfon, Eyal Gera, Lena Shnarch, Leshem Dankin, Marina Choshen, Ranit Danilevsky, Yoav Aharonov, Noam Katz,  Slonim, Active Learning for BERT: An Empirical Study, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Alon Liat Ein-Dor",
                    "Ariel Halfon",
                    "Eyal Gera",
                    "Lena Shnarch",
                    "Leshem Dankin",
                    "Marina Choshen",
                    "Ranit Danilevsky",
                    "Yoav Aharonov",
                    "Noam Katz",
                    " Slonim"
                ],
                "title": "Active Learning for BERT: An Empirical Study",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_102",
            "content": "UNKNOWN, None, 2015, Bayesian convolutional neural networks with bernoulli approximate variational inference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Bayesian convolutional neural networks with bernoulli approximate variational inference",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_103",
            "content": "Yarin Gal, Riashat Islam, Zoubin Ghahramani, Deep bayesian active learning with image data, 2017, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Yarin Gal",
                    "Riashat Islam",
                    "Zoubin Ghahramani"
                ],
                "title": "Deep bayesian active learning with image data",
                "pub_date": "2017",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "129-ARR_v1_104",
            "content": "Mingfei Gao, Zizhao Zhang, Guo Yu, \u00d6 Sercan,  Ar\u0131k, S Larry, Tomas Davis,  Pfister, Consistency-based semi-supervised active learning: Towards minimizing labeling cost, 2020, European Conference on Computer Vision, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Mingfei Gao",
                    "Zizhao Zhang",
                    "Guo Yu",
                    "\u00d6 Sercan",
                    " Ar\u0131k",
                    "S Larry",
                    "Tomas Davis",
                    " Pfister"
                ],
                "title": "Consistency-based semi-supervised active learning: Towards minimizing labeling cost",
                "pub_date": "2020",
                "pub_title": "European Conference on Computer Vision",
                "pub": "Springer"
            }
        },
        {
            "ix": "129-ARR_v1_105",
            "content": "Tianyu Gao, Adam Fisch, Danqi Chen, Making pre-trained language models better few-shot learners, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Tianyu Gao",
                    "Adam Fisch",
                    "Danqi Chen"
                ],
                "title": "Making pre-trained language models better few-shot learners",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "129-ARR_v1_106",
            "content": "Jiannan Guo, Haochen Shi, Yangyang Kang, Kun Kuang, Siliang Tang, Zhuoren Jiang, Changlong Sun, Fei Wu, Yueting Zhuang, Semisupervised active learning for semi-supervised models: Exploit adversarial examples with graph-based virtual labels, 2021, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Jiannan Guo",
                    "Haochen Shi",
                    "Yangyang Kang",
                    "Kun Kuang",
                    "Siliang Tang",
                    "Zhuoren Jiang",
                    "Changlong Sun",
                    "Fei Wu",
                    "Yueting Zhuang"
                ],
                "title": "Semisupervised active learning for semi-supervised models: Exploit adversarial examples with graph-based virtual labels",
                "pub_date": "2021",
                "pub_title": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_107",
            "content": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Momentum contrast for unsupervised visual representation learning, 2020, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Kaiming He",
                    "Haoqi Fan",
                    "Yuxin Wu",
                    "Saining Xie",
                    "Ross Girshick"
                ],
                "title": "Momentum contrast for unsupervised visual representation learning",
                "pub_date": "2020",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_108",
            "content": "Alex Holub, Pietro Perona, Michael C Burl, Entropy-based active learning for object recognition, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Alex Holub",
                    "Pietro Perona",
                    "Michael C Burl"
                ],
                "title": "Entropy-based active learning for object recognition",
                "pub_date": "2008",
                "pub_title": "2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops",
                "pub": "IEEE"
            }
        },
        {
            "ix": "129-ARR_v1_109",
            "content": "Peiyun Hu, Zack Lipton, Anima Anandkumar, Deva Ramanan, Active learning with partial feedback, 2019, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Peiyun Hu",
                    "Zack Lipton",
                    "Anima Anandkumar",
                    "Deva Ramanan"
                ],
                "title": "Active learning with partial feedback",
                "pub_date": "2019",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_110",
            "content": "UNKNOWN, None, 2005, Automated variable weighting in k-means type clustering. IEEE transactions on pattern analysis and machine intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": null,
                "title": null,
                "pub_date": "2005",
                "pub_title": "Automated variable weighting in k-means type clustering. IEEE transactions on pattern analysis and machine intelligence",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_111",
            "content": "Jeff Johnson, Matthijs Douze, Herv\u00e9 J\u00e9gou, Billion-scale similarity search with gpus, 2019, IEEE Transactions on Big Data, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Jeff Johnson",
                    "Matthijs Douze",
                    "Herv\u00e9 J\u00e9gou"
                ],
                "title": "Billion-scale similarity search with gpus",
                "pub_date": "2019",
                "pub_title": "IEEE Transactions on Big Data",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_112",
            "content": "Katharina Kann, Kyunghyun Cho, Samuel , Towards realistic practices in lowresource natural language processing: The development set, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Katharina Kann",
                    "Kyunghyun Cho",
                    "Samuel "
                ],
                "title": "Towards realistic practices in lowresource natural language processing: The development set",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "129-ARR_v1_113",
            "content": "Siddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, Christopher Manning, Mind your outliers! investigating the negative impact of outliers on active learning for visual question answering, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Siddharth Karamcheti",
                    "Ranjay Krishna",
                    "Li Fei-Fei",
                    "Christopher Manning"
                ],
                "title": "Mind your outliers! investigating the negative impact of outliers on active learning for visual question answering",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "129-ARR_v1_114",
            "content": "Andreas Kirsch, Joost Van Amersfoort, Yarin Gal, Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning, 2019, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Andreas Kirsch",
                    "Joost Van Amersfoort",
                    "Yarin Gal"
                ],
                "title": "Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning",
                "pub_date": "2019",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_115",
            "content": "Martin Krallinger, Obdulia Rabal, A Saber,  Akhondi, Overview of the biocreative VI chemical-protein interaction track, 2017, BioCreative evaluation Workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Martin Krallinger",
                    "Obdulia Rabal",
                    "A Saber",
                    " Akhondi"
                ],
                "title": "Overview of the biocreative VI chemical-protein interaction track",
                "pub_date": "2017",
                "pub_title": "BioCreative evaluation Workshop",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_116",
            "content": "UNKNOWN, None, 2016, Temporal ensembling for semi-supervised learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Temporal ensembling for semi-supervised learning",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_117",
            "content": "Dong-Hyun Lee, Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks, 2013, ICML Workshop on challenges in representation learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Dong-Hyun Lee"
                ],
                "title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
                "pub_date": "2013",
                "pub_title": "ICML Workshop on challenges in representation learning",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_118",
            "content": "Xin Li, Dan Roth, Learning question classifiers, 2002, The 19th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Xin Li",
                    "Dan Roth"
                ],
                "title": "Learning question classifiers",
                "pub_date": "2002",
                "pub_title": "The 19th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_119",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_120",
            "content": "Ilya Loshchilov, Frank Hutter, Decoupled weight decay regularization, 2019, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Ilya Loshchilov",
                    "Frank Hutter"
                ],
                "title": "Decoupled weight decay regularization",
                "pub_date": "2019",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_121",
            "content": "UNKNOWN, None, , Loic Barrault, and Nikolaos Aletras. 2021a. Bayesian active learning with pretrained language models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Loic Barrault, and Nikolaos Aletras. 2021a. Bayesian active learning with pretrained language models",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_122",
            "content": "Katerina Margatina, Giorgos Vernikos, Active learning by acquiring contrastive examples, , Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Katerina Margatina",
                    "Giorgos Vernikos"
                ],
                "title": "Active learning by acquiring contrastive examples",
                "pub_date": null,
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "129-ARR_v1_123",
            "content": "Marius Mosbach, Maksym Andriushchenko, Dietrich Klakow, On the stability of fine-tuning {bert}: Misconceptions, explanations, and strong baselines, 2021, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Marius Mosbach",
                    "Maksym Andriushchenko",
                    "Dietrich Klakow"
                ],
                "title": "On the stability of fine-tuning {bert}: Misconceptions, explanations, and strong baselines",
                "pub_date": "2021",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_124",
            "content": "Subhabrata Mukherjee, Ahmed Awadallah, Uncertainty-aware self-training for few-shot text classification, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Subhabrata Mukherjee",
                    "Ahmed Awadallah"
                ],
                "title": "Uncertainty-aware self-training for few-shot text classification",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_125",
            "content": "Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Scikit-learn: Machine learning in python, 2011, Journal of machine Learning research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Fabian Pedregosa",
                    "Ga\u00ebl Varoquaux",
                    "Alexandre Gramfort",
                    "Vincent Michel",
                    "Bertrand Thirion",
                    "Olivier Grisel",
                    "Mathieu Blondel",
                    "Peter Prettenhofer",
                    "Ron Weiss",
                    "Vincent Dubourg"
                ],
                "title": "Scikit-learn: Machine learning in python",
                "pub_date": "2011",
                "pub_title": "Journal of machine Learning research",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_126",
            "content": "Alexander Ratner, H Stephen, Henry Bach, Jason Ehrenberg, Sen Fries, Christopher Wu,  R\u00e9, Snorkel: Rapid training data creation with weak supervision, 2017, Proceedings of the VLDB Endowment, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Alexander Ratner",
                    "H Stephen",
                    "Henry Bach",
                    "Jason Ehrenberg",
                    "Sen Fries",
                    "Christopher Wu",
                    " R\u00e9"
                ],
                "title": "Snorkel: Rapid training data creation with weak supervision",
                "pub_date": "2017",
                "pub_title": "Proceedings of the VLDB Endowment",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_127",
            "content": "Kevin Mamshad Nayeem Rizve,  Duarte, S Yogesh, Mubarak Rawat,  Shah, In defense of pseudo-labeling: An uncertainty-aware pseudolabel selection framework for semi-supervised learning, 2021, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Kevin Mamshad Nayeem Rizve",
                    " Duarte",
                    "S Yogesh",
                    "Mubarak Rawat",
                    " Shah"
                ],
                "title": "In defense of pseudo-labeling: An uncertainty-aware pseudolabel selection framework for semi-supervised learning",
                "pub_date": "2021",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_128",
            "content": "Chuck Rosenberg, Henry Hebert,  Schneiderman, Semi-supervised self-training of object detection models, 2005, Proceedings of the IEEE Workshops on Application of Computer Vision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Chuck Rosenberg",
                    "Henry Hebert",
                    " Schneiderman"
                ],
                "title": "Semi-supervised self-training of object detection models",
                "pub_date": "2005",
                "pub_title": "Proceedings of the IEEE Workshops on Application of Computer Vision",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_129",
            "content": "Matthias Rottmann, Karsten Kahl, Hanno Gottschalk, Deep bayesian active semisupervised learning, 2018, 17th IEEE International Conference on Machine Learning and Applications (ICMLA), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Matthias Rottmann",
                    "Karsten Kahl",
                    "Hanno Gottschalk"
                ],
                "title": "Deep bayesian active semisupervised learning",
                "pub_date": "2018",
                "pub_title": "17th IEEE International Conference on Machine Learning and Applications (ICMLA)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "129-ARR_v1_130",
            "content": "Dongyu Ru, Jiangtao Feng, Lin Qiu, Hao Zhou, Mingxuan Wang, Weinan Zhang, Yong Yu, Lei Li, Active sentence learning by adversarial uncertainty sampling in discrete space, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Dongyu Ru",
                    "Jiangtao Feng",
                    "Lin Qiu",
                    "Hao Zhou",
                    "Mingxuan Wang",
                    "Weinan Zhang",
                    "Yong Yu",
                    "Lei Li"
                ],
                "title": "Active sentence learning by adversarial uncertainty sampling in discrete space",
                "pub_date": "2020",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_131",
            "content": "Timo Schick, Hinrich Sch\u00fctze, Exploiting cloze-questions for few-shot text classification and natural language inference, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Timo Schick",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "Exploiting cloze-questions for few-shot text classification and natural language inference",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_132",
            "content": "Ozan Sener, Silvio Savarese, Active learning for convolutional neural networks: A core-set approach, 2018, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Ozan Sener",
                    "Silvio Savarese"
                ],
                "title": "Active learning for convolutional neural networks: A core-set approach",
                "pub_date": "2018",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_133",
            "content": "Artem Shelmanov, Dmitri Puzyrev, Lyubov Kupriyanova, Denis Belyakov, Daniil Larionov, Nikita Khromov, Olga Kozlova, Ekaterina Artemova, V Dmitry, Alexander Dylov,  Panchenko, Active learning for sequence tagging with deep pre-trained models and Bayesian uncertainty estimates, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Artem Shelmanov",
                    "Dmitri Puzyrev",
                    "Lyubov Kupriyanova",
                    "Denis Belyakov",
                    "Daniil Larionov",
                    "Nikita Khromov",
                    "Olga Kozlova",
                    "Ekaterina Artemova",
                    "V Dmitry",
                    "Alexander Dylov",
                    " Panchenko"
                ],
                "title": "Active learning for sequence tagging with deep pre-trained models and Bayesian uncertainty estimates",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_134",
            "content": "Oriane Sim\u00e9oni, Mateusz Budnik, Yannis Avrithis, Guillaume Gravier, Rethinking deep active learning: Using unlabeled data at model training, 2020, the 25th International Conference on Pattern Recognition (ICPR), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Oriane Sim\u00e9oni",
                    "Mateusz Budnik",
                    "Yannis Avrithis",
                    "Guillaume Gravier"
                ],
                "title": "Rethinking deep active learning: Using unlabeled data at model training",
                "pub_date": "2020",
                "pub_title": "the 25th International Conference on Pattern Recognition (ICPR)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "129-ARR_v1_135",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Richard Socher",
                    "Alex Perelygin",
                    "Jean Wu",
                    "Jason Chuang",
                    "Christopher Manning",
                    "Andrew Ng",
                    "Christopher Potts"
                ],
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "129-ARR_v1_136",
            "content": "UNKNOWN, None, 2019, Combining mixmatch and active learning for better accuracy with fewer labels, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Combining mixmatch and active learning for better accuracy with fewer labels",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_137",
            "content": "Katrin Tomanek, Udo Hahn, Semisupervised active learning for sequence labeling, 2009, Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Katrin Tomanek",
                    "Udo Hahn"
                ],
                "title": "Semisupervised active learning for sequence labeling",
                "pub_date": "2009",
                "pub_title": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_138",
            "content": "Laurens Van Der Maaten, Geoffrey Hinton, Visualizing data using t-sne, 2008, Journal of machine learning research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Laurens Van Der Maaten",
                    "Geoffrey Hinton"
                ],
                "title": "Visualizing data using t-sne",
                "pub_date": "2008",
                "pub_title": "Journal of machine learning research",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_139",
            "content": "Keze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang, Liang Lin, Cost-effective active learning for deep image classification, 2016, IEEE Transactions on Circuits and Systems for Video Technology, .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Keze Wang",
                    "Dongyu Zhang",
                    "Ya Li",
                    "Ruimao Zhang",
                    "Liang Lin"
                ],
                "title": "Cost-effective active learning for deep image classification",
                "pub_date": "2016",
                "pub_title": "IEEE Transactions on Circuits and Systems for Video Technology",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_140",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger,  Drame, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "Thomas Wolf",
                    "Lysandre Debut",
                    "Victor Sanh",
                    "Julien Chaumond",
                    "Clement Delangue",
                    "Anthony Moi",
                    "Pierric Cistac",
                    "Tim Rault",
                    "Remi Louf",
                    "Morgan Funtowicz",
                    "Joe Davison",
                    "Sam Shleifer",
                    "Clara Patrick Von Platen",
                    "Yacine Ma",
                    "Julien Jernite",
                    "Canwen Plu",
                    "Teven Xu",
                    "Sylvain Scao",
                    "Mariama Gugger",
                    " Drame"
                ],
                "title": "Transformers: State-of-the-art natural language processing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_141",
            "content": "Yue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao, Chao Zhang, Fine-tuning pretrained language model with weak supervision: A contrastive-regularized self-training approach, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": [
                    "Yue Yu",
                    "Simiao Zuo",
                    "Haoming Jiang",
                    "Wendi Ren",
                    "Tuo Zhao",
                    "Chao Zhang"
                ],
                "title": "Fine-tuning pretrained language model with weak supervision: A contrastive-regularized self-training approach",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "129-ARR_v1_142",
            "content": "Michelle Yuan, Hsuan-Tien Lin, Jordan Boyd-Graber, Cold-start active learning through self-supervised language modeling, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": [
                    "Michelle Yuan",
                    "Hsuan-Tien Lin",
                    "Jordan Boyd-Graber"
                ],
                "title": "Cold-start active learning through self-supervised language modeling",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "129-ARR_v1_143",
            "content": "Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, Alexander Ratner, WRENCH: A comprehensive benchmark for weak supervision, 2021, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, .",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": [
                    "Jieyu Zhang",
                    "Yue Yu",
                    "Yinghao Li",
                    "Yujing Wang",
                    "Yaming Yang",
                    "Mao Yang",
                    "Alexander Ratner"
                ],
                "title": "WRENCH: A comprehensive benchmark for weak supervision",
                "pub_date": "2021",
                "pub_title": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_144",
            "content": "UNKNOWN, None, 2020, Revisiting few-sample bert fine-tuning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b55",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Revisiting few-sample bert fine-tuning",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_145",
            "content": "Xiang Zhang, Junbo Zhao, Yann Lecun, Character-level convolutional networks for text classification, 2015, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b56",
                "authors": [
                    "Xiang Zhang",
                    "Junbo Zhao",
                    "Yann Lecun"
                ],
                "title": "Character-level convolutional networks for text classification",
                "pub_date": "2015",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v1_146",
            "content": "Yuekai Zhao, Haoran Zhang, Shuchang Zhou, Zhihua Zhang, Active learning approaches to enhancing neural machine translation, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b57",
                "authors": [
                    "Yuekai Zhao",
                    "Haoran Zhang",
                    "Shuchang Zhou",
                    "Zhihua Zhang"
                ],
                "title": "Active learning approaches to enhancing neural machine translation",
                "pub_date": "2020",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "129-ARR_v1_0@0",
            "content": "ACTUNE: Uncertainty-Aware Active Self-Training for Active Fine-Tuning of Pretrained Language Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_0",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_2@0",
            "content": "Although fine-tuning pre-trained language models (PLMs) renders strong performance in many NLP tasks, it relies on excessive labeled data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_2",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_2@1",
            "content": "Recently, researchers have resorted to active fine-tuning for enhancing the label efficiency of PLM fine-tuning, but existing methods of this type usually ignore the potential of unlabeled data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_2",
            "start": 139,
            "end": 332,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_2@2",
            "content": "We develop ACTUNE, a new framework that improves the label efficiency of active PLM fine-tuning by unleashing the power of unlabeled data via self training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_2",
            "start": 334,
            "end": 489,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_2@3",
            "content": "AC-TUNE switches between data annotation and model self-training based on uncertainty: the unlabeled samples of high-uncertainty are selected for annotation, while the ones from lowuncertainty regions are used for model selftraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_2",
            "start": 491,
            "end": 723,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_2@4",
            "content": "Additionally, we design (1) a regionaware sampling strategy to avoid redundant samples when querying annotations and (2) a momentum-based memory bank to dynamically aggregate the model's pseudo labels to suppress label noise in self-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_2",
            "start": 725,
            "end": 966,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_2@5",
            "content": "Experiments on 6 text classification datasets show that ACTUNE outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM fine-tuning by 56.2% on average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_2",
            "start": 968,
            "end": 1173,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_4@0",
            "content": "Fine-tuning pre-trained language models (PLMs) has achieved enormous success in natural language processing (NLP) (Devlin et al., 2019;Liu et al., 2019;Brown et al., 2020), one of which is the competitive performance it offers when consuming only a few labeled data (Bansal et al., 2020;Gao et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_4",
            "start": 0,
            "end": 304,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_4@1",
            "content": "However, there are still significant gaps between few-shot and fully-supervised PLM fine-tuning in many classification tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_4",
            "start": 306,
            "end": 430,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_4@2",
            "content": "Besides, the performance of few-shot PLM fine-tuning can vary substantially with different sets of training data (Bragg et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_4",
            "start": 432,
            "end": 565,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_4@3",
            "content": "Therefore, there is a crucial need for PLM fine-tuning approaches with better label-efficiency and being robust to selection of training data, especially for applications where labeled data are scarce and expensive to obtain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_4",
            "start": 567,
            "end": 791,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_5@0",
            "content": "Towards this goal, researchers have resorted to active fine-tuning of PLMs and achieved comparable performance to fully-supervised methods with much less annotated samples (Ein-Dor et al., 2020;Margatina et al., 2021a,b;Yuan et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_5",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_5@1",
            "content": "Nevertheless, they usually neglect unlabeled data, which can be useful for improving label efficiency for PLM fine-tuning (Du et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_5",
            "start": 240,
            "end": 379,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_5@2",
            "content": "To leverage those unlabeled data to improve label efficiency of active learning, efforts have been made in the semi-supervised active learning literature (Wang et al., 2016;Rottmann et al., 2018;Sim\u00e9oni et al., 2020), but the proposed query strategies can return highly redundant samples due to limited representation power, resulting in suboptimal label efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_5",
            "start": 381,
            "end": 746,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_5@3",
            "content": "Moreover, they usually rely on pseudo-labeling to utilize unlabeled data, which requires greater (yet often absent) care to denoise the pseudo labels, otherwise the errors could accumulate and deteriorate the model performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_5",
            "start": 748,
            "end": 974,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_5@4",
            "content": "This phenomenon can be even more severe for PLMs, as the finetuning process often suffers from the instability issue caused by different weight initialization and data orders (Dodge et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_5",
            "start": 976,
            "end": 1171,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_5@5",
            "content": "Thus, it still remains open and challenging to design robust and label efficient method for active PLM fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_5",
            "start": 1173,
            "end": 1287,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_6@0",
            "content": "To tackle above challenges, we propose AC-TUNE, a new method that improves the label efficiency and robustness of active PLM fine-tuning with self-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_6",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_6@1",
            "content": "Based on the estimated uncertainty of data, ACTUNE chooses from one of the following cases in each learning round: (1) when the average uncertainty of a region is low, we trust the model's prediction and select most certain predictions within the region for self-training;",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_6",
            "start": 157,
            "end": 428,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_7@0",
            "content": "(2) when the average uncertainty of a region is high, indicating inadequate observations for parameter learning, we actively annotate most uncertain samples within the region to improve the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_7",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_7@1",
            "content": "Different from existing AL methods that only leverage uncertainty for querying labels, our uncertaintydriven self-training paradigm gradually unleash the data with low uncertainty via self-training, while reducing the chance of error propagation triggered by highly-uncertain mis-labeled data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_7",
            "start": 197,
            "end": 489,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_8@0",
            "content": "To further boost the performance on downstream tasks, we design two techniques, namely regionaware sampling (RS) and momentum-based memory bank (MMB) to improve the query strategies and suppress label noise for ACTUNE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_8",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_8@1",
            "content": "Inspired by the fact that existing uncertainty-based AL methods often end up choosing uncertain yet repetitive data (Ein-Dor et al., 2020;Margatina et al., 2021b), we design a region-aware sampling technique to promote both diversity and representativeness by leveraging the representation power of PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_8",
            "start": 219,
            "end": 522,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_8@2",
            "content": "Specifically, we first estimate the uncertainties of the unlabeled data with PLMs, then cluster the data using their PLM representations and weigh the data by the corresponding uncertainty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_8",
            "start": 524,
            "end": 712,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_8@3",
            "content": "Such a clustering scheme partitions the embedding space into small sub-regions with an emphasis on highlyuncertain samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_8",
            "start": 714,
            "end": 836,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_8@4",
            "content": "Finally, by sampling over multiple high-uncertainty regions, our strategy selects data with high uncertainty and low redundancy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_8",
            "start": 838,
            "end": 965,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_9@0",
            "content": "To rectify the erroneous pseudo labels derived by self-training, we design a simple but effective way to select low-uncertainty data for self-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_9",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_9@1",
            "content": "Our method is motivated by the fact that fine-tuning PLMs suffer from instability issues -distinct initializations and data orders can result in a large variance of the task performance (Dodge et al., 2020;Mosbach et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_9",
            "start": 152,
            "end": 379,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_9@2",
            "content": "However, previous approaches only select pseudo-labeled data based on the prediction of the current round and therefore are less reliable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_9",
            "start": 381,
            "end": 518,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_9@3",
            "content": "In contrast, we maintain a dynamic memory bank to save the predictions of unlabeled samples for later use.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_9",
            "start": 520,
            "end": 625,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_9@4",
            "content": "we propose a momentum updating method to dynamically aggregate the predictions from preceding rounds (Laine and Aila, 2016) and select low-uncertainty samples based on aggregated prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_9",
            "start": 627,
            "end": 816,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_9@5",
            "content": "As a consequence, only the samples with high prediction confidence over multiple rounds will be used for self-training, which mitigates the issue of label noise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_9",
            "start": 818,
            "end": 978,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_9@6",
            "content": "We highlight that our active self-training approach is an efficient substitution to existing AL methods, requiring ignorable extra computational cost.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_9",
            "start": 980,
            "end": 1129,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_10@0",
            "content": "Our key contributions are: (1) an active selftraining paradigm ACTUNE that integrates the benefit of self-training and active learning in a principled way to minimize the labeling cost for finetuning PLMs; (2) a region-aware querying strategy to enforce both the informativeness and the diversity of queried samples during AL; (3) a simple and effective momentum-based method to harness the predictions for preceding rounds to alleviate the label noise in self-training and (4) experiments on 6 benchmarks demonstrating ACTUNE improves the label efficiency over existing self-training and active learning baselines by 56.2%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_10",
            "start": 0,
            "end": 623,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_11@0",
            "content": "2 Uncertainty-aware Active Self-training",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_11",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_12@0",
            "content": "Problem Formulation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_12",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_13@0",
            "content": "We study active fine-tuning of pre-trained language models for text classification, formulated as follows: Given a small number of labeled samples X l = {(x i , y i )} L i=1 and unlabeled samples",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_13",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_14@0",
            "content": "X u = {x j } U j=1 (|X l | |X u |)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_14",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_15@0",
            "content": ", we aim to fine-tune a pre-trained language model f (x; \u03b8) : X \u2192 Y in an interactive way: we perform active self-training for T rounds with the total labeling budget b. In each round, we aim to query B = b/T samples denoted as B from X u to fine-tune a pre-trained language model f (x; \u03b8) with both X l , B and X u to maximize the performance on downstream text classification tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_15",
            "start": 0,
            "end": 383,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_15@1",
            "content": "Here X = X l \u222a X u denotes all samples and Y = {1, 2, \u2022 \u2022 \u2022 , C} is the label set, where C is the number of classes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_15",
            "start": 385,
            "end": 500,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_16@0",
            "content": "Overview of ACTUNE Framework",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_16",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_17@0",
            "content": "We now present our active self-training paradigm ACTUNE underpinned by estimated uncertainty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_17",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_17@1",
            "content": "We begin the active self-training loop by finetuning a BERT f (\u03b8 (0) ) on the initial labeled data X L .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_17",
            "start": 94,
            "end": 197,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_17@2",
            "content": "Formally, we solve the following optimization problem",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_17",
            "start": 199,
            "end": 251,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_18@0",
            "content": "min \u03b8 1 |X L | (x i ,y i )\u2208X L CE f (x i ; \u03b8 (0) ), y i , (1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_18",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_19@0",
            "content": "In round t (1 \u2264 t \u2264 T ) of the active self-training procedure, we first calculate the uncertainty score based on a given function a (t) i = a(x i , \u03b8 (t) ) 1 for all x i \u2208 X u .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_19",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_19@1",
            "content": "Then, we query labeled samples and generate pseudo-labels for unlabeled data X u simultaneously to facilitate self-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_19",
            "start": 178,
            "end": 301,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_19@2",
            "content": "For each sample x i , the pseudo-label y is calculated based on the current model's output: 3), ( 4)) until convergence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_19",
            "start": 303,
            "end": 422,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_19@3",
            "content": "2. Select sample set Q (t) for AL and S (t) for self-training from Xu based on Eq. ( 11) or (13).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_19",
            "start": 424,
            "end": 520,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_19@4",
            "content": "3. Augment the labeled set XL = XL \u222a Q (t)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_19",
            "start": 522,
            "end": 563,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_19@5",
            "content": ".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_19",
            "start": 565,
            "end": 565,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_19@6",
            "content": "4. Obtain \u03b8 (t) by finetuning f (\u2022; \u03b8 t ) with LST ( Eq. ( 14)) using AdamW.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_19",
            "start": 567,
            "end": 642,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_19@7",
            "content": "5. Update memory bank g(x; \u03b8 t ) with Eq. ( 10) or ( 12).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_19",
            "start": 644,
            "end": 700,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_20@0",
            "content": "y = argmax j\u2208Y f (x; \u03b8 (t) ) j ,(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_20",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_21@0",
            "content": "Output: The final fine-tuned model f (\u2022; \u03b8 T ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_21",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_22@0",
            "content": "where f (x; \u03b8 (t) ) \u2208 R C is a probability simplex and [f (x; \u03b8 (t) )] j is the j-th entry.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_22",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_22@1",
            "content": "The procedure of ACTUNE is summarized in Algorithm 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_22",
            "start": 92,
            "end": 144,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_23@0",
            "content": "Region-aware Sampling for Active",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_23",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_24@0",
            "content": "Learning on High-uncertainty Data After obtaining the uncertainty for unlabeled data, we aim to query annotation for high-uncertainty samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_24",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_24@1",
            "content": "However, directly sampling the most uncertain samples gives suboptimal result since uncertainty-based sampling tends to query repetitive data (Ein-Dor et al., 2020) and results in poor representativeness of the overall data distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_24",
            "start": 143,
            "end": 379,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_25@0",
            "content": "To tackle this issue, we propose region-aware sampling to capture both uncertainty and diversity during active self-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_25",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_25@1",
            "content": "Specifically, in the tth round, we first conduct the weighted K-means clustering (Huang et al., 2005), which weights samples based on their uncertainty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_25",
            "start": 126,
            "end": 277,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_25@2",
            "content": "Denote K the number of clusters and v (t) i = BERT(x i ) the representation of x i from the penultimate layer of BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_25",
            "start": 279,
            "end": 396,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_25@3",
            "content": "The weighted K-means first initializes the center of each each cluster \u00b5 i (1 \u2264 i \u2264 K) via K-Means++ (Arthur and Vassilvitskii, 2007).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_25",
            "start": 398,
            "end": 531,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_25@4",
            "content": "Then, it jointly updates the centroid of each cluster and assigns each sample to cluster c i as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_25",
            "start": 533,
            "end": 627,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_26@0",
            "content": "c (t) i = argmin k=1,...,K vi \u2212 \u00b5 k 2 ,(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_26",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_27@0",
            "content": "\u00b5 (t) k = x i \u2208C (t) k a(xi, \u03b8 (t) ) \u2022 v (t) i x\u2208C (t) k a(xi, \u03b8 (t) )(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_27",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_28@0",
            "content": "where",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_28",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_29@0",
            "content": "C (t) k = {x (t) i |c (t) i",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_29",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_30@0",
            "content": "= k}(k = 1, . . . , K) stands for the k-th cluster.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_30",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_30@1",
            "content": "The above two steps in Eq. ( 3), (4) are repeated until convergence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_30",
            "start": 52,
            "end": 119,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_30@2",
            "content": "Compared with vanilla K-Means method, the weighting scheme increases the density of the samples with high uncertainty, thus enabling the K-Means methods to discover clusters with high uncertainty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_30",
            "start": 121,
            "end": 316,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_30@3",
            "content": "After obtaining K regions with the corresponding data C (t) k , we calculate the uncertainty of each region as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_30",
            "start": 318,
            "end": 427,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_31@0",
            "content": "u (t) k = U (C (t) k ) + \u03b2I(C (t) k )(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_31",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_32@0",
            "content": "where",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_32",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_33@0",
            "content": "U (C (t) k ) = 1 |C (t) k | x i \u2208C (t) k a(xi, \u03b8 (t) )(6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_33",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_34@0",
            "content": "stands for the average uncertainty of samples and",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_34",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_35@0",
            "content": "I(C (t) k ) = \u2212 j\u2208C f (t) j log f (t) j (7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_35",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_36@0",
            "content": "stands for the inter-class diversity within cluster k and f",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_36",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_37@0",
            "content": "(t) j = i 1{ y i =j} |C (t) k |",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_37",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_38@0",
            "content": "represents the frequency of class j on cluster k. Notably, the term U (C",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_38",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_39@0",
            "content": "(t)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_39",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_40@0",
            "content": "k ) assigns higher score for clusters with more uncertain samples, and I(C (t) k ) grants higher scores for clusters containing samples with more diverse predicted classes from pseudo labels since such clusters would be closer to the decision boundary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_40",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_41@0",
            "content": "Then, we rank the clusters in an ascending order according to u (t) k .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_41",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_41@1",
            "content": "A high score indicates high uncertainty of the model in these regions, and we need to actively annotate the associated instances to reduce uncertainty and improve the model's performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_41",
            "start": 72,
            "end": 258,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_41@2",
            "content": "We adopt a hierarchical sampling strategy: we first select the M clusters with the highest uncertainty, and then sample b = B M data with the highest uncertainty to form the batch Q (t) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_41",
            "start": 260,
            "end": 446,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_41@3",
            "content": "2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_41",
            "start": 448,
            "end": 448,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_42@0",
            "content": "K (t) a = top-M k\u2208{1,...,K} u (t) k , Q (t) = k\u2208K (t) a C (t) a,k where C (t) a,k = Top-b x i \u2208C (t) k a(xi, \u03b8 (t) ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_42",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_43@0",
            "content": "(8) We remark that such a hierarchical sampling strategy queries most uncertain samples from different regions, thus the uncertainty and diversity of queried samples can be both achieved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_43",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_44@0",
            "content": "Self-training for Most Confident Data from Low-uncertainty Regions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_44",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_45@0",
            "content": "For self-training, we aim to select unlabeled samples which are most likely to have been correctly classified by the current model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_45",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_45@1",
            "content": "This requires the sample to have low uncertainty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_45",
            "start": 132,
            "end": 180,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_45@2",
            "content": "Therefore, we select the top k samples from the M lowest uncertainty regions to form the acquired batch S (t) :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_45",
            "start": 182,
            "end": 292,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_46@0",
            "content": "C (t) s = k\u2208K (t) s C (t) k where K (t) s = bottom-M k\u2208{1,...,K} u (t) k , S (t) = bottom-k x i \u2208C (t) s a(xi, \u03b8 (t) ),(9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_46",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_47@0",
            "content": "Momentum-based Memory Bank for Selftraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_47",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_47@1",
            "content": "As PLMs are sensitive to the stochasticity involved in fine-tuning, the model suffers from the instability issue -different weight initialization and data orders may result in different predictions on the same dataset (Dodge et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_47",
            "start": 45,
            "end": 283,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_48@0",
            "content": "Additionally, if the model gives inconsistent predictions in different rounds for a specific sample, then it is potentially uncertain about the sample, and adding it to the training set may harm the active self-training process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_48",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_48@1",
            "content": "For example, for a twoclass classification problem, suppose we obtain f (x; \u03b8 (t\u22121) ) = [0.65, 0.35] for sample x the round (t\u22121) and f (x; \u03b8 (t) ) = [0.05, 0.95] for the round t.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_48",
            "start": 229,
            "end": 407,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_49@0",
            "content": "Although the model is quite 'confident' on the class of x when we only consider the result of the round t, it gives contradictory predictions over these two consecutive rounds, which indicates that the model is still uncertain to which class x belongs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_49",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_50@0",
            "content": "To effectively mitigate the noise and stabilize the active self-training process, we maintain a dynamic memory bank to save the results from previous rounds, and use momentum update (He et al., 2020;Laine and Aila, 2016) to aggregate the results from both the previous and current rounds.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_50",
            "start": 0,
            "end": 287,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_50@1",
            "content": "Then, during active self-training, we will select samples with the highest aggregated score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_50",
            "start": 289,
            "end": 380,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_50@2",
            "content": "In this way, only those samples that the model is certain about over all previous rounds will be selected for self-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_50",
            "start": 382,
            "end": 505,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_50@3",
            "content": "We design two variants for the memory bank, namely prediction-based and value-based aggregation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_50",
            "start": 507,
            "end": 602,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_50@4",
            "content": "Prediction based Momentum Update.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_50",
            "start": 604,
            "end": 636,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_50@5",
            "content": "We adopt an exponential moving average approach to aggregate the prediction g(x; \u03b8 (t) ) on round t as g(x; \u03b8",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_50",
            "start": 638,
            "end": 746,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_51@0",
            "content": "(t) ) = m t \u00d7f (x; \u03b8 (t) )+(1\u2212m t )\u00d7g(x; \u03b8 (t\u22121) ), (10",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_51",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_52@0",
            "content": ") where m t = (1 \u2212 t T )m L + t T m H (0 < m L \u2264 m H \u2264 1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_52",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_53@0",
            "content": ") is a momentum coefficient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_53",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_53@1",
            "content": "We gradually increase the weight for models on later rounds, since they are trained with more labeled data thus being able to provide more reliable predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_53",
            "start": 29,
            "end": 188,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_53@2",
            "content": "Then, we calculate the uncertainty based on g(x; \u03b8 (t) ) and rewrite Eq. ( 9) and (2) as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_53",
            "start": 190,
            "end": 277,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_54@0",
            "content": "S (t) = bottom-k x i \u2208C (t) s a x i , g(x; \u03b8 (t) ), \u03b8 (t) y = argmax j\u2208Y g(x; \u03b8 (t) ) j ,(11)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_54",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_55@0",
            "content": "Value-based Momentum Update.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_55",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_55@1",
            "content": "For methods that do not directly use prediction for uncertainty estimation, we aggregate the uncertainty value as g(x; \u03b8 (t) ) = m t \u00d7a(x; \u03b8 (t) )+(1\u2212m t )\u00d7g(x; \u03b8 (t\u22121) ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_55",
            "start": 29,
            "end": 199,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_56@0",
            "content": "(12) Then, we use Eq. ( 12) to sample low-uncertainty data for self-training as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_56",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_57@0",
            "content": "S (t) = bottom-k x i \u2208C (t) s g(x i , \u03b8 (t) ), y = argmax j\u2208Y f (x; \u03b8 (t) ) j . (13",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_57",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_58@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_58",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_59@0",
            "content": "By aggregating the prediction results over previous rounds, we filter the sample with inconsistent predictions to suppress noisy labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_59",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_60@0",
            "content": "Model Learning and Update",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_60",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_61@0",
            "content": "After obtaining both the labeled data and pseudolabeled data, we fine-tune a new pre-trained BERT model \u03b8 (t+1) on them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_61",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_61@1",
            "content": "Although we only include low-uncertainty samples during self-training, it is difficult to eliminate all the wrong pseudo-labels, and such mislabeled samples can still hurt model performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_61",
            "start": 121,
            "end": 310,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_61@2",
            "content": "To suppress such label noise, we use a threshold-based strategy to further remove noisy labels by selecting samples that agree with the corresponding pseudo labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_61",
            "start": 312,
            "end": 475,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_61@3",
            "content": "The loss objective of optimizing \u03b8 (t+1) is",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_61",
            "start": 477,
            "end": 519,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_62@0",
            "content": "LST = 1 |XL \u222a Q (t) | x i \u2208X L \u222aQ (t) CE f (xi; \u03b8 (t+1) ), yi + \u03bb |S (t) | x i \u2208S (t) \u03c9i CE f ( xi; \u03b8 (t+1) ), yi , (14)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_62",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_63@0",
            "content": "where \u03bb is a hyper-parameter balancing the weight between clean and pseudo labels, and \u03c9 i = 1{ f (x i ; \u03b8 (t+1) ) y i > \u03b3} stands for the thresholding function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_63",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_63@1",
            "content": "Complexity Analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_63",
            "start": 162,
            "end": 181,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_63@2",
            "content": "The running time of AC-TUNE is mainly consisted of two parts: the inference time O(|X u |) and the time for K-Means clustering O(dK|X u |), where d is the dimension of the BERT feature v. Note that the clustering can be efficiently implemented with FAISS (Johnson et al., 2019), and will not excessively increase the total running time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_63",
            "start": 183,
            "end": 518,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_63@3",
            "content": "For self-training, the size of the memory bank g(x; \u03b8) is proportional to |X u |, while the extra computation of maintaining this dictionary is ignorable since we do not inference over the unlabeled data for multiple times in each round as BALD (Gal et al., 2017) does.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_63",
            "start": 520,
            "end": 788,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_63@4",
            "content": "The running time of ACTUNE will be shown in section C. Note that when compared with active learning baselines, we do not augment the train set with pseudolabeled data (Eq. ( 9)) to ensure fair comparisons.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_63",
            "start": 790,
            "end": 994,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_64@0",
            "content": "Main Result",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_64",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_65@0",
            "content": "Figure 1 reports the performance of ACTUNE and the baselines on 4 benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_65",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_65@1",
            "content": "From the results, we have the following observations:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_65",
            "start": 78,
            "end": 130,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_66@0",
            "content": "\u2022 ACTUNE consistently outperforms baselines in most of the cases. Different from studies in the computer vision (CV) domain (Sim\u00e9oni et al., 2020) where the model does not perform well in the low-data regime, pre-trained LM has achieved competitive performance with only a few labeled data, which makes further improvements to the vanilla fine-tuning challenging. Nevertheless, AC-TUNE surpasses baselines in more than 90% of the rounds and achieves 0.4%-0.7% and 0.3%-1.5% absolute gain at the end of AL and SSAL respectively. Figure 2 quantitatively measures the number of labels needed for the most advanced active learning model and self-training model (UST) to outperform ACTUNE with 1000 labels. These baselines need >2000 clean labeled samples to reach the performance as ours. ACTUNE saves on average 56.2% and 57.0% of the labeled samples than most advanced active learning and selftraining baselines respectively, which justifies its promising performance under low-resource scenarios. Such improvements show the merits of two key designs under our active self-training framework: the region-aware sampling for active learning and the momentum-based memory bank for robust selftraining, which will be discussed in the section 3.5. \u2022 Compared with the previous AL baselines, AC-TUNE can bring consistent performance gain, while previous semi-supervised active learning methods cannot. For instance, BASS is based on BALD for active learning, but sometimes it performs even worse than BALD with the same number of labeled data (see Fig. 5(b) and Fig. 1(f)). This is mainly because previous methods simply combine noisy pseudo labels with clean labels for training without explicitly rectifying the wrongly-labeled data, which will cause the LM to overfit these hazardous labels. Moreover, previous methods do not exploit momentum updates to stabilize the learning process, as there are oscillations in the beginning rounds. In contrast, ACTUNE achieves a more stable learning process and enables an active selftraining process to benefit from more labeled data. \u2022 The self-training methods (ST & UST) achieve superior performance with limited labels. However, they mainly focus on leveraging unlabeled data for improving the performance, while our results demonstrate that adaptive selecting the most useful data for fine-tuning is also important for improving the performance. With a powerful querying policy, ACTUNE can improve these self-training baselines by 1.05% in terms of accuracy on average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_66",
            "start": 0,
            "end": 2508,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_67@0",
            "content": "Extension to Weakly-supervised Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_67",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_68@0",
            "content": "ACTUNE can be naturally extended to weaklysupervised classification, where X l is a set of data annotated by linguistic patterns or rules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_68",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_68@1",
            "content": "Since the initial label set is noisy, then the model trained with Eq. ( 1) will overfit to the label noise, and we can actively query labeled data to refine the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_68",
            "start": 139,
            "end": 305,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_69@0",
            "content": "We conduct experiments on the TREC and Chemprot dataset 4 , where we first use Snorkel (Ratner et al., 2017) to obtain weak label set X l , then fine-tune the pre-trained LM f (\u03b8 (0) ) on X l . After that, we adopt ACTUNE for active self-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_69",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_70@0",
            "content": "Fig. 5 shows the results of these two datasets 5 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_70",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_70@1",
            "content": "When combining ACTUNE with CAL, the performance is unsatisfactory.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_70",
            "start": 51,
            "end": 116,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_70@2",
            "content": "We argue it is because CAL requires clean labels to calculate uncertainties.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_70",
            "start": 118,
            "end": 193,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_70@3",
            "content": "When labels are inaccurate, it will prevent AC-TUNE from querying informative samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_70",
            "start": 195,
            "end": 280,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_70@4",
            "content": "In contrast, ACTUNE achieves the best performance over baselines when using Entropy as the uncertainty measure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_70",
            "start": 282,
            "end": 392,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_70@5",
            "content": "The performance gain is more notable on the TREC dataset, where we achieve 96.68% accuracy, close to the fully supervised performance (96.80%) with only \u223c6% of clean labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_70",
            "start": 394,
            "end": 566,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_70@6",
            "content": "self-training, ACTUNE can further boost the performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_70",
            "start": 568,
            "end": 623,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_70@7",
            "content": "This indicates that ACTUNE is a general active self-training approach, as it can serve as an efficient plug-in module for existing AL methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_70",
            "start": 625,
            "end": 766,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_71@0",
            "content": "Combination with Other AL Methods",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_71",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_72@0",
            "content": "Ablation and Hyperparameter Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_72",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_73@0",
            "content": "The Effect of Different Components in AC-TUNE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_73",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_73@1",
            "content": "We inspect different components of ACTUNE, including the region-sampling (RS), momentum-based memory bank (MMB), and weighted clustering (WClus) 6 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_73",
            "start": 47,
            "end": 194,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_73@2",
            "content": "Experimental results (Fig. 4(b)) shows that all the three components contribute to the final performance, as removing any of them hurts the classification accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_73",
            "start": 196,
            "end": 359,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_73@3",
            "content": "Also, we find that when removing MMB, the performance hurts most in the beginning rounds, which indicates that MMB effectively suppresses label noise when the model's capacity is weak.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_73",
            "start": 361,
            "end": 544,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_73@4",
            "content": "Con-versely, removing WClus hurts the performance on later rounds, as it enables the model to select most informative samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_73",
            "start": 546,
            "end": 671,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_74@0",
            "content": "Hyperparameter Study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_74",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_74@1",
            "content": "We study two hyperparameters, namely \u03b2 and K used in querying labels. Figure 6(e) and 6(f) show the results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_74",
            "start": 22,
            "end": 129,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_74@2",
            "content": "In general, the model is insensitive to \u03b2 as the performance difference is less than 0.6%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_74",
            "start": 131,
            "end": 220,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_74@3",
            "content": "The model cannot perform well with smaller K since it cannot pinpoint to high-uncertainty regions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_74",
            "start": 222,
            "end": 319,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_74@4",
            "content": "For larger K, the performance also drops as some of the high-uncertainty regions can be outliers and sampling from them would hurt the model performance (Karamcheti et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_74",
            "start": 321,
            "end": 499,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_75@0",
            "content": "A Closer Look at the Momentum-based Memory Bank. To examine the role of MMB, we show the overall accuracy of pseudo-labels on AG News dataset in Fig. 6(g).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_75",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_75@1",
            "content": "From the result, it is clear that the momentum-based memory bank can stabilize the active self-training process, as the accuracy of pseudo labels increases around 1%, especially in later rounds. Fig 6(h) and 3(e) illustrates the model performance with different m L and m H .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_75",
            "start": 156,
            "end": 430,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_75@2",
            "content": "Overall, we find that our model is robust to different choices as ACTUNE outperform the baseline without momentum update consistently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_75",
            "start": 432,
            "end": 565,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_75@3",
            "content": "Moreover, we find that the larger m H will generally lead to better performance in later rounds.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_75",
            "start": 567,
            "end": 662,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_75@4",
            "content": "This is mainly because in later rounds, the model's prediction is more reliable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_75",
            "start": 664,
            "end": 743,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_75@5",
            "content": "Conversely, at the beginning of the training, the model's prediction might be oscillating on unlabeled data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_75",
            "start": 745,
            "end": 852,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_75@6",
            "content": "In this case, using a smaller m L will favor samples with consistent predictions Value, mL=0.7, mH=0.9 Value, mL=0.9, mH=0.9 Prob, m=0.7, mH=0.9 Prob, m=0.9, mH=0.9 No Momentum",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_75",
            "start": 854,
            "end": 1029,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_75@7",
            "content": "(d) Entropy Value, mL=0.7, mH=0.9 Value, mL=0.9, mH=0.9 Prob, m=0.7, mH=0.9 Prob, m=0.9, mH=0.9",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_75",
            "start": 1031,
            "end": 1125,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_76@0",
            "content": "No Momentum (e) CAL to improve the robustness of active self-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_76",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_77@0",
            "content": "Another finding is that for different AL methods, the optimal memory bank can be different.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_77",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_77@1",
            "content": "For Entropy, probability-based memory bank leads to a better result, while for CAL, simple aggregating over uncertainty score achieves better performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_77",
            "start": 92,
            "end": 245,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_77@2",
            "content": "This is mainly because the method used in CAL is more complicated, and using probability-based memory bank may hurt the uncertainty calculation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_77",
            "start": 247,
            "end": 390,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_78@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_78",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_79@0",
            "content": "Active Learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_79",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_79@1",
            "content": "Active learning has been widely applied to various NLP tasks (Yuan et al., 2020;Zhao et al., 2020;Shelmanov et al., 2021;Karamcheti et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_79",
            "start": 17,
            "end": 162,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_79@2",
            "content": "So far, AL methods can be categorized into uncertainty-based methods (Gal et al., 2017;Margatina et al., 2021a,b), diversitybased methods (Ru et al., 2020;Sener and Savarese, 2018) and hybrid methods (Yuan et al., 2020;Ash et al., 2020;Kirsch et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_79",
            "start": 164,
            "end": 420,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_79@3",
            "content": "Ein-Dor et al. (2020) offer an empirical study of active learning with PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_79",
            "start": 422,
            "end": 497,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_79@4",
            "content": "In our study, we leverage the power of unlabeled instances via self-training to further promote the performance of AL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_79",
            "start": 499,
            "end": 616,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_79@5",
            "content": "Semi-supervised Active Learning (SSAL).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_79",
            "start": 618,
            "end": 656,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_79@6",
            "content": "Gao et al. (2020); Song et al. (2019); Guo et al. (2021) design query strategies for specific semi-supervised methods, Tomanek and Hahn (2009); Rottmann et al. (2018); Sim\u00e9oni et al. (2020) exploit the mostcertain samples from the unlabeled with pseudolabeling to augment the training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_79",
            "start": 658,
            "end": 946,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_79@7",
            "content": "So far, most of the SSAL approaches are designed for CV domain and it remains unknown how this paradigm performs with PLMs on NLP tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_79",
            "start": 948,
            "end": 1083,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_79@8",
            "content": "In contrast, we propose ACTUNE to effectively leverage unlabeled data during finetuing PLMs for NLP tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_79",
            "start": 1085,
            "end": 1190,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_79@9",
            "content": "Self-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_79",
            "start": 1192,
            "end": 1205,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_79@10",
            "content": "Self-training first generates pseudo labels for high-confidence samples, then fits a new model on pseudo labeled data to improve the generalization ability (Rosenberg et al., 2005;Lee, 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_79",
            "start": 1207,
            "end": 1397,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_79@11",
            "content": "However, it is known to be vulnerable to error propagation (Arazo et al., 2020;Rizve et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_79",
            "start": 1399,
            "end": 1497,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_79@12",
            "content": "To alleviate this, we adopt a simple momentumbased method to select high confidence samples, effectively reducing the pseudo labels noise for active learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_79",
            "start": 1499,
            "end": 1656,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_79@13",
            "content": "Note that although Mukherjee and Awadallah (2020); Rizve et al. ( 2021) also leverage uncertainty information for self-training, their focus is on developing better self-training methods, while we aim to jointly query high-uncertainty samples and generate pseudo-labels for low-uncertainty samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_79",
            "start": 1658,
            "end": 1955,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_79@14",
            "content": "The experiments in Sec. 3 show that with appropriate querying methods, ACTUNE can further improve the performance of self-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_79",
            "start": 1957,
            "end": 2087,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_80@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_80",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_81@0",
            "content": "In this paper, we develop ACTUNE, a general active self-training framework for enhancing both label efficiency and model performance in fine-tuning pre-trained language models (PLMs).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_81",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_81@1",
            "content": "We propose a region-aware sampling approach to guarantee both the uncertainty the diversity for querying labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_81",
            "start": 184,
            "end": 295,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_81@2",
            "content": "To combat the label noise propagation issue, we design a momentum-based memory bank to effectively utilize the model predictions for preceding AL rounds.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_81",
            "start": 297,
            "end": 449,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_81@3",
            "content": "Empirical results on 6 public text classification benchmarks suggest the superiority of ACTUNE to conventional active learning and semi-supervised active learning methods for fine-tuning PLMs with limited resources.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_81",
            "start": 451,
            "end": 665,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_81@4",
            "content": "(Holub et al., 2008) 461s 646s BALD (Gal et al., 2017) 4595s 6451s ALPS (Yuan et al., 2020) 488s 677s BADGE (Ash et al., 2020) 554s 1140s CAL (Margatina et al., 2021b) 493s 688s REVIVAL (Guo et al., Our implementation of ACTUNE will be published upon acceptance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_81",
            "start": 667,
            "end": 928,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_82@0",
            "content": "We use AdamW (Loshchilov and Hutter, 2019) 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_82",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_82@1",
            "content": "Specifically, we search T 1 from 10 to 2000, T 2 from 1000 to 5000, T 3 from 10 to 500, \u03be from 0 to 1, and \u03bb from 0 to 0.5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_82",
            "start": 46,
            "end": 168,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_82@2",
            "content": "All results are reported as the average over three runs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_82",
            "start": 170,
            "end": 225,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_83@0",
            "content": "In our experiments, we keep \u03b2 = 0.5, \u03bb = 1 for all datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_83",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_83@1",
            "content": "For other parameters, we use a grid search to find the optimal setting for each datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_83",
            "start": 61,
            "end": 149,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_83@2",
            "content": "Specifically, we search \u03b3 from [0.5, 0.6, 0.7], m L from [0.6, 0.7, 0.8], m H from [0.8, 0.9, 1].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_83",
            "start": 151,
            "end": 247,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_83@3",
            "content": "For AC-TUNE with Entropy, we use probability based aggregation and for ACTUNE with CAL, we use value based aggregation by default.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_83",
            "start": 249,
            "end": 378,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_84@0",
            "content": "For other SSAL methods, we mainly tune their key hyperparameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_84",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_84@1",
            "content": "Note that Entropy (Holub et al., 2008), BALD (Gal et al., 2017), ALPS (Yuan et al., 2020), BADGE (Ash et al., 2020) do not introduce any new hyperparameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_84",
            "start": 66,
            "end": 222,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_84@2",
            "content": "For CAL (Margatina et al., 2021b), we tune the number for KNN k from [5,10,20] and report the best performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_84",
            "start": 224,
            "end": 334,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_84@3",
            "content": "For ST (Lee, 2013), CEAL (Wang et al., 2016) & BASS (Rottmann et al., 2018), it uses a threshold \u03b4 for selecting high-confidence data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_84",
            "start": 336,
            "end": 469,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_84@4",
            "content": "We tune \u03b4 from [0.6, 0.7, 0.8, 0.9] to report the best performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_84",
            "start": 471,
            "end": 537,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_84@5",
            "content": "For UST (Mukherjee and Awadallah, 2020), we tune the number of lowuncertainty samples used in the next round from [1024,2048,4096].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_84",
            "start": 539,
            "end": 669,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_84@6",
            "content": "For COSINE (Yu et al., 2021), we set the weight for confidence regularization \u03bb as 0.1, the threshold \u03c4 for selecting high-confidence data from [0.7, 0.9] and the update period of selftraining from [50,100,150].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_84",
            "start": 671,
            "end": 881,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_84@7",
            "content": "For REVIVAL (Guo et al., 2021), it calculates uncertainty with adversarial perturbation, we tune the size of the perturbation from [1e \u2212 3, 1e \u2212 4, 1e \u2212 5].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_84",
            "start": 883,
            "end": 1038,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_85@0",
            "content": "C Runtime Analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_85",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_86@0",
            "content": "Table 3 shows the time in one active learning round of ACTUNE and baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_86",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_86@1",
            "content": "Here we highlight that the additional time for region-aware sampling and momentum-based memory bank is rather small compared with the inference time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_86",
            "start": 77,
            "end": 225,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_86@2",
            "content": "Among all baselines, we find that the running time of clustering-based method is faster than the original reported time in the paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_86",
            "start": 227,
            "end": 359,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_86@3",
            "content": "This is because we use FAISS (Johnson et al., 2019) instead of SKLearn (Pedregosa et al., 2011) for clustering, which accelerates the clustering step significantly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_86",
            "start": 361,
            "end": 524,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_86@4",
            "content": "Also, we find that BALD and REVIVAL are not so efficient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_86",
            "start": 526,
            "end": 582,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_86@5",
            "content": "For BALD, it needs to infer the uncertainty of the model by passing the data to model with multitple times.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_86",
            "start": 584,
            "end": 690,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_86@6",
            "content": "Such an operation will make the total inference time for PLMs very long.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_86",
            "start": 692,
            "end": 763,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_86@7",
            "content": "For REVIVAL, we find that calculating the adversarial gradient needs extra forward passes and backward passes, which could be time-consuming for PLMs with millions of parameters 7 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_86",
            "start": 765,
            "end": 945,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_87@0",
            "content": "First, since our focus is on fine-tuning pre-trained language models, we use the representation of [CLS] token for classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_87",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_87@1",
            "content": "In the future work, we can consider using prompt tuning (Gao et al., 2021;Schick and Sch\u00fctze, 2021), a more dataefficient method for adopting pre-trained language models on classification tasks to further promote the efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_87",
            "start": 131,
            "end": 358,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_87@2",
            "content": "Also, due to the computational resource constraints, we do not use larger pre-trained language models such as RoBERTa-large (Liu et al.,7 The original model is proposed with CV tasks and they use ResNet-18 as the backbone which only contains 11M parameters (around 10% of the parameters of Roberta-base).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_87",
            "start": 360,
            "end": 663,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_87@3",
            "content": "2019) which shown even better performance with only a few labels (Du et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_87",
            "start": 665,
            "end": 747,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_87@4",
            "content": "Last, apart from the text classification task, we can also extend our work into other tasks such as sequence labeling and natural language inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_87",
            "start": 749,
            "end": 897,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_88@0",
            "content": "Here we give an example of our querying strategy on AG News and Pubmed dataset for the 1st round of active self-training process in figure 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_88",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_88@1",
            "content": "Note that we use t-SNE algorithm (Van der Maaten and Hinton, 2008) for dimension reduction, and the black triangle stands for the queried samples while other circles stands for the unlabeled data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_88",
            "start": 142,
            "end": 337,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_88@2",
            "content": "Different colors stands for different classes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_88",
            "start": 339,
            "end": 384,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_88@3",
            "content": "From the comparision, we can see that the existing uncertainty based methods such as Entropy and CAL, are suffered from the issue of limited diversity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_88",
            "start": 386,
            "end": 536,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_88@4",
            "content": "However, when combined with ACTUNE, the diversity is much improved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_88",
            "start": 538,
            "end": 604,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_88@5",
            "content": "Such results, compared with the main results in figure 1, demonstrate the efficacy of ACTUNE empirically.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_88",
            "start": 606,
            "end": 710,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_89@0",
            "content": "Eric Arazo, Diego Ortego, Paul Albert, E O' Noel, Kevin Connor,  Mcguinness, Pseudolabeling and confirmation bias in deep semisupervised learning, 2020, 2020 International Joint Conference on Neural Networks (IJCNN), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_89",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_90@0",
            "content": "David Arthur, Sergei Vassilvitskii, K-means++: The advantages of careful seeding, 2007, Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_90",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_91@0",
            "content": "Jordan Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, Alekh Agarwal, Deep batch active learning by diverse, uncertain gradient lower bounds, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_91",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_92@0",
            "content": "Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai, Andrew Mccallum, Self-supervised meta-learning for few-shot natural language classification tasks, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_92",
            "start": 0,
            "end": 303,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_93@0",
            "content": "Iz Beltagy, Kyle Lo, Arman Cohan, SciB-ERT: A pretrained language model for scientific text, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_93",
            "start": 0,
            "end": 276,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_94@0",
            "content": "Jonathan Bragg, Arman Cohan, Kyle Lo, Iz Beltagy, Flex: Unifying evaluation for few-shot nlp, 2021, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_94",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_95@0",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Language models are fewshot learners, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_95",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_96@0",
            "content": "Franck Dernoncourt, Ji Lee, PubMed 200k RCT: a dataset for sequential sentence classification in medical abstracts, 2017, Proceedings of the Eighth International Joint Conference on Natural Language Processing, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_96",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_97@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_97",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_98@0",
            "content": "UNKNOWN, None, 2002, 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_98",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_99@0",
            "content": "Rotem Dror, Gili Baumer, Segev Shlomov, Roi Reichart, The hitchhiker's guide to testing statistical significance in natural language processing, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_99",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_100@0",
            "content": "Jingfei Du, Edouard Grave, Beliz Gunel, Vishrav Chaudhary, Onur Celebi, Michael Auli, Veselin Stoyanov, Alexis Conneau, Self-training improves pre-training for natural language understanding, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_100",
            "start": 0,
            "end": 342,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_101@0",
            "content": "Alon Liat Ein-Dor, Ariel Halfon, Eyal Gera, Lena Shnarch, Leshem Dankin, Marina Choshen, Ranit Danilevsky, Yoav Aharonov, Noam Katz,  Slonim, Active Learning for BERT: An Empirical Study, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_101",
            "start": 0,
            "end": 290,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_102@0",
            "content": "UNKNOWN, None, 2015, Bayesian convolutional neural networks with bernoulli approximate variational inference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_102",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_103@0",
            "content": "Yarin Gal, Riashat Islam, Zoubin Ghahramani, Deep bayesian active learning with image data, 2017, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_103",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_104@0",
            "content": "Mingfei Gao, Zizhao Zhang, Guo Yu, \u00d6 Sercan,  Ar\u0131k, S Larry, Tomas Davis,  Pfister, Consistency-based semi-supervised active learning: Towards minimizing labeling cost, 2020, European Conference on Computer Vision, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_104",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_105@0",
            "content": "Tianyu Gao, Adam Fisch, Danqi Chen, Making pre-trained language models better few-shot learners, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_105",
            "start": 0,
            "end": 278,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_106@0",
            "content": "Jiannan Guo, Haochen Shi, Yangyang Kang, Kun Kuang, Siliang Tang, Zhuoren Jiang, Changlong Sun, Fei Wu, Yueting Zhuang, Semisupervised active learning for semi-supervised models: Exploit adversarial examples with graph-based virtual labels, 2021, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_106",
            "start": 0,
            "end": 327,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_107@0",
            "content": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Momentum contrast for unsupervised visual representation learning, 2020, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_107",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_108@0",
            "content": "Alex Holub, Pietro Perona, Michael C Burl, Entropy-based active learning for object recognition, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_108",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_109@0",
            "content": "Peiyun Hu, Zack Lipton, Anima Anandkumar, Deva Ramanan, Active learning with partial feedback, 2019, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_109",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_110@0",
            "content": "UNKNOWN, None, 2005, Automated variable weighting in k-means type clustering. IEEE transactions on pattern analysis and machine intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_110",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_111@0",
            "content": "Jeff Johnson, Matthijs Douze, Herv\u00e9 J\u00e9gou, Billion-scale similarity search with gpus, 2019, IEEE Transactions on Big Data, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_111",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_112@0",
            "content": "Katharina Kann, Kyunghyun Cho, Samuel , Towards realistic practices in lowresource natural language processing: The development set, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_112",
            "start": 0,
            "end": 357,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_113@0",
            "content": "Siddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, Christopher Manning, Mind your outliers! investigating the negative impact of outliers on active learning for visual question answering, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_113",
            "start": 0,
            "end": 368,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_114@0",
            "content": "Andreas Kirsch, Joost Van Amersfoort, Yarin Gal, Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning, 2019, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_114",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_115@0",
            "content": "Martin Krallinger, Obdulia Rabal, A Saber,  Akhondi, Overview of the biocreative VI chemical-protein interaction track, 2017, BioCreative evaluation Workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_115",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_116@0",
            "content": "UNKNOWN, None, 2016, Temporal ensembling for semi-supervised learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_116",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_117@0",
            "content": "Dong-Hyun Lee, Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks, 2013, ICML Workshop on challenges in representation learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_117",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_118@0",
            "content": "Xin Li, Dan Roth, Learning question classifiers, 2002, The 19th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_118",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_119@0",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_119",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_120@0",
            "content": "Ilya Loshchilov, Frank Hutter, Decoupled weight decay regularization, 2019, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_120",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_121@0",
            "content": "UNKNOWN, None, , Loic Barrault, and Nikolaos Aletras. 2021a. Bayesian active learning with pretrained language models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_121",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_122@0",
            "content": "Katerina Margatina, Giorgos Vernikos, Active learning by acquiring contrastive examples, , Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_122",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_123@0",
            "content": "Marius Mosbach, Maksym Andriushchenko, Dietrich Klakow, On the stability of fine-tuning {bert}: Misconceptions, explanations, and strong baselines, 2021, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_123",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_124@0",
            "content": "Subhabrata Mukherjee, Ahmed Awadallah, Uncertainty-aware self-training for few-shot text classification, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_124",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_125@0",
            "content": "Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Scikit-learn: Machine learning in python, 2011, Journal of machine Learning research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_125",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_126@0",
            "content": "Alexander Ratner, H Stephen, Henry Bach, Jason Ehrenberg, Sen Fries, Christopher Wu,  R\u00e9, Snorkel: Rapid training data creation with weak supervision, 2017, Proceedings of the VLDB Endowment, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_126",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_127@0",
            "content": "Kevin Mamshad Nayeem Rizve,  Duarte, S Yogesh, Mubarak Rawat,  Shah, In defense of pseudo-labeling: An uncertainty-aware pseudolabel selection framework for semi-supervised learning, 2021, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_127",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_128@0",
            "content": "Chuck Rosenberg, Henry Hebert,  Schneiderman, Semi-supervised self-training of object detection models, 2005, Proceedings of the IEEE Workshops on Application of Computer Vision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_128",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_129@0",
            "content": "Matthias Rottmann, Karsten Kahl, Hanno Gottschalk, Deep bayesian active semisupervised learning, 2018, 17th IEEE International Conference on Machine Learning and Applications (ICMLA), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_129",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_130@0",
            "content": "Dongyu Ru, Jiangtao Feng, Lin Qiu, Hao Zhou, Mingxuan Wang, Weinan Zhang, Yong Yu, Lei Li, Active sentence learning by adversarial uncertainty sampling in discrete space, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_130",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_131@0",
            "content": "Timo Schick, Hinrich Sch\u00fctze, Exploiting cloze-questions for few-shot text classification and natural language inference, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_131",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_132@0",
            "content": "Ozan Sener, Silvio Savarese, Active learning for convolutional neural networks: A core-set approach, 2018, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_132",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_133@0",
            "content": "Artem Shelmanov, Dmitri Puzyrev, Lyubov Kupriyanova, Denis Belyakov, Daniil Larionov, Nikita Khromov, Olga Kozlova, Ekaterina Artemova, V Dmitry, Alexander Dylov,  Panchenko, Active learning for sequence tagging with deep pre-trained models and Bayesian uncertainty estimates, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_133",
            "start": 0,
            "end": 405,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_134@0",
            "content": "Oriane Sim\u00e9oni, Mateusz Budnik, Yannis Avrithis, Guillaume Gravier, Rethinking deep active learning: Using unlabeled data at model training, 2020, the 25th International Conference on Pattern Recognition (ICPR), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_134",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_135@0",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_135",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_136@0",
            "content": "UNKNOWN, None, 2019, Combining mixmatch and active learning for better accuracy with fewer labels, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_136",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_137@0",
            "content": "Katrin Tomanek, Udo Hahn, Semisupervised active learning for sequence labeling, 2009, Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_137",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_138@0",
            "content": "Laurens Van Der Maaten, Geoffrey Hinton, Visualizing data using t-sne, 2008, Journal of machine learning research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_138",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_139@0",
            "content": "Keze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang, Liang Lin, Cost-effective active learning for deep image classification, 2016, IEEE Transactions on Circuits and Systems for Video Technology, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_139",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_140@0",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger,  Drame, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_140",
            "start": 0,
            "end": 463,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_141@0",
            "content": "Yue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao, Chao Zhang, Fine-tuning pretrained language model with weak supervision: A contrastive-regularized self-training approach, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_141",
            "start": 0,
            "end": 370,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_142@0",
            "content": "Michelle Yuan, Hsuan-Tien Lin, Jordan Boyd-Graber, Cold-start active learning through self-supervised language modeling, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_142",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_143@0",
            "content": "Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, Alexander Ratner, WRENCH: A comprehensive benchmark for weak supervision, 2021, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_143",
            "start": 0,
            "end": 245,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_144@0",
            "content": "UNKNOWN, None, 2020, Revisiting few-sample bert fine-tuning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_144",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_145@0",
            "content": "Xiang Zhang, Junbo Zhao, Yann Lecun, Character-level convolutional networks for text classification, 2015, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_145",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "129-ARR_v1_146@0",
            "content": "Yuekai Zhao, Haoran Zhang, Shuchang Zhou, Zhihua Zhang, Active learning approaches to enhancing neural machine translation, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v1_146",
            "start": 0,
            "end": 201,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "129-ARR_v1_0",
            "tgt_ix": "129-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_0",
            "tgt_ix": "129-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_1",
            "tgt_ix": "129-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_1",
            "tgt_ix": "129-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_0",
            "tgt_ix": "129-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_2",
            "tgt_ix": "129-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_4",
            "tgt_ix": "129-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_5",
            "tgt_ix": "129-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_6",
            "tgt_ix": "129-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_7",
            "tgt_ix": "129-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_8",
            "tgt_ix": "129-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_9",
            "tgt_ix": "129-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_10",
            "tgt_ix": "129-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_3",
            "tgt_ix": "129-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_3",
            "tgt_ix": "129-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_3",
            "tgt_ix": "129-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_3",
            "tgt_ix": "129-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_3",
            "tgt_ix": "129-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_3",
            "tgt_ix": "129-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_3",
            "tgt_ix": "129-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_3",
            "tgt_ix": "129-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_3",
            "tgt_ix": "129-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_0",
            "tgt_ix": "129-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_11",
            "tgt_ix": "129-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_13",
            "tgt_ix": "129-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_14",
            "tgt_ix": "129-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_12",
            "tgt_ix": "129-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_12",
            "tgt_ix": "129-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_12",
            "tgt_ix": "129-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_12",
            "tgt_ix": "129-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_0",
            "tgt_ix": "129-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_15",
            "tgt_ix": "129-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_17",
            "tgt_ix": "129-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_18",
            "tgt_ix": "129-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_19",
            "tgt_ix": "129-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_20",
            "tgt_ix": "129-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_21",
            "tgt_ix": "129-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_16",
            "tgt_ix": "129-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_16",
            "tgt_ix": "129-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_16",
            "tgt_ix": "129-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_16",
            "tgt_ix": "129-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_16",
            "tgt_ix": "129-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_16",
            "tgt_ix": "129-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_16",
            "tgt_ix": "129-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_0",
            "tgt_ix": "129-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_22",
            "tgt_ix": "129-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_24",
            "tgt_ix": "129-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_25",
            "tgt_ix": "129-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_26",
            "tgt_ix": "129-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_27",
            "tgt_ix": "129-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_28",
            "tgt_ix": "129-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_29",
            "tgt_ix": "129-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_30",
            "tgt_ix": "129-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_31",
            "tgt_ix": "129-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_32",
            "tgt_ix": "129-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_33",
            "tgt_ix": "129-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_34",
            "tgt_ix": "129-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_35",
            "tgt_ix": "129-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_36",
            "tgt_ix": "129-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_37",
            "tgt_ix": "129-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_38",
            "tgt_ix": "129-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_39",
            "tgt_ix": "129-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_40",
            "tgt_ix": "129-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_41",
            "tgt_ix": "129-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_42",
            "tgt_ix": "129-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_0",
            "tgt_ix": "129-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_43",
            "tgt_ix": "129-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_45",
            "tgt_ix": "129-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_46",
            "tgt_ix": "129-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_47",
            "tgt_ix": "129-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_48",
            "tgt_ix": "129-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_49",
            "tgt_ix": "129-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_50",
            "tgt_ix": "129-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_51",
            "tgt_ix": "129-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_52",
            "tgt_ix": "129-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_53",
            "tgt_ix": "129-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_54",
            "tgt_ix": "129-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_55",
            "tgt_ix": "129-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_56",
            "tgt_ix": "129-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_57",
            "tgt_ix": "129-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_58",
            "tgt_ix": "129-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_44",
            "tgt_ix": "129-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_44",
            "tgt_ix": "129-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_44",
            "tgt_ix": "129-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_44",
            "tgt_ix": "129-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_44",
            "tgt_ix": "129-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_44",
            "tgt_ix": "129-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_44",
            "tgt_ix": "129-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_44",
            "tgt_ix": "129-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_44",
            "tgt_ix": "129-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_44",
            "tgt_ix": "129-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_44",
            "tgt_ix": "129-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_44",
            "tgt_ix": "129-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_44",
            "tgt_ix": "129-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_44",
            "tgt_ix": "129-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_44",
            "tgt_ix": "129-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_44",
            "tgt_ix": "129-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_0",
            "tgt_ix": "129-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_59",
            "tgt_ix": "129-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_61",
            "tgt_ix": "129-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_62",
            "tgt_ix": "129-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_60",
            "tgt_ix": "129-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_60",
            "tgt_ix": "129-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_60",
            "tgt_ix": "129-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_60",
            "tgt_ix": "129-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_0",
            "tgt_ix": "129-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_63",
            "tgt_ix": "129-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_65",
            "tgt_ix": "129-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_64",
            "tgt_ix": "129-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_64",
            "tgt_ix": "129-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_64",
            "tgt_ix": "129-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_0",
            "tgt_ix": "129-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_68",
            "tgt_ix": "129-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_69",
            "tgt_ix": "129-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_67",
            "tgt_ix": "129-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_67",
            "tgt_ix": "129-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_67",
            "tgt_ix": "129-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_67",
            "tgt_ix": "129-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_0",
            "tgt_ix": "129-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_70",
            "tgt_ix": "129-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_0",
            "tgt_ix": "129-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_71",
            "tgt_ix": "129-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_73",
            "tgt_ix": "129-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_74",
            "tgt_ix": "129-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_75",
            "tgt_ix": "129-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_76",
            "tgt_ix": "129-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_72",
            "tgt_ix": "129-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_72",
            "tgt_ix": "129-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_72",
            "tgt_ix": "129-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_72",
            "tgt_ix": "129-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_72",
            "tgt_ix": "129-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_72",
            "tgt_ix": "129-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_0",
            "tgt_ix": "129-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_77",
            "tgt_ix": "129-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_78",
            "tgt_ix": "129-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_78",
            "tgt_ix": "129-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_0",
            "tgt_ix": "129-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_79",
            "tgt_ix": "129-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_80",
            "tgt_ix": "129-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_80",
            "tgt_ix": "129-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_82",
            "tgt_ix": "129-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_80",
            "tgt_ix": "129-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_80",
            "tgt_ix": "129-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_81",
            "tgt_ix": "129-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_84",
            "tgt_ix": "129-ARR_v1_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_85",
            "tgt_ix": "129-ARR_v1_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_80",
            "tgt_ix": "129-ARR_v1_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_80",
            "tgt_ix": "129-ARR_v1_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_80",
            "tgt_ix": "129-ARR_v1_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_83",
            "tgt_ix": "129-ARR_v1_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_80",
            "tgt_ix": "129-ARR_v1_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_86",
            "tgt_ix": "129-ARR_v1_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_80",
            "tgt_ix": "129-ARR_v1_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_87",
            "tgt_ix": "129-ARR_v1_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v1_0",
            "tgt_ix": "129-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_1",
            "tgt_ix": "129-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_2",
            "tgt_ix": "129-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_2",
            "tgt_ix": "129-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_2",
            "tgt_ix": "129-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_2",
            "tgt_ix": "129-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_2",
            "tgt_ix": "129-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_2",
            "tgt_ix": "129-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_3",
            "tgt_ix": "129-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_4",
            "tgt_ix": "129-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_4",
            "tgt_ix": "129-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_4",
            "tgt_ix": "129-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_4",
            "tgt_ix": "129-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_5",
            "tgt_ix": "129-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_5",
            "tgt_ix": "129-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_5",
            "tgt_ix": "129-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_5",
            "tgt_ix": "129-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_5",
            "tgt_ix": "129-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_5",
            "tgt_ix": "129-ARR_v1_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_6",
            "tgt_ix": "129-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_6",
            "tgt_ix": "129-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_7",
            "tgt_ix": "129-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_7",
            "tgt_ix": "129-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_8",
            "tgt_ix": "129-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_8",
            "tgt_ix": "129-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_8",
            "tgt_ix": "129-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_8",
            "tgt_ix": "129-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_8",
            "tgt_ix": "129-ARR_v1_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_9",
            "tgt_ix": "129-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_9",
            "tgt_ix": "129-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_9",
            "tgt_ix": "129-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_9",
            "tgt_ix": "129-ARR_v1_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_9",
            "tgt_ix": "129-ARR_v1_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_9",
            "tgt_ix": "129-ARR_v1_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_9",
            "tgt_ix": "129-ARR_v1_9@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_10",
            "tgt_ix": "129-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_11",
            "tgt_ix": "129-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_12",
            "tgt_ix": "129-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_13",
            "tgt_ix": "129-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_14",
            "tgt_ix": "129-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_15",
            "tgt_ix": "129-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_15",
            "tgt_ix": "129-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_16",
            "tgt_ix": "129-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_17",
            "tgt_ix": "129-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_17",
            "tgt_ix": "129-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_17",
            "tgt_ix": "129-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_18",
            "tgt_ix": "129-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_19",
            "tgt_ix": "129-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_19",
            "tgt_ix": "129-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_19",
            "tgt_ix": "129-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_19",
            "tgt_ix": "129-ARR_v1_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_19",
            "tgt_ix": "129-ARR_v1_19@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_19",
            "tgt_ix": "129-ARR_v1_19@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_19",
            "tgt_ix": "129-ARR_v1_19@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_19",
            "tgt_ix": "129-ARR_v1_19@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_20",
            "tgt_ix": "129-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_21",
            "tgt_ix": "129-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_22",
            "tgt_ix": "129-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_22",
            "tgt_ix": "129-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_23",
            "tgt_ix": "129-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_24",
            "tgt_ix": "129-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_24",
            "tgt_ix": "129-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_25",
            "tgt_ix": "129-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_25",
            "tgt_ix": "129-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_25",
            "tgt_ix": "129-ARR_v1_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_25",
            "tgt_ix": "129-ARR_v1_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_25",
            "tgt_ix": "129-ARR_v1_25@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_26",
            "tgt_ix": "129-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_27",
            "tgt_ix": "129-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_28",
            "tgt_ix": "129-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_29",
            "tgt_ix": "129-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_30",
            "tgt_ix": "129-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_30",
            "tgt_ix": "129-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_30",
            "tgt_ix": "129-ARR_v1_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_30",
            "tgt_ix": "129-ARR_v1_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_31",
            "tgt_ix": "129-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_32",
            "tgt_ix": "129-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_33",
            "tgt_ix": "129-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_34",
            "tgt_ix": "129-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_35",
            "tgt_ix": "129-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_36",
            "tgt_ix": "129-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_37",
            "tgt_ix": "129-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_38",
            "tgt_ix": "129-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_39",
            "tgt_ix": "129-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_40",
            "tgt_ix": "129-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_41",
            "tgt_ix": "129-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_41",
            "tgt_ix": "129-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_41",
            "tgt_ix": "129-ARR_v1_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_41",
            "tgt_ix": "129-ARR_v1_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_42",
            "tgt_ix": "129-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_43",
            "tgt_ix": "129-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_44",
            "tgt_ix": "129-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_45",
            "tgt_ix": "129-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_45",
            "tgt_ix": "129-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_45",
            "tgt_ix": "129-ARR_v1_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_46",
            "tgt_ix": "129-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_47",
            "tgt_ix": "129-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_47",
            "tgt_ix": "129-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_48",
            "tgt_ix": "129-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_48",
            "tgt_ix": "129-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_49",
            "tgt_ix": "129-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_50",
            "tgt_ix": "129-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_50",
            "tgt_ix": "129-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_50",
            "tgt_ix": "129-ARR_v1_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_50",
            "tgt_ix": "129-ARR_v1_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_50",
            "tgt_ix": "129-ARR_v1_50@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_50",
            "tgt_ix": "129-ARR_v1_50@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_51",
            "tgt_ix": "129-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_52",
            "tgt_ix": "129-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_53",
            "tgt_ix": "129-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_53",
            "tgt_ix": "129-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_53",
            "tgt_ix": "129-ARR_v1_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_54",
            "tgt_ix": "129-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_55",
            "tgt_ix": "129-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_55",
            "tgt_ix": "129-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_56",
            "tgt_ix": "129-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_57",
            "tgt_ix": "129-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_58",
            "tgt_ix": "129-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_59",
            "tgt_ix": "129-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_60",
            "tgt_ix": "129-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_61",
            "tgt_ix": "129-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_61",
            "tgt_ix": "129-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_61",
            "tgt_ix": "129-ARR_v1_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_61",
            "tgt_ix": "129-ARR_v1_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_62",
            "tgt_ix": "129-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_63",
            "tgt_ix": "129-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_63",
            "tgt_ix": "129-ARR_v1_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_63",
            "tgt_ix": "129-ARR_v1_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_63",
            "tgt_ix": "129-ARR_v1_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_63",
            "tgt_ix": "129-ARR_v1_63@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_64",
            "tgt_ix": "129-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_65",
            "tgt_ix": "129-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_65",
            "tgt_ix": "129-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_66",
            "tgt_ix": "129-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_67",
            "tgt_ix": "129-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_68",
            "tgt_ix": "129-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_68",
            "tgt_ix": "129-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_69",
            "tgt_ix": "129-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_70",
            "tgt_ix": "129-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_70",
            "tgt_ix": "129-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_70",
            "tgt_ix": "129-ARR_v1_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_70",
            "tgt_ix": "129-ARR_v1_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_70",
            "tgt_ix": "129-ARR_v1_70@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_70",
            "tgt_ix": "129-ARR_v1_70@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_70",
            "tgt_ix": "129-ARR_v1_70@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_70",
            "tgt_ix": "129-ARR_v1_70@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_71",
            "tgt_ix": "129-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_72",
            "tgt_ix": "129-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_73",
            "tgt_ix": "129-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_73",
            "tgt_ix": "129-ARR_v1_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_73",
            "tgt_ix": "129-ARR_v1_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_73",
            "tgt_ix": "129-ARR_v1_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_73",
            "tgt_ix": "129-ARR_v1_73@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_74",
            "tgt_ix": "129-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_74",
            "tgt_ix": "129-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_74",
            "tgt_ix": "129-ARR_v1_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_74",
            "tgt_ix": "129-ARR_v1_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_74",
            "tgt_ix": "129-ARR_v1_74@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_75",
            "tgt_ix": "129-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_75",
            "tgt_ix": "129-ARR_v1_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_75",
            "tgt_ix": "129-ARR_v1_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_75",
            "tgt_ix": "129-ARR_v1_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_75",
            "tgt_ix": "129-ARR_v1_75@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_75",
            "tgt_ix": "129-ARR_v1_75@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_75",
            "tgt_ix": "129-ARR_v1_75@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_75",
            "tgt_ix": "129-ARR_v1_75@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_76",
            "tgt_ix": "129-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_77",
            "tgt_ix": "129-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_77",
            "tgt_ix": "129-ARR_v1_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_77",
            "tgt_ix": "129-ARR_v1_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_78",
            "tgt_ix": "129-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_79",
            "tgt_ix": "129-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_79",
            "tgt_ix": "129-ARR_v1_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_79",
            "tgt_ix": "129-ARR_v1_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_79",
            "tgt_ix": "129-ARR_v1_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_79",
            "tgt_ix": "129-ARR_v1_79@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_79",
            "tgt_ix": "129-ARR_v1_79@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_79",
            "tgt_ix": "129-ARR_v1_79@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_79",
            "tgt_ix": "129-ARR_v1_79@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_79",
            "tgt_ix": "129-ARR_v1_79@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_79",
            "tgt_ix": "129-ARR_v1_79@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_79",
            "tgt_ix": "129-ARR_v1_79@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_79",
            "tgt_ix": "129-ARR_v1_79@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_79",
            "tgt_ix": "129-ARR_v1_79@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_79",
            "tgt_ix": "129-ARR_v1_79@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_79",
            "tgt_ix": "129-ARR_v1_79@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_80",
            "tgt_ix": "129-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_81",
            "tgt_ix": "129-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_81",
            "tgt_ix": "129-ARR_v1_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_81",
            "tgt_ix": "129-ARR_v1_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_81",
            "tgt_ix": "129-ARR_v1_81@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_81",
            "tgt_ix": "129-ARR_v1_81@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_82",
            "tgt_ix": "129-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_82",
            "tgt_ix": "129-ARR_v1_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_82",
            "tgt_ix": "129-ARR_v1_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_83",
            "tgt_ix": "129-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_83",
            "tgt_ix": "129-ARR_v1_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_83",
            "tgt_ix": "129-ARR_v1_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_83",
            "tgt_ix": "129-ARR_v1_83@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_84",
            "tgt_ix": "129-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_84",
            "tgt_ix": "129-ARR_v1_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_84",
            "tgt_ix": "129-ARR_v1_84@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_84",
            "tgt_ix": "129-ARR_v1_84@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_84",
            "tgt_ix": "129-ARR_v1_84@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_84",
            "tgt_ix": "129-ARR_v1_84@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_84",
            "tgt_ix": "129-ARR_v1_84@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_84",
            "tgt_ix": "129-ARR_v1_84@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_85",
            "tgt_ix": "129-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_86",
            "tgt_ix": "129-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_86",
            "tgt_ix": "129-ARR_v1_86@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_86",
            "tgt_ix": "129-ARR_v1_86@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_86",
            "tgt_ix": "129-ARR_v1_86@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_86",
            "tgt_ix": "129-ARR_v1_86@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_86",
            "tgt_ix": "129-ARR_v1_86@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_86",
            "tgt_ix": "129-ARR_v1_86@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_86",
            "tgt_ix": "129-ARR_v1_86@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_87",
            "tgt_ix": "129-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_87",
            "tgt_ix": "129-ARR_v1_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_87",
            "tgt_ix": "129-ARR_v1_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_87",
            "tgt_ix": "129-ARR_v1_87@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_87",
            "tgt_ix": "129-ARR_v1_87@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_88",
            "tgt_ix": "129-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_88",
            "tgt_ix": "129-ARR_v1_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_88",
            "tgt_ix": "129-ARR_v1_88@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_88",
            "tgt_ix": "129-ARR_v1_88@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_88",
            "tgt_ix": "129-ARR_v1_88@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_88",
            "tgt_ix": "129-ARR_v1_88@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_89",
            "tgt_ix": "129-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_90",
            "tgt_ix": "129-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_91",
            "tgt_ix": "129-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_92",
            "tgt_ix": "129-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_93",
            "tgt_ix": "129-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_94",
            "tgt_ix": "129-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_95",
            "tgt_ix": "129-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_96",
            "tgt_ix": "129-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_97",
            "tgt_ix": "129-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_98",
            "tgt_ix": "129-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_99",
            "tgt_ix": "129-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_100",
            "tgt_ix": "129-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_101",
            "tgt_ix": "129-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_102",
            "tgt_ix": "129-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_103",
            "tgt_ix": "129-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_104",
            "tgt_ix": "129-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_105",
            "tgt_ix": "129-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_106",
            "tgt_ix": "129-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_107",
            "tgt_ix": "129-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_108",
            "tgt_ix": "129-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_109",
            "tgt_ix": "129-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_110",
            "tgt_ix": "129-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_111",
            "tgt_ix": "129-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_112",
            "tgt_ix": "129-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_113",
            "tgt_ix": "129-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_114",
            "tgt_ix": "129-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_115",
            "tgt_ix": "129-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_116",
            "tgt_ix": "129-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_117",
            "tgt_ix": "129-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_118",
            "tgt_ix": "129-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_119",
            "tgt_ix": "129-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_120",
            "tgt_ix": "129-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_121",
            "tgt_ix": "129-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_122",
            "tgt_ix": "129-ARR_v1_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_123",
            "tgt_ix": "129-ARR_v1_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_124",
            "tgt_ix": "129-ARR_v1_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_125",
            "tgt_ix": "129-ARR_v1_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_126",
            "tgt_ix": "129-ARR_v1_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_127",
            "tgt_ix": "129-ARR_v1_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_128",
            "tgt_ix": "129-ARR_v1_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_129",
            "tgt_ix": "129-ARR_v1_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_130",
            "tgt_ix": "129-ARR_v1_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_131",
            "tgt_ix": "129-ARR_v1_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_132",
            "tgt_ix": "129-ARR_v1_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_133",
            "tgt_ix": "129-ARR_v1_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_134",
            "tgt_ix": "129-ARR_v1_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_135",
            "tgt_ix": "129-ARR_v1_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_136",
            "tgt_ix": "129-ARR_v1_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_137",
            "tgt_ix": "129-ARR_v1_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_138",
            "tgt_ix": "129-ARR_v1_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_139",
            "tgt_ix": "129-ARR_v1_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_140",
            "tgt_ix": "129-ARR_v1_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_141",
            "tgt_ix": "129-ARR_v1_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_142",
            "tgt_ix": "129-ARR_v1_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_143",
            "tgt_ix": "129-ARR_v1_143@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_144",
            "tgt_ix": "129-ARR_v1_144@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_145",
            "tgt_ix": "129-ARR_v1_145@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v1_146",
            "tgt_ix": "129-ARR_v1_146@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1174,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "129-ARR",
        "version": 1
    }
}